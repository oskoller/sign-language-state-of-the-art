
@inproceedings{_anovellossfunctionfortheoverallriskcriterionbaseddiscriminativetrainingofhmmmodels_,
  title = {{{ANOVELLOSSFUNCTIONFORTHEOVERALLRISKCRITERIONBASEDDISCRIMINATIVETRAININGOFHMMMODELS}}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GAE6VLGY/i00_2887.html}
}

@incollection{_front_2015,
  title = {Front {{Matter}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  year = {2015},
  pages = {i-xii},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.fmatter},
  abstract = {The prelims comprise: Half-Title Page Series Page Title Page Copyright Page Table of Contents Notes on Contributors},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015_Front Matter.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HPKHCM63/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@misc{_ilsp_2010,
  title = {{{ILSP}} - {{R}}\&{{D}} Projects - {{DICTA}}-{{SIGN}} - {{Sign Language Recognition}}, {{Generation}} and ?Odelling with Application in {{Deaf Communication}}},
  year = {2010},
  abstract = {Coordinator:Eleni Efthimiou Duration:36 Months Start: February 2009 Program: ICT (FP7) Grant Agreement Number: 231135 Project URL: http://www.dictasign.},
  howpublished = {http://www.ilsp.gr/dicta\_eng.html},
  keywords = {appropriate processing,demonstrator,east anglia,eu project,language processing,lexicon,linguistic knowledge,national de la recherche scientifique}
}

@incollection{_index_2015,
  title = {Index},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  year = {2015},
  pages = {369--371},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.index},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015_Index.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8HIL4HJL/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@incollection{_index_2015a,
  title = {Index of {{Sign Languages}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  year = {2015},
  pages = {372--372},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.indsp},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015_Index of Sign Languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GAVVCVTC/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@book{_pattern_,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown ...},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pattern Recognition and Machine Learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/E8TKRFNI/978-0-387-31073-2.html}
}

@misc{_wsdgan_,
  title = {{{WSD}}-{{GAN}}: {{Word Sense Disambiguation Using Generative}}... - {{Google Scholar}}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JVER3VA3/scholar.html},
  howpublished = {https://scholar.google.de/scholar?hl=en\&as\_sdt=0\%2C5\&q=WSD-GAN\%3A+Word+Sense+Disambiguation+Using+Generative+Adversarial+Networks\&btnG=}
}

@inproceedings{abad_automatic_2008,
  title = {Automatic {{Classification}} and {{Transcription}} of {{Telephone Speech}} in {{Radio Broadcast Data}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Computational Processing}} of the {{Portuguese Language}} ({{PROPOR}})},
  author = {Abad, Alberto and Meinedo, Hugo and Neto, Jo{\~a}o},
  year = {2008},
  pages = {172--181},
  publisher = {{Springer}},
  address = {{Aveiro, Portugal}}
}

@inproceedings{abad_incorporating_2008,
  title = {Incorporating {{Acoustical Modelling}} of {{Phone Transitions}} in an {{Hybrid}} \{\vphantom\}{{ANN}}/{{HMM}}\vphantom\{\} {{Speech Recognizer}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Abad, A. and Neto, J.},
  year = {2008},
  address = {{Brisbane, Australia}}
}

@inproceedings{abad_l2f_2010,
  title = {The {{L2F Language Verification Systems}} for {{Albayzin}}-2010 {{Evaluation}}},
  booktitle = {Proc. {{Fala}} 2010, {{Vigo}}, {{Spain}}},
  author = {Abad, A. and Koller, O. and Trancoso, I.},
  year = {2010},
  month = nov,
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Abad et al/Abad et al_2010_The L2F Language Verification Systems for Albayzin-2010 Evaluation.pdf}
}

@inproceedings{abad_porting_2009,
  title = {Porting an \{\vphantom\}{{European Portuguese}}\vphantom\{\} {{Broadcast News Recognition System}} to \{\vphantom\}{{Brazilian Portuguese}}\vphantom\{\}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Abad, A. and Trancoso, I. and Neto, N. and Viana, M. C},
  year = {2009},
  month = sep,
  address = {{Brighton, UK}}
}

@article{abboud_facial_2004,
  title = {Facial Expression Recognition and Synthesis Based on an Appearance Model},
  author = {Abboud, Bouchra and Davoine, Franck and Dang, M{\^o}},
  year = {2004},
  month = sep,
  volume = {19},
  pages = {723--740},
  issn = {0923-5965},
  doi = {10.1016/j.image.2004.05.009},
  abstract = {This article addresses the issue of expressive face modelling using an active appearance model for facial expression recognition and synthesis. We consider the six universal emotional categories namely joy, anger, fear, disgust, sadness and surprise. After a description of the active appearance model (computed with 3 or only one PCA), we address the active appearance model contribution to automatic facial expression recognition. Then we propose a new method for analysis and synthesis allowing, from a single photo, to cancel the facial expression on a given face and to artificially synthesize novel expressions on this same face. In this last framework, we propose two facial expression modelling approaches.},
  journal = {Signal Processing: Image Communication},
  number = {8}
}

@inproceedings{abdel-hamid_applying_2012,
  title = {Applying Convolutional Neural Networks Concepts to Hybrid {{NN}}-{{HMM}} Model for Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Abdel-Hamid}, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Penn, Gerald},
  year = {2012},
  pages = {4277--4280},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2012/Abdel-Hamid et al/Abdel-Hamid et al_2012_Applying convolutional neural networks concepts to hybrid NN-HMM model for.pdf}
}

@article{abid_dynamic_2015,
  title = {Dynamic {{Sign Language Recognition}} for {{Smart Home Interactive Application Using Stochastic Linear Formal Grammar}}},
  author = {Abid, M.R. and Petriu, E.M. and Amjadian, E.},
  year = {2015},
  month = mar,
  volume = {64},
  pages = {596--605},
  issn = {0018-9456},
  doi = {10.1109/TIM.2014.2351331},
  abstract = {This paper presents the state-of-the art dynamic sign language recognition (DSLR) system for smart home interactive applications. Our novel DSLR system comprises two main subsystems: an image processing (IP) module and a stochastic linear formal grammar (SLFG) module. Our IP module enables us to recognize the individual words of the sign language (i.e., a single gesture). In this module, we used the bag-of-features (BOFs) and a local part model approach for bare hand dynamic gesture recognition from a video. We used dense sampling to extract local 3-D multiscale whole-part features. We adopted 3-D histograms of a gradient orientation descriptor to represent features. The \$k\$ -means++ method was applied to cluster the visual words. Dynamic hand gesture classification was conducted using the BOFs and nonlinear support vector machine methods. We used a multiscale local part model to preserve temporal context. The SLFG module analyzes the sentences of the sign language (i.e., sequences of gestures) and determines whether or not they are syntactically valid. Therefore, the DSLR system is not only able to rule out ungrammatical sentences, but it can also make predictions about missing gestures, which, in turn, increases the accuracy of our recognition task. Our IP module alone seals the accuracy of 97\% and outperforms any existing bare hand dynamic gesture recognition system. However, by exploiting syntactic pattern recognition, the SLFG module raises this accuracy by 1.65\%. This makes the aggregate performance of the DSLR system as accurate as 98.65\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Abid et al/Abid et al_2015_Dynamic Sign Language Recognition for Smart Home Interactive Application Using.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WR6DDK3N/articleDetails.html},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  number = {3}
}

@article{adaloglou_comprehensive_2020,
  title = {A {{Comprehensive Study}} on {{Sign Language Recognition Methods}}},
  author = {Adaloglou, Nikolas and Chatzis, Theocharis and Papastratis, Ilias and Stergioulas, Andreas and Papadopoulos, Georgios Th and Zacharopoulou, Vassia and Xydopoulos, George J. and Atzakas, Klimnis and Papazachariou, Dimitris and Daras, Petros},
  year = {2020},
  abstract = {In this paper, a comparative experimental assessment of computer vision-based methods for sign language recognition is conducted. By implementing the most recent deep neural network methods in this field, a thorough evaluation on multiple publicly available datasets is performed. The aim of the present study is to provide insights on sign language recognition, focusing on mapping non-segmented video streams to glosses. For this task, two new sequence training criteria, known from the fields of speech and scene text recognition, are introduced. Furthermore, a plethora of pretraining schemes is thoroughly discussed. Finally, a new RGB+D dataset for the Greek sign language is created. To the best of our knowledge, this is the first sign language dataset where sentence and gloss level annotations are provided for a video capture.},
  archivePrefix = {arXiv},
  eprint = {2007.12530},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Adaloglou et al_2020_A Comprehensive Study on Sign Language Recognition Methods.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NZ8KNMWP/2007.html},
  journal = {arXiv:2007.12530 [cs]},
  keywords = {augment=brightness,augment=contrast,augment=hue,augment=randomcrop,augment=randomframedrop,augment=saturation,authorgroup=CERTH,authorgroup=Patras,capturedBy=vision,cnnresiduals=0,Computer Science - Computer Vision and Pattern Recognition,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=28.9/29.1,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,continuous;vocab=310;signer=7;sl=GSL;capturedBy=vision,dataSet=CSL,dataSet=Phoenix14,isolated,isolated;vocab=310;signer=7;sl=GSL;capturedBy=vision,loss=CE,loss=CTC,loss=other,modal=fullframe,nn=1dcnn,nn=2dcnn,nn=blstm,paperinfo,paperinfo;authorgroup=CERTH;authorgroup=Patras;shorttitle=Comprehensive Study,re-align=0,shorttitle=Comprehensive Study,signer=50,signer=7,signer=9,sl=CSL,sl=DGS,sl=GSL,slreco,slreco;modal=fullframe;nn=2dcnn;nn=1dcnn;nn=blstm;cnnresiduals=0;re-align=0;augment=randomcrop;augment=randomframedrop;augment=brightness;augment=contrast;augment=saturation;augment=hue;loss=CTC;loss=other;loss=CE,vocab<500,vocab=1080,vocab=178,vocab=310,vocab>=1000,wer=28.9/29.1},
  primaryClass = {cs}
}

@incollection{adam_dissemination_2015,
  title = {Dissemination and {{Transfer}} of {{Knowledge}} to the {{Deaf Community}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Adam, Robert},
  year = {2015},
  pages = {41--52},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch3},
  abstract = {There is currently a great emphasis on knowledge transfer during the research process. The ultimate aim of knowledge transfer is to appropriately inform and in the long term empower the Deaf community. This chapter discusses the origins of linguistic and cultural research into the Deaf community as a research population. It focuses on the examination of the public engagement process as undertaken in British universities, with specific reference to the Deafness Cognition and Language Research Centre (DCAL). Public engagement has become more relevant in recent years as research funders became more aware and more concerned about how researchers and the public interact with each other. Finally this work will be used to make some recommendations concerning information dissemination and the transfer of knowledge to the Deaf community.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Adam_2015_Dissemination and Transfer of Knowledge to the Deaf Community.pdf},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{adda-decker_corpusbased_2003,
  title = {A {{Corpus}}-{{Based Decompounding Algorithm}} for {{German Lexical Modeling}} in {{LVCSR}}},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {{Adda-Decker}, Martine},
  year = {2003},
  pages = {257--260},
  address = {{Geneva, Switzerland}},
  abstract = {In this paper a corpus-based decompounding algorithm is described and applied for German LVCSR. The decompounding algorithm contributes to address two major problems for LVCSR: lexical coverage and letter-to-sound conversion. The idea of the algorithm is simple: given a word start of length only few different characters can continue an admissible word in the language. But concerning compounds, if word start reaches a constituent word boundary, the set of successor characters can theoretically include any character. The algorithm has been applied to a 300M word corpus with 2.6M distinct words. 800k decomposition rules have been extracted automatically. OOV (out of vocabulary) word reductions of 25\% to 50\% relative have been achieved using word lists from 65k to 600k words. Pronunciation dictionaries have been developed for the LIMSI 300k German recognition system. As no language specific knowledge is required beyond the text corpus, the algorithm can apply more generally to any compounding language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Adda-Decker_2003_A Corpus-Based Decompounding Algorithm for German Lexical Modeling in LVCSR.pdf},
  language = {en}
}

@inproceedings{adda-decker_morphological_2000,
  title = {Morphological {{Decomposition}} for {{Asr}} in {{German}}},
  booktitle = {Workshop on {{Phonetics}} and {{Phonology}} in {{ASR}}, {{Saarbr\"ucken}}, {{Germany}}},
  author = {{Adda-Decker}, Martine and Adda, Gilles},
  year = {2000},
  pages = {129--143},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Adda-Decker_Adda_2000_Morphological decomposition for ASR in German.pdf}
}

@inproceedings{adithya_artificial_2013,
  title = {Artificial Neural Network Based Method for {{Indian}} Sign Language Recognition},
  booktitle = {Proc. {{IEEE Conf}}. on {{Information Communication Technologies}}},
  author = {Adithya, V. and Vinod, P. R. and Gopalakrishnan, U.},
  year = {2013},
  month = apr,
  pages = {1080--1085},
  doi = {10.1109/CICT.2013.6558259},
  abstract = {Sign Language is a language which uses hand gestures, facial expressions and body movements for communication. A sign language consists of either word level signs or fingerspelling. It is the only communication mean for the deaf-dumb community. But the hearing people never try to learn the sign language. So the deaf people cannot interact with the normal people without a sign language interpreter. This causes the isolation of deaf people in the society. So a system that automatically recognizes the sign language is necessary. The implementation of such a system provides a platform for the interaction of hearing disabled people with the rest of the world without an interpreter. In this paper, we propose a method for the automatic recognition of fingerspelling in Indian sign language. The proposed method uses digital image processing techniques and artificial neural network for recognizing different signs.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Adithya et al_2013_Artificial neural network based method for Indian sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DB86YFSN/6558259.html}
}

@phdthesis{aepli_parsing_2018,
  title = {Parsing {{Approaches}} for {{Swiss German}}},
  author = {Aepli, No{\"e}mi},
  year = {2018},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Aepli_2018_Parsing Approaches for Swiss German.pdf},
  school = {University of Zurich},
  type = {{{PhD Thesis}}}
}

@article{afouras_deep_2018,
  title = {Deep {{Audio}}-Visual {{Speech Recognition}}},
  author = {Afouras, T. and Chung, J. S. and Senior, A. and Vinyals, O. and Zisserman, A.},
  year = {2018},
  pages = {1--1},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2018.2889052},
  abstract = {The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem \textendash{} unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) we compare two models for lip reading, one using a CTC loss, and the other using a sequence-to-sequence loss. Both models are built on top of the transformer self-attention architecture; (2) we investigate to what extent lip reading is complementary to audio speech recognition, especially when the audio signal is noisy; (3) we introduce and publicly release two new datasets for audio-visual speech recognition: LRS2-BBC, consisting of thousands of natural sentences from British television; and LRS3-TED, consisting of hundreds of hours of TED and TEDx talks obtained from YouTube. The models that we train surpass the performance of all previous work on lip reading benchmark datasets by a significant margin.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Afouras et al_2018_Deep Audio-visual Speech Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4JN4RRBZ/8585066.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@inproceedings{agarwal_sign_2013,
  title = {Sign Language Recognition Using {{Microsoft Kinect}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Contemporary Computing}} ({{IC3}})},
  author = {Agarwal, A. and Thakur, M. K.},
  year = {2013},
  month = aug,
  pages = {181--185},
  doi = {10.1109/IC3.2013.6612186},
  abstract = {In last decade lot of efforts had been made by research community to create sign language recognition system which provide a medium of communication for differently-abled people and their machine translations help others having trouble in understanding such sign languages. Computer vision and machine learning can be collectively applied to create such systems. In this paper, we present a sign language recognition system which makes use of depth images that were captured using a Microsoft Kinect\textregistered{} camera. Using computer vision algorithms, we develop a characteristic depth and motion profile for each sign language gesture. The feature matrix thus generated was trained using a multi-class SVM classifier and the final results were compared with existing techniques. The dataset used is of sign language gestures for the digits 0-9.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Agarwal_Thakur_2013_Sign language recognition using Microsoft Kinect.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X3XDEEQ4/6612186.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=10;sl=CSL;capturedBy=vision;capturedBy=depth,modal=movement,modal=shape,sl=CSL,slreco,slreco;modal=shape;modal=movement,vocab<50,vocab=10}
}

@article{aghaahmadi_clustering_2013,
  title = {Clustering {{Persian}} Viseme Using Phoneme Subspace for Developing Visual Speech Application},
  author = {Aghaahmadi, Mohammad and Dehshibi, Mohammad Mahdi and Bastanfard, Azam and Fazlali, Mahmood},
  year = {2013},
  pages = {1--21},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2013/Aghaahmadi et al/Aghaahmadi et al_2013_Clustering Persian viseme using phoneme subspace for developing visual speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8DWUIA44/s11042-012-1128-7.html},
  journal = {Multimedia Tools and Applications}
}

@inproceedings{agris_rapid_2006,
  title = {Rapid {{Signer Adaptation}} for {{Isolated Sign Language Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {von Agris, U. and Schneider, D. and Zieren, J. and Kraiss, K.-},
  year = {2006},
  month = jun,
  pages = {159--159},
  doi = {10.1109/CVPRW.2006.165},
  abstract = {Research in the field of sign language recognition has not yet addressed the problem of interpersonal variance in large vocabulary on the classification level. Current recognition systems are designed for signer-dependent operation. Applied to signer-independent tasks, they show poor performance even when increasing the number of training signers. Better results can be achieved with dedicated adaptation methods. This paper describes a vision-based recognition system that quickly adapts to unknown signers. A combination of Maximum Likelihood Linear Regression and Maximum A Posteriori estimation was implemented and modified to consider the specifics of sign languages, such as one-handed signs. An extensive evaluation was performed in supervised and unsupervised mode on a vocabulary of 153 isolated signs. The proposed adaptation approach significantly increases accuracy even with a small amount of adaptation data. Supervised adaptation with 80 adaptation sequences yields a recognition accuracy of 78.6\%, which is a relative improvement of 41.6\% compared to the signerindependent baseline.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Agris et al_2006_Rapid Signer Adaptation for Isolated Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SL5BQVS3/1640605.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=153;signer=4;sl=BSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=4,sl=BSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<500,vocab=153}
}

@article{ahmed_review_2018,
  title = {A {{Review}} on {{Systems}}-{{Based Sensory Gloves}} for {{Sign Language Recognition State}} of the {{Art}} between 2007 and 2017},
  author = {Ahmed, Mohamed Aktham and Zaidan, Bilal Bahaa and Zaidan, Aws Alaa and Salih, Mahmood Maher and bin Lakulu, Muhammad Modi},
  year = {2018},
  month = jul,
  volume = {18},
  pages = {2208},
  doi = {10.3390/s18072208},
  abstract = {Loss of the ability to speak or hear exerts psychological and social impacts on the affected persons due to the lack of proper communication. Multiple and systematic scholarly interventions that vary according to context have been implemented to overcome disability-related difficulties. Sign language recognition (SLR) systems based on sensory gloves are significant innovations that aim to procure data on the shape or movement of the human hand. Innovative technology for this matter is mainly restricted and dispersed. The available trends and gaps should be explored in this research approach to provide valuable insights into technological environments. Thus, a review is conducted to create a coherent taxonomy to describe the latest research divided into four main categories: development, framework, other hand gesture recognition, and reviews and surveys. Then, we conduct analyses of the glove systems for SLR device characteristics, develop a roadmap for technology evolution, discuss its limitations, and provide valuable insights into technological environments. This will help researchers to understand the current options and gaps in this area, thus contributing to this line of research.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ahmed et al_2018_A Review on Systems-Based Sensory Gloves for Sign Language Recognition State of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IEYVFCQY/2208.html},
  journal = {Sensors},
  language = {en},
  number = {7}
}

@inproceedings{ahn_realtime_2015,
  title = {Real-{{Time Head Orientation}} from a {{Monocular Camera Using Deep Neural Network}}},
  booktitle = {Asian {{Conf}}. on {{Computer Vision}} ({{ACCV}})},
  author = {Ahn, Byungtae and Park, Jaesik and Kweon, In So},
  year = {2015},
  pages = {82--96},
  publisher = {{Springer}},
  address = {{Singapore}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Ahn et al/Ahn et al_2015_Real-Time Head Orientation from a Monocular Camera Using Deep Neural Network.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Ahn et al/Ahn et al_2015_Real-Time Head Orientation from a Monocular Camera Using Deep Neural Network.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3TMZHVSW/978-3-319-16811-1_6.html}
}

@article{ahonen_face_2006,
  title = {Face {{Description}} with {{Local Binary Patterns}}: {{Application}} to {{Face Recognition}}},
  shorttitle = {Face {{Description}} with {{Local Binary Patterns}}},
  author = {Ahonen, T. and Hadid, A. and Pietikainen, M.},
  year = {2006},
  month = dec,
  volume = {28},
  pages = {2037--2041},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2006.244},
  abstract = {This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2006/Ahonen et al/Ahonen et al_2006_Face Description with Local Binary Patterns.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N98887VA/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {12}
}

@article{akakin_robust_2011,
  title = {Robust Classification of Face and Head Gestures in Video},
  author = {Akak{\i}n, Hatice {\c C}{\i}nar and Sankur, B{\"u}lent},
  year = {2011},
  volume = {29},
  pages = {470--483},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2011/Akakın_Sankur/Akakın_Sankur_2011_Robust classification of face and head gestures in video.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CA839SUA/S0262885611000242.html},
  journal = {Image and Vision Computing},
  number = {7}
}

@inproceedings{akmeliawati_realtime_2007,
  title = {Real-{{Time Malaysian Sign Language Translation}} Using {{Colour Segmentation}} and {{Neural Network}}},
  booktitle = {Proc. {{IEEE Instrumentation Measurement Technology Conference}} ({{IMTC}})},
  author = {Akmeliawati, R. and Ooi, M. P. and Kuang, Y. C.},
  year = {2007},
  month = may,
  pages = {1--6},
  doi = {10.1109/IMTC.2007.379311},
  abstract = {In this paper we present an automatic visual-based sign language translation system. Our proposed automatic sign-language translator provides a real-time English translation of the Malaysia SL. To date, there have been studies on sign language recognition based on visual approach (video camera). However, the emphasis on these works is limited to a small lexicon of sign language or solely focuses on fingerspelling, which takes different approaches respectively. In practical sense, fingerspelling is used if a word cannot be expressed via sign language. Our sign language translator can recognise both fingerspelling and sign gestures that involve static and motion signs. Trained neural networks are used to identify the signs to translate into English.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Akmeliawati et al_2007_Real-Time Malaysian Sign Language Translation using Colour Segmentation and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EI5MRXVA/4258110.html}
}

@inproceedings{aktas_recognizing_2019,
  title = {Recognizing {{Non}}-{{Manual Signs}} in {{Turkish Sign Language}}},
  booktitle = {2019 {{Ninth International Conference}} on {{Image Processing Theory}}, {{Tools}} and {{Applications}} ({{IPTA}})},
  author = {Akta{\c s}, M{\"u}jde and G{\"o}kberk, Berk and Akarun, Lale},
  year = {2019},
  month = nov,
  pages = {1--6},
  issn = {2154-512X},
  doi = {10.1109/IPTA.2019.8936081},
  abstract = {Recognition of non-manual components in sign language has been a neglected topic, partly due to the absence of annotated non-manual sign datasets. We have collected a dataset of videos with non-manual signs, displaying facial expressions and head movements and prepared frame-level annotations. In this paper, we present the Turkish Sign Language (TSL) non-manual signs dataset and provide a baseline system for non-manual sign recognition. A deep learning based recognition system is proposed, in which the pre-trained ResNet Convolutional Neural Network (CNN) is employed to recognize question, negation side to side and negation up-down, affirmation and pain movements and expressions. Our subject independent method achieves 78.49\% overall frame-level accuracy on 483 TSL videos performed by six subjects, who are native TSL signers. Prediction results of consecutive frames are filtered for analyzing the qualitative results.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Aktaş et al_2019_Recognizing Non-Manual Signs in Turkish Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/35C982SC/8936081.html},
  keywords = {convolutional neural nets,deep learning based recognition system,facial expression recognition,learning (artificial intelligence),natural language processing,non-manual sign analysis,nonmanual components,nonmanual sign datasets,nonmanual sign recognition,pre-trained ResNet convolutional neural network,prepared frame-level annotations,sign language recognition,Sign language recognition,Turkish Sign Language nonmanual signs dataset}
}

@article{alaghband_feph_2020,
  title = {{{FePh}}: {{An Annotated Facial Expression Dataset}} for the {{RWTH}}-{{PHOENIX}}-{{Weather}} 2014 {{Dataset}}},
  shorttitle = {{{FePh}}},
  author = {Alaghband, Marie and Yousefi, Niloofar and Garibay, Ivan},
  year = {2020},
  month = mar,
  abstract = {Facial expressions are important parts of both gesture and sign language recognition systems. Despite the recent advances in both fields, annotated facial expression dataset in the context of sign language are still scarce resources. In this manuscript, we introduce a continuous sign language facial expression dataset, comprising over \$3000\$ annotated images of the RWTH-PHOENIX-Weather 2014 development set. Unlike the majority of currently existing facial expression datasets, FePh provides sequenced semi-blurry facial images with different head poses, orientations, and movements. In addition, in the majority of images, identities are mouthing the words, which makes the data more challenging. To annotate this dataset we consider primary, secondary, and tertiary dyads of seven basic emotions of "sad", "surprise", "fear", "angry", "neutral", "disgust", and "happy". We also considered the "None" class if the image's facial expression could not be described by any of the aforementioned emotions. Although we provide FePh in the context of facial expression and sign language, it has a wider application in gesture recognition and Human Computer Interaction (HCI) systems. The dataset will be publicly available.},
  archivePrefix = {arXiv},
  eprint = {2003.08759},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Alaghband et al_2020_FePh.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JB8TPKW7/2003.html},
  journal = {arXiv:2003.08759 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{alayrac_unsupervised_2016,
  title = {Unsupervised Learning from Narrated Instruction Videos},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Alayrac, Jean-Baptiste and Bojanowski, Piotr and Agrawal, Nishant and Sivic, Josef and Laptev, Ivan and {Lacoste-Julien}, Simon},
  year = {2016},
  pages = {4575--4583},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2016/Alayrac et al/Alayrac et al_2016_Unsupervised learning from narrated instruction videos.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DB2RGGKP/Alayrac_Unsupervised_Learning_From_CVPR_2016_paper.html}
}

@article{albanie_bsl1k_2020,
  title = {{{BSL}}-{{1K}}: {{Scaling}} up Co-Articulated Sign Language Recognition Using Mouthing Cues},
  shorttitle = {{{BSL}}-{{1K}}},
  author = {Albanie, Samuel and Varol, G{\"u}l and Momeni, Liliane and Afouras, Triantafyllos and Chung, Joon Son and Fox, Neil and Zisserman, Andrew},
  year = {2020},
  month = jul,
  abstract = {Recent progress in fine-grained gesture and action classification, and machine translation, point to the possibility of automated sign language recognition becoming a reality. A key stumbling block in making progress towards this goal is a lack of appropriate training data, stemming from the high complexity of sign annotation and a limited supply of qualified annotators. In this work, we introduce a new scalable approach to data collection for sign recognition in continuous videos. We make use of weakly-aligned subtitles for broadcast footage together with a keyword spotting method to automatically localise sign-instances for a vocabulary of 1,000 signs in 1,000 hours of video. We make the following contributions: (1) We show how to use mouthing cues from signers to obtain high-quality annotations from video data - the result is the BSL-1K dataset, a collection of British Sign Language (BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train strong sign recognition models for co-articulated signs in BSL and that these models additionally form excellent pretraining for other sign languages and benchmarks - we exceed the state of the art on both the MSASL and WLASL benchmarks. Finally, (3) we propose new large-scale evaluation sets for the tasks of sign recognition and sign spotting and provide baselines which we hope will serve to stimulate research in this area.},
  archivePrefix = {arXiv},
  eprint = {2007.12131},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Albanie et al_2020_BSL-1K.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QEF75NHR/2007.html},
  journal = {arXiv:2007.12131 [cs]},
  keywords = {capturedBy=vision,Computer Science - Computer Vision and Pattern Recognition,dataSet=MS-ASL,dataSet=WSASL,isolated,isolated;vocab=1000;signer=222;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=1064;signer=40;sl=BSL;capturedBy=vision,isolated;vocab=2000;signer=119;sl=ASL;capturedBy=vision;dataSet=WSASL,modal=bodyjoints,modal=fullframe,modal=head,signer=119,signer=222,signer=40,sl=ASL,sl=BSL,slreco,slreco;modal=bodyjoints;signer=40,slreco;modal=fullframe;signer=119,slreco;modal=fullframe;signer=222,slreco;modal=head;modal=fullframe;signer=40,vocab=1000,vocab=1064,vocab=2000,vocab>=1000},
  primaryClass = {cs}
}

@article{aleksic_automatic_2006,
  title = {Automatic Facial Expression Recognition Using Facial Animation Parameters and Multistream {{HMMs}}},
  author = {Aleksic, Petar S. and Katsaggelos, Aggelos K.},
  year = {2006},
  volume = {1},
  pages = {3--11},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2006/Aleksic_Katsaggelos/Aleksic_Katsaggelos_2006_Automatic facial expression recognition using facial animation parameters and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WWEQBFF5/login.html},
  journal = {Information Forensics and Security, IEEE Transactions on},
  number = {1}
}

@article{ali_werd_2017,
  title = {{{WERd}}: {{Using}} Social Text Spelling Variants for Evaluating Dialectal Speech Recognition},
  shorttitle = {{{WERd}}},
  author = {Ali, Ahmed and Nakov, Preslav and Bell, Peter and Renals, Steve},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ali et al_2017_WERd.pdf},
  journal = {arXiv preprint arXiv:1709.07484}
}

@incollection{allen_deaf_2015,
  title = {The {{Deaf Community}} as a ``{{Special Linguistic Demographic}}''},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Allen, Thomas E.},
  year = {2015},
  pages = {21--40},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch2},
  abstract = {This chapter point outs the fact that the much of the published research on individuals who are deaf derives from population models that embrace medical, rehabilitation, and normative educational descriptions of the population subgroups of interest. It examines that an alternative strategy for enumeration, one based on linguistic diversity, will lead to a valuable (and very different) knowledge base, which may contribute to the improvement of society and of the lives of deaf individuals. The chapter discusses the predominant medical frameworks that have determined the course of demographic studies to date, and describes their shortcomings and limitations. It summarizes some of the issues that are pervasive in existing demographic analyses of this population, and makes some specific suggestions about how questions might be posed to participants. Demographic studies of deaf individuals have often failed to separate the very different constructs of language and modality.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Allen_2015_The Deaf Community as a “Special Linguistic Demographic”.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4E49CXIF/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{almajai_improved_2016,
  title = {Improved Speaker Independent Lip Reading Using Speaker Adaptive Training and Deep Neural Networks},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Almajai, I. and Cox, S. and Harvey, R. and Lan, Y.},
  year = {2016},
  month = mar,
  pages = {2722--2726},
  doi = {10.1109/ICASSP.2016.7472172},
  abstract = {Recent improvements in tracking and feature extraction mean that speaker-dependent lip-reading of continuous speech using a medium size vocabulary (around 1000 words) is realistic. However, the recognition of previously unseen speakers has been found to be a very challenging task, because of the large variation in lip-shapes across speakers and the lack of large, tracked databases of visual features, which are very expensive to produce. By adapting a technique that is established in speech recognition but has not previously been used in lip-reading, we show that error-rates for speaker-independent lip-reading can be very significantly reduced. Furthermore, we show that error-rates can be even further reduced by the additional use of Deep Neural Networks (DNN). We also find that there is no need to map phonemes to visemes for context-dependent visual speech transcription.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2016/Almajai et al/Almajai et al_2016_Improved speaker independent lip reading using speaker adaptive training and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6IHWDIWD/abs_all.html}
}

@article{almeida_coupling_2015,
  title = {Coupling {{Natural Language Processing}} and {{Animation Synthesis}} in {{Portuguese Sign Language Translation}}},
  author = {Almeida, In{\^e}s and Coheur, Lu{\'i}sa and Candeias, Sara},
  year = {2015},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Almeida et al/Almeida et al_2015_Coupling Natural Language Processing and Animation Synthesis in Portuguese Sign.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Almeida et al/Almeida et al_2015_Coupling Natural Language Processing and Animation Synthesis in Portuguese Sign.pdf},
  journal = {Vision and Language}
}

@article{almeida_feature_2014,
  title = {Feature Extraction in {{Brazilian Sign Language Recognition}} Based on Phonological Structure and Using {{RGB}}-{{D}} Sensors},
  author = {Almeida, S{\'i}lvia Grasiella Moreira and Guimar{\~a}es, Frederico Gadelha and Ram{\'i}rez, Jaime Arturo},
  year = {2014},
  volume = {41},
  pages = {7259--7271},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014/Almeida et al/Almeida et al_2014_Feature extraction in Brazilian Sign Language Recognition based on phonological.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TK827HBI/S0957417414003042.html},
  journal = {Expert Systems with Applications},
  keywords = {capturedBy=depth,capturedBy=vision,cnn=0,isolated,isolated;vocab=34;signer=1;sl=Libras;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=1,sl=Libras,slreco,slreco;modal=2hmovement;modal=2hlocation;modal=2hshape;modal=2horientation,subunits=1,svm=1,vocab<50,vocab=34},
  number = {16}
}

@inproceedings{almohimeed_arabic_2010,
  title = {An {{Arabic Sign Language}} Corpus for Instructional Language in School},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Almohimeed, Abdulaziz and Wald, Mike and Damper, Robert},
  year = {2010},
  pages = {81--82},
  address = {{Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Almohimeed et al/Almohimeed et al_2010_An Arabic Sign Language corpus for instructional language in school.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2DB2MJQK/271106.html}
}

@article{alon_unified_2009,
  title = {A Unified Framework for Gesture Recognition and Spatiotemporal Gesture Segmentation},
  author = {Alon, Jonathan and Athitsos, Vassilis and Yuan, Quan and Sclaroff, Stan},
  year = {2009},
  volume = {31},
  pages = {1685--1699},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2009/Alon et al/Alon et al_2009_A unified framework for gesture recognition and spatiotemporal gesture.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MHPBGIFD/login.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {9}
}

@misc{amaliamendes_eurac_2009,
  title = {{{EURAC Research}} - {{Language Technologies}} - the \{african\} {{Varieties}} of \{portuguese\}},
  author = {{Am\'alia Mendes}},
  year = {2009},
  month = jan,
  howpublished = {http://www.eurac.edu/Focus/Languagetechnologies/african+varieties+of+portoguese\_en.htm},
  keywords = {PALOP Corpora,portuguese variety differences}
}

@article{amaral_prototype_2007,
  title = {A {{Prototype System}} for {{Selective Dissemination}} of {{Broadcast News}} in \{\vphantom\}{{European Portuguese}}\vphantom\{\}},
  author = {Amaral, R. and Meinedo, H. and Caseiro, D. and Trancoso, I. and Neto, J.},
  year = {2007},
  volume = {2007},
  journal = {EURASIP Journal on Advances in Signal Processing}
}

@inproceedings{Andreas_2016_CVPR,
  title = {Neural {{Module Networks}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  year = {2016},
  month = jun
}

@inproceedings{anina_ouluvs2_2015,
  title = {{{OuluVS2}}: {{A}} Multi-View Audiovisual Database for Non-Rigid Mouth Motion Analysis},
  shorttitle = {{{OuluVS2}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Anina, I. and Zhou, Ziheng and Zhao, Guoying and Pietikainen, M.},
  year = {2015},
  month = may,
  pages = {1--5},
  doi = {10.1109/FG.2015.7163155},
  abstract = {Visual speech constitutes a large part of our nonrigid facial motion and contains important information that allows machines to interact with human users, for instance, through automatic visual speech recognition (VSR) and speaker verification. One of the major obstacles to research of non-rigid mouth motion analysis is the absence of suitable databases. Those available for public research either lack a sufficient number of speakers or utterances or contain constrained view points, which limits their representativeness and usefulness. This paper introduces a newly collected multi-view audiovisual database for non-rigid mouth motion analysis. It includes more than 50 speakers uttering three types of utterances and more importantly, thousands of videos simultaneously recorded by six cameras from five different views spanned between the frontal and profile views. Moreover, a simple VSR system has been developed and tested on the database to provide some baseline performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Anina et al/Anina et al_2015_OuluVS2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X67JJ43H/abs_all.html}
}

@inproceedings{antonakos_survey_2015,
  title = {A Survey on Mouth Modeling and Analysis for {{Sign Language}} Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Antonakos, E. and Roussos, A. and Zafeiriou, S.},
  year = {2015},
  month = may,
  pages = {1--7},
  address = {{Lubljana, Slovenia}},
  doi = {10.1109/FG.2015.7163162},
  abstract = {Around 70 million Deaf worldwide use Sign Languages (SLs) as their native languages. At the same time, they have limited reading/writing skills in the spoken language. This puts them at a severe disadvantage in many contexts, including education, work, usage of computers and the Internet. Automatic Sign Language Recognition (ASLR) can support the Deaf in many ways, e.g. by enabling the development of systems for Human-Computer Interaction in SL and translation between sign and spoken language. Research in ASLR usually revolves around automatic understanding of manual signs. Recently, ASLR research community has started to appreciate the importance of non-manuals, since they are related to the lexical meaning of a sign, the syntax and the prosody. Nonmanuals include body and head pose, movement of the eyebrows and the eyes, as well as blinks and squints. Arguably, the mouth is one of the most involved parts of the face in non-manuals. Mouth actions related to ASLR can be either mouthings, i.e. visual syllables with the mouth while signing, or non-verbal mouth gestures. Both are very important in ASLR. In this paper, we present the first survey on mouth non-manuals in ASLR. We start by showing why mouth motion is important in SL and the relevant techniques that exist within ASLR. Since limited research has been conducted regarding automatic analysis of mouth motion in the context of ALSR, we proceed by surveying relevant techniques from the areas of automatic mouth expression and visual speech recognition which can be applied to the task. Finally, we conclude by presenting the challenges and potentials of automatic analysis of mouth motion in the context of ASLR.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Antonakos et al/Antonakos et al_2015_A survey on mouth modeling and analysis for Sign Language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TDTRXBTT/articleDetails.html}
}

@inproceedings{antonakos_unsupervised_2012,
  title = {Unsupervised Classification of Extreme Facial Events Using Active Appearance Models Tracking for Sign Language Videos},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Antonakos, E. and Pitsikalis, V. and Rodomagoulakis, I. and Maragos, P.},
  year = {2012},
  month = sep,
  pages = {1409--1412},
  doi = {10.1109/ICIP.2012.6467133},
  abstract = {We propose an Unsupervised method for Extreme States Classification (UnESC) on feature spaces of facial cues of interest. The method is built upon Active Appearance Models (AAM) face tracking and on feature extraction of Global and Local AAMs. UnESC is applied primarily on facial pose, but is shown to be extendable for the case of local models on the eyes and mouth. Given the importance of facial events in Sign Languages we apply the UnESC on videos from two sign language corpora, both American (ASL) and Greek (GSL) yielding promising qualitative and quantitative results. Apart from the detection of extreme facial states, the proposed Un-ESC also has impact for SL corpora lacking any facial annotations.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2012/Antonakos et al/Antonakos et al_2012_Unsupervised classification of extreme facial events using active appearance.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZVCNFWVH/abs_all.html}
}

@techreport{aradilla_improving_2005,
  title = {Improving {{Speech Recognition Using}} a {{Data}}-{{Driven Approach}} - {{Idiap Publications}}},
  author = {Aradilla, Guillermo and Vepa, Jithendra and Bourlard, Herv{\'e}},
  year = {2005},
  month = apr,
  pages = {1--10},
  address = {{Martigny, Switzerland}},
  institution = {{IDIAP}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Aradilla et al_2005_Improving Speech Recognition Using a Data-Driven Approach - Idiap Publications.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IUZ5CJ39/387.html},
  language = {en},
  number = {rr05-66}
}

@article{aran_beliefbased_2009,
  title = {A Belief-Based Sequential Fusion Approach for Fusing Manual Signs and Non-Manual Signals},
  author = {Aran, Oya and Burger, Thomas and Caplier, Alice and Akarun, Lale},
  year = {2009},
  volume = {42},
  pages = {812--822},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2009/Aran et al/Aran et al_2009_A belief-based sequential fusion approach for fusing manual signs and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/23FVGZBP/S0031320308004019.html},
  journal = {Pattern Recognition},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=19;signer=8;sl=TSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=head,signer=7,signer=8,sl=TSL,slreco,slreco;modal=2hshape;modal=2hmovement;modal=2hlocation;modal=head,vocab<50,vocab=19},
  number = {5}
}

@inproceedings{aran_database_2007,
  title = {A Database of Non-Manual Signs in Turkish Sign Language},
  booktitle = {Signal {{Processing}} and {{Communications Applications}}, 2007. {{SIU}} 2007. {{IEEE}} 15th},
  author = {Aran, Oya and Ari, I. and Guvensan, A. and Haberdar, Hakan and Kurr, Z. and Turkmen, I. and Uyar, Asl{\i} and Akarun, Lale},
  year = {2007},
  pages = {1--4},
  publisher = {{IEEE}}
}

@inproceedings{aran_multiclass_2008,
  title = {Multi-Class Classification Strategies for {{Fisher}} Scores of Gesture and Sign Sequences},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Aran, Oya and Akarun, Lale},
  year = {2008},
  month = dec,
  pages = {1--4},
  address = {{Tampa, FL, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2008/Aran_Akarun/Aran_Akarun_2008_Multi-class classification strategies for Fisher scores of gesture and sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/53EDH2MD/abs_all.html},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=19;signer=8;sl=TSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=head,signer=8,sl=TSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=head,vocab<50,vocab=19}
}

@article{aran_multiclass_2010,
  title = {A Multi-Class Classification Strategy for {{Fisher}} Scores: {{Application}} to Signer Independent Sign Language Recognition},
  shorttitle = {A Multi-Class Classification Strategy for {{Fisher}} Scores},
  author = {Aran, Oya and Akarun, Lale},
  year = {2010},
  month = may,
  volume = {43},
  pages = {1776--1788},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2009.12.002},
  abstract = {Fisher kernels combine the powers of discriminative and generative classifiers by mapping the variable-length sequences to a new fixed length feature space, called the Fisher score space. The mapping is based on a single generative model and the classifier is intrinsically binary. We propose a multi-class classification strategy that applies a multi-class classification on each Fisher score space and combines the decisions of multi-class classifiers. We experimentally show that the Fisher scores of one class provide discriminative information for the other classes as well. We compare several multi-class classification strategies for Fisher scores generated from the hidden Markov models of sign sequences. The proposed multi-class classification strategy increases the classification accuracy in comparison with the state of the art strategies based on combining binary classifiers. To reduce the computational complexity of the Fisher score extraction and the training phases, we also propose a score space selection method and show that, similar or even higher accuracies can be obtained by using only a subset of the score spaces. Based on the proposed score space selection method, a signer adaptation technique is also presented that does not require any re-training.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Aran_Akarun/Aran_Akarun_2010_A multi-class classification strategy for Fisher scores.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Aran_Akarun/Aran_Akarun_2010_A multi-class classification strategy for Fisher scores.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/G4N5XBVP/222576818_A_multi-class_classification_strategy_for_Fisher_scores_Application_to_signer_indepen.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XTUED6EB/S0031320309004476.html},
  journal = {Pattern Recognition},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=19;signer=8;sl=TSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=8,sl=TSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=19},
  number = {5}
}

@incollection{aran_recognizing_2006,
  title = {Recognizing Two Handed Gestures with Generative, Discriminative and Ensemble Methods via {{Fisher}} Kernels},
  booktitle = {Multimedia {{Content Representation}}, {{Classification}} and {{Security}}},
  author = {Aran, Oya and Akarun, Lale},
  year = {2006},
  pages = {159--166},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2006/Aran_Akarun/Aran_Akarun_2006_Recognizing two handed gestures with generative, discriminative and ensemble.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V3DMHHHI/11848035_23.html}
}

@article{aran_signtutor_2009,
  title = {{{SignTutor}}: {{An Interactive System}} for {{Sign Language Tutoring}}},
  shorttitle = {{{SignTutor}}},
  author = {Aran, O. and Ari, I. and Akarun, L. and Sankur, B. and Benoit, A. and Caplier, A. and Campr, P. and Carrillo, A. H. and Fanard, F. X.},
  year = {2009},
  month = jan,
  volume = {16},
  pages = {81--93},
  issn = {1070-986X},
  doi = {10.1109/MMUL.2009.17},
  abstract = {Language learning can only advance with practice and corrective feedback. The interactive system, SignTutor, evaluates users' signing and gives multimodal feedback to help improve signing.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2009/Aran et al/Aran et al_2009_SignTutor.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MBU58RC5/4800278.html},
  journal = {IEEE MultiMedia},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=19;signer=8;sl=TSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=head,signer=8,sl=TSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=head,vocab<50,vocab=19},
  number = {1}
}

@article{arapi_deepdynamichand_2018,
  title = {{{DeepDynamicHand}}: {{A Deep Neural Architecture}} for {{Labeling Hand Manipulation Strategies}} in {{Video Sources Exploiting Temporal Information}}},
  shorttitle = {{{DeepDynamicHand}}},
  author = {Arapi, Visar and Della Santina, Cosimo and Bacciu, Davide and Bianchi, Matteo and Bicchi, Antonio},
  year = {2018},
  month = dec,
  volume = {12},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2018.00086},
  abstract = {Humans are capable of complex manipulation interactions with the environment, relying on the intrinsic adaptability and compliance of their hands. Recently, soft robotic manipulation has attempted to reproduce such an extraordinary behavior, through the design of deformable yet robust end-effectors. To this goal, the investigation of human behavior has become crucial to correctly inform technological developments of robotic hands that can successfully exploit environmental constraint as humans actually do. Among the different tools robotics can leverage on to achieve this objective, deep learning has emerged as a promising approach for the study and then the implementation of neuro-scientific observations on the artificial side. However, current approaches tend to neglect the dynamic nature of hand pose recognition problems, limiting the effectiveness of these techniques in identifying sequences of manipulation primitives underpinning action generation, e.g., during purposeful interaction with the environment. In this work, we propose a vision-based supervised Hand Pose Recognition method which, for the first time, takes into account temporal information to identify meaningful sequences of actions in grasping and manipulation tasks. More specifically, we apply Deep Neural Networks to automatically learn features from hand posture images that consist of frames extracted from grasping and manipulation task videos with objects and external environmental constraints. For training purposes, videos are divided into intervals, each associated to a specific action by a human supervisor. The proposed algorithm combines a Convolutional Neural Network to detect the hand within each video frame and a Recurrent Neural Network to predict the hand action in the current frame, while taking into consideration the history of actions performed in the previous frames. Experimental validation has been performed on two datasets of dynamic hand-centric strategies, where subjects regularly interact with objects and environment. Proposed architecture achieved a very good classification accuracy on both datasets, reaching performance up to 94\%, and outperforming state of the art techniques. The outcomes of this study can be successfully applied to robotics, e.g., for planning and control of soft anthropomorphic manipulators.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Arapi et al_2018_DeepDynamicHand.pdf},
  journal = {Frontiers in Neurorobotics},
  pmcid = {PMC6304372},
  pmid = {30618707}
}

@inproceedings{ariesta_sentence_2018,
  title = {Sentence {{Level Indonesian Sign Language Recognition Using 3D Convolutional Neural Network}} and {{Bidirectional Recurrent Neural Network}}},
  booktitle = {2018 {{Indonesian Association}} for {{Pattern Recognition International Conference}} ({{INAPR}})},
  author = {Ariesta, M. C. and Wiryana, F. and Zahra, {and} A.},
  year = {2018},
  month = sep,
  pages = {16--22},
  doi = {10.1109/INAPR.2018.8627016},
  abstract = {Sign Language Recognition (SLR) is a relatively challenging research field which allows opportunity for improvements. In this research, we propose sentence-level SLR using deep learning method by combining Convolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network (Bi-RNN). Specifically, 3D CNN is implemented to extract features from each video frame and bidirectional-RNN is implemented to extract the unique features from the video frame's sequential behavior, which later generate a possible sentence. There are two key takeaways from this paper. The first is our proposed dataset of Indonesian Sign Language (SIBI) which is comprised of 30 sentences in SIBI. The second is our novel approach of using deep learning and Connectionist Temporal Classification (CTC) loss function in sentence-level SLR. The result shows that the hyperparameter used, in this case Hyperparameter 1, achieves the best result. Also, this research found that deeper network does not necessarily guarantee good results. A bigger number of dataset also affects the performance of the system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2018/Ariesta et al/Ariesta et al_2018_Sentence Level Indonesian Sign Language Recognition Using 3D Convolutional.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8QLNUPXQ/8627016.html},
  keywords = {capturedBy=vision,CTC loss,isolated,isolated;vocab=30;signer=10;sl=IndonesianSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,SIBI,signer=10,sl=IndonesianSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=30}
}

@incollection{arisoy_language_2018,
  title = {Language {{Modeling}} for {{Turkish Text}} and {{Speech Processing}}},
  booktitle = {Turkish {{Natural Language Processing}}},
  author = {Ar{\i}soy, Ebru and Sara{\c c}lar, Murat},
  editor = {Oflazer, Kemal and Sara{\c c}lar, Murat},
  year = {2018},
  pages = {69--92},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-90165-7_4},
  abstract = {This chapter presents an overview of language modeling followed by a discussion of the challenges in Turkish language modeling. Sub-lexical units are commonly used to reduce the high out-of-vocabulary (OOV) rates of morphologically rich languages. These units are either obtained by morphological analysis or by unsupervised statistical techniques. For Turkish, the morphological analysis yields word segmentations both at the lexical and surface forms which can be used as sub-lexical language modeling units. Discriminative language models, which outperform generative models for various tasks, allow for easy integration of morphological and syntactic features into language modeling. The chapter provides a review of both generative and discriminative approaches for Turkish language modeling.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Arısoy_Saraçlar_2018_Language Modeling for Turkish Text and Speech Processing.pdf},
  isbn = {978-3-319-90165-7},
  language = {en},
  series = {Theory and {{Applications}} of {{Natural Language Processing}}}
}

@article{arulampalam_tutorial_2002,
  title = {A Tutorial on Particle Filters for Online Nonlinear/Non-{{Gaussian Bayesian}} Tracking},
  author = {Arulampalam, M. Sanjeev and Maskell, Simon and Gordon, Neil},
  year = {2002},
  volume = {50},
  pages = {174--188},
  abstract = {Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or ``particle'') representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2002/Arulampalam et al/Arulampalam et al_2002_A tutorial on particle filters for online nonlinear-non-Gaussian Bayesian.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UBU9ZE4F/summary.html},
  journal = {IEEE TRANSACTIONS ON SIGNAL PROCESSING}
}

@article{arulkumaran_brief_2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = aug,
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archivePrefix = {arXiv},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2017/Arulkumaran et al/Arulkumaran et al_2017_A Brief Survey of Deep Reinforcement Learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PUP2BHZ9/1708.html},
  journal = {arXiv:1708.05866 [cs, stat]},
  primaryClass = {cs, stat}
}

@unpublished{aschenberner_phonemeviseme_2005,
  title = {Phoneme-{{Viseme Mapping}} for {{German Video}}-{{Realistic Audio}}-{{Visual}}-{{Speech}}-{{Synthesis}}},
  author = {Aschenberner, Bianca and Weiss, Christian},
  year = {2005},
  address = {{Bonn}},
  abstract = {In this working paper we introduce a German viseme set which we already use in our data-driven audio-visual synthesis system. The viseme set is essential for speech driven audio-visual synthesis due to the fact that the selection of appropriate video segments is based on the visemically transcribed input text. For text-to-speech synthesis, a transcription of the input text into the phonemic representation is used, in order to avoid ambiguous meanings and to acquire the correct pronunciation of the underlying input text. The transcription also serves as label in unit-selection based synthesis systems. Likewise, the visual synthesis requires a transcription that represents analogue to the phonemes, the visual counterpart which is called visemes in related literature and serves also as unit label in our data-driven video-realistic synthesis system. A viseme could include several phonemes, since we can not for example visually differentiate between pronouncing a /b / or a /p/. Thus, both sounds are bilabial plosives and consequently, have the same lip-based realisation. Therefore, we worked out an inventory of German viseme classes and developed a phoneme-viseme mapping for German that we currently use in our data-driven exemplary audio-visual synthesis system. The visemes are represented in a SAMPA-like labelling. For transcribing a large corpus, we trained a maximum entropy model for automatic visemic transcription and thus received feasible results. 1 1},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2005/Aschenberner_Weiss/Aschenberner_Weiss_2005_Phoneme-Viseme Mapping for German Video-Realistic Audio-Visual-Speech-Synthesis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K8B3QNWZ/summary.html},
  type = {{{IKP}} - {{Working}}  {{Paper NF}} 11}
}

@inproceedings{asghar_color_2008,
  title = {Color {{Image Segmentation Using Multilevel Clustering Approach}}},
  booktitle = {Digital {{Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}}), 2008},
  author = {Asghar, A. and Rao, N.I.},
  year = {2008},
  pages = {519--524},
  doi = {10.1109/DICTA.2008.54},
  abstract = {In this paper, we present a new approach for automatic color image segmentation. It is a multilevel clustering method based on a new proposed non-parametric clustering algorithm, called adaptive medoidshift (AMS) and normalized cuts (N-cut). The AMS algorithm is a modification of recently presented medoidshift algorithm by transforming its global fixed bandwidth to local automatically chosen bandwidth for every data point. The AMS method locally clusters the image color composition by considering their spatial distribution, resulting into uniform segments. Then the segmented regions are represented by graph structure and finally N-cut method performs optimized global grouping into meaningful salient regions that convey semantic information of image. The experiments show that proposed segmentation method provides good segmentation results on variety of color images.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2008/Asghar_Rao/Asghar_Rao_2008_Color Image Segmentation Using Multilevel Clustering Approach.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z6463INQ/abs_all.html}
}

@article{ashraf_painful_2009,
  title = {The Painful Face \textendash{} {{Pain}} Expression Recognition Using Active Appearance Models},
  author = {Ashraf, Ahmed Bilal and Lucey, Simon and Cohn, Jeffrey F. and Chen, Tsuhan and Ambadar, Zara and Prkachin, Kenneth M. and Solomon, Patricia E.},
  year = {2009},
  month = nov,
  volume = {27},
  pages = {1788--1796},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2009.05.007},
  abstract = {Pain is typically assessed by patient self-report. Self-reported pain, however, is difficult to interpret and may be impaired or in some circumstances (i.e., young children and the severely ill) not even possible. To circumvent these problems behavioral scientists have identified reliable and valid facial indicators of pain. Hitherto, these methods have required manual measurement by highly skilled human observers. In this paper we explore an approach for automatically recognizing acute pain without the need for human observers. Specifically, our study was restricted to automatically detecting pain in adult patients with rotator cuff injuries. The system employed video input of the patients as they moved their affected and unaffected shoulder. Two types of ground truth were considered. Sequence-level ground truth consisted of Likert-type ratings by skilled observers. Frame-level ground truth was calculated from presence/absence and intensity of facial actions previously associated with pain. Active appearance models (AAM) were used to decouple shape and appearance in the digitized face images. Support vector machines (SVM) were compared for several representations from the AAM and of ground truth of varying granularity. We explored two questions pertinent to the construction, design and development of automatic pain detection systems. First, at what level (i.e., sequence- or frame-level) should datasets be labeled in order to obtain satisfactory automatic pain detection performance? Second, how important is it, at both levels of labeling, that we non-rigidly register the face?},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2009/Ashraf et al/Ashraf et al_2009_The painful face – Pain expression recognition using active appearance models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WARBE8WD/S0262885609000985.html},
  journal = {Image and Vision Computing},
  number = {12},
  series = {Visual and Multimodal Analysis of Human Spontaneous Behaviour:}
}

@article{assael_lipnet_2016,
  title = {{{LipNet}}: {{End}}-to-{{End Sentence}}-Level {{Lipreading}}},
  shorttitle = {{{LipNet}}},
  author = {Assael, Yannis M. and Shillingford, Brendan and Whiteson, Shimon and {de Freitas}, Nando},
  year = {2016},
  month = nov,
  volume = {1},
  abstract = {Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung \& Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton \& Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2\% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4\% word-level state-of-the-art accuracy (Gergen et al., 2016).},
  archivePrefix = {arXiv},
  eprint = {1611.01599},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2016/Assael et al/Assael et al_2016_LipNet.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GS29W2W6/1611.html},
  journal = {arXiv:1611.01599 [cs]},
  primaryClass = {cs}
}

@article{assael_lipnet_2016a,
  title = {{{LipNet}}: {{Sentence}}-Level {{Lipreading}}},
  shorttitle = {{{LipNet}}},
  author = {Assael, Yannis M. and Shillingford, Brendan and Whiteson, Shimon and {de Freitas}, Nando},
  year = {2016},
  month = nov,
  abstract = {Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung \& Zisserman, 2016a). All existing works, however, perform only word classification, not sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton \& Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level, using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 93.4\% accuracy, outperforming experienced human lipreaders and the previous 79.6\% state-of-the-art accuracy.},
  archivePrefix = {arXiv},
  eprint = {1611.01599},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2016/Assael et al/Assael et al_2016_LipNet.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P4II8N65/1611.html},
  journal = {arXiv:1611.01599 [cs]},
  primaryClass = {cs}
}

@article{assaleh_continuous_2010,
  title = {Continuous {{Arabic Sign Language Recognition}} in {{User Dependent Mode}}},
  author = {Assaleh, K. and Shanableh, T. and Fanaswala, M. and Amin, F. and Bajaj, H.},
  year = {2010},
  month = mar,
  volume = {02},
  pages = {19},
  doi = {10.4236/jilsa.2010.21003},
  abstract = {Arabic Sign Language recognition is an emerging field of research. Previous attempts at automatic vision-based recog-nition of Arabic Sign Language mainly focused on finger spelling and recognizing isolated gestures. In this paper we report the first continuous Arabic Sign Language by building on existing research in feature extraction and pattern recognition. The development of the presented work required collecting a continuous Arabic Sign Language database which we designed and recorded in cooperation with a sign language expert. We intend to make the collected database available for the research community. Our system which we based on spatio-temporal feature extraction and hidden Markov models has resulted in an average word recognition rate of 94\%, keeping in the mind the use of a high perplex-ity vocabulary and unrestrictive grammar. We compare our proposed work against existing sign language techniques based on accumulated image difference and motion estimation. The experimental results section shows that the pro-posed work outperforms existing solutions in terms of recognition accuracy.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Assaleh et al_2010_Continuous Arabic Sign Language Recognition in User Dependent Mode.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WVQ4QNXN/PaperInformation.html},
  journal = {Journal of Intelligent Learning Systems and Applications},
  keywords = {capturedBy=vision,continuous,continuous;vocab=80;signer=1;sl=ArSL;capturedBy=vision,isolated,isolated;vocab=23;signer=3;sl=ArSL;capturedBy=vision,modal=2hlocation,signer=1,signer=3,sl=ArSL,slreco,slreco;modal=2hlocation,vocab<100,vocab<50,vocab=23,vocab=80},
  language = {en}
}

@inproceedings{assan_videobased_1997,
  title = {Video-Based Sign Language Recognition Using {{Hidden Markov Models}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Assan, Marcell and Grobel, Kirsti},
  year = {1997},
  month = sep,
  pages = {97--109},
  doi = {10.1007/BFb0052992},
  abstract = {This paper is concerned with the video-based recognition of signs. Concentrating on the manual parameters of sign language, the system aims for the signer dependent recognition of 262 different signs taken from Sign Language of the Netherlands. For Hidden Markov Modelling a sign is considered a doubly stochastic process, represented by an unobservable state sequence. The observations emitted by the states are regarded as feature vectors, that are extracted from video frames. This work deals with three topics: Firstly the recognition of isolated signs, secondly the influence of variations of the feature vector on the recognition rate and thirdly an approach for the recognition of connected signs. The system achieves recognition rates up to 94\% for isolated signs and 73\% for a reduced vocabulary of connected signs.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1997/Assan_Grobel/Assan_Grobel_1997_Video-based sign language recognition using Hidden Markov Models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EAF7HJKV/10.html},
  isbn = {978-3-540-64424-8 978-3-540-69782-4},
  keywords = {capturedBy=coloredGlove,continuous,continuous;vocab=26;signer=1;sl=NGT;capturedBy=coloredGlove,isolated,isolated;vocab=262;signer=2;sl=NGT;capturedBy=coloredGlove,modal=2hlocation,modal=2horientation,modal=2hshape,signer=1,signer=2,sl=NGT,slreco,slreco;modal=2hlocation;modal=2hshape;modal=2horientation,vocab<50,vocab<500,vocab=26,vocab=262},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{asteriadis_natural_2009,
  title = {A Natural Head Pose and Eye Gaze Dataset},
  booktitle = {Proceedings of the {{International Workshop}} on {{Affective}}-{{Aware Virtual Agents}} and {{Social Robots}}},
  author = {Asteriadis, Stylianos and Soufleros, Dimitris and Karpouzis, Kostas and Kollias, Stefanos},
  year = {2009},
  pages = {1},
  publisher = {{ACM}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2009/Asteriadis et al/Asteriadis et al_2009_A natural head pose and eye gaze dataset.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AQXM33MS/citation.html}
}

@article{ateniese_hacking_2013,
  title = {Hacking {{Smart Machines}} with {{Smarter Ones}}: {{How}} to {{Extract Meaningful Data}} from {{Machine Learning Classifiers}}},
  shorttitle = {Hacking {{Smart Machines}} with {{Smarter Ones}}},
  author = {Ateniese, Giuseppe and Felici, Giovanni and Mancini, Luigi V. and Spognardi, Angelo and Villani, Antonio and Vitali, Domenico},
  year = {2013},
  month = jun,
  abstract = {Machine Learning (ML) algorithms are used to train computers to perform a variety of complex tasks and improve with experience. Computers learn how to recognize patterns, make unintended decisions, or react to a dynamic environment. Certain trained machines may be more effective than others because they are based on more suitable ML algorithms or because they were trained through superior training sets. Although ML algorithms are known and publicly released, training sets may not be reasonably ascertainable and, indeed, may be guarded as trade secrets. While much research has been performed about the privacy of the elements of training sets, in this paper we focus our attention on ML classifiers and on the statistical information that can be unconsciously or maliciously revealed from them. We show that it is possible to infer unexpected but useful information from ML classifiers. In particular, we build a novel meta-classifier and train it to hack other classifiers, obtaining meaningful information about their training sets. This kind of information leakage can be exploited, for example, by a vendor to build more effective classifiers or to simply acquire trade secrets from a competitor's apparatus, potentially violating its intellectual property rights.},
  archivePrefix = {arXiv},
  eprint = {1306.4447},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ateniese et al_2013_Hacking Smart Machines with Smarter Ones.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6MDT7QIF/1306.html},
  journal = {arXiv:1306.4447 [cs, stat]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ateniese_hacking_2015,
  title = {Hacking Smart Machines with Smarter Ones: {{How}} to Extract Meaningful Data from Machine Learning Classifiers},
  shorttitle = {Hacking Smart Machines with Smarter Ones},
  author = {Ateniese, Giuseppe and Mancini, Luigi V. and Spognardi, Angelo and Villani, Antonio and Vitali, Domenico and Felici, Giovanni},
  year = {2015},
  month = sep,
  volume = {10},
  pages = {137--150},
  issn = {1747-8405},
  doi = {10.1504/IJSN.2015.071829},
  abstract = {Machine-learning ML enables computers to learn how to recognise patterns, make unintended decisions, or react to a dynamic environment. The effectiveness of trained machines varies because of more suitable ML algorithms or because superior training sets. Although ML algorithms are known and publicly released, training sets may not be reasonably ascertainable and, indeed, may be guarded as trade secrets. In this paper we focus our attention on ML classifiers and on the statistical information that can be unconsciously or maliciously revealed from them. We show that it is possible to infer unexpected but useful information from ML classifiers. In particular, we build a novel meta-classifier and train it to hack other classifiers, obtaining meaningful information about their training sets. Such information leakage can be exploited, for example, by a vendor to build more effective classifiers or to simply acquire trade secrets from a competitor's apparatus, potentially violating its intellectual property rights.},
  journal = {International Journal of Security and Networks},
  number = {3}
}

@inproceedings{athitsos_american_2008,
  title = {The {{American Sign Language Lexicon Video Dataset}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Athitsos, V. and Neidle, C. and Sclaroff, S. and Nash, J. and Stefan, A. and Yuan, Q. and Thangali, A.},
  year = {2008},
  month = jun,
  pages = {1--8},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2008/Athitsos et al/Athitsos et al_2008_The American Sign Language Lexicon Video Dataset.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/T9R7IN83/abs_all.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=108;signer=2;sl=ASL;capturedBy=vision,modal=2hlocation,signer=2,sl=ASL,slreco,slreco;modal=2hlocation,vocab<500,vocab=108}
}

@incollection{athitsos_database_2004,
  title = {Database {{Indexing Methods}} for {{3D Hand Pose Estimation}}},
  booktitle = {Gesture-{{Based Communication}} in {{Human}}-{{Computer Interaction}}},
  author = {Athitsos, Vassilis and Sclaroff, Stan},
  editor = {Camurri, Antonio and Volpe, Gualtiero},
  year = {2004},
  month = jan,
  pages = {288--299},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Estimation of 3D hand pose is useful in many gesture recognition applications, ranging from human-computer interaction to recognition of sign languages. In this paper, 3D hand pose estimation is treated as a database indexing problem. Given an input image of a hand, the most similar images in a large database of hand images are retrieved. The hand pose parameters of the retrieved images are used as estimates for the hand pose in the input image. Lipschitz embeddings are used to map edge images of hands into a Euclidean space. Similarity queries are initially performed in this Euclidean space, to quickly select a small set of candidate matches. These candidate matches are finally ranked using the more computationally expensive chamfer distance. Using Lipschitz embeddings to select likely candidate matches greatly reduces retrieval time over applying the chamfer distance to the entire database, without significant losses in accuracy.},
  copyright = {\textcopyright 2004 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2004/Athitsos_Sclaroff/Athitsos_Sclaroff_2004_Database Indexing Methods for 3D Hand Pose Estimation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HIDFRMJ5/10.html},
  isbn = {978-3-540-21072-6 978-3-540-24598-8},
  number = {2915},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{athitsos_databasebased_2010,
  title = {A Database-Based Framework for Gesture Recognition},
  author = {Athitsos, Vassilis and Wang, Haijing and Stefan, Alexandra},
  year = {2010},
  volume = {14},
  pages = {511--526},
  issn = {1617-4909},
  doi = {10.1007/s00779-009-0276-x},
  abstract = {Gestures are an important modality for human\textendash machine communication. Computer vision modules performing gesture recognition can be important components of intelligent homes, assistive environments, and human\textendash computer interfaces. A key problem in recognizing gestures is that the appearance of a gesture can vary widely depending on variables such as the person performing the gesture, or the position and orientation of the camera. This paper presents a database-based approach for addressing this problem. The large variability in appearance among different examples of the same gesture is addressed by creating large gesture databases, that store enough exemplars from each gesture to capture the variability within that gesture. This database-based approach is applied to two gesture recognition problems: handshape categorization and motion-based recognition of American Sign Language signs. A key aspect of our approach is the use of database indexing methods, in order to address the challenge of searching large databases without violating the time constraints of an online interactive system, where system response times of over a few seconds are oftentimes considered unacceptable. Our experiments demonstrate the benefits of the proposed database-based framework, and the feasibility of integrating large gesture databases into online interacting systems.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Athitsos et al/Athitsos et al_2010_A database-based framework for gesture recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NKUT8N5G/abstract.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R8GM65R2/j578207337201763.html},
  journal = {Personal and Ubiquitous Computing},
  keywords = {capturedBy=vision,isolated,isolated;vocab=921;signer=3;sl=ASL;capturedBy=vision,modal=movement,signer=3,sl=ASL,slreco,slreco;modal=movement,vocab<1000,vocab=921},
  number = {6}
}

@inproceedings{athitsos_estimating_2003,
  title = {Estimating {{3D}} Hand Pose from a Cluttered Image},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Athitsos, Vassilis and Sclaroff, Stan},
  year = {2003},
  month = jun,
  volume = {2},
  pages = {II--432},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2003/Athitsos_Sclaroff/Athitsos_Sclaroff_2003_Estimating 3D hand pose from a cluttered image.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/95DS85WA/login.html}
}

@inproceedings{audhkhasi_forget_2019,
  title = {Forget a {{Bit}} to {{Learn Better}}: {{Soft Forgetting}} for {{CTC}}-{{Based Automatic Speech Recognition}}},
  shorttitle = {Forget a {{Bit}} to {{Learn Better}}},
  booktitle = {Interspeech 2019},
  author = {Audhkhasi, Kartik and Saon, George and T{\"u}ske, Zolt{\'a}n and Kingsbury, Brian and Picheny, Michael},
  year = {2019},
  month = sep,
  pages = {2618--2622},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2841},
  abstract = {Prior work has shown that connectionist temporal classification (CTC)-based automatic speech recognition systems perform well when using bidirectional long short-term memory (BLSTM) networks unrolled over the whole speech utterance. This is because whole-utterance BLSTMs better capture longterm context. We hypothesize that this also leads to overfitting and propose soft forgetting as a solution. During training, we unroll the BLSTM network only over small non-overlapping chunks of the input utterance. We randomly pick a chunk size for each batch instead of a fixed global chunk size. In order to retain some utterance-level information, we encourage the hidden states of the BLSTM network to approximate those of a pre-trained whole-utterance BLSTM. Our experiments on the 300-hour English Switchboard dataset show that soft forgetting improves the word error rate (WER) above a competitive wholeutterance phone CTC BLSTM by an average of 7-9\% relative. We obtain WERs of 9.1\%/17.4\% using speaker-independent and 8.7\%/16.8\% using speaker-adapted models respectively on the Hub5-2000 Switchboard/CallHome test sets. We also show that soft forgetting improves the WER when the model is used with limited temporal context for streaming recognition. Finally, we present some empirical insights into the regularization and data augmentation effects of soft forgetting.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Audhkhasi et al_2019_Forget a Bit to Learn Better.pdf},
  language = {en}
}

@article{avola_exploiting_2019,
  title = {Exploiting {{Recurrent Neural Networks}} and {{Leap Motion Controller}} for the {{Recognition}} of {{Sign Language}} and {{Semaphoric Hand Gestures}}},
  author = {Avola, D. and Bernardi, M. and Cinque, L. and Foresti, G. L. and Massaroni, C.},
  year = {2019},
  month = jan,
  volume = {21},
  pages = {234--245},
  issn = {1520-9210},
  doi = {10.1109/TMM.2018.2856094},
  abstract = {Hand gesture recognition is still a topic of great interest for the computer vision community. In particular, sign language and semaphoric hand gestures are two foremost areas of interest due to their importance in human-human communication and human-computer interaction, respectively. Any hand gesture can be represented by sets of feature vectors that change over time. Recurrent neural networks (RNNs) are suited to analyze this type of set thanks to their ability to model the long-term contextual information of temporal sequences. In this paper, an RNN is trained by using as features the angles formed by the finger bones of the human hands. The selected features, acquired by a leap motion controller sensor, are chosen because the majority of human hand gestures produce joint movements that generate truly characteristic corners. The proposed method, including the effectiveness of the selected angles, was initially tested by creating a very challenging dataset composed by a large number of gestures defined by the American sign language. On the latter, an accuracy of over 96\% was achieved. Afterwards, by using the Shape Retrieval Contest (SHREC) dataset, a wide collection of semaphoric hand gestures, the method was also proven to outperform in accuracy competing approaches of the current literature.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Avola et al_2019_Exploiting Recurrent Neural Networks and Leap Motion Controller for the.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LZ3LJVWA/8410764.html},
  journal = {IEEE Transactions on Multimedia},
  keywords = {capturedBy=depth,isolated,isolated;vocab=30;signer=20;sl=ASL;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=20,sl=ASL,slreco,slreco;modal=2hshape;modal=2horientation;modal=2hmovement;modal=2hlocation,vocab<50,vocab=30},
  number = {1}
}

@inproceedings{awad_novel_2009,
  title = {Novel {{Boosting Framework}} for {{Subunit}}-{{Based Sign Language Recognition}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Awad, G. and Han, J. and Sutherland, A.},
  year = {2009},
  month = nov,
  pages = {2729--2732},
  doi = {10.1109/ICIP.2009.5414159},
  abstract = {Recently, a promising research direction has emerged in sign language recognition (SLR) aimed at breaking up signs into manageable subunits. This paper presents a novel SL learning technique based on boosted subunits. Three main contributions distinguish the proposed work from traditional approaches: (1) A novel boosting framework is developed to recognize SL. The learning is based on subunits instead of the whole sign, which is more scalable for the recognition task. (2) Feature selection is performed to learn a small set of discriminative combinations of subunits and SL features. (3) A joint learning strategy is adopted to share subunits across sign classes, which leads to a better performance classifiers. Our experiments show that compared to Dynamic Time Warping (DTW) when applied on the whole sign, our proposed technique gives better results.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2009/Awad et al/Awad et al_2009_Novel boosting framework for subunit-based sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KT9ZBQRX/abs_all.html},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=20;signer=1;sl=BSL;capturedBy=coloredGlove,modal=movement,modal=shape,signer=1,sl=BSL,slreco,slreco;modal=shape;modal=movement,vocab<50,vocab=20}
}

@inproceedings{awad_unified_2006,
  title = {A {{Unified System}} for {{Segmentation}} and {{Tracking}} of {{Face}} and {{Hands}} in {{Sign Language Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Awad, G. and {Junwei Han} and Sutherland, A.},
  year = {2006},
  month = aug,
  volume = {1},
  pages = {239--242},
  doi = {10.1109/ICPR.2006.194},
  abstract = {This paper presents a unified system for segmentation and tracking of face and hands in a sign language recognition using a single camera. Unlike much related work that uses colour gloves, we detect skin by combining 3 useful features: colour, motion and position. These features together, represent the skin colour pixels that are more likely to be foreground pixels and are within a predicted position range. We extend the previous research in occlusion detection to handle occlusion between any of the skin objects using a Kalman filter based algorithm. The tracking improves the segmentation by reducing the search space and the segmentation enhances the overall tracking process. The algorithm is tested on several video sequences from a standard database and can provide a very low error rate},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Awad et al_2006_A Unified System for Segmentation and Tracking of Face and Hands in Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F6FPT6VB/1698877.html}
}

@inproceedings{baccouche_sequential_2011,
  title = {Sequential {{Deep Learning}} for {{Human Action Recognition}}},
  booktitle = {Proc.  {{Int}}. {{Conf}}. on {{Human Behavior Unterstanding}}},
  author = {Baccouche, Moez and Mamalet, Franck and Wolf, Christian and Garcia, Christophe and Baskurt, Atilla},
  year = {2011},
  pages = {29--39},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-25446-8_4},
  abstract = {We propose in this paper a fully automated deep model, which learns to classify human actions without using any prior knowledge. The first step of our scheme, based on the extension of Convolutional Neural Networks to 3D, automatically learns spatio-temporal features. A Recurrent Neural Network is then trained to classify each sequence considering the temporal evolution of the learned features for each timestep. Experimental results on the KTH dataset show that the proposed approach outperforms existing deep models, and gives comparable results with the best related works.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2011/Baccouche et al/Baccouche et al_2011_Sequential Deep Learning for Human Action Recognition.pdf},
  isbn = {978-3-642-25445-1},
  series = {{{HBU}}'11}
}

@inproceedings{baccouche_spatiotemporal_2012,
  title = {Spatio-{{Temporal Convolutional Sparse Auto}}-{{Encoder}} for {{Sequence Classification}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Baccouche, Moez and Mamalet, Franck and Wolf, Christian and Garcia, Christophe and Baskurt, Atilla},
  year = {2012},
  pages = {124.1-124.12},
  publisher = {{British Machine Vision Association}},
  doi = {10.5244/C.26.124},
  isbn = {1-901725-46-4},
  language = {en}
}

@article{badrinarayanan_segnet_2015,
  title = {{{SegNet}}: {{A Deep Convolutional Encoder}}-{{Decoder Architecture}} for {{Image Segmentation}}},
  shorttitle = {{{SegNet}}},
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  year = {2015},
  month = nov,
  abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the fully convolutional network (FCN) architecture and its variants. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. The design of SegNet was primarily motivated by road scene understanding applications. Hence, it is efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent. We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge. We show that SegNet provides competitive performance although it is significantly smaller than other architectures. We also provide a Caffe implementation of SegNet and a webdemo at http://mi.eng.cam.ac.uk/projects/segnet/},
  archivePrefix = {arXiv},
  eprint = {1511.00561},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Badrinarayanan et al/Badrinarayanan et al_2015_SegNet.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GJZUTWT3/1511.html},
  journal = {arXiv:1511.00561 [cs]},
  primaryClass = {cs}
}

@article{bahar_empirical_2017,
  title = {Empirical {{Investigation}} of {{Optimization Algorithms}} in {{Neural Machine Translation}}},
  author = {Bahar, Parnia and Alkhouli, Tamer and Peter, Jan-Thorsten and Brix, Christopher Jan-Steffen and Ney, Hermann},
  year = {2017},
  month = jun,
  volume = {108},
  pages = {13--25},
  issn = {1804-0462},
  doi = {10.1515/pralin-2017-0005},
  abstract = {Training neural networks is a non-convex and a high-dimensional optimization problem. In this paper, we provide a comparative study of the most popular stochastic optimization techniques used to train neural networks. We evaluate the methods in terms of convergence speed, translation quality, and training stability. In addition, we investigate combinations that seek to improve optimization in terms of these aspects. We train state-of-the-art attention-based models and apply them to perform neural machine translation. We demonstrate our results on two tasks: WMT 2016 En\textrightarrow Ro and WMT 2015 De\textrightarrow En.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2017/Bahar et al/Bahar et al_2017_Empirical Investigation of Optimization Algorithms in Neural Machine Translation.pdf},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  language = {en},
  number = {1}
}

@inproceedings{bahdanau_endtoend_2016,
  title = {End-to-End Attention-Based Large Vocabulary Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bahdanau, D. and Chorowski, J. and Serdyuk, D. and Brakel, P. and Bengio, Y.},
  year = {2016},
  month = mar,
  pages = {4945--4949},
  doi = {10.1109/ICASSP.2016.7472618},
  abstract = {Many state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are hybrids of neural networks and Hidden Markov Models (HMMs). Recently, more direct end-to-end methods have been investigated, in which neural architectures were trained to model sequences of characters [1,2]. To our knowledge, all these approaches relied on Connectionist Temporal Classification [3] modules. We investigate an alternative method for sequence modelling based on an attention mechanism that allows a Recurrent Neural Network (RNN) to learn alignments between sequences of input frames and output labels. We show how this setup can be applied to LVCSR by integrating the decoding RNN with an n-gram language model and by speeding up its operation by constraining selections made by the attention mechanism and by reducing the source sequence lengths by pooling information over time. Recognition accuracies similar to other HMM-free RNN-based approaches are reported for the Wall Street Journal corpus.}
}

@inproceedings{bahdanau_neural_2015,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Learning Representations}} ({{ICLR}})},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2015},
  month = may,
  address = {{San Diego, CA, USA}},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014/Bahdanau et al/Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AUHFVEZR/1409.html}
}

@article{bahl_maximum_1983,
  title = {A {{Maximum Likelihood Approach}} to {{Continuous Speech Recognition}}},
  author = {Bahl, L. R. and Jelinek, F. and Mercer, R. L.},
  year = {1983},
  month = mar,
  volume = {PAMI-5},
  pages = {179--190},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.1983.4767370},
  abstract = {Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1983/Bahl et al/Bahl et al_1983_A Maximum Likelihood Approach to Continuous Speech Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/72VXJV32/4767370.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {maximum likelihood},
  number = {2}
}

@article{baker_dragon_1975,
  title = {The {{DRAGON}} System\textendash{{An}} Overview},
  author = {Baker, J.},
  year = {1975},
  month = feb,
  volume = {23},
  pages = {24--29},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1975.1162650},
  abstract = {This paper briefly describes the major features of the DRAGON speech understanding system. DRAGON makes systematic use of a general abstract model to represent each of the knowledge sources necessary for automatic recognition of continuous speech. The model\textendash that of a probabilistic function of a Markov process\textendash is very flexible and leads to features which allow DRAGON to function despite high error rates from individual knowledge sources. Repeated use of a simple abstract model produces a system which is simple in structure, but powerful in capabilities.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Baker_1975_The DRAGON system–An overview.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/USCXICEF/1162650.html},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  number = {1}
}

@phdthesis{baker_stochastic_1975,
  title = {Stochastic Modeling as a Means of Automatic Speech Recognition},
  author = {Baker, James K.},
  year = {1975},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1975/Baker/Baker_1975_Stochastic modeling as a means of automatic speech recognition.pdf},
  school = {Dept. of Comp. Science, Carnegie-Mellon Univ.}
}

@incollection{baker_stochastic_1975a,
  title = {Stochastic {{Modeling}} for {{Automatic Speech Understanding}}},
  booktitle = {Speech {{Recognition}}: {{Invited Papers Presented}} at the 1974 {{IEEE Symposium}}},
  author = {Baker, J. K.},
  editor = {Reddy, D. R.},
  year = {1975},
  month = jan,
  pages = {521--542},
  publisher = {{Academic Press}},
  abstract = {Speech Recognition: Invited Papers Presented at the 1974 IEEE Symposium discusses several topics, including speech recognition systems, systems organization, acoustic-phonetics, parameter extraction, as well as syntax and semantics. Organized into five parts encompassing 20 chapters, this compilation of papers starts with an overview of the basic structure of speech understanding systems. This text then discusses the practical applications of automatic speech recognition in several areas, including quality control inspection, automated material handling, direct communication with computers, and inventory taking and control. Other chapters consider the operational methods for applying higher level of information to decode the acoustic ambiguities encountered when recognizing larger vocabularies and continuous speech. The final chapter deals with stochastic modeling, which is a valuable and versatile procedure for automatic speech analysis. This book is a valuable resource for scientists and researchers in the fields of artificial intelligence, acoustic-phonetics, linguistics, and computer architecture.},
  googlebooks = {eJac7g7YfZIC},
  isbn = {978-0-323-14619-7},
  language = {en}
}

@inproceedings{baker_stochastic_1990,
  title = {Stochastic Modeling for Automatic Speech Understanding},
  booktitle = {Readings in Speech Recognition},
  author = {Baker, James K.},
  year = {1990},
  pages = {297--307},
  publisher = {{Morgan Kaufmann Publishers Inc.}}
}

@article{bakis_continuous_1976,
  title = {Continuous Speech Recognition via Centisecond Acoustic States},
  author = {Bakis, Raimo},
  year = {1976},
  volume = {59},
  pages = {S97--S97},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1976/Bakis/Bakis_1976_Continuous speech recognition via centisecond acoustic states.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FQ5IUK3H/1.html},
  journal = {The Journal of the Acoustical Society of America},
  number = {S1}
}

@inproceedings{balayn_datadriven_2018,
  title = {Data-Driven Development of {{Virtual Sign Language Communication Agents}}},
  booktitle = {2018 27th {{IEEE International Symposium}} on {{Robot}} and {{Human Interactive Communication}} ({{RO}}-{{MAN}})},
  author = {Balayn, A. and Brock, H. and Nakadai, K.},
  year = {2018},
  month = aug,
  pages = {370--377},
  doi = {10.1109/ROMAN.2018.8525717},
  abstract = {Engaging deaf and hearing people in common discussions requires interfaces to help them understand each other, such as robot agents that translate spoken language into Sign Language (SL) expressions and vice-versa. However, the recognition and generation of signed sentences is a complex task of high dimensionality that cannot be solved in sufficient quality yet. Thus, it is necessary to develop new technologies of improved performances. The sequence to sequence neural network model, traditionally used for machine translation, is adapted to the above two tasks by treating a SL sequence as a multi-dimensional sentence. We defined an encoding of the SL annotations and conducted experiments on the network structure to define a most accurate translation model. This study proves the network trainable and possibly applicable in real-life with an extended dataset, which shall be tested for deployment in virtual translation assistants in the following.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Balayn et al_2018_Data-driven development of Virtual Sign Language Communication Agents.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DZMT92VZ/8525717.html}
}

@inproceedings{ballati_assessing_2018,
  title = {Assessing {{Virtual Assistant Capabilities}} with {{Italian Dysarthric Speech}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Ballati, Fabio and Corno, Fulvio and De Russis, Luigi},
  year = {2018},
  pages = {93--101},
  address = {{Galway, Ireland}},
  doi = {10.1145/3234695.3236354},
  abstract = {The usage of smartphone-based virtual assistants (e.g., Siri or Google Assistant) is growing, and their spread was most possible by the increasing capabilities of natural language processing, and generally has a positive impact on device accessibility, e.g., for people with disabilities. However, people with dysarthria or other speech impairments may be unable to use these virtual assistants with proficiency. This paper investigates to which extent people with ALSinduced dysarthria can be understood and get consistent answers by three widely used smartphone-based assistants, namely Siri, Google Assistant, and Cortana. In particular, we focus on the recognition of Italian dysarthric speech, to study the behavior of the virtual assistants with this specific population for which there are no relevant studies available. We collected and recorded suitable speech samples from people with dysarthria in a dedicated center of the Molinette hospital, in Turin, Italy. Starting from those recordings, the differences between such assistants, in terms of speech recognition and consistency in answer, are investigated and discussed. Results highlight different performance among the virtual assistants. For speech recognition, Google Assistant is the most promising, with around 25\% of word error rate per sentence. Consistency in answer, instead, sees Siri and Google Assistant provide coherent answers around 60\% of times.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J8GQSZKC/Ballati et al. - 2018 - Assessing Virtual Assistant Capabilities with Ital.pdf},
  isbn = {978-1-4503-5650-3},
  language = {en}
}

@inproceedings{baltrusaitis_openface_2018,
  title = {{{OpenFace}} 2.0: {{Facial Behavior Analysis Toolkit}}},
  shorttitle = {{{OpenFace}} 2.0},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Baltrusaitis, T. and Zadeh, A. and Lim, Y. C. and Morency, L.},
  year = {2018},
  month = may,
  pages = {59--66},
  doi = {10.1109/FG.2018.00019},
  abstract = {Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Baltrusaitis et al_2018_OpenFace 2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DVVDPDP6/8373812.html}
}

@article{bank_variation_2011,
  title = {Variation in Mouth Actions with Manual Signs in {{Sign Language}} of the {{Netherlands}} ({{NGT}}).},
  author = {Bank, Richard and Crasborn, Onno and {van Hout}, Roeland},
  year = {2011},
  volume = {14},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2011/Bank et al/Bank et al_2011_Variation in mouth actions with manual signs in Sign Language of the.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PABKPUZ9/advanced.html},
  journal = {Sign Language \& Linguistics},
  number = {2}
}

@book{baranowski_performing_2013,
  title = {Performing Object Detection Operations via a Graphics Processing Unit},
  author = {Baranowski, Mateusz Jerzy and Gupta, Shalini and Albuz, Elif},
  year = {2013},
  month = sep
}

@article{barczak_new_2011,
  title = {A New {{2D}} Static Hand Gesture Colour Image Dataset for Asl Gestures},
  author = {Barczak, A. L. C. and Reyes, N. H. and Abastillas, M. and Piccio, A. and Susnjak, T.},
  year = {2011},
  volume = {15},
  pages = {12--20},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2011/Barczak et al/Barczak et al_2011_A new 2D static hand gesture colour image dataset for asl gestures.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6T6QRGKW/4514.html},
  journal = {Research Letters in the Information and Mathematical Sciences},
  keywords = {capturedBy=vision,capturedBy=wristcover,isolated,isolated;vocab=36;signer=5;sl=ASL;capturedBy=vision;capturedBy=wristcover,modal=shape,signer=5,sl=ASL,slreco,slreco;modal=shape,vocab<50,vocab=36}
}

@article{barnard_matching_2003,
  title = {Matching Words and Pictures},
  author = {Barnard, Kobus and Duygulu, Pinar and Forsyth, David and De Freitas, Nando and Blei, David M. and Jordan, Michael I.},
  year = {2003},
  volume = {3},
  pages = {1107--1135},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2003/Barnard et al/Barnard et al_2003_Matching words and pictures.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZXDMW5GG/citation.html},
  journal = {The Journal of Machine Learning Research}
}

@inproceedings{bartlett_recognizing_2005,
  title = {Recognizing Facial Expression: Machine Learning and Application to Spontaneous Behavior},
  shorttitle = {Recognizing Facial Expression},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bartlett, Marian Stewart and Littlewort, Gwen and Frank, Mark and Lainscsek, Claudia and Fasel, Ian and Movellan, Javier},
  year = {2005},
  volume = {2},
  pages = {568--573},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2005/Bartlett et al/Bartlett et al_2005_Recognizing facial expression.pdf}
}

@book{battison_lexical_1978,
  title = {Lexical Borrowing in {{American}} Sign Language.},
  author = {Battison, Robbin},
  year = {1978},
  publisher = {{Linstok Press}},
  address = {{Silver Spring, Maryland}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IAWZMI89/eric.ed.gov.html}
}

@inproceedings{bauer_automatic_1999,
  title = {Towards an {{Automatic Sign Language Translation System}}},
  booktitle = {Proc. of the {{International Workshop}} on {{Physicality}} and {{Tangibility}} in {{Interaction}}: {{Towards New Paradigms}} for {{Interaction Beyond}} the {{Desktop}}},
  author = {Bauer, B. and Nie{\ss}en, S. and Hienz, H.},
  year = {1999},
  month = oct,
  address = {{Siena, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1999/Bauer et al/Bauer et al_1999_Towards an Automatic Sign Language Translation System.pdf},
  keywords = {capturedBy=coloredGlove,continuous,continuous;vocab=100;signer=1;sl=DGS;capturedBy=coloredGlove,modal=2hlocation,modal=2hshape,signer=1,sl=DGS,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<500,vocab=100}
}

@incollection{bauer_automatic_2002,
  title = {Towards an {{Automatic Sign Language Recognition System Using Subunits}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Bauer, Britta and Kraiss, Karl-Friedrich},
  editor = {Wachsmuth, Ipke and Sowa, Timo},
  year = {2002},
  volume = {2298},
  pages = {123--173},
  publisher = {{Springer Berlin / Heidelberg}},
  abstract = {This paper is concerned with the automatic recognition of German continuous sign language. For the most user-friendliness only one single color video camera is used for image recording. The statistical approach is based on the Bayes decision rule for minimum error rate. Following speech recognition system design, which are in general based on subunits, here the idea of an automatic sign language recognition system using subunits rather than models for whole signs will be outlined. The advantage of such a system will be a future reduction of necessary training material. Furthermore, a simplified enlargement of the existing vocabulary is expected. Since it is difficult to define subunits for sign language, this approach employs totally self-organized subunits called fenone. K-means algorithm is used for the definition of such fenones. The software prototype of the system is currently evaluated in experiments.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bauer_Kraiss_2002_Towards an Automatic Sign Language Recognition System Using Subunits.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X4SFJUAJ/abstract.html},
  isbn = {978-3-540-43678-2},
  keywords = {continuous,continuous;vocab=12,modal=location,modal=orientation,modal=shape,slreco,slreco;modal=shape;modal=location;modal=orientation,vocab<50,vocab=12},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{bauer_videobased_2002,
  title = {Video-Based Sign Recognition Using Self-Organizing Subunits},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Bauer, B. and Kraiss, K. -F},
  year = {2002},
  month = aug,
  volume = {2},
  pages = {434--437},
  address = {{Quebec, Canada}},
  doi = {10.1109/ICPR.2002.1048332},
  abstract = {This paper deals with the automatic recognition of German signs. The statistical approach is based on the Bayes decision rule for minimum error rate. Following speech recognition system designs, which are in general based on phonemes, here the idea of an automatic sign language recognition system using subunits rather than models for whole signs is outlined. The advantage of such a system will be a future reduction of necessary training material. Furthermore, a simplified enlargement of the existing vocabulary is expected, as new signs can be added to the vocabulary database without re-training the existing hidden Markov models (HMMs) for subunits. Since it is difficult to define subunits for sign language, this approach employs totally self-organized subunits. In first experiences a recognition accuracy of 92,5\% was achieved for 100 signs, which were previously trained. For 50 new signs an accuracy of 81\% was achieved without retraining of subunit-HMMs.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2002/Bauer_Kraiss/Bauer_Kraiss_2002_Video-based sign recognition using self-organizing subunits.pdf},
  isbn = {0-7695-1695-X},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=100;signer=1;sl=DGS;capturedBy=coloredGlove,isolated;vocab=50;signer=1;sl=DGS;capturedBy=coloredGlove,modal=2hlocation,modal=2hshape,signer=1,sl=DGS,slreco,slreco;modal=2hshape;modal=2hlocation,vocab<100,vocab<500,vocab=100,vocab=50},
  language = {English}
}

@article{baum_inequality_1967,
  title = {An Inequality with Applications to Statistical Estimation for Probabilistic Functions of {{Markov}} Processes and to a Model for Ecology},
  author = {Baum, Leonard E. and Eagon, John Alonzo},
  year = {1967},
  month = may,
  volume = {73},
  pages = {360--363},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1967/Baum_Eagon/Baum_Eagon_1967_An inequality with applications to statistical estimation for probabilistic.pdf},
  journal = {Bulletin of the American Mathematical Society},
  number = {3}
}

@inproceedings{baum_inequality_1972,
  title = {An {{Inequality}} and {{Associated Maximization Technique}} in {{Statistical Estimation}} for {{Probabilistic Functions}} of {{Markov Processes}}},
  booktitle = {Inequalities {{III}}: {{Proceedings}} of the {{Third Symposium}} on {{Inequalities}}},
  author = {Baum, Leonard E.},
  editor = {Shisha, Oved},
  year = {1972},
  month = apr,
  pages = {1--8},
  publisher = {{Academic Press}},
  address = {{New York, NY, USA}},
  keywords = {2000,book,nlp}
}

@article{baum_maximization_1970,
  title = {A {{Maximization Technique Occurring}} in the {{Statistical Analysis}} of {{Probabilistic Functions}} of {{Markov Chains}}},
  author = {Baum, Leonard E. and Petrie, Ted and Soules, George and Weiss, Norman},
  year = {1970},
  month = feb,
  volume = {41},
  pages = {164--171},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177697196},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Baum et al_1970_A Maximization Technique Occurring in the Statistical Analysis of Probabilistic.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NH2W738D/1177697196.html},
  journal = {The Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR287613},
  number = {1},
  zmnumber = {0188.49603}
}

@article{baum_statistical_1966,
  title = {Statistical Inference for Probabilistic Functions of Finite State {{Markov}} Chains},
  author = {Baum, Leonard E. and Petrie, Ted},
  year = {1966},
  volume = {37},
  pages = {1554--1563},
  journal = {Annals of Mathematical Statistics},
  number = {6}
}

@article{bayes_lii_1763,
  title = {{{LII}}. {{An}} Essay towards Solving a Problem in the Doctrine of Chances. {{By}} the Late {{Rev}}. {{Mr}}. {{Bayes}}, {{F}}. {{R}}. {{S}}. Communicated by {{Mr}}. {{Price}}, in a Letter to {{John Canton}}, {{A}}. {{M}}. {{F}}. {{R}}. {{S}}},
  author = {Bayes, Thomas},
  year = {1763},
  month = jan,
  volume = {53},
  pages = {370--418},
  issn = {0261-0523,},
  doi = {10.1098/rstl.1763.0053},
  abstract = {Extract
Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bayes_1763_LII.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JKW8RM4D/370.html},
  journal = {Philosophical Transactions},
  language = {en}
}

@inproceedings{bear_resolution_2014,
  title = {Resolution Limits on Visual Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Bear, H.L. and Harvey, R. and Theobald, B.-J. and Lan, Yuxuan},
  year = {2014},
  month = oct,
  pages = {1371--1375},
  doi = {10.1109/ICIP.2014.7025274},
  abstract = {Visual-only speech recognition is dependent upon a number of factors that can be difficult to control, such as: lighting; identity; motion; emotion and expression. But some factors, such as video resolution are controllable, so it is surprising that there is not yet a systematic study of the effect of resolution on lip-reading. Here we use a new data set, the Rosetta Raven data, to train and test recognizers so we can measure the affect of video resolution on recognition accuracy. We conclude that, contrary to common practice, resolution need not be that great for automatic lip-reading. However it is highly unlikely that automatic lip-reading can work reliably when the distance between the bottom of the lower lip and the top of the upper lip is less than four pixels at rest.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bear et al_2014_Resolution limits on visual speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ANET9FIG/login.html}
}

@article{bear_speakerindependent_,
  title = {Speaker-Independent Machine Lip-Reading with Speaker-Dependent Viseme Classifiers},
  author = {Bear, Helen L. and Cox, Stephen J. and Harvey, Richard W.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bear et al_Speaker-independent machine lip-reading with speaker-dependent viseme.pdf}
}

@inproceedings{beaudoin_motionmotif_2008,
  title = {Motion-Motif Graphs},
  booktitle = {Proc. {{ACM SIGGRAPH}}/{{Eurographics Symposium}} on {{Computer Animation}}},
  author = {Beaudoin, Philippe and Coros, Stelian and {van de Panne}, Michiel and Poulin, Pierre},
  year = {2008},
  pages = {117--126},
  publisher = {{Eurographics Association}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Beaudoin et al_2008_Motion-motif graphs.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UZGBC3QP/citation.html}
}

@unpublished{belissen_automatic_2019,
  title = {Automatic Recognition of {{Sign Language}} Structures in {{RGB}} Videos: The Detection of Pointing and Lexical Signs},
  shorttitle = {Automatic Recognition of {{Sign Language}} Structures in {{RGB}} Videos},
  author = {Belissen, Valentin and Gouiff{\`e}s, Mich{\`e}le and Braffort, Annelies},
  year = {2019},
  abstract = {This work presents a generic approach to tackle continuous Sign Language Recognition (SLR) in ordinary RGB Sign Language videos. While usual SLR systems only analyze SL through the lexical level, it is shown here that both lexical and SL-specific features can be accurately detected and localized. This paves the way to better understanding Sign Language linguistics, as well as benefiting the Deaf community in need of SL-specific tools like video querying. Generic human motion features are first extracted from videos, so that a very compact modeling is obtained and future training time is limited. A Recurrent Neural Network is then trained on those generic features to recognize lexicon and SL-specific structures like pointing. Applied to the French Sign Language corpus Dicta-Sign, pointing signs detection gets a 78\% sequence-wise F1-score on 4 seconds chunks. The network also gets a 70\% sequence-wise F1-score for the detection of lexical signs with less than 50 instances in the training set. These are very promising results for a first SLR trial on a French Sign Language corpus, given its relatively short total duration, its low image resolution and frame rate, and the unconstrained nature of the recorded dialogs.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Belissen et al_2019_Automatic recognition of Sign Language structures in RGB videos.pdf},
  keywords = {Computer Vision,Pointing signs,Sign Language,Sign Language Linguistics,Sign Language Recognition}
}

@book{bellman_dynamic_1957,
  title = {Dynamic Programming},
  author = {Bellman, Richard Ernest},
  year = {1957},
  publisher = {{Princeton Univ. Press}},
  address = {{Princeton, NJ}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bellman_1957_Dynamic programming.pdf},
  language = {eng},
  lccn = {519.8 BEL},
  series = {Rand Research Study}
}

@article{bellman_theory_1954,
  title = {The Theory of Dynamic Programming},
  author = {Bellman, Richard},
  year = {1954},
  month = nov,
  volume = {60},
  pages = {503--515},
  issn = {0002-9904, 1936-881X},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bellman_1954_The theory of dynamic programming.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QQ3AW6JP/1183519147.html},
  journal = {Bulletin of the American Mathematical Society},
  language = {EN},
  mrnumber = {MR0067459},
  number = {6},
  zmnumber = {0057.12503}
}

@article{bellugi_comparison_1972,
  title = {A Comparison of Sign Language and Spoken Language},
  author = {Bellugi, Ursula and Fischer, Susan},
  year = {1972},
  month = jan,
  volume = {1},
  pages = {173--200},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(72)90018-2},
  abstract = {Evidence is presented which suggests that a sign in the American Sign Language takes longer to produce than a spoken word, but that a proposition takes about the same amount of time to produce in either language, or either modality for some signers. Properties of American Sign Language which can account for both of these facts are then discussed.
R\'esum\'e
Les donn\'ees pr\'esent\'ees ont montr\'e que la production d'un geste dans le American Sign Language (ASL) demande plus de temps que celle d'un mot parl\'e, mais que la production d'une proposition prend \'a peu pr\'es le m\^eme temps dans les deux langages, et prend le m\^eme temps avec chacun des proc\'ed\'es, pour les sujets bilingues. Suit une discussion des propri\'et\'es du ASL qui rend compte de ces faits.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bellugi_Fischer_1972_A comparison of sign language and spoken language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/58A93G3B/0010027772900182.html},
  journal = {Cognition},
  number = {2}
}

@inproceedings{Bendale_2016_CVPR,
  title = {Towards {{Open Set Deep Networks}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bendale, Abhijit and Boult, Terrance E.},
  year = {2016},
  month = jun
}

@phdthesis{bender10:phd,
  title = {Robust {{Machine Translation}} for {{Multi}}-{{Domain Tasks}}},
  author = {Bender, Oliver},
  year = {2010},
  month = mar,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bender_2010_Robust Machine Translation for Multi-Domain Tasks.pdf},
  school = {RWTH Aachen University}
}

@article{benedicto_where_2004,
  title = {Where Did All the Arguments Go?: {{Argument}}-Changing Properties of Classifiers in {{ASL}}},
  shorttitle = {Where Did All the Arguments Go?},
  author = {Benedicto, Elena and Brentari, Diane},
  year = {2004},
  volume = {22},
  pages = {743--810},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Benedicto_Brentari_2004_Where did all the arguments go.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J2FUNNEK/s11049-003-4698-2.html},
  journal = {Natural Language \& Linguistic Theory},
  number = {4}
}

@book{benesty_springer_2008,
  title = {Springer {{Handbook}} of {{Speech Processing}}},
  author = {Benesty, J. and Sondhi, M. and Huang, Y.},
  year = {2008},
  edition = {First},
  publisher = {{Springer}},
  address = {{Berlin/New York}},
  isbn = {3-540-49125-2}
}

@inproceedings{bengio_asynchronous_2003,
  title = {An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Samy},
  year = {2003},
  month = dec,
  pages = {1213--1220},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio_2003_An asynchronous hidden markov model for audio-visual speech recognition.pdf}
}

@techreport{bengio_asynchronous_2003a,
  title = {An {{Asynchronous Hidden Markov Model}} for {{Audio}}-{{Visual Speech Recognition}}},
  author = {Bengio, Samy},
  year = {2003},
  month = jan,
  pages = {1--12},
  institution = {{IDIAP}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio_2002_An Asynchronous Hidden Markov Model for Audio-Visual Speech Recognition.pdf},
  number = {Idiap-RR 02-26}
}

@article{bengio_connectionist_1993,
  title = {A Connectionist Approach to Speech Recognition},
  author = {Bengio, Yoshua},
  year = {1993},
  volume = {7},
  pages = {647--667},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio_1993_A connectionist approach to speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7IBIQJ3Q/S0218001493000327.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {04}
}

@inproceedings{bengio_curriculum_2009,
  title = {Curriculum {{Learning}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Learning}} ({{ICML}})},
  author = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  year = {2009},
  month = jun,
  pages = {41--48},
  address = {{New York, NY, USA}},
  doi = {10.1145/1553374.1553380},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  isbn = {978-1-60558-516-1},
  series = {{{ICML}} '09}
}

@article{bengio_estimating_2013,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  year = {2013},
  month = aug,
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{\textbackslash em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  archivePrefix = {arXiv},
  eprint = {1308.3432},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio et al_2013_Estimating or Propagating Gradients Through Stochastic Neurons for Conditional.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z4FUG2B8/1308.html},
  journal = {arXiv:1308.3432 [cs]},
  primaryClass = {cs}
}

@inproceedings{bengio_input_1995,
  title = {An Input Output {{HMM}} Architecture},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Frasconi, Paolo},
  year = {1995},
  pages = {427--434},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1995/Bengio_Frasconi/Bengio_Frasconi_1995_An input output HMM architecture.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio_Frasconi_1995_An input output HMM architecture.pdf}
}

@article{bengio_inputoutput_1996,
  title = {Input-Output {{HMMs}} for Sequence Processing},
  author = {Bengio, Yoshua and Frasconi, Paolo},
  year = {1996},
  volume = {7},
  pages = {1231--1249},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio_Frasconi_1996_Input-output HMMs for sequence processing.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3CJG2BLV/536317.html},
  journal = {IEEE Transactions on Neural Networks},
  number = {5}
}

@article{bengio_learning_1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  year = {1994},
  volume = {5},
  pages = {157--166},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio et al_1994_Learning long-term dependencies with gradient descent is difficult.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6Z4XPBKJ/279181.html},
  journal = {IEEE Transactions on Neural Networks},
  number = {2}
}

@article{bengio_markovian_1999,
  title = {Markovian Models for Sequential Data},
  author = {Bengio, Yoshua},
  year = {1999},
  volume = {2},
  pages = {129--162},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio_1999_Markovian models for sequential data.pdf},
  journal = {Neural computing surveys},
  number = {1049}
}

@article{bengio_representation_2013,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Y. and Courville, A and Vincent, P.},
  year = {2013},
  month = aug,
  volume = {35},
  pages = {1798--1828},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2013/Bengio et al/Bengio et al_2013_Representation Learning.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bengio et al_2013_Representation Learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7V2GX4KA/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8}
}

@inproceedings{benites_twist_2018,
  title = {Twist {{Bytes}} - {{German Dialect Identification}} with {{Data Mining Optimization}}},
  booktitle = {Proceedings of the {{Fifth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}} ({{VarDial}} 2018)},
  author = {Benites, Fernando and Grubenmann, Ralf and {von D{\"a}niken}, Pius and {von Gr{\"u}nigen}, Dirk and Deriu, Jan and Cieliebak, Mark},
  year = {2018},
  month = aug,
  pages = {218--227},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Fe, New Mexico, USA}},
  abstract = {We describe our approaches used in the German Dialect Identification (GDI) task at the VarDial Evaluation Campaign 2018. The goal was to identify to which out of four dialects spoken in German speaking part of Switzerland a sentence belonged to. We adopted two different meta classifier approaches and used some data mining insights to improve the preprocessing and the meta classifier parameters. Especially, we focused on using different feature extraction methods and how to combine them, since they influenced very differently the performance of the system. Our system achieved second place out of 8 teams, with a macro averaged F-1 of 64.6\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Benites et al_2018_Twist Bytes - German Dialect Identification with Data Mining Optimization.pdf}
}

@inproceedings{benites_twistbytes_2019,
  title = {{{TwistBytes}} - {{Identification}} of {{Cuneiform Languages}} and {{German Dialects}} at {{VarDial}} 2019},
  booktitle = {Proceedings of the {{Sixth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Benites, Fernando and {von D{\"a}niken}, Pius and Cieliebak, Mark},
  year = {2019},
  month = jun,
  pages = {194--201},
  publisher = {{Association for Computational Linguistics}},
  address = {{Ann Arbor, Michigan}},
  doi = {10.18653/v1/W19-1421},
  abstract = {We describe our approaches for the German Dialect Identification (GDI) and the Cuneiform Language Identification (CLI) tasks at the VarDial Evaluation Campaign 2019. The goal was to identify dialects of Swiss German in GDI and Sumerian and Akkadian in CLI. In GDI, the system should distinguish four dialects from the German-speaking part of Switzerland. Our system for GDI achieved third place out of 6 teams, with a macro averaged F-1 of 74.6\%. In CLI, the system should distinguish seven languages written in cuneiform script. Our system achieved third place out of 8 teams, with a macro averaged F-1 of 74.7\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Benites et al_2019_TwistBytes - Identification of Cuneiform Languages and German Dialects at.pdf}
}

@article{berger_maximum_1996,
  title = {A {{Maximum Entropy Approach}} to {{Natural Language Processing}}},
  author = {Berger, A. L and Pietra, V.J.D and Pietra, S.A.D},
  year = {1996},
  volume = {22},
  pages = {39--71},
  journal = {Computational Linguistics},
  number = {1}
}

@inproceedings{beringer_human_2004,
  title = {Human Language Acquisition Methods in a Machine Learning Task},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Beringer, Nicole},
  year = {2004},
  month = oct,
  pages = {2233--2236},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Beringer_2004_Human language acquisition methods in a machine learning task.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Beringer_2004_Human language acquisition methods in a machine learning task2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/67SF6VH9/i04_2233.html}
}

@incollection{berkhin_survey_2006,
  title = {A Survey of Clustering Data Mining Techniques},
  booktitle = {Grouping Multidimensional Data},
  author = {Berkhin, Pavel},
  year = {2006},
  pages = {25--71},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Berkhin_2006_A survey of clustering data mining techniques.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9SBABDW9/3-540-28349-8_2.html}
}

@inproceedings{berkling_analysis_1994,
  title = {Analysis of {{Phoneme}}-{{Based Features}} for {{Language Identification}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Berkling, K. and Arai, T. and Barnard, E.},
  year = {1994},
  volume = {1},
  pages = {289--292}
}

@article{berry_extended_2011,
  title = {An {{Extended Multiresolution Approach}} to {{Mouth Specific AAM Fitting}} for {{Speech Recognition}}},
  author = {Berry, Craig and Kokaram, Anil and Harte, Naomi},
  year = {2011},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Berry et al_2011_An Extended Multiresolution Approach to Mouth Specific AAM Fitting for Speech.pdf}
}

@inproceedings{beser_recognition_2018,
  title = {Recognition of Sign Language Using Capsule Networks},
  booktitle = {Proc. {{Signal Processing}} and {{Communications Applications Conf}}. ({{SIU}})},
  author = {Be{\c s}er, F. and Kizrak, M. A. and Bolat, B. and Yildirim, T.},
  year = {2018},
  month = may,
  pages = {1--4},
  doi = {10.1109/SIU.2018.8404385},
  abstract = {Hearing and speech impaired persons continue to communicate with the help of lip reading or hand and face movements also known as a sign language. Ensuring the disabled persons participation in life and increasing their quality of life are achievable through healthy and effective communication with other people. In this work; digits of the sign language were recognized with \%94.2 validation accuracy by Capsule Networks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Beşer et al_2018_Recognition of sign language using capsule networks.pdf}
}

@inproceedings{bettinger_modelling_2002,
  title = {Modelling {{Facial Behaviours}}.},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Bettinger, Franck and Cootes, Timothy F. and Taylor, Christopher J.},
  year = {2002},
  pages = {1--10},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bettinger et al_2002_Modelling Facial Behaviours.pdf}
}

@book{beulen_phonetische_1999,
  title = {Phonetische {{Entscheidungsb\"aume}} F\"ur Die Automatische {{Spracherkennung}} Mit Gro\ss em {{Vokabular}}},
  author = {Beulen, Klaus},
  year = {1999},
  publisher = {{Mainz}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Beulen_1999_Phonetische Entscheidungsbäume für die automatische Spracherkennung mit großem.pdf}
}

@phdthesis{beyerlein00:dmi,
  title = {Diskriminative {{Modellkombination}} in {{Spracherkennungssystemen}} Mit Gro\ss em {{Wortschatz}}},
  author = {Beyerlein, Peter},
  year = {2000},
  month = oct,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Beyerlein_2000_Diskriminative Modellkombination in Spracherkennungssystemen mit großem.pdf},
  school = {RWTH Aachen University}
}

@phdthesis{bianchini_signwriting_2012,
  title = {{{SignWriting}} et Son Application \`a La {{Langue}} Des {{Signes Italienne}}},
  author = {Bianchini, Claudia},
  year = {2012},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2012/Bianchini/Bianchini_2012_SignWriting et son application à la Langue des Signes Italienne.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bianchini_2012_SignWriting et son application à la Langue des Signes Italienne.pdf},
  school = {UNIVERSIT\'E DE PARIS VIII}
}

@inproceedings{bigham_how_2017,
  title = {On {{How Deaf People Might Use Speech}} to {{Control Devices}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Bigham, Jeffrey P. and Kushalnagar, Raja and Huang, Ting-Hao Kenneth and Flores, Juan Pablo and Savage, Saiph},
  year = {2017},
  pages = {383--384},
  address = {{New York, NY, USA}},
  doi = {10.1145/3132525.3134821},
  abstract = {Smart devices connected to the Internet are proliferating.To reduce costs of devices that havetraditionally been inexpensive(toasters, microwaves, printers, etc), manyof these devices have chosen to use a speech interface rather than a visual one. This transition has been hastened by the increasing capabilities of speech interfaces,exemplifiedbyproducts likeAmazon Echo and Apple'sSiri.A consequence of these products moving to voice control is that people who are deaf and hard of hearing (DHH) may be unable to use them. In this paper, we briefly introduce two technical approaches we are pursuingfor enabling DHH people to provide input to these devices: (i) human computationworkflows for understanding "deaf speech," and (ii) mobile interfaces that can be instructed to speak on the user's behalf.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bigham et al_2017_On How Deaf People Might Use Speech to Control Devices.pdf},
  isbn = {978-1-4503-4926-0},
  series = {{{ASSETS}} '17}
}

@inproceedings{bilge_zeroshot_2019,
  title = {Zero-{{Shot Sign Language Recognition}}: {{Can Textual Data Uncover Sign Languages}}?},
  shorttitle = {Zero-{{Shot Sign Language Recognition}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Bilge, Yunus Can and {Ikizler-Cinbis}, Nazli and Cinbis, Ramazan Gokberk},
  year = {2019},
  month = jul,
  address = {{Cardiff, UK}},
  abstract = {We introduce the problem of zero-shot sign language recognition (ZSSLR), where the goal is to leverage models learned over the seen sign class examples to recognize the instances of unseen signs. To this end, we propose to utilize the readily available descriptions in sign language dictionaries as an intermediate-level semantic representation for knowledge transfer. We introduce a new benchmark dataset called ASL-Text that consists of 250 sign language classes and their accompanying textual descriptions. Compared to the ZSL datasets in other domains (such as object recognition), our dataset consists of limited number of training examples for a large number of classes, which imposes a significant challenge. We propose a framework that operates over the body and hand regions by means of 3D-CNNs, and models longer temporal relationships via bidirectional LSTMs. By leveraging the descriptive text embeddings along with these spatio-temporal representations within a zero-shot learning framework, we show that textual data can indeed be useful in uncovering sign languages. We anticipate that the introduced approach and the accompanying dataset will provide a basis for further exploration of this new zero-shot learning problem.},
  archivePrefix = {arXiv},
  eprint = {1907.10292},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bilge et al_2019_Zero-Shot Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VR3NEMGI/1907.html},
  keywords = {capturedBy=vision,Computer Science - Computer Vision and Pattern Recognition,isolated,isolated;vocab=50;signer=1;sl=ASL;capturedBy=vision,modal=fullframe,modal=shape,signer=1,sl=ASL,slreco,slreco;modal=fullframe,slreco;modal=fullframe;modal=shape,slreco;modal=shape,vocab<100,vocab=50}
}

@inproceedings{bing_adaptation_2008,
  title = {The {{Adaptation Schemes In}} \{\vphantom\}{{PR}}-{{SVM}}\vphantom\{\} {{Based Language Recognition}}},
  booktitle = {Proc. {{Int}}. {{Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  author = {Bing, Xu and Yan, Song and Dai, Lirong},
  year = {2008},
  pages = {1--4},
  address = {{Kunming, China}},
  keywords = {machines;,machines;universal,model;natural,model;phonetic-based,n-gram,PR-SVM;adaptation,processing;speech,recognition;low-order,recognition;support,schemes;language,systems;support,vector}
}

@article{bisani_jointsequence_2008,
  title = {Joint-Sequence Models for Grapheme-to-Phoneme Conversion},
  author = {Bisani, Maximilian and Ney, Hermann},
  year = {2008},
  month = may,
  volume = {50},
  pages = {434--451},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2008.01.002},
  abstract = {Grapheme-to-phoneme conversion is the task of finding the pronunciation of a word given its written form. It has important applications in text-to-speech and speech recognition. Joint-sequence models are a simple and theoretically stringent probabilistic framework that is applicable to this problem. This article provides a self-contained and detailed description of this method. We present a novel estimation algorithm and demonstrate high accuracy on a variety of databases. Moreover, we study the impact of the maximum approximation in training and transcription, the interaction of model size parameters, n-best list generation, confidence measures, and phoneme-to-grapheme conversion. Our software implementation of the method proposed in this work is available under an Open Source license.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bisani_Ney_2008_Joint-sequence models for grapheme-to-phoneme conversion.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZTQGE226/S0167639308000046.html},
  journal = {Speech Communication},
  number = {5}
}

@inproceedings{bleiken_using_2016,
  title = {Using a {{Language Technology Infrastructure}} for {{German}} in Order to {{Anonymize German Sign Language Corpus Data}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Bleiken, Julian and Hanke, Thomas and Salden, Uta and Wagner, Sven},
  year = {2016},
  month = may,
  pages = {3303--3306},
  address = {{Portoro\v{z}, Slovenia}}
}

@incollection{bluche_comparison_,
  title = {A {{Comparison}} of {{Sequence}}-{{Trained Deep Neural Networks}} and {{Recurrent Neural Networks Optical Modeling}} for {{Handwriting Recognition}}},
  booktitle = {{{SpringerLink}}},
  author = {Bluche, Th{\'e}odore and Ney, Hermann and Kermorvant, Christopher},
  pages = {199--210},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-11397-5_15},
  abstract = {Long Short-Term Memory Recurrent Neural Networks are the current state-of-the-art in handwriting recognition. In speech recognition, Deep Multi-Layer Perceptrons (DeepMLPs) have become the standard acoustic model for Hidden Markov Models (HMMs). Although handwriting and speech recognition systems tend to include similar components and techniques, DeepMLPs are not used as optical model in unconstrained large vocabulary handwriting recognition. In this paper, we compare Bidirectional LSTM-RNNs with DeepMLPs for this task. We carried out experiments on two public databases of multi-line handwritten documents: Rimes and IAM. We show that the proposed hybrid systems yield performance comparable to the state-of-the-art, regardless of the type of features (hand-crafted or pixel values) and the neural network optical model (DeepMLP or RNN).},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bluche et al_A Comparison of Sequence-Trained Deep Neural Networks and Recurrent Neural.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MJDKQT6A/10.html},
  language = {en}
}

@inproceedings{bluche_framewise_2015,
  title = {Framewise and {{CTC}} Training of {{Neural Networks}} for Handwriting Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Bluche, T. and Ney, H. and Louradour, J. and Kermorvant, C.},
  year = {2015},
  month = aug,
  pages = {81--85},
  doi = {10.1109/ICDAR.2015.7333730},
  abstract = {In recent years, Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) trained with the Connectionist Temporal Classification (CTC) objective won many international handwriting recognition evaluations. The CTC algorithm is based on a forward-backward procedure, avoiding the need of a segmentation of the input before training. The network outputs are characters labels, and a special non-character label. On the other hand, in the hybrid Neural Network / Hidden Markov Models (NN/HMM) framework, networks are trained with framewise criteria to predict state labels. In this paper, we show that CTC training is close to forward-backward training of NN/HMMs, and can be extended to more standard HMM topologies. We apply this method to Multi-Layer Perceptrons (MLPs), and investigate the properties of CTC, namely the modeling of character by single labels and the role of the special label.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bluche et al_2015_Framewise and CTC training of Neural Networks for handwriting recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7693H84U/7333730.html}
}

@article{bodapati_neural_2019,
  title = {Neural {{Word Decomposition Models}} for {{Abusive Language Detection}}},
  author = {Bodapati, Sravan Babu and Gella, Spandana and Bhattacharjee, Kasturi and {Al-Onaizan}, Yaser},
  year = {2019},
  month = oct,
  abstract = {User generated text on social media often suffers from a lot of undesired characteristics including hatespeech, abusive language, insults etc. that are targeted to attack or abuse a specific group of people. Often such text is written differently compared to traditional text such as news involving either explicit mention of abusive words, obfuscated words and typological errors or implicit abuse i.e., indicating or targeting negative stereotypes. Thus, processing this text poses several robustness challenges when we apply natural language processing techniques developed for traditional text. For example, using word or token based models to process such text can treat two spelling variants of a word as two different words. Following recent work, we analyze how character, subword and byte pair encoding (BPE) models can be aid some of the challenges posed by user generated text. In our work, we analyze the effectiveness of each of the above techniques, compare and contrast various word decomposition techniques when used in combination with others. We experiment with finetuning large pretrained language models, and demonstrate their robustness to domain shift by studying Wikipedia attack, toxicity and Twitter hatespeech datasets},
  archivePrefix = {arXiv},
  eprint = {1910.01043},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bodapati et al_2019_Neural Word Decomposition Models for Abusive Language Detection.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FABSNGZA/1910.html},
  journal = {arXiv:1910.01043 [cs]},
  primaryClass = {cs}
}

@inproceedings{bojar_findings_2016,
  title = {Findings of the 2016 {{Conference}} on {{Machine Translation}}.},
  booktitle = {Proc. of {{First Conf}}. {{On Machine Translation}} ({{WMT}})},
  author = {Bojar, Ondrej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Logacheva, Varvara and Monz, Christof},
  year = {2016},
  pages = {131--198},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bojar et al_2016_Findings of the 2016 Conference on Machine Translation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TXDDA2ML/307240.html}
}

@inproceedings{borg_phonologicallymeaningful_2020,
  title = {Phonologically-Meaningful {{Subunits}} for {{Deep Learning}}-Based {{Sign Language Recognition}}},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Borg, Mark and Camilleri, Kenneth P},
  year = {2020},
  month = aug,
  pages = {18},
  address = {{online}},
  abstract = {The large majority of sign language recognition systems based on deep learning adopt a word model approach. Here we present a system that works with subunits, rather than word models. We propose a pipelined approach to deep learning that uses a factorisation algorithm to derive hand motion features, embedded within a low-rank trajectory space. Recurrent neural networks are then trained on these embedded features for subunit recognition, followed by a second-stage neural network for sign recognition. Our evaluation shows that our proposed solution compares well in accuracy against the state of the art, providing added benefits of better interpretability and phonologically-meaningful subunits that can operate across different signers and sign languages.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Borg_Camilleri_2020_Phonologically-meaningful Subunits for Deep Learning-based Sign Language.pdf},
  keywords = {continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=-/28.1,paperinfo;authorgroup=Malta;shorttitle=Phonologically-meaningful Subunits,slreco;modal=bodyjoints;modal=2hshape;modal=2hmovement;nn=blstm;cnnresiduals=0;re-align=1;loss=CE;loss=CTC},
  language = {en}
}

@inproceedings{borg_sign_2019,
  title = {Sign {{Language Detection}} ``in the {{Wild}}'' with {{Recurrent Neural Networks}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Borg, M. and Camilleri, K. P.},
  year = {2019},
  month = may,
  pages = {1637--1641},
  doi = {10.1109/ICASSP.2019.8683257},
  abstract = {We propose a multi-layer RNN for sign language detection. The system uses features extracted automatically from a 2-stream convolutional neural network (CNN) that takes video image data and motion data as input. We also created a dataset of videos containing signing "in the wild" to be used for training and evaluation purposes. We compare our system against the state-of-the-art, and attain an improvement of around 18\%, indicating that our network is able to leverage dynamic information of hand motion during detection.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Borg_Camilleri_2019_Sign Language Detection “in the Wild” with Recurrent Neural Networks.pdf},
  keywords = {continuous}
}

@inproceedings{bourel_robust_2002,
  title = {Robust Facial Expression Recognition Using a State-Based Model of Spatially-Localised Facial Dynamics},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Bourel, F. and Chibelushi, C.C. and Low, A.A.},
  year = {2002},
  month = may,
  pages = {106--111},
  doi = {10.1109/AFGR.2002.1004141},
  abstract = {The paper proposes a new approach for the robust recognition of facial expressions from video sequences. The goal of the work presented, is to develop robust recognition techniques that will overcome some limitations of current techniques, such as their sensitivity to partial occlusion of the face, and noisy data. The paper investigates a representation of facial expressions which is based on a spatially-localised geometric facial model coupled to a state-based model of facial motion. The experiments show that the proposed facial expression recognition framework yields relatively little degradation in recognition rate, when faces are partially occluded, or under a variety of levels of noise introduced at the feature tracker level},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bourel et al_2002_Robust facial expression recognition using a state-based model of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KF4IFRT9/abs_all.html}
}

@book{bourlard_connectionist_1993,
  title = {Connectionist {{Speech Recognition}}: A {{Hybrid Approach}}},
  shorttitle = {Connectionist Speech Recognition},
  author = {Bourlard, Herve A. and Morgan, Nelson},
  year = {1993},
  publisher = {{Kluwer Academic Publishers}},
  address = {{Norwell, MA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bourlard_Morgan_1993_Connectionist Speech Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WFIWRU9U/books.html}
}

@inproceedings{bourlard_links_1989,
  title = {Links {{Between Markov Models}} and {{Multilayer Perceptrons}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bourlard, Herv{\'e} and Wellekens, C. J.},
  year = {1989},
  month = nov,
  pages = {502--507},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bourlard_Wellekens_1989_Links Between Markov Models and Multilayer Perceptrons.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EMGS6E5C/163-links-between-markov-models-and-multilayer-perceptrons.html}
}

@techreport{bourlard_multistream_1996,
  title = {Multi-Stream Speech Recognition},
  author = {Bourlard, Herv{\'e} and Dupont, St{\'e}phane and Ris, Christophe},
  year = {1996},
  month = dec,
  pages = {1--14},
  address = {{Martigny, Switzerland}},
  institution = {{IDIAP}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bourlard et al_1996_Multi-stream speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/47RC6HFB/82370.html},
  language = {en},
  number = {IDIAP-RR 96-07},
  type = {Techreport}
}

@inproceedings{bourlard_new_1996,
  title = {A New {{ASR}} Approach Based on Independent Processing and Recombination of Partial Frequency Bands},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{ICSLP}})},
  author = {Bourlard, Herv{\'e} and Dupont, St{\'e}phane},
  year = {1996},
  month = oct,
  volume = {1},
  pages = {426--429},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bourlard_Dupont_1996_A new ASR approach based on independent processing and recombination of partial.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XBVADBKT/abs_all.html}
}

@inproceedings{bourlard_subbandbased_1997,
  title = {Subband-Based Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bourlard, Herv{\'e} and Dupont, St{\'e}phane},
  year = {1997},
  volume = {2},
  pages = {1251--1254},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bourlard_Dupont_1997_Subband-based speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/48EETJMR/abs_all.html}
}

@incollection{bousmalis_domain_2016,
  title = {Domain {{Separation Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {343--351},
  publisher = {{Curran Associates, Inc.}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bousmalis et al_2016_Domain Separation Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LM87LKLQ/6254-domain-separation-networks.html}
}

@inproceedings{bouzid_3d_2012,
  title = {Towards a 3d Signing Avatar from Signwriting Notation},
  booktitle = {Proceedings of the 13th International Conference on {{Computers Helping People}} with {{Special Needs}} - {{Volume Part II}}},
  author = {Bouzid, Yosra and Jbali, Maher and El Ghoul, Oussama and Jemni, Mohamed},
  year = {2012},
  month = jul,
  pages = {229--236},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-31534-3_36},
  abstract = {Many transcription systems, like SignWriting, have been suggested in the last decades to describe sign language in a written form. But, these systems have some limitations, they are not easily understood and adopted by the members of deaf community who usually use video and avatar-based systems to access information. In this context, we present in this paper a new tool for automatically generating 3D animation sequences from SW notation. The SW notation is provided as input in an XML based format called SWML (SignWriting Markup Language). This tool aims to improve the reading and writing capabilities of a deaf person who has no special training to read and write in SL.},
  isbn = {978-3-642-31533-6},
  series = {{{ICCHP}}'12}
}

@inproceedings{bowden_linguistic_2004,
  title = {A {{Linguistic Feature Vector}} for the {{Visual Interpretation}} of {{Sign Language}}},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Bowden, Richard and Windridge, David and Kadir, Timor and Zisserman, Andrew and Brady, Michael},
  year = {2004},
  month = may,
  pages = {390--401},
  address = {{Czech Republic, Prague}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2004/Bowden et al/Bowden et al_2004_A Linguistic Feature Vector for the Visual Interpretation of Sign Language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bowden et al_2004_A Linguistic Feature Vector for the Visual Interpretation of Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R8HDKKIC/10.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XTW74W97/978-3-540-24670-1_30.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=43;signer=1;sl=BSL;capturedBy=vision,modal=location,modal=movement,modal=orientation,modal=shape,signer=1,sl=BSL,slreco,slreco;modal=shape;modal=orientation;modal=movement;modal=location,vocab<50,vocab=43}
}

@article{bowman_generating_2016,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
  year = {2016},
  month = may,
  abstract = {The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space.},
  archivePrefix = {arXiv},
  eprint = {1511.06349},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F9JPLZ5N/Bowman et al. - 2016 - Generating Sentences from a Continuous Space.pdf},
  journal = {arXiv:1511.06349 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{boyes-braem-2001,
  title = {A Multimedia Bilingual Database for the Lexicon of {{Swiss German Sign Language}}},
  author = {Braem, Penny Boyes},
  year = {2001},
  volume = {4},
  pages = {133--143},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Braem_2001_A multimedia bilingual database for the lexicon of Swiss German Sign Language.pdf},
  journal = {Sign Language \& Linguistics},
  number = {1/2}
}

@article{boyesbraem_functions_2001,
  title = {Functions of the {{Mouthing Component}} in the {{Signing}} of {{Deaf Early}} and {{Late Learners}} of {{Swiss German Sign Language}}},
  author = {Boyes Braem, Penny},
  year = {2001},
  pages = {1--47},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Boyes Braem_2001_Functions of the Mouthing Component in the Signing of Deaf Early and Late.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZREBVPD8/books.html},
  journal = {Foreign Vocabulary in Sign Languages: A Cross-Linguistic Investigation of Word Formation, D. Brentari, Ed. Mahwah, NJ: Erlbaum}
}

@article{boyesbraem_functions_2001a,
  title = {Functions of the Mouthings in the Signing of Deaf Early and Late Learners of {{Swiss German Sign Language}} ({{DSGS}})},
  author = {Boyes Braem, Penny},
  year = {2001},
  pages = {99--132},
  journal = {The hands are the head of the mouth: The mouth as Articulator in Sign Languages}
}

@book{bradski_learning_2008,
  title = {Learning {{OpenCV}}: {{Computer}} Vision with the {{OpenCV}} Library},
  shorttitle = {Learning {{OpenCV}}},
  author = {Bradski, Gary and Kaehler, Adrian},
  year = {2008},
  publisher = {{O'Reilly Media, Inc.}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bradski_Kaehler_2008_Learning OpenCV.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SFZ238BH/books.html}
}

@incollection{braffort_argo_1997,
  title = {{{ARGo}}: {{An}} Architecture for Sign Language Recognition and Interpretation},
  shorttitle = {{{ARGo}}},
  booktitle = {Progress in {{Gestural Interaction}}},
  author = {Braffort, Annelies},
  year = {1997},
  pages = {17--30},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Braffort_1997_ARGo.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ENZCYIME/978-1-4471-0943-3_3.html}
}

@inproceedings{braffort_sign_2010,
  title = {Sign {{Language Corpora}} for {{Analysis}}, {{Processing}} and {{Evaluation}}.},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Braffort, Annelies and Bolot, Laurence and {Ch{\'e}telat-Pel{\'e}}, Emilie and Choisier, Annick and Delorme, Maxime and Filhol, Michael and Segouat, J{\'e}r{\'e}mie and Verrecchia, Cyril and Badin, Flora and Devos, Nad{\`e}ge},
  year = {2010},
  month = may,
  pages = {453--456},
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Braffort et al_2010_Sign Language Corpora for Analysis, Processing and Evaluation.pdf}
}

@inproceedings{bragg_sign_2019,
  title = {Sign {{Language Recognition}}, {{Generation}}, and {{Translation}}: {{An Interdisciplinary Perspective}}},
  shorttitle = {Sign {{Language Recognition}}, {{Generation}}, and {{Translation}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Bragg, Danielle and Koller, Oscar and Bellard, Mary and Berke, Larwan and Boudreault, Patrick and Braffort, Annelies and Caselli, Naomi and Huenerfauth, Matt and Kacorri, Hernisa and Verhoef, Tessa and Vogler, Christian and Ringel Morris, Meredith},
  year = {2019},
  pages = {16--31},
  address = {{New York, NY, USA}},
  doi = {10.1145/3308561.3353774},
  abstract = {Developing successful sign language recognition, generation, and translation systems requires expertise in a wide range of fields, including computer vision, computer graphics, natural language processing, human-computer interaction, linguistics, and Deaf culture. Despite the need for deep interdisciplinary knowledge, existing research occurs in separate disciplinary silos, and tackles separate portions of the sign language processing pipeline. This leads to three key questions: 1) What does an interdisciplinary view of the current landscape reveal? 2) What are the biggest challenges facing the field? and 3) What are the calls to action for people working in the field? To help answer these questions, we brought together a diverse group of experts for a two-day workshop. This paper presents the results of that interdisciplinary workshop, providing key background that is often overlooked by computer scientists, a review of the state-of-the-art, a set of pressing challenges, and a call to action for the research community.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bragg et al_2019_Sign Language Recognition, Generation, and Translation.pdf},
  isbn = {978-1-4503-6676-2},
  series = {{{ASSETS}} '19}
}

@techreport{brand_coupled_1997,
  title = {Coupled Hidden {{Markov}} Models for Modeling Interacting Processes},
  author = {Brand, Matthew},
  year = {1997},
  month = jun,
  pages = {1--28},
  address = {{Cambrigde, MA, USA}},
  institution = {{MIT Media Lab Perceptual Computing/Learning and Common Sense}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brand_1997_Coupled hidden Markov models for modeling interacting processes.pdf},
  language = {en},
  number = {405},
  type = {Techreport}
}

@inproceedings{brand_coupled_1997a,
  title = {Coupled Hidden {{Markov}} Models for Complex Action Recognition},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Brand, M. and Oliver, N. and Pentland, A.},
  year = {1997},
  month = jun,
  pages = {994--999},
  doi = {10.1109/CVPR.1997.609450},
  abstract = {We present algorithms for coupling and training hidden Markov models (HMMs) to model interacting processes, and demonstrate their superiority to conventional HMMs in a vision task classifying two-handed actions. HMMs are perhaps the most successful framework in perceptual computing for modeling and classifying dynamic behaviors, popular because they offer dynamic time warping, a training algorithm and a clear Bayesian semantics. However the Markovian framework makes strong restrictive assumptions about the system generating the signal-that it is a single process having a small number of states and an extremely limited state memory. The single-process model is often inappropriate for vision (and speech) applications, resulting in low ceilings on model performance. Coupled HMMs provide an efficient way to resolve many of these problems, and offer superior training speeds, model likelihoods, and robustness to initial conditions},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brand et al_1997_Coupled hidden Markov models for complex action recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C6IMFUSZ/609450.html}
}

@inproceedings{brand_inverse_1997,
  title = {The "{{Inverse}} Hollywood Problem": From Video to Scripts and Storyboards via Causal Analysis},
  shorttitle = {The" {{Inverse}} Hollywood Problem"},
  booktitle = {Proc. of the  {{AAAI Conf}}. on {{Artificial Intelligence}}},
  author = {Brand, Matthew},
  year = {1997},
  pages = {132--137},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brand_1997_The Inverse hollywood problem.pdf}
}

@inproceedings{brashear_american_2006,
  title = {American Sign Language Recognition in Game Development for Deaf Children},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Brashear, Helene and Henderson, Valerie and Park, Kwang-Hyun and Hamilton, Harley and Lee, Seungyon and Starner, Thad},
  year = {2006},
  month = oct,
  pages = {79--86},
  doi = {10.1145/1168987.1169002},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brashear et al_2006_American sign language recognition in game development for deaf children.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZRZYVNRM/citation.html},
  isbn = {978-1-59593-290-7},
  keywords = {capturedBy=coloredGlove,capturedBy=electronicGlove,continuous,continuous;vocab=22;signer=5;sl=ASL;capturedBy=coloredGlove;capturedBy=electronicGlove,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=5,sl=ASL,slreco,slreco;modal=2hshape;modal=2hlocation;modal=2hmovement,vocab<50,vocab=22}
}

@inproceedings{brashear_using_2003,
  title = {Using Multiple Sensors for Mobile Sign Language Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Symposium}} on {{Wearable Computers}}},
  author = {Brashear, Helene and Starner, Thad and Lukowicz, Paul and Junker, Holger},
  year = {2003},
  month = oct,
  pages = {45--52},
  address = {{White Plains, NY, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brashear et al_2003_Using multiple sensors for mobile sign language recognition.pdf},
  keywords = {capturedBy=mocap,capturedBy=vision,continuous,continuous;vocab=5;signer=1;sl=ASL;capturedBy=vision;capturedBy=mocap,modal=2hlocation,modal=2hshape,signer=1,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<50,vocab=5}
}

@incollection{bregler_learning_1997,
  title = {Learning {{Visual Models}} for {{Lipreading}}},
  booktitle = {Motion-{{Based Recognition}}},
  author = {Bregler, Christoph and Omohundro, Stephen M.},
  editor = {Shah, Mubarak and Jain, Ramesh},
  year = {1997},
  month = jan,
  pages = {301--320},
  publisher = {{Springer Netherlands}},
  abstract = {This chapter describes learning techniques that are the basis of a ``visual speech recognition'' or ``lipreading'' system1. Model-based vision systems currently have the best performance for many visual recognition tasks. For geometrically simple domains, models can sometimes be constructed by hand using CAD-like tools. Such models are difficult and expensive to construct, however, and are inadequate for more complex domains. To do model-based lipreading, we would like a parameterized model of the complex ``space of lip configurations''. Rather than building such a model by hand, our approach is to have the system itself build it using machine learning. The system is given a collection of training images which it uses to automatically construct the models that are later used in recognition.},
  copyright = {\textcopyright 1997 Springer Science+Business Media B.V.},
  isbn = {978-90-481-4870-7 978-94-015-8935-2},
  language = {en},
  number = {9},
  series = {Computational {{Imaging}} and {{Vision}}}
}

@inproceedings{bregler_nonlinear_1995,
  title = {Nonlinear Manifold Learning for Visual Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Bregler, Christoph and Omohundro, Stephen M.},
  year = {1995},
  pages = {494--499},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bregler_Omohundro_1995_Nonlinear manifold learning for visual speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RINB3ZF2/abs_all.html}
}

@inproceedings{bregler_recovering_2000,
  title = {Recovering Non-Rigid {{3D}} Shape from Image Streams},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bregler, Christoph and Hertzmann, Aaron and Biermann, Henning},
  year = {2000},
  volume = {2},
  pages = {690--696},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bregler et al_2000_Recovering non-rigid 3D shape from image streams.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M7247DZS/abs_all.html}
}

@article{breiman_random_2001,
  title = {Random {{Forests}}},
  author = {BREIMAN, LEO},
  year = {2001},
  volume = {45},
  pages = {5--32},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/BREIMAN_2001_Random Forests.pdf},
  journal = {Machine Learning}
}

@incollection{brentari_handshape_2011,
  title = {Handshape in {{Sign Language Phonology}}},
  booktitle = {The {{Blackwell Companion}} to {{Phonology}}},
  author = {Brentari, Diane},
  year = {2011},
  volume = {1},
  pages = {195--222},
  publisher = {{Blackwell}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_2011_Handshape in sign language phonology.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LUG9CUGP/9781444335262.html}
}

@article{brentari_modality_2002,
  title = {Modality Differences in Sign Language Phonology and Morphophonemics},
  author = {Brentari, Diane},
  year = {2002},
  pages = {35--64},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_2002_Modality differences in sign language phonology and morphophonemics.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8JZD6I4Q/books.html},
  journal = {Modality and structure in signed and spoken languages}
}

@book{brentari_prosodic_1998,
  title = {A Prosodic Model of Sign Language Phonology},
  author = {Brentari, D.},
  year = {1998},
  publisher = {{The MIT Press}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_1998_A prosodic model of sign language phonology.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P5SJUDSN/books.html}
}

@article{brentari_representing_2018,
  title = {Representing {{Handshapes}} in {{Sign Languages Using Morphological Templates}}},
  author = {Brentari, Diane},
  year = {2018},
  volume = {13},
  pages = {145},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_2018_Representing Handshapes in Sign Languages Using Morphological Templates.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/49UPBXD6/books.html},
  journal = {Geb\"ardensprachen: Struktur, Erwerb, Verwendung}
}

@book{brentari_sign_2010,
  title = {Sign {{Languages}}},
  editor = {Brentari, Diane},
  year = {2010},
  month = may,
  edition = {First},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK}},
  abstract = {What are the unique characteristics of sign languages that make them so fascinating? What have recent researchers discovered about them, and what do these findings tell us about human language more generally? This thematic and geographic overview examines more than forty sign languages from around the world. It begins by investigating how sign languages have survived and been transmitted for generations, and then goes on to analyse the common characteristics shared by most sign languages: for example, how the use of the visual system affects grammatical structures. The final section describes the phenomena of language variation and change. Drawing on a wide range of examples, the book explores sign languages both old and young, from British, Italian, Asian and American to Israeli, Al-Sayyid Bedouin, African and Nicaraguan. Written in a clear, readable style, it is the essential reference for students and scholars working in sign language studies and deaf studies.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_2010_Sign Languages.pdf},
  isbn = {978-0-521-88370-2},
  language = {en}
}

@incollection{brentari_sign_2011,
  title = {Sign {{Language Phonology}}},
  booktitle = {The {{Handbook}} of {{Phonological Theory}}},
  author = {Brentari, Diane},
  year = {2011},
  pages = {691--721},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781444343069.ch21},
  abstract = {This chapter contains sections titled: Introduction Structure Modality Effects (The Effects of Communication Mode) Iconicity Effects on Phonological Representation Conclusion},
  copyright = {Copyright \textcopyright{} 2011 Blackwell Publishing Ltd.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_2011_Sign Language Phonology.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UAW8E8NU/9781444343069.html},
  isbn = {978-1-4443-4306-9},
  language = {en}
}

@article{brentari_sign_2018,
  title = {Sign {{Language Phonology}}},
  author = {Brentari, Diane and Fenlon, Jordan and Cormier, Kearsy},
  year = {2018},
  month = jul,
  doi = {10.1093/acrefore/9780199384655.013.117},
  abstract = {Sign language phonology is the abstract grammatical component where primitive structural units are combined to create an infinite number of meaningful utterances. Although the notion of phonology is traditionally based on sound systems, phonology also includes the equivalent component of the grammar in sign languages, because it is tied to the grammatical organization, and not to particular content. This definition of phonology helps us see that the term covers all phenomena organized by constituents such as the syllable, the phonological word, and the higher-level prosodic units, as well as the structural primitives such as features, timing units, and autosegmental tiers, and it does not matter if the content is vocal or manual. Therefore, the units of sign language phonology and their phonotactics provide opportunities to observe the interaction between phonology and other components of the grammar in a different communication channel, or modality. This comparison allows us to better understand how the modality of a language influences its phonological system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari et al_2018_Sign Language Phonology.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IZSTPQVS/acrefore-9780199384655-e-117.html},
  journal = {Oxford Research Encyclopedia of Linguistics},
  language = {en}
}

@article{brentari_trilled_1996,
  title = {Trilled Movement: {{Phonetic}} Realization and Formal Representation},
  shorttitle = {Trilled Movement},
  author = {Brentari, Diane},
  year = {1996},
  month = mar,
  volume = {98},
  pages = {43--71},
  issn = {0024-3841},
  doi = {10.1016/0024-3841(95)00032-1},
  abstract = {Evidence from the distribution of `trilled movement' (abbreviated TM) in specific kinds of polymorphemic and monomorphemic forms has been used to argue for a variety of analyses of phonological structure in sign languages. In this article, I begin with a descriptive typology of TM across mono- and polymorphemic forms, and then I propose an analysis of TM, which addresses the following three questions. Is TM a unified phonological property or must it be further sub-divided? To what extent is its distribution predictable in surface forms? What formal mechanisms are needed to capture the predictable aspects of its distribution? I conclude that all TM can be handled by a single distinctive feature in the phonology. Its pattern of distribution can be determined largely on the basis of the sonority values of the properties specified in the underlying structure of the sign in question. For the case of exceptions discussed in this article, where sonority is not capable of predicting TM distribution (namely, in the [protracted/inceptive] aspect forms), I propose a solution using a type of segmental structure. In so doing, I reaffirm the need for segmental structure in ASL representations, and I make specific proposal about the nature of this segmental structure. Further theoretical implications concerning the relationships among features, segments, and syllables are also discussed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_1996_Trilled movement.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KGSS92E4/0024384195000321.html},
  journal = {Lingua},
  number = {1},
  series = {Sign {{Linguistics Phonetics}}, {{Phonology}} and {{Morpho}}-Syntax}
}

@article{brentari_what_2013,
  title = {What Sign Language Creation Teaches Us about Language},
  author = {Brentari, Diane and Coppola, Marie},
  year = {2013},
  volume = {4},
  pages = {201--211},
  issn = {1939-5086},
  doi = {10.1002/wcs.1212},
  abstract = {How do languages emerge? What are the necessary ingredients and circumstances that permit new languages to form? Various researchers within the disciplines of primatology, anthropology, psychology, and linguistics have offered different answers to this question depending on their perspective. Language acquisition, language evolution, primate communication, and the study of spoken varieties of pidgin and creoles address these issues, but in this article we describe a relatively new and important area that contributes to our understanding of language creation and emergence. Three types of communication systems that use the hands and body to communicate will be the focus of this article: gesture, homesign systems, and sign languages. The focus of this article is to explain why mapping the path from gesture to homesign to sign language has become an important research topic for understanding language emergence, not only for the field of sign languages, but also for language in general. WIREs Cogn Sci 2013, 4:201\textendash 211. doi: 10.1002/wcs.1212 This article is categorized under: Psychology {$>$} Language},
  copyright = {Copyright \textcopyright{} 2012 John Wiley \& Sons, Ltd.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brentari_Coppola_2013_What sign language creation teaches us about language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BELJ444T/wcs.html},
  journal = {Wiley Interdisciplinary Reviews: Cognitive Science},
  language = {en},
  number = {2}
}

@inproceedings{bridle_method_1977,
  title = {A Method for Segmenting Acoustic Patterns, with Applications to Automatic Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bridle, J. and Sedgwick, N.},
  year = {1977},
  month = may,
  volume = {2},
  pages = {656--659},
  doi = {10.1109/ICASSP.1977.1170169},
  abstract = {Patterns derived by acoustic analysis of speech may be represented as a sequence of "segments", each described in terms such as length and mean spectrum. We present an automatic method of dividing the pattern from one utterance of a word into a sequence of segments. A "segment evaluation function" measures how well any given portion of the utterance can be represented as a single segment. The segmentation process itself simply finds the sequence of segments which best represents the whole pattern. Results are presented for several words and segment evaluation functions. Applications to automatic recognition of given words in connected speech are discussed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bridle_Sedgwick_1977_A method for segmenting acoustic patterns, with applications to automatic.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4KM7EQ3P/1170169.html}
}

@article{brock_augmenting_a,
  title = {Augmenting {{Sparse Corpora}} for {{Enhanced Sign Language Recognition}} and {{Generation}}},
  author = {Brock, Heike and Rengot, Juliette and Nakadai, Kazuhiro},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brock et al_Augmenting Sparse Corpora for Enhanced Sign Language Recognition and Generation.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brock et al_Augmenting Sparse Corpora for Enhanced Sign Language Recognition and Generation.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/undefined/Brock et al/Brock et al_Augmenting Sparse Corpora for Enhanced Sign Language Recognition and Generation.pdf}
}

@inproceedings{brock_deep_2018,
  title = {Deep {{JSLC}}: {{A Multimodal Corpus Collection}} for {{Data}}-Driven {{Generation}} of {{Japanese Sign Language Expressions}}},
  shorttitle = {Deep {{JSLC}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Brock, Heike and Nakadai, Kazuhiro},
  year = {2018},
  month = may,
  pages = {4247--4252},
  address = {{Myazaki, Japan}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brock_Nakadai_2018_Deep JSLC.pdf},
  keywords = {capturedBy=depth,capturedBy=mocap,capturedBy=vision,continuous,generation;vocab=10;signer=1;sl=JSL;capturedBy=vision;capturedBy=depth;capturedBy=mocap,generationreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape;modal=head;modal=eyebrows;modal=eyeblink,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,modal=eyeblink,modal=eyebrows,modal=head,signer=1,sl=JSL,slreco,vocab<50,vocab=10}
}

@inproceedings{brown_products_2001,
  title = {Products of {{Hidden Markov Models}}.},
  booktitle = {{{AISTATS}}},
  author = {Brown, Andrew D. and Hinton, Geoffrey E.},
  year = {2001},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Brown_Hinton_2001_Products of Hidden Markov Models.pdf}
}

@inproceedings{bruguier_phoebe_2019,
  title = {Phoebe: {{Pronunciation}}-Aware {{Contextualization}} for {{End}}-to-End {{Speech Recognition}}},
  shorttitle = {Phoebe},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bruguier, Antoine and Prabhavalkar, Rohit and Pundak, Golan and Sainath, Tara N.},
  year = {2019},
  month = may,
  pages = {6171--6175},
  issn = {2379-190X, 1520-6149},
  doi = {10.1109/ICASSP.2019.8682441},
  abstract = {End-to-End (E2E) automatic speech recognition (ASR) systems learn word spellings directly from text-audio pairs, in contrast to traditional ASR systems which incorporate a separate pronunciation lexicon. The lexicon allows a traditional system to correctly spell rare words observed only in LM training, if their phonetic pronunciation is known during inference. E2E systems, however, are more likely to misspell rare words.We propose an E2E model which benefits from the best of both worlds: it outputs graphemes, and thus learns to spell words directly, while leveraging pronunciations for words which might be likely in a given context. Our model is based on the recently proposed Contextual Listen, Attend, and Spell (CLAS) model. As in CLAS, our model accepts a set of bias phrases, which are first converted into fixed length embeddings which are provided as additional inputs to the model. Unlike CLAS, which accepts only the textual form of the bias phrases, the proposed model also has access to the corresponding phonetic pronunciations, which improves performance on challenging sets which include words unseen in training. The proposed model provides a 16\% relative word-error-rate reduction over CLAS when both the phonetic and written representation of the context bias phrases are used.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bruguier et al_2019_Phoebe.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6BFTYZ3X/8682441.html},
  keywords = {biasing,Context modeling,Contextual Listen; Attend; and Spell model,Decoding,end-to-end automatic speech recognition,end-to-end speech recognition,LAS,Mathematical model,natural language processing,Phoebe,phonetic pronunciation,Phonetics,Predictive models,pronunciation,pronunciation-aware contextualization,sequence-to-sequence,speech processing,speech recognition,Speech recognition,Training}
}

@article{brummer_focal_2007,
  title = {{{FoCal Multi}}-Class: {{Toolkit}} for {{Evaluation}}, {{Fusion}} and {{Calibration}} of {{Multi}}-Class {{Recognition Scores}}\textemdash{{Tutorial}} and {{User Manual}}\textemdash},
  shorttitle = {{{FoCal Multi}}-Class},
  author = {Br{\"u}mmer, N.},
  year = {2007}
}

@inproceedings{brummer_spescom_2004,
  title = {Spescom \{\vphantom\}{{DataVoice}}\vphantom\{\} \{\vphantom\}{{NIST}}\vphantom\{\} 2004 {{System Description}}},
  booktitle = {Proc. {{NIST Speaker Recognition Evaluation}}},
  author = {Br{\"u}mmer, N.},
  year = {2004},
  pages = {1--8},
  address = {{Toledo, Spain}}
}

@article{bucheli_syntactic_2002,
  title = {The Syntactic Atlas of {{Swiss German}} Dialects: Empirical and Methodological Problems},
  shorttitle = {The Syntactic Atlas of {{Swiss German}} Dialects},
  author = {Bucheli, Claudia and Glaser, Elvira},
  year = {2002},
  volume = {2},
  pages = {41--73},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bucheli_Glaser_2002_The syntactic atlas of Swiss German dialects.pdf},
  journal = {Syntactic microvariation}
}

@inproceedings{budhkar_generative_2019,
  ids = {budhkar\_generative\_2019-1},
  title = {Generative {{Adversarial Networks}} for {{Text Using Word2vec Intermediaries}}},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Representation Learning}} for {{NLP}} ({{RepL4NLP}}-2019)},
  author = {Budhkar, Akshay and Vishnubhotla, Krishnapriya and Hossain, Safwan and Rudzicz, Frank},
  year = {2019},
  pages = {15--26},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4303},
  abstract = {Generative adversarial networks (GANs) have shown considerable success, especially in the realistic generation of images. In this work, we apply similar techniques for the generation of text. We propose a novel approach to handle the discrete nature of text, during training, using word embeddings. Our method is agnostic to vocabulary size and achieves competitive results relative to methods with various discrete gradient estimators.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Budhkar et al_2019_Generative Adversarial Networks for Text Using Word2vec Intermediaries.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4QVBCXWF/Budhkar et al. - 2019 - Generative Adversarial Networks for Text Using Wor.pdf},
  language = {en}
}

@inproceedings{buehler_employing_2010,
  title = {Employing Signed {{TV}} Broadcasts for Automated Learning of {{British Sign Language}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Buehler, Patrick and Everingham, Mark and Zisserman, Andrew},
  year = {2010},
  month = may,
  pages = {22--23},
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Buehler et al_2010_Employing signed TV broadcasts for automated learning of British Sign Language.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=210;signer=3;sl=BSL;capturedBy=vision,modal=2hlocation,modal=2hshape,signer=3,sl=BSL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<500,vocab=210}
}

@inproceedings{buehler_learning_2009,
  title = {Learning Sign Language by Watching {{TV}} (Using Weakly Aligned Subtitles)},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Buehler, Patrick and Zisserman, Andrew and Everingham, Mark},
  year = {2009},
  month = jun,
  pages = {2961--2968},
  address = {{Miami, FL, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2009/Buehler et al/Buehler et al_2009_Learning sign language by watching TV (using weakly aligned subtitles).pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Buehler et al_2009_Learning sign language by watching TV (using weakly aligned subtitles).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5HM3BS5I/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GJFS7VJV/cookiedetectresponse.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=210;signer=3;sl=BSL;capturedBy=vision,modal=2hlocation,modal=2horientation,modal=2hshape,signer=3,sl=BSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=2horientation,vocab<500,vocab=210}
}

@inproceedings{bungeroth_atis_2008,
  title = {The {{ATIS Sign Language Corpus}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Bungeroth, Jan and Stein, Daniel and Dreuw, Philippe and Ney, Hermann and Morrissey, Sara and Way, Andy and {van Zijl}, Lynette},
  year = {2008},
  month = may,
  address = {{Marrakech, Morocco}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bungeroth et al_2008_The ATIS Sign Language Corpus.pdf}
}

@inproceedings{bungeroth_automatic_2005,
  title = {Automatic {{Generation}} of {{German Sign Language Glosses}} from {{German Words}}},
  booktitle = {International {{Workshop}} on {{Gesture}} in {{Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {Bungeroth, Jan and Ney, Hermann},
  year = {2005},
  month = may,
  volume = {3881},
  pages = {49--52},
  address = {{Ile-de-Berder, France}}
}

@inproceedings{bungeroth_german_2006,
  title = {A {{German Sign Language Corpus}} of the {{Domain Weather Report}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Bungeroth, Jan and Stein, Daniel and Dreuw, Philippe and Zahedi, Morteza and Ney, Hermann},
  year = {2006},
  month = may,
  pages = {2000--2003},
  address = {{Genoa, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bungeroth et al_2006_A German Sign Language Corpus of the Domain Weather Report.pdf}
}

@inproceedings{bungeroth_statistical_2004,
  title = {Statistical {{Sign Language Translation}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Bungeroth, Jan and Ney, Hermann},
  year = {2004},
  month = may,
  pages = {105--108},
  address = {{Lisbon, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Bungeroth_Ney_2004_Statistical Sign Language Translation.pdf}
}

@phdthesis{burke_fast_2015,
  title = {Fast Upper Body Pose Estimation for Human-Robot Interaction},
  author = {Burke, Michael Glen},
  year = {2015},
  month = jul,
  address = {{Cambridge, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Burke_2015_Fast upper body pose estimation for human-robot interaction.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QDASB8TF/256305.html},
  language = {en},
  school = {University of Cambridge},
  type = {{{PhD Thesis}}}
}

@article{c.fabianbenitez-quiroz_discriminant_2014,
  title = {Discriminant {{Features}} and {{Temporal Structure}} of {{Nonmanuals}} in {{American Sign Language}}},
  author = {{C. Fabian Benitez-Quiroz} and G{\"o}kg{\"o}z, Kadir and Wilbur, Ronnie B. and Martinez, Aleix M.},
  year = {2014},
  month = feb,
  volume = {9},
  pages = {e86268},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0086268},
  abstract = {To fully define the grammar of American Sign Language (ASL), a linguistic model of its nonmanuals needs to be constructed. While significant progress has been made to understand the features defining ASL manuals, after years of research, much still needs to be done to uncover the discriminant nonmanual components. The major barrier to achieving this goal is the difficulty in correlating facial features and linguistic features, especially since these correlations may be temporally defined. For example, a facial feature (e.g., head moves down) occurring at the end of the movement of another facial feature (e.g., brows moves up), may specify a Hypothetical conditional, but only if this time relationship is maintained. In other instances, the single occurrence of a movement (e.g., brows move up) can be indicative of the same grammatical construction. In the present paper, we introduce a linguistic\textendash computational approach to efficiently carry out this analysis. First, a linguistic model of the face is used to manually annotate a very large set of 2,347 videos of ASL nonmanuals (including tens of thousands of frames). Second, a computational approach is used to determine which features of the linguistic model are more informative of the grammatical rules under study. We used the proposed approach to study five types of sentences \textendash{} Hypothetical conditionals, Yes/no questions, Wh-questions, Wh-questions postposed, and Assertions \textendash{} plus their polarities \textendash{} positive and negative. Our results verify several components of the standard model of ASL nonmanuals and, most importantly, identify several previously unreported features and their temporal relationship. Notably, our results uncovered a complex interaction between head position and mouth shape. These findings define some temporal structures of ASL nonmanuals not previously detected by other approaches.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Benitez-Quiroz et al_2014_Discriminant Features and Temporal Structure of Nonmanuals in American Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LIVDDYFA/article.html},
  journal = {PLOS ONE},
  language = {en},
  number = {2}
}

@inproceedings{cadavid_multimodal_2009,
  title = {Multi-Modal Biometric Modeling and Recognition of the Human Face and Ear},
  booktitle = {Proc. {{IEEE Int}}. {{Workshop Safety}}, {{Security}} \& {{Rescue Robotics}} ({{SSRR}})},
  author = {Cadavid, Steven and Mahoor, Mohammad H. and {Abdel-Mottaleb}, Mohamed},
  year = {2009},
  pages = {1--6},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cadavid et al_2009_Multi-modal biometric modeling and recognition of the human face and ear.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J4QP2XEA/login.html}
}

@inproceedings{cai_fast_2017,
  title = {Fast and {{Accurate Neural Word Segmentation}} for {{Chinese}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Cai, Deng and Zhao, Hai and Zhang, Zhisong and Xin, Yuan and Wu, Yongjian and Huang, Feiyue},
  year = {2017},
  month = jul,
  pages = {608--615},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-2096},
  abstract = {Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. In this paper, we propose a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-to-end, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cai et al_2017_Fast and Accurate Neural Word Segmentation for Chinese.pdf}
}

@inproceedings{cai_neural_2016,
  title = {Neural {{Word Segmentation Learning}} for {{Chinese}}},
  booktitle = {{{ACL}} (1)},
  author = {Cai, Deng and Zhao, Hai},
  year = {2016},
  month = jan,
  abstract = {Most previous approaches to Chinese word segmentation formalize this problem as a character-based sequence labeling task where only contextual information within fixed sized local windows and...},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cai_Zhao_2016_Neural Word Segmentation Learning for Chinese.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CP8AGGUN/forum.html}
}

@inproceedings{camgoz_bosphorussign_2016,
  title = {{{BosphorusSign}}: {{A Turkish Sign Language Recognition Corpus}} in {{Health}} and {{Finance Domains}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Camgoz, Necati Cihan and Kindiroglu, Ahmet Alp and Karabuklu, Serpil and Kelepir, Meltem and Ozsoy, Ayse Sumru and Akarun, Lale},
  year = {2016},
  month = may,
  pages = {1383--1388},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Camgoz et al_2016_BosphorusSign.pdf},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=855;signer=10;sl=TSL;capturedBy=vision;capturedBy=depth,signer=10,sl=TSL,vocab<1000,vocab=855}
}

@inproceedings{camgoz_neural_2018,
  title = {Neural {{Sign Language Translation}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Camgoz, Cihan and Hadfield, Simon and Koller, Oscar and Ney, Hermann and Bowden, Richard},
  year = {2018},
  month = jun,
  pages = {7784--7793},
  address = {{Salt Lake City, UT}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Camgoz et al_2018_Neural Sign Language Translation.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=1066;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,dataSet=Phoenix14T,modal=fullframe,signer=9,sl=DGS,slreco,slreco;modal=fullframe,translation;vocab=2887;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,vocab=1066,vocab>=1000}
}

@incollection{camgoz_sign_2016,
  title = {Sign {{Language Recognition}} for {{Assisting}} the {{Deaf}} in {{Hospitals}}},
  booktitle = {Human {{Behavior Understanding}}},
  author = {Camg{\"o}z, Necati Cihan and K{\i}nd{\i}ro{\u g}lu, Ahmet Alp and Akarun, Lale},
  editor = {Chetouani, Mohamed and Cohn, Jeffrey and Salah, Albert Ali},
  year = {2016},
  volume = {9997},
  pages = {89--101},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46843-3_6},
  abstract = {In this study, a real-time, computer vision based sign language recognition system aimed at aiding hearing impaired users in a hospital setting has been developed. By directing them through a tree of questions, the system allows the user to state their purpose of visit by answering between four to six questions. The deaf user can use sign language to communicate with the system, which provides a written transcript of the exchange. A database collected from six users was used for the experiments. User independent tests without using the tree-based interaction scheme yield a 96.67\% accuracy among 1257 sign samples belonging to 33 sign classes. The experiments evaluated the effectiveness of the system in terms of feature selection and spatio-temporal modelling. The combination of hand position and movement features modelled by Temporal Templates and classified by Random Decision Forests yielded the best results. The tree-based interaction scheme further increased the recognition performance to more than 97.88\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Camgöz et al_2016_Sign Language Recognition for Assisting the Deaf in Hospitals.pdf},
  isbn = {978-3-319-46842-6 978-3-319-46843-3},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=33;signer=6;sl=TSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=6,sl=TSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=33},
  language = {en}
}

@article{camgoz_sign_2020,
  ids = {camgoz\_sign\_2020-1},
  title = {Sign {{Language Transformers}}: {{Joint End}}-to-End {{Sign Language Recognition}} and {{Translation}}},
  shorttitle = {Sign {{Language Transformers}}},
  author = {Camgoz, Necati Cihan and Koller, Oscar and Hadfield, Simon and Bowden, Richard},
  year = {2020},
  month = mar,
  abstract = {Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-theart in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information, simultaneously solving two co-dependant sequence-tosequence learning problems and leads to significant performance gains.},
  archivePrefix = {arXiv},
  eprint = {2003.13830},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Camgoz et al_2020_Sign Language Transformers.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3I7VVPDU/2003.html},
  journal = {arXiv:2003.13830 [cs]},
  keywords = {capturedBy=vision,continuous,continuous;vocab=1066;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,dataSet=Phoenix14T,modal=fullframe,signer=9,sl=DGS,slreco,slreco;modal=fullframe,vocab=1066,vocab>=1000},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{camgoz_subunets_2017,
  title = {{{SubUNets}}: {{End}}-to-End {{Hand Shape}} and {{Continuous Sign Language Recognition}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Camgoz, Necati Cihan and Hadfield, Simon and Koller, Oscar and Bowden, Richard},
  year = {2017},
  month = oct,
  pages = {22--27},
  address = {{Venice, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Camgoz et al_2017_SubUNets.pdf},
  keywords = {augment=?,authorgroup=RWTH,authorgroup=Surrey,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,dataSet=Phoenix14,loss=CTC,modal=fullframe,modal=shape,nn=2dcnn,nn=blstm,nnarch=alexnet,nntrain=blstm,paperinfo,paperinfo;authorgroup=Surrey;authorgroup=RWTH;shorttitle=SubUNets,re-align=1,shorttitle=SubUNets,signer=9,sl=DGS,slreco,slreco;modal=fullframe;modal=shape;dataSet=Phoenix14;wer=40.8/40.7;nn=2dcnn;nnarch=alexnet;cnnresiduals=0;re-align=1;nn=blstm;nntrain=blstm;augment=?;loss=CTC,vocab=1080,vocab>=1000,wer=40.8/40.7}
}

@inproceedings{camgoz2016:3dconv,
  title = {Using {{Convolutional 3D Neural Networks}} for {{User}}-{{Independent Continuous Gesture Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition Workshops}} ({{ICPRW}})},
  author = {Camgoz, Necati Cihan and Hadfield, Simon and Koller, Oscar and Bowden, Richard},
  year = {2016},
  month = dec,
  pages = {49--54},
  address = {{Cancun, Mexico}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Camgoz et al_2016_Using Convolutional 3D Neural Networks for User-Independent Continuous Gesture.pdf}
}

@inproceedings{campbell_comparison_2008,
  title = {A {{Comparison}} of {{Subspace Feature}}-{{Domain Methods}} for {{Language Recognition}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Campbell, W. M. and Sturim, D. E. and {Torres-Carrasquillo}, P. and Reynolds, D. A.},
  year = {2008},
  address = {{Brisbane, Australia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Campbell et al_2008_A Comparison of Subspace Feature-Domain Methods for Language Recognition.PDF}
}

@inproceedings{campbell_covariance_2008,
  title = {A {{Covariance Kernel}} for \{\vphantom\}{{SVM}}\vphantom\{\} {{Language Recognition}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Campbell, W. M},
  year = {2008},
  pages = {4141--4144},
  address = {{Las Vegas, USA}},
  doi = {10.1109/ICASSP.2008.4518566}
}

@inproceedings{campbell_highlevel_2004,
  title = {High-{{Level Speaker Verification}} with {{Support Vector Machines}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Campbell, W. M. and Campbell, J. P. and Reynolds, D. A. and Jones, D. A. and Leek, T. R.},
  year = {2004},
  volume = {4},
  pages = {I-73-76},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Campbell et al_2004_High-Level Speaker Verification with Support Vector Machines.pdf}
}

@article{campbell_support_2006,
  title = {Support {{Vector Machines}} for {{Speaker}} and {{Language Recognition}}},
  author = {Campbell, W. M. and Campbell, J. P. and Reynolds, D. A. and Singer, E. and {Torres-Carrasquillo}, P. A.},
  year = {2006},
  volume = {20},
  pages = {210--229},
  journal = {Computer Speech and Language},
  number = {2-3}
}

@inproceedings{campbell_svm_2006,
  title = {\{\vphantom\}{{SVM}}\vphantom\{\} {{Based Speaker Verification Using}} a \{\vphantom\}{{GMM}}\vphantom\{\} {{Supervector Kernel}} and \{\vphantom\}{{NAP}}\vphantom\{\} {{Variability Compensation}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Campbell, W. M. and Sturim, D. E. and Reynolds, D. A. and Solomonoff, A.},
  year = {2006},
  volume = {1}
}

@inproceedings{campr_collection_2008,
  title = {Collection and Preprocessing of Czech Sign Language Corpus for Sign Language Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Campr, Pavel and Hr{\'u}z, Marek and Trojanov{\'a}, Jana},
  year = {2008},
  month = may,
  pages = {3175--3178},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Campr et al_2008_Collection and preprocessing of czech sign language corpus for sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M455RD6P/CamprP_2008_Collectionand.html}
}

@article{cao_face_2014,
  title = {Face Alignment by Explicit Shape Regression},
  author = {Cao, Xudong and Wei, Yichen and Wen, Fang and Sun, Jian},
  year = {2014},
  volume = {107},
  pages = {177--190},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cao et al_2012_Face alignment by explicit shape regression.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9VJPS2QN/face-alignment-by-explicit-shape-regression.html},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {2}
}

@inproceedings{cao_realtime_2017,
  title = {Realtime {{Multi}}-{{Person 2D Pose Estimation Using Part Affinity Fields}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  year = {2017},
  month = jul,
  pages = {7291--7299},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cao et al_2017_Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F7PZ9KWG/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html}
}

@incollection{capek_studying_2015,
  title = {Studying {{Sign Language Processing Using Functional Neuroimaging Techniques}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Capek, Cheryl M. and Neville, Helen J.},
  year = {2015},
  pages = {319--335},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch17},
  abstract = {This chapter focuses on four main techniques for investigating the neural organization of cognitive functions: functional magnetic resonance imaging (fMRI), event-related potential (ERP), magnetoencephalography (MEG), and transcranial magnetic stimulation (TMS). For each technique, a brief description of the methodology is followed by particular considerations for sign language research \textendash{} experimental manipulation, selection of participant groups, and particular communication issues for testing deaf participants. The magnetic resonance imaging (MRI) technique produces highly detailed pictures of the structure of both hard and soft tissue of the body, including the brain. FMRI measures relative blood oxygenation-level-dependent (or BOLD) states across time and is an indirect measure of neural activity. Unlike fMRI, ERP provides a direct measure of neural processing and exquisite temporal resolution, of the order of milliseconds (msec).},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Capek_Neville_2015_Studying Sign Language Processing Using Functional Neuroimaging Techniques.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PHPHUMAS/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{cappelletta_phonemetoviseme_2012,
  title = {Phoneme-to-Viseme {{Mapping}} for {{Visual Speech Recognition}}.},
  booktitle = {{{ICPRAM}}},
  author = {Cappelletta, Luca and Harte, Naomi},
  year = {2012},
  pages = {322--329},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cappelletta_Harte_2012_Phoneme-to-viseme Mapping for Visual Speech Recognition.pdf}
}

@inproceedings{caridakis_automatic_2008,
  title = {Automatic Sign Language Recognition: Vision Based Feature Extraction and Probabilistic Recognition Scheme from Multiple Cues},
  shorttitle = {Automatic Sign Language Recognition},
  booktitle = {Proc {{Int}}. {{Conf}}. on {{PErvasive Technologies Related}} to {{Assistive Environments}}},
  author = {Caridakis, George and Diamanti, Olga and Karpouzis, Kostas and Maragos, Petros},
  year = {2008},
  pages = {89},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Caridakis et al_2008_Automatic sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CSZ5TXZA/citation.html}
}

@article{caridakis_non_2012,
  title = {Non Parametric, Self Organizing, Scalable Modeling of Spatiotemporal Inputs: {{The}} Sign Language Paradigm},
  shorttitle = {Non Parametric, Self Organizing, Scalable Modeling of Spatiotemporal Inputs},
  author = {Caridakis, G. and Karpouzis, K. and Drosopoulos, A. and Kollias, S.},
  year = {2012},
  month = dec,
  volume = {36},
  pages = {157--166},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2012.10.001},
  abstract = {Modeling and recognizing spatiotemporal, as opposed to static input, is a challenging task since it incorporates input dynamics as part of the problem. The vast majority of existing methods tackle the problem as an extension of the static counterpart, using dynamics, such as input derivatives, at feature level and adopting artificial intelligence and machine learning techniques originally designed for solving problems that do not specifically address the temporal aspect. The proposed approach deals with temporal and spatial aspects of the spatiotemporal domain in a discriminative as well as coupling manner. Self Organizing Maps (SOM) model the spatial aspect of the problem and Markov models its temporal counterpart. Incorporation of adjacency, both in training and classification, enhances the overall architecture with robustness and adaptability. The proposed scheme is validated both theoretically, through an error propagation study, and experimentally, on the recognition of individual signs, performed by different, native Greek Sign Language users. Results illustrate the architecture's superiority when compared to Hidden Markov Model techniques and variations both in terms of classification performance and computational cost.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Caridakis et al_2012_Non parametric, self organizing, scalable modeling of spatiotemporal inputs.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SI5F6MVE/S0893608012002626.html},
  journal = {Neural Networks},
  keywords = {capturedBy=vision,isolated,isolated;vocab=118;signer=3;sl=GSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=3,sl=GSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<500,vocab=118}
}

@article{caridakis_nonmanual_2014,
  title = {Non-Manual Cues in Automatic Sign Language Recognition},
  author = {Caridakis, George and Asteriadis, Stylianos and Karpouzis, Kostas},
  year = {2014},
  month = oct,
  volume = {18},
  pages = {37--46},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Caridakis et al_2014_Non-manual cues in automatic sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CK4KGFBH/s00779-012-0615-1.html},
  journal = {Personal and ubiquitous computing},
  number = {1}
}

@article{caridakis_somm_2010,
  title = {{{SOMM}}: {{Self}} Organizing {{Markov}} Map for Gesture Recognition},
  shorttitle = {{{SOMM}}},
  author = {Caridakis, George and Karpouzis, Kostas and Drosopoulos, Athanasios and Kollias, Stefanos},
  year = {2010},
  month = jan,
  volume = {31},
  pages = {52--59},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2009.09.009},
  abstract = {Present work introduces a probabilistic recognition scheme for hand gestures. Self organizing feature maps are used to model spatiotemporal information extracted through image processing. Two models are built for each gesture category and, along with appropriate distance metrics, produce a validated classification mechanism that performs consistently during experiments on acted gestures video sequences. The main focus of current work is to tackle intra and inter user variability during gesture performance by adding flexibility to the decoding procedure and allowing the algorithm to perform an optimal trajectory search while the processing speed of both the feature extraction and the recognition process indicate that the proposed architecture is appropriate for real time and large scale lexicon applications.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Caridakis et al_2010_SOMM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R3JZSIW3/S0167865509002372.html},
  journal = {Pattern Recognition Letters},
  number = {1}
}

@article{carreira_quo_2017,
  title = {Quo {{Vadis}}, {{Action Recognition}}? {{A New Model}} and the {{Kinetics Dataset}}},
  shorttitle = {Quo {{Vadis}}, {{Action Recognition}}?},
  author = {Carreira, Joao and Zisserman, Andrew},
  year = {2017},
  month = may,
  abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9\% on HMDB-51 and 98.0\% on UCF-101.},
  archivePrefix = {arXiv},
  eprint = {1705.07750},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Carreira_Zisserman_2017_Quo Vadis, Action Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IW6CKIUK/1705.html},
  journal = {arXiv:1705.07750 [cs]},
  primaryClass = {cs}
}

@article{carreiras_lexical_2008,
  title = {Lexical Processing in {{Spanish Sign Language}} ({{LSE}})},
  author = {Carreiras, Manuel and {Guti{\'e}rrez-Sigut}, Eva and Baquero, Silvia and Corina, David},
  year = {2008},
  month = jan,
  volume = {58},
  pages = {100--122},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.05.004},
  abstract = {Lexical access is concerned with how the spoken or visual input of language is projected onto the mental representations of lexical forms. To date, most theories of lexical access have been based almost exclusively on studies of spoken languages and/or orthographic representations of spoken languages. Relatively few studies have examined how lexical access takes place in deaf users of signed languages. This paper examines whether two properties, lexical familiarity and phonological neighborhood, which are known to influence recognition in spoken languages, influence lexical access in Spanish Sign Language\textemdash Lengua de Signos Espanola (LSE). Our results indicate that the representational factors of lexical familiarity and phonological neighborhood can be observed in native and non-native deaf users of LSE. In addition, the present data provides evidence for the importance of sub-lexical properties in sign language processing.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Carreiras et al_2008_Lexical processing in Spanish Sign Language (LSE).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WNNXESFC/S0749596X07000678.html},
  journal = {Journal of Memory and Language},
  number = {1}
}

@inproceedings{caseiro_comparative_2003,
  title = {A {{Comparative Description}} of \{\vphantom\}{{GtoP}}\vphantom\{\} {{Modules}} for \{\vphantom\}{{Portuguese}}\vphantom\{\} and \{\vphantom\}{{Mirandese}}\vphantom\{\} {{Using Finite State Transducers}}},
  booktitle = {Proc. {{International Congress}} of {{Phonetic Sciences}} ({{ICPhS}})},
  author = {Caseiro, D. and Trancoso, I. and Viana, C. and Barros, M.},
  year = {2003},
  address = {{Barcelona, Spain}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Caseiro et al_2003_A Comparative Description of GtoP Modules for Portuguese and Mirandese .pdf}
}

@phdthesis{caseiro_finite-state_2003,
  title = {Finite-{{State Methods}} in {{Automatic Speech Recognition}}},
  author = {Caseiro, D.},
  year = {2003},
  address = {{Lisbon, Portugal}},
  school = {Instituto Superior T\'ecnico, Universidade T\'ecnica de Lisboa}
}

@article{caseiro_grapheme-to-phone_2002,
  title = {Grapheme-to-{{Phone Using Finite}}-{{State Transducers}}},
  author = {Caseiro, D. and Trancoso, L. and Oliveira, L. and Viana, C.},
  year = {2002},
  pages = {215--218},
  journal = {Proc. IEEE Workshop on Speech Synthesis}
}

@article{caselli_asllex_2017,
  title = {{{ASL}}-{{LEX}}: {{A}} Lexical Database of {{American Sign Language}}},
  shorttitle = {{{ASL}}-{{LEX}}},
  author = {Caselli, Naomi K. and Sehyr, Zed Sevcikova and {Cohen-Goldberg}, Ariel M. and Emmorey, Karen},
  year = {2017},
  month = apr,
  volume = {49},
  pages = {784--801},
  issn = {1554-3528},
  doi = {10.3758/s13428-016-0742-0},
  abstract = {ASL-LEX is a lexical database that catalogues information about nearly 1,000 signs in American Sign Language (ASL). It includes the following information: subjective frequency ratings from 25\textendash 31 deaf signers, iconicity ratings from 21\textendash 37 hearing non-signers, videoclip duration, sign length (onset and offset), grammatical class, and whether the sign is initialized, a fingerspelled loan sign, or a compound. Information about English translations is available for a subset of signs (e.g., alternate translations, translation consistency). In addition, phonological properties (sign type, selected fingers, flexion, major and minor location, and movement) were coded and used to generate sub-lexical frequency and neighborhood density estimates. ASL-LEX is intended for use by researchers, educators, and students who are interested in the properties of the ASL lexicon. An interactive website where the database can be browsed and downloaded is available at http://asl-lex.org.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Caselli et al_2017_ASL-LEX.pdf},
  journal = {Behavior Research Methods},
  language = {en},
  number = {2}
}

@article{caselli_lexical_2014,
  title = {Lexical Access in Sign Language: A Computational Model},
  shorttitle = {Lexical Access in Sign Language},
  author = {Caselli, Naomi K. and {Cohen-Goldberg}, Ariel M.},
  year = {2014},
  volume = {5},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2014.00428},
  abstract = {Psycholinguistic theories have predominantly been built upon data from spoken language, which leaves open the question: How many of the conclusions truly reflect language-general principles as opposed to modality-specific ones? We take a step toward answering this question in the domain of lexical access in recognition by asking whether a single cognitive architecture might explain diverse behavioral patterns in signed and spoken language. Chen and Mirman (2012) presented a computational model of word processing that unified opposite effects of neighborhood density in speech production, perception, and written word recognition. Neighborhood density effects in sign language also vary depending on whether the neighbors share the same handshape or location. We present a spreading activation architecture that borrows the principles proposed by Chen and Mirman (2012), and show that if this architecture is elaborated to incorporate relatively minor facts about either 1) the time course of sign perception or 2) the frequency of sub-lexical units in sign languages, it produces data that match the experimental findings from sign languages. This work serves as a proof of concept that a single cognitive architecture could underlie both sign and word recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Caselli_Cohen-Goldberg_2014_Lexical access in sign language.pdf},
  journal = {Frontiers in Psychology},
  language = {English}
}

@article{castaldo_compensation_2007,
  title = {Compensation of {{Nuisance Factors}} for {{Speaker}} and {{Language Recognition}}},
  author = {Castaldo, F. and Colibro, D. and Dalmasso, E. and Laface, P. and Vair, C.},
  year = {2007},
  volume = {15},
  pages = {1969--1978},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {7}
}

@article{castaldo_loquendopolitecnico_2010,
  title = {Loquendo-{{Politecnico Di}} \{\vphantom\}{{Torino}}\vphantom\{\} {{System}} for the 2009 {{NIST Language Recognition Evaluation}}},
  author = {Castaldo, Fabio and Colibro, Daniele and Cumani, Sandro and Dalmasso, Emanuele and Laface, Pietro and Vair, Claudio},
  year = {2010},
  journal = {Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)}
}

@article{cedras_motionbased_1995,
  title = {Motion-Based Recognition a Survey},
  author = {C{\'e}dras, Claudette and Shah, Mubarak},
  year = {1995},
  month = mar,
  volume = {13},
  pages = {129--155},
  issn = {0262-8856},
  doi = {10.1016/0262-8856(95)93154-K},
  abstract = {Motion-based recognition deals with the recognition of an object or its motion based on motion in a sequence of images. In this approach, a sequence containing a large number of frames is used to extract motion information. The advantage is that a longer sequence leads to recognition of higher level motions, like walking or running, which consists of a complex and coordinated series of events that cannot be understood by looking at only a few frames. This paper provides a review of recent developments in the computer vision aspect of motionbased recognition. We will identify two main steps in motionbased recognition. The first step is the extraction of motion information and its organization into motion models. The second step consists of the matching of some unknown input with a constructed model. Several methods for the recognition of objects and motions will then be reported. They include methods such as cyclic motion detection and recognition, lipreading, hand gestures interpretation, motion verb recognition and temporal textures classification. Tracking and recognition of human motion, like walking, skipping and running will also be discussed. Finally, we will conclude the paper with some thoughts about future directions for motionbased recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cédras_Shah_1995_Motion-based recognition a survey.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UHUHDKEI/026288569593154K.html},
  journal = {Image and Vision Computing},
  number = {2}
}

@article{cetingul_discriminative_2006,
  title = {Discriminative Analysis of Lip Motion Features for Speaker Identification and Speech-Reading},
  author = {Cetingul, H. Ertan and Yemez, Y{\"u}cel and Erzin, Engin and Tekalp, A. Murat},
  year = {2006},
  volume = {15},
  pages = {2879--2891},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cetingul et al_2006_Discriminative analysis of lip motion features for speaker identification and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3Z7I3N7A/login.html},
  journal = {Image Processing, IEEE Transactions on},
  number = {10}
}

@inproceedings{chai_communication_2015,
  title = {Communication Tool for the Hard of Hearings: {{A}} Large Vocabulary Sign Language Recognition System},
  shorttitle = {Communication Tool for the Hard of Hearings},
  booktitle = {Int. {{Conf}}. {{Affective Computing}} and {{Intelligent Interaction}} ({{ACII}})},
  author = {Chai, Xiujuan and Wang, Hanjie and Yin, Fang and Chen, Xilin},
  year = {2015},
  month = sep,
  pages = {781--783},
  address = {{Xian, China}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chai et al_2015_Communication tool for the hard of hearings.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PVIRHW9A/7344659.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=1000;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=1000;signer=7;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=1,signer=7,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab=1000,vocab>=1000}
}

@techreport{chai_devisign_2014,
  title = {The {{DEVISIGN Large Vocabulary}} of {{Chinese Sign Language Database}} and {{Baseline Evaluations}}},
  author = {Chai, Xiujuan and Wang, Hanjie and Chen, Xilin},
  year = {2014},
  pages = {1--27},
  institution = {{Key Lab of Intelligent Information Processing of Chinese Academy of Sciences}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chai et al_2014_The DEVISIGN Large Vocabulary of Chinese Sign Language Database and Baseline.pdf},
  keywords = {capturedBy=depth,capturedBy=vision,dataSet=Devisign-D,dataSet=Devisign-G,dataSet=Devisign-L,isolated,isolated;vocab=2000;signer=8;sl=CSL;capturedBy=vision;capturedBy=depth;dataSet=Devisign-L,isolated;vocab=36;signer=8;sl=CSL;capturedBy=vision;capturedBy=depth;dataSet=Devisign-G,isolated;vocab=500;signer=8;sl=CSL;capturedBy=vision;capturedBy=depth;dataSet=Devisign-D,modal=2hlocation,modal=2hshape,modal=bodyjoints,signer=8,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=bodyjoints,vocab<1000,vocab<50,vocab=2000,vocab=36,vocab=500,vocab>=1000}
}

@inproceedings{chai_sign_2013,
  title = {Sign Language Recognition and Translation with Kinect},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Chai, Xiujuan and Li, Guang and Lin, Yushun and Xu, Zhihao and Tang, Yili and Chen, Xilin and Zhou, Ming},
  year = {2013},
  month = apr,
  volume = {655},
  pages = {demo},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chai et al_2013_Sign language recognition and translation with kinect.pdf},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=239;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hmovement,sl=CSL,slreco,slreco;modal=2hmovement,vocab<500,vocab=239}
}

@inproceedings{chai_visualcomm_2013,
  title = {{{VisualComm}}: A Tool to Support Communication between Deaf and Hearing Persons with the {{Kinect}}},
  shorttitle = {{{VisualComm}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Chai, Xiujuan and Li, Guang and Chen, Xilin and Zhou, Ming and Wu, Guobin and Li, Hanjing},
  year = {2013},
  pages = {76},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chai et al_2013_VisualComm.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J3H9QI55/citation.html}
}

@inproceedings{chan_euroma_1995,
  title = {{{EUROM}}-{{A}} Spoken Language Resource for the {{EU}}},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Chan, D. and Fourcin, A. and Gibbon, D. and Granstrom, B. and Huckvale, M. and Kokkinakis, G. and Kvale, K. and Lamel, L. and Lindberg, B. and Moreno, A. and {others}},
  year = {1995},
  volume = {95},
  pages = {867--870},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chan et al_1995_EUROM-A spoken language resource for the EU.pdf}
}

@article{chan_listen_2015,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  year = {2015},
  month = aug,
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  archivePrefix = {arXiv},
  eprint = {1508.01211},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chan et al_2015_Listen, Attend and Spell.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/55UKR237/1508.html},
  journal = {arXiv:1508.01211 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{chan_listen_2016,
  title = {Listen, Attend and Spell: {{A}} Neural Network for Large Vocabulary Conversational Speech Recognition},
  shorttitle = {Listen, Attend and Spell},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc and Vinyals, Oriol},
  year = {2016},
  month = mar,
  pages = {4960--4964},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2016.7472621},
  abstract = {We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes speech utterances directly to characters without pronunciation models, HMMs or other components of traditional speech recognizers. In LAS, the neural network architecture subsumes the acoustic, pronunciation and language models making it not only an end-to-end trained system but an end-to-end model. In contrast to DNN-HMM, CTC and most other models, LAS makes no independence assumptions about the probability distribution of the output character sequences given the acoustic sequence. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits each character conditioned on all previous characters, and the entire acoustic sequence. On a Google voice search task, LAS achieves a WER of 14.1\% without a dictionary or an external language model and 10.3\% with language model rescoring over the top 32 beams. In comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\% on the same set.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chan et al_2016_Listen, attend and spell.pdf},
  keywords = {acoustic sequence,Acoustics,attention-based recurrent network decoder,channel bank filters,CLDNN-HMM model,Context,CTC,decoding,Decoding,encoding,end-to-end model,end-to-end speech recognition,end-to-end trained system,filter bank spectra,Google voice search task,hidden Markov models,Hidden Markov models,language models,large vocabulary conversational speech recognition,LAS,listen-attend and spell neural speech recognizer,neural attention,neural net architecture,neural network,neural network architecture,output character sequences,probability distribution,pyramidal recurrent network encoder,recurrent neural nets,Recurrent neural network,Speech,speech coding,speech recognition,Speech recognition,speech utterances,statistical distributions,Training}
}

@incollection{channon_research_2015,
  title = {Research {{Methods}} for {{Studying}} the {{Form}} of {{Signs}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Channon, Rachel},
  year = {2015},
  pages = {123--140},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch8},
  abstract = {This chapter discusses methodological considerations related to researching the form of a sign \textendash{} research that can be subsumed to the phonology, phonetics, prosody and iconicity of sign languages. Phonetics is responsible for the physical aspects of signs, and its primary task is to consider the non-contrastive constraints on phonology \textendash{} those elements that are not part of the mental representation, but are a necessary part of the physical instantiation of a sign. Prosody has two major functions: to group elements and to provide information about intentions and emotional states. The research process, possible methods, and potential problems are considered in a step-by-step manner, which proceeds from finding a research question to collecting the data, finding signers, determining the equipment needed, and storing, annotating, and analyzing the data. There are several types of equipment to consider: elicitation material or prompts, video cameras, and specialized equipment.},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@article{charayaphan_image_1992,
  title = {Image Processing System for Interpreting Motion in {{American Sign Language}}},
  author = {Charayaphan, C. and Marble, A. E.},
  year = {1992},
  month = sep,
  volume = {14},
  pages = {419--425},
  issn = {0141-5425},
  doi = {10.1016/0141-5425(92)90088-3},
  abstract = {In this paper, an image processing algorithm is presented for the interpretation of the American Sign Language (ASL), which is one of the sign languages used by the majority of the deaf community. The process involves detection of hand motion, tracking the hand location based on the motion and classification of signs using adaptive clustering of stop positions, simple shape of the trajectory, and matching of the hand shape at the stop position.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Charayaphan_Marble_1992_Image processing system for interpreting motion in American Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X3S9SUH9/0141542592900883.html},
  journal = {Journal of Biomedical Engineering},
  keywords = {capturedBy=vision,isolated,isolated;vocab=31;signer=1;sl=ASL;capturedBy=vision,modal=location,modal=movement,signer=1,sl=ASL,slreco,slreco;modal=movement;modal=location,vocab<50,vocab=31},
  number = {5}
}

@article{charles_automatic_2014,
  title = {Automatic and {{Efficient Human Pose Estimation}} for {{Sign Language Videos}}},
  author = {Charles, James and Pfister, Tomas and Everingham, Mark and Zisserman, Andrew},
  year = {2014},
  month = oct,
  volume = {110},
  pages = {70--90},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-013-0672-6},
  abstract = {We present a fully automatic arm and hand tracker that detects joint positions over continuous sign language video sequences of more than an hour in length. To achieve this, we make contributions in four areas: (i) we show that the overlaid signer can be separated from the background TV broadcast using co-segmentation over all frames with a layered model; (ii) we show that joint positions (shoulders, elbows, wrists) can be predicted per-frame using a random forest regressor given only this segmentation and a colour model; (iii) we show that the random forest can be trained from an existing semi-automatic, but computationally expensive, tracker; and, (iv) introduce an evaluator to assess whether the predicted joint positions are correct for each frame. The method is applied to 20 signing footage videos with changing background, challenging imaging conditions, and for different signers. Our framework outperforms the state-of-the-art long term tracker by Buehler et al.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/75QNT5AT/Charles et al. - 2014 - Automatic and Efficient Human Pose Estimation for .pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {1}
}

@inproceedings{charles2016personalizing,
  title = {Personalizing {{Video Pose Estimation}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Charles, James and Pfister, Tomas and Magee, Derek and Hogg, David and Zisserman, Andrew},
  year = {2016},
  address = {{Las Vegas, NV, USA}},
  archivePrefix = {arXiv},
  award = {Oral},
  eprint = {1511.06676},
  eprinttype = {arxiv},
  webpdf = {http://arxiv.org/pdf/1511.06676v1.pdf}
}

@article{chen_audiovisual_1998,
  title = {Audio-Visual Integration in Multimodal Communication},
  author = {Chen, Tsuhan and Rao, R.R.},
  year = {1998},
  month = may,
  volume = {86},
  pages = {837--852},
  issn = {0018-9219},
  doi = {10.1109/5.664274},
  abstract = {We review recent research that examines audio-visual integration in multimodal communication. The topics include bimodality in human speech, human and automated lip reading, facial animation, lip synchronization, joint audio-video coding, and bimodal speaker verification. We also study the enabling technologies for these research topics, including automatic facial-feature tracking and audio-to-visual mapping. Recent progress in audio-visual research shows that joint processing of audio and video provides advantages that are not available when the audio and video are processed independently},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chen_Rao_1998_Audio-visual integration in multimodal communication.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/D6S3EBD7/abs_all.html},
  journal = {Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)},
  number = {5}
}

@techreport{chen_empirical_1998,
  title = {An {{Empirical Study}} of {{Smoothing Techniques}} for {{Language Modeling}}},
  author = {Chen, S. and Goodman, J.},
  year = {1998},
  pages = {1--63},
  institution = {{Computer Science Group, Harvard University}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chen_Goodman_1998_An Empirical Study of Smoothing Techniques for Language Modeling.pdf},
  number = {TR-10-98},
  type = {Technical {{Report}}}
}

@inproceedings{chen_joint_2019,
  title = {Joint {{Grapheme}} and {{Phoneme Embeddings}} for {{Contextual End}}-to-{{End ASR}}},
  booktitle = {Interspeech 2019},
  author = {Chen, Zhehuai and Jain, Mahaveer and Wang, Yongqiang and Seltzer, Michael L. and Fuegen, Christian},
  year = {2019},
  month = sep,
  pages = {3490--3494},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1434},
  abstract = {End-to-end approaches to automatic speech recognition, such as Listen-Attend-Spell (LAS), blend all components of a traditional speech recognizer into a unified model. Although this simplifies training and decoding pipelines, a unified model is hard to adapt when mismatch exists between training and test data, especially if this information is dynamically changing. The Contextual LAS (CLAS) framework tries to solve this problem by encoding contextual entities into fixed-dimensional embeddings and utilizing an attention mechanism to model the probabilities of seeing these entities. In this work, we improve the CLAS approach by proposing several new strategies to extract embeddings for the contextual entities. We compare these embedding extractors based on graphemic and phonetic input and/or output sequences and show that an encoder-decoder model trained jointly towards graphemes and phonemes outperforms other approaches. Leveraging phonetic information obtains better discrimination for similarly written graphemic sequences and also helps the model generalize better to graphemic sequences unseen in training. We show significant improvements over the original CLAS approach and also demonstrate that the proposed method scales much better to a large number of contextual entities across multiple domains.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chen et al_2019_Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR.pdf},
  language = {en}
}

@inproceedings{chen_parallelizing_2020,
  title = {Parallelizing {{Adam Optimizer}} with {{Blockwise Model}}-{{Update Filtering}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Kai and Ding, Haisong and Huo, Qiang},
  year = {2020},
  month = may,
  pages = {3027--3031},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9052983},
  abstract = {Recently Adam has become a popular stochastic optimization method in deep learning area. To parallelize Adam in a distributed system, synchronous stochastic gradient (SSG) technique is widely used, which is inefficient due to heavy communication cost. In this paper, we attempt to parallelize Adam with blockwise model-update filtering (BMUF) instead. BMUF synchronizes model-update periodically and introduces a block momentum to improve performance. We propose a novel way to modify the estimated moment buffers of Adam and figure out a simple yet effective trick for hyper-parameter setting under BMUF framework. Experimental results on large scale English optical character recognition (OCR) task and large vocabulary continuous speech recognition (LVCSR) task show that BMUF-Adam achieves almost a linear speedup without recognition accuracy degradation and outperforms SSG-based method in terms of speedup, scalability and recognition accuracy.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chen et al_2020_Parallelizing Adam Optimizer with Blockwise Model-Update Filtering.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VBF3BFPT/9052983.html},
  keywords = {Adam,BMUF,communication efficient,distributed optimization,scalable training}
}

@inproceedings{chen_scalable_2016,
  title = {Scalable Training of Deep Learning Machines by Incremental Block Training with Intra-Block Parallel Optimization and Blockwise Model-Update Filtering},
  booktitle = {2016 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Kai and Huo, Qiang},
  year = {2016},
  month = mar,
  pages = {5880--5884},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2016.7472805},
  abstract = {We present a new approach to scalable training of deep learning machines by incremental block training with intra-block parallel optimization to leverage data parallelism and blockwise model-update filtering to stabilize learning process. By using an implementation on a distributed GPU cluster with an MPI-based HPC machine learning framework to coordinate parallel job scheduling and collective communication, we have trained successfully deep bidirectional long short-term memory (LSTM) recurrent neural networks (RNNs) and fully-connected feed-forward deep neural networks (DNNs) for large vocabulary continuous speech recognition on two benchmark tasks, namely 309-hour Switchboard-I task and 1,860-hour "Switch-board+Fisher" task. We achieve almost linear speedup up to 16 GPU cards on LSTM task and 64 GPU cards on DNN task, with either no degradation or improved recognition accuracy in comparison with that of running a traditional mini-batch based stochastic gradient descent training on a single GPU.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chen_Huo_2016_Scalable training of deep learning machines by incremental block training with.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FAVZUNZC/7472805.html},
  keywords = {blockwise model-update filtering,collective communication,Computational modeling,Data models,deep bidirectional long short-term memory,deep learning,distributed GPU cluster,DNN,feedforward neural nets,filtering theory,fully-connected feedforward deep neural network,Graphics processing units,Incremental block training,intrablock parallel optimization,learning (artificial intelligence),leverage data parallelism,LSTM,LVCSR,Machine learning,minibatch based stochastic gradient descent training,MPI-based HPC machine learning framework,optimisation,Optimization,parallel job scheduling,parallel optimization,Parallel processing,recurrent neural network,RNN,scalable incremental block training,scheduling,speech recognition,switchboard-I task,switchboard+Fisher task,time 1860 hour,time 309 hour,Training,vocabulary continuous speech recognition}
}

@article{cheng_fully_2020,
  title = {Fully {{Convolutional Networks}} for {{Continuous Sign Language Recognition}}},
  author = {Cheng, Ka Leong and Yang, Zhaoyang and Chen, Qifeng and Tai, Yu-Wing},
  year = {2020},
  month = jul,
  abstract = {Continuous sign language recognition (SLR) is a challenging task that requires learning on both spatial and temporal dimensions of signing frame sequences. Most recent work accomplishes this by using CNN and RNN hybrid networks. However, training these networks is generally non-trivial, and most of them fail in learning unseen sequence patterns, causing an unsatisfactory performance for online recognition. In this paper, we propose a fully convolutional network (FCN) for online SLR to concurrently learn spatial and temporal features from weakly annotated video sequences with only sentence-level annotations given. A gloss feature enhancement (GFE) module is introduced in the proposed network to enforce better sequence alignment learning. The proposed network is end-to-end trainable without any pre-training. We conduct experiments on two large scale SLR datasets. Experiments show that our method for continuous SLR is effective and performs well in online recognition.},
  archivePrefix = {arXiv},
  eprint = {2007.12402},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cheng et al_2020_Fully Convolutional Networks for Continuous Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9DS3LCKW/2007.html},
  journal = {arXiv:2007.12402 [cs]},
  keywords = {augment=randomcrop,augment=temp-scale,authorgroup=HKUST,authorgroup=Kwai,authorgroup=Tencent,capturedBy=vision,cnnresiduals=0,Computer Science - Computer Vision and Pattern Recognition,continuous,continuous;vocab=1066;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T;wer=23.3/25.1,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=24.6/24.6,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,dataSet=Phoenix14,dataSet=Phoenix14T,loss=CE,loss=CTC,modal=fullframe,nn=1dcnn,nn=2dcnn,nnarch=2d-resnet,paperinfo,paperinfo;authorgroup=HKUST;authorgroup=Tencent;authorgroup=Kwai;shorttitle=Fully Conv Networks,re-align=0,shorttitle=Fully Conv Networks,shorttitle=Fully Convolutional Networks,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;nn=2dcnn;nn=1dcnn;nnarch=2d-resnet;cnnresiduals=0;re-align=0;augment=randomcrop;augment=temp-scale;loss=CE;loss=CTC,vocab<500,vocab=1066,vocab=1080,vocab=178,vocab>=1000,wer=23.3/25.1,wer=24.6/24.6},
  primaryClass = {cs}
}

@article{cheng_novel_2015,
  title = {A {{Novel Phonology}}- and {{Radical}}-{{Coded Chinese Sign Language Recognition Framework Using Accelerometer}} and {{Surface Electromyography Sensors}}},
  author = {Cheng, Juan and Chen, Xun and Liu, Aiping and Peng, Hu},
  year = {2015},
  month = sep,
  volume = {15},
  pages = {23303--23324},
  issn = {1424-8220},
  doi = {10.3390/s150923303},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cheng et al_2015_A Novel Phonology- and Radical-Coded Chinese Sign Language Recognition.pdf},
  journal = {Sensors},
  keywords = {capturedBy=emg,capturedBy=mocap,isolated,isolated;vocab=223;signer=5;sl=CSL;capturedBy=emg;capturedBy=mocap,modal=2horientation,modal=2hshape,signer=5,sl=CSL,slreco,slreco;modal=2horientation;modal=2hshape,vocab<500,vocab=223},
  language = {en},
  number = {9}
}

@article{cheok_review_2019,
  title = {A Review of Hand Gesture and Sign Language Recognition Techniques},
  author = {Cheok, Ming Jin and Omar, Zaid and Jaward, Mohamed Hisham},
  year = {2019},
  month = jan,
  volume = {10},
  pages = {131--153},
  issn = {1868-808X},
  doi = {10.1007/s13042-017-0705-5},
  abstract = {Hand gesture recognition serves as a key for overcoming many difficulties and providing convenience for human life. The ability of machines to understand human activities and their meaning can be utilized in a vast array of applications. One specific field of interest is sign language recognition. This paper provides a thorough review of state-of-the-art techniques used in recent hand gesture and sign language recognition research. The techniques reviewed are suitably categorized into different stages: data acquisition, pre-processing, segmentation, feature extraction and classification, where the various algorithms at each stage are elaborated and their merits compared. Further, we also discuss the challenges and limitations faced by gesture recognition research in general, as well as those exclusive to sign language recognition. Overall, it is hoped that the study may provide readers with a comprehensive introduction into the field of automated gesture and sign language recognition, and further facilitate future research efforts in this area.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cheok et al_2019_A review of hand gesture and sign language recognition techniques.pdf},
  journal = {International Journal of Machine Learning and Cybernetics},
  language = {en},
  number = {1}
}

@inproceedings{cherniavsky_activity_2008,
  title = {Activity Detection in Conversational Sign Language Video for Mobile Telecommunication},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Cherniavsky, N. and Ladner, R. E. and Riskin, E. A.},
  year = {2008},
  month = sep,
  pages = {1--6},
  doi = {10.1109/AFGR.2008.4813363},
  abstract = {The goal of the MobileASL project is to increase accessibility by making the mobile telecommunications network available to the signing Deaf community. Video cell phones enable Deaf users to communicate in their native language, American Sign Language (ASL). However, encoding and transmission of real-time video over cell phones is a power-intensive task that can quickly drain the battery.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cherniavsky et al_2008_Activity detection in conversational sign language video for mobile.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RNWZ2322/4813363.html}
}

@phdthesis{cherniavsky_activity_2009,
  title = {Activity Analysis of Sign Language Video for Mobile Telecommunication},
  author = {Cherniavsky, Neva and Ladner, Richard E. and Riskin, Eve A.},
  year = {2009},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cherniavsky et al_2009_Activity analysis of sign language video for mobile telecommunication.pdf},
  school = {University of Washington},
  type = {{{PhD Thesis}}}
}

@inproceedings{cherniavsky_variable_2007,
  title = {Variable Frame Rate for Low Power Mobile Sign Language Communication},
  booktitle = {Proceedings of the 9th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility},
  author = {Cherniavsky, Neva and Cavender, Anna C. and Ladner, Richard E. and Riskin, Eve A.},
  year = {2007},
  month = oct,
  pages = {163--170},
  publisher = {{Association for Computing Machinery}},
  address = {{Tempe, Arizona, USA}},
  doi = {10.1145/1296843.1296872},
  abstract = {The MobileASL project aims to increase accessibility by enabling Deaf people to communicate over video cell phones in their native language, American Sign Language (ASL). Real-time video over cell phones can be a computationally intensive task that quickly drains the battery, rendering the cell phone useless. Properties of conversational sign language allow us to save power and bits: namely, lower frame rates are possible when one person is not signing due to turn-taking, and signing can potentially employ a lower frame rate than fingerspelling. We conduct a user study with native signers to examine the intelligibility of varying the frame rate based on activity in the video. We then describe several methods for automatically determining the activity of signing or not signing from the video stream in real-time. Our results show that varying the frame rate during turn-taking is a good way to save power without sacrificing intelligibility, and that automatic activity analysis is feasible.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cherniavsky et al_2007_Variable frame rate for low power mobile sign language communication.pdf},
  isbn = {978-1-59593-573-1},
  keywords = {activity analysis,deaf community,low power,mobile telephone use,sign language},
  series = {Assets '07}
}

@article{cheung_local_2012,
  title = {A Local Region Based Approach to Lip Tracking},
  author = {Cheung, Yiu-ming and Liu, Xin and You, Xinge},
  year = {2012},
  month = sep,
  volume = {45},
  pages = {3336--3347},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2012.02.024},
  abstract = {Lip tracking has played a significant role in a lip reading system. In this paper, we present a local region based approach to lip tracking, which consists of two phases: (i) lip contour extraction for the first lip frame, and followed by (ii) lip tracking in the subsequent lip frames. Initially, we construct a localized color active color model provided that the foreground and background regions around the object are locally different in color space. In the first phase, we find a combined semi-ellipse around the lip as the initial evolving curve and compute the localized energies for curve evolution such that the lip image is separated into lip and non-lip regions. Then, we utilize a 16-point deformable model (Wang et al., 2004 [20]) with geometric constraint to achieve lip contour extraction. In the second phase, we present a dynamic selection of the radius of local regions associated with the extracted lip contour of the previous frame to realize lip tracking. The proposed approach not only adapts to the lip movement, but it is also robust against the appearance of teeth, tongue and black hole. Extensive experiments show the efficiency of the proposed lip tracking algorithm in comparison with the existing methods.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cheung et al_2012_A local region based approach to lip tracking.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8H3X4K3W/S0031320312001021.html},
  journal = {Pattern Recognition},
  number = {9}
}

@article{chitu_comparison_2007,
  title = {Comparison between Different Feature Extraction Techniques for Audio-Visual Speech Recognition},
  author = {Chitu, Alin G. and Rothkrantz, Leon JM and Wiggers, Pascal and Wojdel, Jacek C.},
  year = {2007},
  volume = {1},
  pages = {7--20},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chitu et al_2007_Comparison between different feature extraction techniques for audio-visual.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9UIXMDA4/link.springer.com.html},
  journal = {Journal on Multimodal User Interfaces},
  number = {1}
}

@incollection{chiu_automatic_2012,
  title = {Automatic {{Visual Speech Recognition}}},
  booktitle = {Speech {{Enhancement}}, {{Modeling}} and {{Recognition}}- {{Algorithms}} and {{Applications}}},
  author = {Chiu, Alin and J.M., Lon},
  editor = {Ramakrishnan, S},
  year = {2012},
  month = mar,
  publisher = {{InTech}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chiu_J.M._2012_Automatic Visual Speech Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GSUQT2G4/towards-robust-visual-speech-recognition.html},
  isbn = {978-953-51-0291-5}
}

@article{chiu_speech_2017,
  title = {Speech Recognition for Medical Conversations},
  author = {Chiu, Chung-Cheng and Tripathi, Anshuman and Chou, Katherine and Co, Chris and Jaitly, Navdeep and Jaunzeikare, Diana and Kannan, Anjuli and Nguyen, Patrick and Sak, Hasim and Sankar, Ananth and Tansuwan, Justin and Wan, Nathan and Wu, Yonghui and Zhang, Xuedong},
  year = {2017},
  month = nov,
  abstract = {In this paper we document our experiences with developing speech recognition for medical transcription - a system that automatically transcribes doctor-patient conversations. Towards this goal, we built a system along two different methodological lines - a Connectionist Temporal Classification (CTC) phoneme based model and a Listen Attend and Spell (LAS) grapheme based model. To train these models we used a corpus of anonymized conversations representing approximately 14,000 hours of speech. Because of noisy transcripts and alignments in the corpus, a significant amount of effort was invested in data cleaning issues. We describe a two-stage strategy we followed for segmenting the data. The data cleanup and development of a matched language model was essential to the success of the CTC based models. The LAS based models, however were found to be resilient to alignment and transcript noise and did not require the use of language models. CTC models were able to achieve a word error rate of 20.1\%, and the LAS models were able to achieve 18.3\%. Our analysis shows that both models perform well on important medical utterances and therefore can be practical for transcribing medical conversations.},
  archivePrefix = {arXiv},
  eprint = {1711.07274},
  eprinttype = {arxiv},
  journal = {arXiv:1711.07274 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{cho_describing_2015,
  title = {Describing {{Multimedia Content}} Using {{Attention}}-Based {{Encoder}}\textendash{{Decoder Networks}}},
  author = {Cho, Kyunghyun and Courville, Aaron and Bengio, Yoshua},
  year = {2015},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cho et al_2015_Describing Multimedia Content using Attention-based Encoder–Decoder Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/444JGEUV/abs_all.html}
}

@article{cho_learning_2014,
  title = {Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine Translation},
  author = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cho et al_2014_Learning phrase representations using rnn encoder-decoder for statistical.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/URMGAIQQ/1406.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UTT6EQ3D/1406.html},
  journal = {arXiv preprint arXiv:1406.1078}
}

@inproceedings{cho_properties_2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}-{{Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  booktitle = {Proceedings of {{SSST}}@{{EMNLP}} 2014, {{Eighth Workshop}} on {{Syntax}}, {{Semantics}} and {{Structure}} in {{Statistical Translation}}},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  editor = {Wu, Dekai and Carpuat, Marine and Carreras, Xavier and Vecchi, Eva Maria},
  year = {2014},
  pages = {103--111},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  isbn = {978-1-937284-96-1}
}

@article{chum_largescale_2010,
  title = {Large-{{Scale Discovery}} of {{Spatially Related Images}}},
  author = {Chum, O. and Matas, J.},
  year = {2010},
  month = feb,
  volume = {32},
  pages = {371--377},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2009.166},
  abstract = {We propose a randomized data mining method that finds clusters of spatially overlapping images. The core of the method relies on the min-Hash algorithm for fast detection of pairs of images with spatial overlap, the so-called cluster seeds. The seeds are then used as visual queries to obtain clusters which are formed as transitive closures of sets of partially overlapping images that include the seed. We show that the probability of finding a seed for an image cluster rapidly increases with the size of the cluster. The properties and performance of the algorithm are demonstrated on data sets with 104, 105, and 5 ?? 106 images. The speed of the method depends on the size of the database and the number of clusters. The first stage of seed generation is close to linear for databases sizes up to approximately 234 ?? 1010 images. On a single 2.4 GHz PC, the clustering process took only 24 minutes for a standard database of more than 100,000 images, i.e., only 0.014 seconds per image.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chum_Matas_2010_Large-Scale Discovery of Spatially Related Images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3N7VEQTA/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {2}
}

@article{chung_empirical_2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archivePrefix = {arXiv},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chung et al_2014_Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/82GTV78X/1412.html},
  journal = {arXiv:1412.3555 [cs]},
  primaryClass = {cs}
}

@inproceedings{chung_lip_2017,
  title = {Lip {{Reading Sentences}} in the {{Wild}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chung, Joon Son and Senior, Andrew and Vinyals, Oriol and Zisserman, Andrew},
  year = {2017},
  month = jul,
  pages = {3444--3453},
  address = {{Honolulu, HI, USA}},
  abstract = {The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem - unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS) network that learns to transcribe videos of mouth motion to characters; (2) a curriculum learning strategy to accelerate training and to reduce overfitting; (3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television. The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that visual information helps to improve speech recognition performance even when the audio is available.},
  archivePrefix = {arXiv},
  eprint = {1611.05358},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chung et al_2017_Lip Reading Sentences in the Wild.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/367H3H2K/1611.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/74ZWF5VU/1611.html}
}

@inproceedings{chung_lip_2017a,
  title = {Lip Reading in Profile},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Chung, Joon Son and Zisserman, Andrew},
  year = {2017},
  month = sep,
  pages = {1--11},
  address = {{York, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chung_Zisserman_2017_Lip reading in profile.pdf}
}

@article{ciaramello_computational_2011,
  title = {A {{Computational Intelligibility Model}} for {{Assessment}} and {{Compression}} of {{American Sign Language Video}}},
  author = {Ciaramello, F.M. and Hemami, S.S.},
  year = {2011},
  month = nov,
  volume = {20},
  pages = {3014--3027},
  issn = {1057-7149},
  doi = {10.1109/TIP.2011.2132730},
  abstract = {Real-time, two-way transmission of American Sign Language (ASL) video over cellular networks provides natural communication among members of the Deaf community. As a communication tool, compressed ASL video must be evaluated according to the intelligibility of the conversation, not according to conventional definitions of video quality. Guided by linguistic principles and human perception of ASL, this paper proposes a full-reference computational model of intelligibility for ASL (CIM-ASL) that is suitable for evaluating compressed ASL video. The CIM-ASL measures distortions only in regions relevant for ASL communication, using spatial and temporal pooling mechanisms that vary the contribution of distortions according to their relative impact on the intelligibility of the compressed video. The model is trained and evaluating using ground truth experimental data collected in three separate studies. The CIM-ASL provides accurate estimates of subjective intelligibility and demonstrates statistically significant improvements over computational models traditionally used to estimate video quality. The CIM-ASL is incorporated into an H.264 compliant video coding framework, creating a closed-loop encoding system optimized explicitly for ASL intelligibility. The ASL-optimized encoder achieves bitrate reductions between 10\% and 42\%, without reducing intelligibility, when compared to a general purpose H.264 encoder.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ciaramello_Hemami_2011_A Computational Intelligibility Model for Assessment and Compression of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VNAV5DIR/abs_all.html},
  journal = {IEEE Transactions on Image Processing},
  number = {11}
}

@article{ciresan_multicolumn_,
  title = {Multi-Column {{Deep Neural Networks}} for {{Image Classification Supplementary Online Material}}},
  author = {Ciresan, Dan and Meier, Ueli and Schmidhuber, J{\"u}rgen},
  volume = {12},
  pages = {13--18},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ciresan et al_Multi-column Deep Neural Networks for Image Classification Supplementary Online.pdf},
  journal = {Descente du gradient}
}

@book{clark_how_2012,
  title = {How to Write {{American Sign Language}}},
  author = {Clark, Adrean},
  year = {2012},
  publisher = {{ASLwrite}}
}

@inproceedings{clematide_cluzh_2017,
  title = {{{CLUZH}} at {{VarDial GDI}} 2017: {{Testing}} a Variety of Machine Learning Tools for the Classification of {{Swiss German}} Dialects},
  shorttitle = {{{CLUZH}} at {{VarDial GDI}} 2017},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}} ({{VarDial}})},
  author = {Clematide, Simon and Makarov, Peter},
  year = {2017},
  pages = {170--177}
}

@article{clevert_fast_2015,
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  year = {2015},
  month = nov,
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs also avoid a vanishing gradient via the identity for positive values. However ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero. Zero means speed up learning because they bring the gradient closer to the unit natural gradient. We show that the unit natural gradient differs from the normal gradient by a bias shift term, which is proportional to the mean activation of incoming units. Like batch normalization, ELUs push the mean towards zero, but with a significantly smaller computational footprint. While other activation functions like LReLUs and PReLUs also have negative values, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the propagated variation and information. Therefore ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. Consequently dependencies between ELUs are much easier to model and distinct concepts are less likely to interfere. We found that ELUs lead not only to faster learning, but also to better generalization performance once networks have many layers ({$>$}4). ELU networks were among top 10 reported CIFAR-10 results and yielded the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELUs considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
  archivePrefix = {arXiv},
  eprint = {1511.07289},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Clevert et al_2015_Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C5QKSB2A/1511.html},
  journal = {arXiv:1511.07289 [cs]},
  primaryClass = {cs}
}

@phdthesis{cloutier_prototyping_2016,
  title = {Prototyping a {{Portable}}, {{Affordable Sign Language Glove}}},
  author = {Cloutier, Princesa},
  year = {2016},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cloutier_2016_Prototyping a Portable, Affordable Sign Language Glove.pdf},
  school = {Worcester Polytechnic Institute},
  type = {Bachelor's {{Thesis}}}
}

@article{Cohen1960,
  title = {A {{Coefficient}} of {{Agreement}} for {{Nominal Scales}}},
  author = {Cohen, J.},
  year = {1960},
  volume = {20},
  pages = {37--46},
  journal = {Educational and Psychological Measurement},
  number = {1}
}

@article{cohn_timing_2004,
  title = {The Timing of Facial Motion in Posed and Spontaneous Smiles},
  author = {Cohn, J. F. and Schmidt, K. L.},
  year = {2004},
  month = jun,
  volume = {02},
  pages = {121--132},
  issn = {0219-6913},
  doi = {10.1142/S021969130400041X},
  abstract = {Almost all work in automatic facial expression analysis has focused on recognition of prototypic expressions rather than dynamic changes in appearance over time. To investigate the relative contribution of dynamic features to expression recognition, we used automatic feature tracking to measure the relation between amplitude and duration of smile onsets in spontaneous and deliberate smiles of 81 young adults of Euro- and African-American background. Spontaneous smiles were of smaller amplitude and had a larger and more consistent relation between amplitude and duration than deliberate smiles. A linear discriminant classifier using timing and amplitude measures of smile onsets achieved a 93\% recognition rate. Using timing measures alone, recognition rate declined only marginally to 89\%. These findings suggest that by extracting and representing dynamic as well as morphological features, automatic facial expression analysis can begin to discriminate among the message values of morphologically similar expressions.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cohn_Schmidt_2004_The timing of facial motion in posed and spontaneous smiles.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FWM92J9W/S021969130400041X.html},
  journal = {International Journal of Wavelets, Multiresolution and Information Processing},
  number = {02}
}

@article{cooke_audiovisual_2006,
  title = {An Audio-Visual Corpus for Speech Perception and Automatic Speech Recognition},
  author = {Cooke, Martin and Barker, Jon and Cunningham, Stuart and Shao, Xu},
  year = {2006},
  volume = {120},
  pages = {2421--2424},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooke et al_2006_An audio-visual corpus for speech perception and automatic speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P2HZJF4P/1.html},
  journal = {The Journal of the Acoustical Society of America},
  number = {5}
}

@inproceedings{cooper_large_2007,
  title = {Large {{Lexicon Detection}} of {{Sign Language}}},
  booktitle = {Human\textendash{{Computer Interaction}}},
  author = {Cooper, Helen and Bowden, Richard},
  year = {2007},
  month = oct,
  pages = {88--97},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-75773-3_10},
  abstract = {This paper presents an approach to large lexicon sign recognition that does not require tracking. This overcomes the issues of how to accurately track the hands through self occlusion in unconstrained video, instead opting to take a detection strategy, where patterns of motion are identified. It is demonstrated that detection can be achieved with only minor loss of accuracy compared to a perfectly tracked sequence using coloured gloves. The approach uses two levels of classification. In the first, a set of viseme classifiers detects the presence of sub-Sign units of activity. The second level then assembles visemes into word level Sign using Markov chains. The system is able to cope with a large lexicon and is more expandable than traditional word level approaches. Using as few as 5 training examples the proposed system has classification rates as high as 74.3\% on a randomly selected 164 sign vocabulary performing at a comparable level to other tracking based systems.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper_Bowden_2007_Large Lexicon Detection of Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4GBUXXI9/978-3-540-75773-3_10.html},
  isbn = {978-3-540-75772-6 978-3-540-75773-3},
  keywords = {capturedBy=vision,isolated,isolated;vocab=164;signer=1;sl=BSL;capturedBy=vision,modal=2hmovement,modal=orientation,modal=shape,signer=1,sl=BSL,slreco,slreco;modal=shape;modal=orientation;modal=2hmovement,vocab<500,vocab=164},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{cooper_learning_2009,
  title = {Learning Signs from Subtitles: {{A}} Weakly Supervised Approach to Sign Language Recognition},
  shorttitle = {Learning Signs from Subtitles},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cooper, H. and Bowden, R.},
  year = {2009},
  month = jun,
  pages = {2568--2574},
  address = {{Miami, FL}},
  doi = {10.1109/CVPR.2009.5206647},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper_Bowden_2009_Learning signs from subtitles.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=23;signer=1;sl=BSL;capturedBy=vision,modal=2hlocation,signer=1,sl=BSL,slreco,slreco;modal=2hlocation,vocab<50,vocab=23}
}

@inproceedings{cooper_reading_2011,
  title = {Reading the Signs: {{A}} Video Based Sign Dictionary},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Cooper, H. and Pugeault, N. and Bowden, R.},
  year = {2011},
  month = nov,
  pages = {914--919},
  doi = {10.1109/ICCVW.2011.6130349},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper et al_2011_Reading the signs.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V5KB4AE8/abs_all.html},
  keywords = {capturedBy=vision,dictionaries,isolated,isolated;vocab=984;signer=1;sl=GSL;capturedBy=vision,linguistics concept,Markov model,modal=location,modal=movement,modal=shape,Pragmatics,query processing,sign classifier,sign feature,sign level recognition,signer=1,sl=GSL,slreco,slreco;modal=shape;modal=location;modal=movement,user query,video based sign dictionary,visual sign recognition,vocab<1000,vocab=984}
}

@inproceedings{cooper_sign_2007,
  title = {Sign Language Recognition Using Boosted Volumetric Features},
  booktitle = {Proc. {{IAPR Conf}}. on {{Machine Vision Applications}}},
  author = {Cooper, H. and Bowden, R.},
  year = {2007},
  month = may,
  pages = {359--362},
  address = {{Tokyo, Japan}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper_Bowden_2007_Sign language recognition using boosted volumetric features.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=5;signer=9;sl=BSL;capturedBy=vision,modal=2hmovement,signer=9,sl=BSL,slreco,slreco;modal=2hmovement,vocab<50,vocab=5}
}

@inproceedings{cooper_sign_2009,
  title = {Sign {{Language Recognition}}: {{Working}} with {{Limited Corpora}}},
  shorttitle = {Sign {{Language Recognition}}},
  booktitle = {Universal {{Access}} in {{Human}}-{{Computer Interaction}}. {{Applications}} and {{Services}}},
  author = {Cooper, Helen and Bowden, Richard},
  editor = {Stephanidis, Constantine},
  year = {2009},
  volume = {5616},
  pages = {472--481},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The availability of video format sign language corpora limited. This leads to a desire for techniques which do not rely on large, fully-labelled datasets. This paper covers various methods for learning sign either from small data sets or from those without ground truth labels. To avoid non-trivial tracking issues; sign detection is investigated using volumetric spatio-temporal features. Following this the advantages of recognising the component parts of sign rather than the signs themselves is demonstrated and finally the idea of using a weakly labelled data set is considered and results shown for work in this area.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper_Bowden_2009_Sign Language Recognition2.pdf},
  isbn = {978-3-642-02713-0},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{cooper_sign_2010,
  title = {Sign {{Language Recognition}} Using {{Linguistically Derived Sub}}-{{Units}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Cooper, Helen and Bowden, Richard},
  year = {2010},
  month = may,
  pages = {57--61},
  address = {{Valletta, Malta}},
  abstract = {Sign Language Recognition using Linguistically Derived Sub-Units Helen Cooper, Richard Bowden University Of Surrey Guildford, UK. H.M.Cooper@Surrey.ac.uk, R.Bowden@Surrey.ac.uk Abstract This work proposes to learn linguistically-derived sub-unit ...},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper_Bowden_2010_Sign Language Recognition using Linguistically Derived Sub-Units.pdf},
  keywords = {capturedBy=vision,face side,hand motion,isolated,isolated;vocab=164;signer=1;sl=BSL;capturedBy=vision,lexicon,modal=2hlocation,modal=2hmovement,non dominant hand,on line learning,positive data,read 2011-week49,signer=1,sl=BSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<500,vocab=164}
}

@phdthesis{cooper_sign_2010a,
  title = {Sign {{Language Recognition}} : {{Generalising}} to {{More Complex Corpora}}},
  author = {Cooper, Helen M.},
  year = {2010},
  month = aug,
  address = {{Guildford, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper_2010_Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=164;signer=1;sl=BSL;capturedBy=vision,isolated;vocab=5;signer=9;sl=BSL;capturedBy=vision,modal=2hlocation,modal=movement,modal=shape,signer=1,signer=9,sl=BSL,slreco,slreco;modal=shape;modal=2hlocation;modal=movement,vocab<50,vocab<500,vocab=164,vocab=5},
  school = {University Of Surrey}
}

@incollection{cooper_sign_2011,
  title = {Sign {{Language Recognition}}},
  booktitle = {Visual {{Analysis}} of {{Humans}}},
  author = {Cooper, Helen and Holt, Brian and Bowden, Richard},
  editor = {Moeslund, Thomas B. and Hilton, Adrian and Kr{\"u}ger, Volker and Sigal, Leonid},
  year = {2011},
  month = jan,
  pages = {539--562},
  publisher = {{Springer London}},
  abstract = {This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a pr\'ecis of sign linguistics and their impact on the field. The types of data available and the relative merits are explored allowing examination of the features which can be extracted. Classifying the manual aspects of sign (similar to gestures) is then discussed from a tracking and non-tracking viewpoint before summarising some of the approaches to the non-manual aspects of sign languages. Methods for combining the sign classification results into full SLR are given showing the progression towards speech recognition techniques and the further adaptations required for the sign specific case. Finally the current frontiers are discussed and the recent research presented. This covers the task of continuous sign recognition, the work towards true signer independence, how to effectively combine the different modalities of sign, making use of the current linguistic research and adapting to larger more noisy data sets.},
  copyright = {\textcopyright 2011 Springer-Verlag London Limited},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper et al_2011_Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/E4227EM9/978-0-85729-997-0_27.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GM27TPVQ/Cooper et al. - 2011 - Sign Language Recognition.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KDCQQNNR/978-0-85729-997-0_27.html},
  isbn = {978-0-85729-996-3 978-0-85729-997-0},
  language = {en}
}

@article{cooper_sign_2012,
  title = {Sign Language Recognition Using Sub-Units},
  author = {Cooper, Helen and Ong, Eng-Jon and Pugeault, Nicolas and Bowden, Richard},
  year = {2012},
  volume = {13},
  pages = {2205--2231},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cooper et al_2012_Sign language recognition using sub-units.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QCRAGGED/citation.html},
  journal = {The Journal of Machine Learning Research},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=164;signer=1;sl=BSL;capturedBy=vision,isolated;vocab=20;signer=6;sl=GSL;capturedBy=vision;capturedBy=depth,isolated;vocab=40;signer=15;sl=DGS;capturedBy=vision,isolated;vocab=984;signer=1;sl=GSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=1,signer=15,signer=6,sl=BSL,sl=DGS,sl=GSL,slreco,slreco;modal=2hmovement;modal=2hlocation;modal=2hshape;modal=bodyjoints,vocab<1000,vocab<50,vocab<500,vocab=164,vocab=20,vocab=40,vocab=984},
  number = {1}
}

@article{cootes_active_2001,
  title = {Active {{Appearance Models}}},
  author = {Cootes, T. F and Edwards, G. J and Taylor, C. J},
  year = {2001},
  volume = {23},
  pages = {681},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cootes et al_2001_Active Appearance Models.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {6}
}

@article{corradini_hybrid_2000,
  title = {A Hybrid Stochastic-Connectionist Approach to Gesture Recognition},
  author = {Corradini, Andrea and Boehme, Hans-Joachim and Gross, Horst-Michael},
  year = {2000},
  volume = {9},
  pages = {177--203},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Corradini et al_2000_A hybrid stochastic-connectionist approach to gesture recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JEVHZGXQ/S0218213000000148.html},
  journal = {International Journal on Artificial Intelligence Tools},
  number = {02}
}

@article{costa_filho_fully_2017,
  title = {A Fully Automatic Method for Recognizing Hand Configurations of {{Brazilian}} Sign Language},
  author = {Costa Filho, Cicero Ferreira Fernandes and de Souza, Robson Silva and dos Santos, Jonilson Roque and dos Santos, B{\'a}rbara Lobato and Costa, Marly Guimar{\~a}es Fernandes and Costa Filho, Cicero Ferreira Fernandes and de Souza, Robson Silva and dos Santos, Jonilson Roque and dos Santos, B{\'a}rbara Lobato and Costa, Marly Guimar{\~a}es Fernandes},
  year = {2017},
  month = mar,
  volume = {33},
  pages = {78--89},
  issn = {2446-4740},
  doi = {10.1590/2446-4740.03816},
  abstract = {AbstractIntroductionSign language is a collection of gestures, postures, movements, and facial expressions used by deaf people. The Brazilian sign language is Libras. The use of Libras has been increased among the deaf communities, but is still not disseminated outside this community. Sign language recognition is a field of research, which intends to help the deaf community communication with non-hearing-impaired people. In this context, this paper describes a new method for recognizing hand configurations of Libras - using depth maps obtained with a Kinect\textregistered{} sensor.MethodsThe proposed method comprises three phases: hand segmentation, feature extraction, and classification. The segmentation phase is independent from the background and depends only on pixel value. The feature extraction process is independent from rotation and translation. The features are extracted employing two techniques: (2D)2LDA and (2D)2PCA. The classification employs two classifiers: a novelty classifier and a KNN classifier. A robust database is constructed for classifier evaluation, with 12,200 images of Libras and 200 gestures of each hand configuration.ResultsThe best accuracy obtained was 96.31\%.ConclusionThe best gesture recognition accuracy obtained is much higher than the studies previously published. It must be emphasized that this recognition rate is obtained for different conditions of hand rotation and proximity of the depth camera, and with a depth camera resolution of only 640\texttimes 480 pixels. This performance must be also credited to the feature extraction technique, and to the size standardization and normalization processes used previously to feature extraction step.Keywords~Deaf community; Sign language; Gesture recognition; Novelty classifier; kNN classifier; Libras},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Costa Filho et al_2017_A fully automatic method for recognizing hand configurations of Brazilian sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IB68PVER/scielo.html},
  journal = {Research on Biomedical Engineering},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=61;signer=10;sl=Libras;capturedBy=vision;capturedBy=depth,modal=shape,signer=10,sl=Libras,slreco,slreco;modal=shape,vocab<100,vocab=61},
  number = {1}
}

@article{covington_problems_2009,
  title = {Problems for a {{Sign Language Planning Agency}}},
  author = {COVINGTON, VIRGINIA},
  year = {2009},
  volume = {1976},
  pages = {85--106},
  issn = {1613-3668},
  doi = {10.1515/ijsl.1976.11.85},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/COVINGTON_2009_Problems for a Sign Language Planning Agency.pdf},
  journal = {International Journal of the Sociology of Language},
  number = {11}
}

@inproceedings{crasborn_annotation_2014,
  title = {An Annotation Scheme for the Linguistic Study of Mouth Actions in Sign Languages},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Crasborn, Onno and Bank, Richard},
  year = {2014},
  month = may,
  address = {{Reykjavik, Iceland}}
}

@misc{crasborn_corpus_2008,
  title = {The {{Corpus NGT}}. {{A}} Digital Open Access Corpus of Movies and Annotations of {{Sign Language}} of the {{Netherlands}}},
  author = {Crasborn, Onno and Zwitserlood, Inge and Ros, Johan},
  year = {2008},
  copyright = {Centre for Language Studies, Radboud Universiteit Nijmegen},
  howpublished = {https://www.ru.nl/corpusngtuk/}
}

@inproceedings{crasborn_corpus_2008a,
  title = {The {{Corpus NGT}}: An Online Corpus for Professionals and Laymen},
  shorttitle = {The {{Corpus NGT}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Crasborn, Onno and Zwitserlood, Inge},
  year = {2008},
  month = may,
  pages = {44--49},
  address = {{Marrakech, Morocco}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Crasborn_Zwitserlood_2008_The Corpus NGT.pdf}
}

@article{crasborn_frequency_2008,
  title = {Frequency Distribution and Spreading Behavior of Different Types of Mouth Actions in Three Sign Languages.},
  author = {Crasborn, Onno and Van Der Kooij, Els and Waters, Dafydd and Woll, Bencie and Mesch, Johanna},
  year = {2008},
  volume = {11},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Crasborn et al_2008_Frequency distribution and spreading behavior of different types of mouth.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SA8ZU553/advanced.html},
  journal = {Sign Language \& Linguistics},
  number = {1}
}

@article{crasborn_linguistic_2006,
  title = {A Linguistic Analysis of the Use of the Two Hands in Sign Language Poetry},
  author = {Crasborn, Onno},
  year = {2006},
  month = nov,
  volume = {23},
  pages = {65--77},
  issn = {0929-7332, 1569-9919},
  doi = {10.1075/avt.23.09cra},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UXHSX7UI/2006 - A linguistic analysis of the use of the two hands .pdf},
  journal = {Linguistics in the Netherlands},
  language = {en},
  number = {1}
}

@article{crasborn_phonology_2013,
  title = {The Phonology of Focus in {{Sign Language}} of the {{Netherlands}}},
  author = {Crasborn, Onno and Kooij, Els Van Der},
  year = {2013},
  month = nov,
  volume = {49},
  pages = {515--565},
  issn = {0022-2267, 1469-7742},
  doi = {10.1017/S0022226713000054},
  abstract = {Signed languages are similar to spoken languages in the overall organisation of their grammars, displaying a prosodic level of organisation that is not isomorphic to the syntactic organisation. Their rich inventory of manual and non-manual features allows for a prolific range of functions if used prosodically. New data from Sign Language of the Netherlands (NGT, Nederlandse Gebarentaal) are discussed to demonstrate that focused constituents are not marked by a single prosodic feature, but rather by multiple properties that can also have other functions in the prosodic phonology of the language. These findings are integrated in an overall model of sign language prosody that emphasises the distinction between phonetic appearance and phonological representation and that allows for the interaction of linguistic and paralinguistic cues in visual communication.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Crasborn_Kooij_2013_The phonology of focus in Sign Language of the Netherlands1.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/INTTHHQB/8F80AB82CBE6546A9BC87F8C0155F448.html},
  journal = {Journal of Linguistics},
  language = {en},
  number = {3}
}

@article{crasborn_relative_1997,
  title = {Relative Orientation in Sign Language Phonology},
  author = {Crasborn, Onno and {van der Kooij}, Els},
  year = {1997},
  volume = {14},
  pages = {37--48},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Crasborn_van der Kooij_1997_Relative orientation in sign language phonology2.pdf},
  journal = {Linguistics in the Netherlands}
}

@incollection{crasborn_transcription_2015,
  title = {Transcription and {{Notation Methods}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Crasborn, Onno A.},
  year = {2015},
  pages = {74--88},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch5},
  abstract = {This chapter discusses various methods of notating sign forms, focusing on the manual aspect of signing, for which the largest number of different systems has been proposed. It also discusses how notation and transcription can aid present-day researchers that make use of multimodal annotation tools to directly time-link text to videos. The chapter determines how phonetic transcription and other types of notation may be used in the present context of the availability of large sign language corpora to researchers and the general public, and it includes practical suggestions and references to studies that have followed certain methodologies. It focuses on various ways in which a basic annotation of a corpus in terms of glosses and sentence-level translations can be enriched with representations of aspects of the form of signs using notation methods that have been developed for other purposes.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Crasborn_2015_Transcription and Notation Methods2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R8WLK9QM/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@article{creutz_unsupervised_2007a,
  title = {Unsupervised Models for Morpheme Segmentation and Morphology Learning},
  author = {Creutz, Mathias and Lagus, Krista},
  year = {2007},
  month = jan,
  volume = {4},
  pages = {1--34},
  issn = {15504875},
  doi = {10.1145/1187415.1187418},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Creutz_Lagus_2007_Unsupervised models for morpheme segmentation and morphology learning.pdf},
  journal = {ACM Transactions on Speech and Language Processing},
  language = {en},
  number = {1}
}

@article{cui_appearancebased_2000,
  title = {Appearance-Based Hand Sign Recognition from Intensity Image Sequences},
  author = {Cui, Yuntao and Weng, Juyang},
  year = {2000},
  volume = {78},
  pages = {157--176},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cui_Weng_2000_Appearance-based hand sign recognition from intensity image sequences.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FAASC5CF/S1077314200908373.html},
  journal = {Computer Vision and Image Understanding (CVIU)},
  keywords = {capturedBy=vision,isolated,isolated;vocab=28;sl=ASL;capturedBy=vision,modal=location,modal=movement,modal=shape,sl=ASL,slreco,slreco;modal=shape;modal=movement;modal=location,vocab<50,vocab=28},
  number = {2}
}

@article{cui_deep_2019,
  title = {A {{Deep Neural Framework}} for {{Continuous Sign Language Recognition}} by {{Iterative Training}}},
  author = {Cui, R. and Liu, H. and Zhang, C.},
  year = {2019},
  volume = {0},
  pages = {1--1},
  issn = {1520-9210},
  doi = {10.1109/TMM.2018.2889563},
  abstract = {This work develops a vision-based continuous sign language (SL) recognition framework with deep neural networks, which directly transcribes videos of SL sentences to sequences of ordered gloss labels. Previous methods dealing with continuous SL recognition usually employ hidden Markov models with limited capacity to capture the temporal information. In contrast, our proposed architecture adopts deep convolutional neural networks with stacked temporal fusion layers as the feature extraction module, and bi-directional recurrent neural networks as the sequence learning module. We propose an iterative optimization process for our architecture to fully exploit the representation capability of deep neural networks with limited data. We first train the end-to-end recognition model for alignment proposal, and then use the alignment proposal as strong supervisory information to directly tune the feature extraction module. This training process can run iteratively to achieve improvements on the recognition performance. We further contribute by exploring the multimodal fusion of RGB images and optical flow in sign language. Our method is evaluated on two challenging SL recognition benchmarks, and outperforms the state-of-the-art by a relative improvement of more than 15\% on both databases.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cui et al_2019_A Deep Neural Framework for Continuous Sign Language Recognition by Iterative.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2722XDP6/8598757.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JW2FPBSP/8598757.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VQ634AQS/8598757.html},
  journal = {IEEE Transactions on Multimedia},
  keywords = {augment=intensity-noises,augment=spatial-scale,augment=temp-scale,authorgroup=Tsinghua,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Phoenix14,dataSet=Signum,iterative training,loss=CTC,loss=KL,loss=other,modal=fullframe,modal=fullframemotion,modal=shape,multimodal fusion,nn=1dcnn,nn=2dcnn,nn=blstm,nnarch=inception,nnarch=vgg-s,nntrain=2dcnn+1dcnn+blstm,paperinfo,paperinfo;authorgroup=Tsinghua;shorttitle=Iterative Training,re-align=1,sequence learning,shorttitle=Iterative Training,signer=1,signer=9,sl=DGS,slreco,slreco;modal=fullframe;dataSet=Phoenix14;wer=23.8/24.4;nn=2dcnn;nn=1dcnn;nnarch=inception;cnnresiduals=0;re-align=1;nn=blstm;augment=temp-scale;augment=intensity-noises;augment=spatial-scale;nntrain=2dcnn+1dcnn+blstm;loss=CTC;loss=KL;loss=other,slreco;modal=fullframe;modal=fullframemotion;dataSet=Phoenix14;wer=23.1/22.9;nn=2dcnn;nn=1dcnn;nnarch=inception;cnnresiduals=0;re-align=1;nn=blstm;augment=temp-scale;augment=intensity-noises;augment=spatial-scale;loss=CTC;loss=KL;loss=other,slreco;modal=fullframe;modal=fullframemotion;dataSet=Signum;wer=3.0;nn=2dcnn;nn=1dcnn;nnarch=vgg-s;cnnresiduals=0;re-align=1;nn=blstm;augment=temp-scale;augment=intensity-noises;augment=spatial-scale;nntrain=2dcnn+1dcnn+blstm;loss=CTC;loss=KL;loss=other,slreco;modal=fullframemotion;dataSet=Phoenix14;wer=37.9/37.6;nn=2dcnn;nn=1dcnn;nnarch=inception;cnnresiduals=0;re-align=1;nn=blstm;augment=temp-scale;augment=intensity-noises;augment=spatial-scale;nntrain=2dcnn+1dcnn+blstm;loss=CTC;loss=KL;loss=other,slreco;modal=shape;dataSet=Phoenix14;wer=31.7/31.5;nn=2dcnn;nn=1dcnn;nnarch=vgg-s;cnnresiduals=0;re-align=1;nn=blstm;augment=temp-scale;augment=intensity-noises;augment=spatial-scale;nntrain=2dcnn+1dcnn+blstm;loss=CTC;loss=KL;loss=other,vocab<500,vocab=1080,vocab=455,vocab>=1000,wer=23.1/22.9,wer=23.8/24.4,wer=3.0,wer=31.7/31.5,wer=37.9/37.6}
}

@inproceedings{cui_recurrent_2017,
  title = {Recurrent {{Convolutional Neural Networks}} for {{Continuous Sign Language Recognition}} by {{Staged Optimization}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cui, Runpeng and Liu, Hu and Zhang, Changshui},
  year = {2017},
  month = jul,
  pages = {7361--7369},
  address = {{Honolulu, HI, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2017/Cui et al/Cui et al_2017_Recurrent Convolutional Neural Networks for Continuous Sign Language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cui et al_2017_Recurrent Convolutional Neural Networks for Continuous Sign Language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Cui et al_2017_Recurrent Convolutional Neural Networks for Continuous Sign Language2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/36M6FMFU/Cui_Recurrent_Convolutional_Neural_CVPR_2017_paper.html},
  keywords = {augment=temp-scale,authorgroup=Tsinghua,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,dataSet=Phoenix14,loss=CTC,loss=KL,loss=other,modal=shape,nn=1dcnn,nn=2dcnn,nn=blstm,nnarch=vgg-s,nntrain=2dcnn+1dcnn+blstm,paperinfo,paperinfo;authorgroup=Tsinghua;shorttitle=Staged Optimization,re-align=1,shorttitle=Staged Optimization,signer=9,sl=DGS,slreco,slreco;modal=shape;dataSet=Phoenix14;wer=39.4/38.7;nn=2dcnn;nn=1dcnn;nnarch=vgg-s;cnnresiduals=0;re-align=1;nn=blstm;augment=temp-scale;nntrain=2dcnn+1dcnn+blstm;loss=CTC;loss=KL;loss=other,vocab=1080,vocab>=1000,wer=39.4/38.7}
}

@article{curiel_implementation_2014,
  title = {Implementation of an {{Automatic Sign Language Lexical Annotation Framework}} Based on {{Propositional Dynamic Logic}}},
  author = {Curiel, Arturo and Collet, Christophe},
  year = {2014},
  month = mar,
  abstract = {In this paper, we present the implementation of an automatic Sign Language (SL) sign annotation framework based on a formal logic, the Propositional Dynamic Logic (PDL). Our system relies heavily on the use of a specific variant of PDL, the Propositional Dynamic Logic for Sign Language (PDLSL), which lets us describe SL signs as formulae and corpora videos as labeled transition systems (LTSs). Here, we intend to show how a generic annotation system can be constructed upon these underlying theoretical principles, regardless of the tracking technologies available or the input format of corpora. With this in mind, we generated a development framework that adapts the system to specific use cases. Furthermore, we present some results obtained by our application when adapted to one distinct case, 2D corpora analysis with pre-processed tracking information. We also present some insights on how such a technology can be used to analyze 3D real-time data, captured with a depth device.},
  archivePrefix = {arXiv},
  eprint = {1403.6392},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Curiel_Collet_2014_Implementation of an Automatic Sign Language Lexical Annotation Framework based.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PS2UX3GQ/1403.html},
  journal = {arXiv:1403.6392 [cs]},
  primaryClass = {cs}
}

@article{curiel_sign_2014,
  title = {Sign {{Language Lexical Recognition With Propositional Dynamic Logic}}},
  author = {Curiel, Arturo and Collet, Christophe},
  year = {2014},
  month = mar,
  abstract = {This paper explores the use of Propositional Dynamic Logic (PDL) as a suitable formal framework for describing Sign Language (SL), the language of deaf people, in the context of natural language processing. SLs are visual, complete, standalone languages which are just as expressive as oral languages. Signs in SL usually correspond to sequences of highly specific body postures interleaved with movements, which make reference to real world objects, characters or situations. Here we propose a formal representation of SL signs, that will help us with the analysis of automatically-collected hand tracking data from French Sign Language (FSL) video corpora. We further show how such a representation could help us with the design of computer aided SL verification tools, which in turn would bring us closer to the development of an automatic recognition system for these languages.},
  archivePrefix = {arXiv},
  eprint = {1403.6636},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Curiel_Collet_2014_Sign Language Lexical Recognition With Propositional Dynamic Logic.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F85UU7MA/1403.html},
  journal = {arXiv:1403.6636 [cs]},
  primaryClass = {cs}
}

@article{dachkovsky_visual_2013,
  title = {Visual Intonation in Two Sign Languages},
  author = {Dachkovsky, Svetlana and Healy, Christina and Sandler, Wendy},
  year = {2013},
  month = aug,
  volume = {30},
  pages = {211--252},
  issn = {0952-6757, 1469-8188},
  doi = {10.1017/S0952675713000122},
  abstract = {In a detailed comparison of the intonational systems of two unrelated languages, Israeli Sign Language and American Sign Language, we show certain similarities as well as differences in the distribution of several articulations of different parts of the face and motions of the head. Differences between the two languages are explained on the basis of pragmatic notions related to information structure, such as accessibility and contingency, providing novel evidence that the system is inherently intonational, and only indirectly related to syntax. The study also identifies specific ways in which the physical modality in which language is expressed influences intonational structure.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dachkovsky et al_2013_Visual intonation in two sign languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WCK2RE3T/C08E71864BC04B710560470FC0D3F4FE.html},
  journal = {Phonology},
  language = {en},
  number = {2}
}

@article{dahl_contextdependent_2012,
  title = {Context-{{Dependent Pre}}-{{Trained Deep Neural Networks}} for {{Large}}-{{Vocabulary Speech Recognition}}},
  author = {Dahl, G. E. and Yu, D. and Deng, L. and Acero, A.},
  year = {2012},
  month = jan,
  volume = {20},
  pages = {30--42},
  issn = {1558-7916},
  doi = {10.1109/TASL.2011.2134090},
  abstract = {We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8\% and 9.2\% (or relative error reduction of 16.0\% and 23.2\%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dahl et al_2012_Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BFVPESUM/5740583.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {1}
}

@phdthesis{dahmen_invariant_2001,
  title = {Invariant {{Image Object Recognition}} Using {{Gaussian Mixture Densities}}},
  author = {Dahmen, J"org},
  year = {2001},
  month = oct,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dahmen_2001_Invariant Image Object Recognition using Gaussian Mixture Densities.pdf},
  school = {Human Language Technology and Pattern Recognition Group, RWTH Aachen University}
}

@inproceedings{dalal_histograms_2005,
  title = {Histograms of Oriented Gradients for Human Detection},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  month = jun,
  volume = {1},
  pages = {886-893 vol. 1},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dalal_Triggs_2005_Histograms of oriented gradients for human detection.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3E5WESE4/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JERD6EFH/1467360.html}
}

@inproceedings{dalsgaard_identification_1992,
  title = {Identification of {{Mono}}-and {{Poly}}-{{Phonemes Using Acoustic}}-{{Phonetic Features Derived}} by a {{Self}}-{{Organising Neural Network}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{ICSLP}})},
  author = {Dalsgaard, P. and Andersen, O.},
  year = {1992}
}

@incollection{das_finding_1997,
  title = {Finding Similar Time Series},
  booktitle = {Principles of {{Data Mining}} and {{Knowledge Discovery}}},
  author = {Das, Gautam and Gunopulos, Dimitrios and Mannila, Heikki},
  year = {1997},
  pages = {88--100},
  publisher = {{Springer}}
}

@article{das_sign_2015,
  title = {Sign {{Language Recognition Using Facial Expression}}},
  author = {Das, Siddhartha Pratim and Talukdar, Anjan Kumar and Sarma, Kandarpa Kumar},
  year = {2015},
  volume = {58},
  pages = {210--216},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2015.08.056},
  abstract = {Vision- based approaches of recognition of sign languages have made spectacular advances in the last few years. These also include many works in the area of speech processing to convert speech to text. A vision-based approach to classify facial gestures (lip movement, eye brow pattern etc.) for communication designed especially for the differently abled persons is a less explored area. In our work, we explore certain approaches to classify facial gestures to enhance its effectiveness and incorporate it to any sign language or vision-based gesture recognition movements for precise decision making. In our work, we have designed a real time system to detect alphabets by recognizing the lip pattern based on texture and shape. The system takes live video input and processes it in real time. Object detector of computer vision toolbox is used to classify the lips from extracted frames of video input. Five consecutive frames are extracted so as to trace the movements caused while speaking a particular syllable. Histogram of oriented gradients (HOG) of extracted lip image is used as features for recognition. The recognizer is designed using Artificial Neural Network (ANN) to recognize four classes viz. the lips movements formed for the four alphabets `A','B','C','D'. The entire systemis modelled and tested for real time performance with a video of 10 frames per second. Experimental results show that the system provides satisfactory performance with recognition rate as high as 90.67\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Das et al_2015_Sign Language Recognition Using Facial Expression.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TG7P2KCJ/S1877050915021675.html},
  journal = {Procedia Computer Science},
  series = {Second {{International Symposium}} on {{Computer Vision}} and the {{Internet}} ({{VisionNet}}'15)}
}

@inproceedings{dasilveira_visual_2003,
  title = {Visual Speech Recognition: A Solution from Feature Extraction to Words Classification},
  shorttitle = {Visual Speech Recognition},
  booktitle = {Brazilian {{Symposium}} on {{Computer Graphics}} and {{Image Processing}}},
  author = {Da Silveira, Luciana Gon{\textbackslash}ccalves and Facon, Jacques and Borges, D{\'i}bio Leandro},
  year = {2003},
  pages = {399--405},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Da Silveira et al_2003_Visual speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VERZPPTC/login.html}
}

@article{davis_automatic_1952,
  title = {Automatic {{Recognition}} of {{Spoken Digits}}},
  author = {Davis, K. H. and Biddulph, R. and Balashek, S.},
  year = {1952},
  month = nov,
  volume = {24},
  pages = {637--642},
  issn = {0001-4966},
  doi = {10.1121/1.1906946},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Davis et al_1952_Automatic Recognition of Spoken Digits.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EQXZCYNY/1.html},
  journal = {The Journal of the Acoustical Society of America},
  number = {6}
}

@article{davis_survey_1975,
  title = {A Survey of Edge Detection Techniques},
  author = {Davis, Larry S.},
  year = {1975},
  month = sep,
  volume = {4},
  pages = {248--270},
  issn = {0146-664X},
  doi = {10.1016/0146-664X(75)90012-X},
  abstract = {Methods of detecting ``edges,'' i.e., boundaries between regions in a picture, are reviewed. Included are both parallel (linear, nonlinear, optimal) and sequential methods, as well as methods using planning or a priori knowledge.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Davis_1975_A survey of edge detection techniques.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GPCPJ7US/0146664X7590012X.html},
  journal = {Computer Graphics and Image Processing},
  number = {3}
}

@article{davydov_weighted_2013,
  title = {{{WEIGHTED OPTICAL FLOW MODEL FOR VISEMES RECOGNITION IN SIGN LANGUAGE TUTORING SOFTWARE}}},
  author = {Davydov, M. V. and Nikolski, Yu V. and Tykhanskyi, S. M.},
  year = {2013},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Davydov et al_2013_WEIGHTED OPTICAL FLOW MODEL FOR VISEMES RECOGNITION IN SIGN LANGUAGE TUTORING.pdf}
}

@inproceedings{DBLP:conf/fgr/KirishimaSNC98,
  title = {Realtime {{Gesture Recognition}} under the {{Multi}}-{{Layered Parallel Recognition Framework}} of {{QVIPS}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Kirishima, Toshiyuki and Sato, Kosuke and Narita, Hirokazu and Chihara, Kunihiro},
  year = {1998},
  month = apr,
  pages = {579--584},
  address = {{Nara, Japan}},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  biburl = {http://dblp2.uni-trier.de/rec/bib/conf/fgr/KirishimaSNC98},
  crossref = {DBLP:conf/fgr/1998}
}

@inproceedings{de_coster_sign_2020,
  title = {Sign Language Recognition with Transformer Networks},
  booktitle = {12th {{International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2020), {{Proceedings}}},
  author = {De Coster, Mathieu and Van Herreweghe, Mieke and Dambre, Joni},
  year = {2020},
  pages = {6018--6024},
  publisher = {{European Language Resources Association (ELRA)}},
  issn = {2522-2686},
  abstract = {Sign languages are complex languages. Research into them is ongoing, supported by large video corpora of which only small parts are annotated. Sign language recognition can be used to speed up the annotation process of these corpora, in order to aid research into sign languages and sign language recognition. Previous research has approached sign language recognition in various ways, using feature extraction techniques or end-to-end deep learning. In this work, we apply a combination of feature extraction using OpenPose for human keypoint estimation and end-to-end feature learning with Convolutional Neural Networks. The proven multi-head attention mechanism used in transformers is applied to recognize isolated signs in the Flemish Sign Language corpus. Our proposed method significantly outperforms the previous state of the art of sign language recognition on the Flemish Sign Language corpus: we obtain an accuracy of 74.7\% on a vocabulary of 100 classes. Our results will be implemented as a suggestion system for sign language corpus annotation.},
  copyright = {Creative Commons Attribution-NonCommercial 4.0 International Public License (CC BY-NC 4.0)},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/De Coster et al_2020_Sign language recognition with transformer networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WY73UE8J/8660743.html},
  isbn = {979-10-95546-34-4},
  keywords = {capturedBy=vision,isolated,isolated;vocab=100;signer=67;sl=FlemishSL;capturedBy=vision,modal=bodyjoints,modal=fullframe,signer=67,sl=FlemishSL,slreco,slreco;modal=bodyjoints,slreco;modal=fullframe,vocab<500,vocab=100},
  language = {eng}
}

@article{dearaujo_accessibility_2014,
  title = {Accessibility as a Service: {{Augmenting}} Multimedia Content with Sign Language Video Tracks},
  shorttitle = {Accessibility as a Service},
  author = {{de Ara{\'u}jo}, Tiago Maritan U. and Ferreira, Felipe Lacet S. and Silva, Danilo Assis Nobre S. and {de Lucena Falcao}, Eduardo and {de Oliveira}, Leonardo Dantas and {de Araujo Domingues}, Leonardo and Nobrega, Yurika Sato and Lima, Hozana Raquel G. and Duarte, Alexandre Nobrega and others},
  year = {2014},
  volume = {45},
  pages = {79},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/de Araújo et al_2014_Accessibility as a service.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/93CFHNTQ/documentSummary\;dn=147949922205839\;res=IELHSS.html},
  journal = {Journal of Research and Practice in Information Technology},
  number = {2}
}

@article{dehak_frontend_2011,
  title = {Front-End Factor Analysis for Speaker Verification},
  author = {Dehak, Najim and Kenny, Patrick J. and Dehak, R{\'e}da and Dumouchel, Pierre and Ouellet, Pierre},
  year = {2011},
  volume = {19},
  pages = {788--798},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dehak et al_2011_Front-end factor analysis for speaker verification.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TVWWWI4P/5545402.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  number = {4}
}

@article{delagorce_modelbased_2011,
  title = {Model-Based 3d Hand Pose Estimation from Monocular Video},
  author = {{de La Gorce}, Martin and Fleet, David J. and Paragios, Nikos},
  year = {2011},
  volume = {33},
  pages = {1793--1805},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/de La Gorce et al_2011_Model-based 3d hand pose estimation from monocular video.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Q663FEHF/login.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {9}
}

@article{demeulder_legal_2015,
  title = {The Legal Recognition of Sign Languages},
  author = {De Meulder, Maartje},
  year = {2015},
  volume = {15},
  pages = {498--506},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/De Meulder_2015_The legal recognition of sign languages.pdf},
  journal = {Sign Language Studies},
  number = {4}
}

@article{dempster_maximum_1977,
  title = {Maximum {{Likelihood}} from {{Incomplete Data}} via the {{EM Algorithm}}},
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  year = {1977},
  volume = {39},
  pages = {1--38},
  issn = {00359246},
  abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dempster et al_1977_Maximum Likelihood from Incomplete Data via the EM Algorithm.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number = {1}
}

@inproceedings{deng_binary_2010,
  title = {Binary Coding of Speech Spectrograms Using a Deep Auto-Encoder},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Deng, Li and Seltzer, Michael L. and Yu, Dong and Acero, Alex and Mohamed, Abdel-rahman and Hinton, Geoff},
  year = {2010},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Deng et al_2010_Binary coding of speech spectrograms using a deep auto-encoder.pdf}
}

@incollection{deng_deep_2014,
  title = {{{DEEP LEARNING}}: {{Methods}} and {{Applications}}},
  author = {Deng, Li and Yu, Dong},
  year = {2014},
  publisher = {{Now Publishers}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Deng_Yu_2014_DEEP LEARNING.pdf}
}

@inproceedings{deng_recent_2013,
  title = {Recent Advances in Deep Learning for Speech Research at {{Microsoft}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Deng, Li and Li, Jinyu and Huang, Jui-Ting and Yao, Kaisheng and Yu, Dong and Seide, Frank and Seltzer, Michael and Zweig, Geoff and He, Xiaodong and Williams, Jason and others},
  year = {2013},
  pages = {8604--8608},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Deng et al_2013_Recent advances in deep learning for speech research at Microsoft.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/869IX57N/6639345.html}
}

@inproceedings{deng_twostep_2002,
  title = {A {{Two}}-Step {{Approach}} Based on {{PaHMM}} for the {{Recognition}} of {{ASL}}},
  booktitle = {Asian {{Conf}}. on {{Computer Vision}} ({{ACCV}})},
  author = {Deng, Jiangwen and Tsui, H. T.},
  year = {2002},
  month = jan,
  pages = {126--131},
  address = {{Melbourne, Australia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Deng_Tsui_2002_A Two-step Approach based on PaHMM for the Recognition of ASL.pdf},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=192;signer=2;sl=ASL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=2,sl=ASL,slreco,slreco;modal=2hshape;modal=2horientation;modal=2hmovement;modal=2hlocation,vocab<500,vocab=192}
}

@incollection{denmark_using_2015,
  title = {Using and {{Developing Language}} and {{Cognitive Assessments}} with {{Deaf Signers}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Denmark, Tanya and Atkinson, Joanna},
  year = {2015},
  pages = {352--368},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch19},
  abstract = {This chapter highlights that researchers interested in assessments need separate, specifically designed tests of language and cognition for deaf sign language users rather than relying on tests designed for users of spoken languages. Tests that are developed specifically for deaf signers and that produce deaf norms are an invaluable tool in both sign language research and clinical practice. When deaf signers are given spoken language-based tests designed to be suitable for hearing populations, there is a potential for linguistic and cultural biases to occur, which can lead to an unreliable assessment. The chapter reviews the language and cognitive assessments that have been developed for deaf children and adults to date, demonstrating examples of good practice. It presents a range of important points to consider when conducting assessments with deaf signers. The chapter concludes with considerations for developing future assessments.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Denmark_Atkinson_2015_Using and Developing Language and Cognitive Assessments with Deaf Signers.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/S3DUX83Z/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@article{derpanis_definition_2008,
  title = {Definition and Recovery of Kinematic Features for Recognition of {{American}} Sign Language Movements},
  author = {Derpanis, K and Wildes, R and Tsotsos, J},
  year = {2008},
  month = dec,
  volume = {26},
  pages = {1650--1662},
  issn = {02628856},
  doi = {10.1016/j.imavis.2008.04.007},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Derpanis et al_2008_Definition and recovery of kinematic features for recognition of American sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BER87DSW/search.html},
  journal = {Image and Vision Computing},
  keywords = {capturedBy=vision,isolated,isolated;vocab=148;signer=3;sl=ASL;capturedBy=vision,modal=movement,signer=3,sl=ASL,slreco,slreco;modal=movement,vocab<500,vocab=148},
  number = {12}
}

@inproceedings{deselaers_bildsuche_2009,
  title = {Bildsuche, {{Objekterkennung}} Und {{Diskriminative Modelle}}},
  booktitle = {Ausgezeichnete {{Informatikdissertationen}} 2008},
  author = {Deselaers, T.},
  year = {2009}
}

@phdthesis{deselaers_image_2008,
  title = {Image {{Retrieval}}, {{Object Recognition}}, and {{Discriminative Models}}},
  author = {Deselaers, Thomas},
  year = {2008},
  month = dec,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Deselaers_2008_Image Retrieval, Object Recognition, and Discriminative Models.pdf},
  school = {Computer Science Department, RWTH Aachen University}
}

@inproceedings{deselaers_pan_2008,
  title = {Pan, {{Zoom}}, {{Scan}} \textendash{} {{Time}}-Coherent, {{Trained Automatic Video Cropping}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Deselaers, Thomas and Dreuw, Philippe and Ney, Hermann},
  year = {2008},
  month = jun,
  address = {{Anchorage, AK, USA}}
}

@phdthesis{desoky:phd2014,
  title = {Sub-{{Word Based Language Modeling}} of {{Morphologically Rich Languages}} for {{LVCSR}}},
  author = {{El-Desoky Mousa}, Amr},
  year = {2014},
  month = jun,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/El-Desoky Mousa_2014_Sub-Word Based Language Modeling of Morphologically Rich Languages for LVCSR.pdf},
  school = {RWTH Aachen University}
}

@article{devlin_bert_2018,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  month = oct,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Devlin et al_2018_BERT.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CTZTPY8D/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  primaryClass = {cs}
}

@article{devos_sign_2015,
  title = {Sign {{Language Typology}}: {{The Contribution}} of {{Rural Sign Languages}}},
  shorttitle = {Sign {{Language Typology}}},
  author = {{de Vos}, Connie and Pfau, Roland},
  year = {2015},
  volume = {1},
  pages = {265--288},
  doi = {10.1146/annurev-linguist-030514-124958},
  abstract = {Since the 1990s, the field of sign language typology has shown that sign languages exhibit typological variation at all relevant levels of linguistic description. These initial typological comparisons were heavily skewed toward the urban sign languages of developed countries, mostly in the Western world. This review reports on the recent contributions made by rural signing varieties, that is, sign languages that have evolved in village communities, often in developing countries, due to a high incidence of deafness. With respect to a number of structural properties, rural sign languages fit into previously established typological classifications. However, they also exhibit unique and typologically marked features that challenge received views on possible sign languages. At the same time, the shared features of geographically dispersed rural signing varieties provide a unique window into the social dynamics that may shape the structures of modern human languages.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/de Vos_Pfau_2015_Sign Language Typology.pdf},
  journal = {Annual Review of Linguistics},
  number = {1}
}

@article{dgscorpus_2,
  title = {{{MY DGS}} \textendash{} Annotated. {{Public}} Corpus of {{German Sign Language}}, 2nd Release},
  author = {Konrad, Reiner and Hanke, Thomas and Langer, Gabriele and Blanck, Dolly and Bleicken, Julian and Hofmann, Ilona and Jeziorski, Olga and K{\"o}nig, Lutz and K{\"o}nig, Susanne and Nishio, Rie and Regen, Anja and Salden, Uta and Wagner, Sven and Worseck, Satu},
  year = {2019},
  publisher = {{Universit\"at Hamburg}},
  doi = {10.25592/dgs-corpus-2.0},
  type = {Languageresource},
  version = {2.0}
}

@article{dharo_language_2008,
  title = {Language {{Model Adaptation For}} a {{Speech}} to {{Sign Language Translation System Using Web Frequencies}} and a {{Map Framework}}},
  author = {D'Haro, Luis Fernando and {San-Segundo}, Rub{\'e}n and de C{\'o}rdoba, Ricardo and Bungeroth, Jan and Stein, Daniel and Ney, Hermann},
  year = {2008},
  month = sep,
  journal = {Proc. of the Ann. Conf. of the Int. Speech Commun. Assoc. (Interspeech)}
}

@inproceedings{dias_hand_2009,
  title = {Hand Movement Recognition for {{Brazilian Sign Language}}: {{A}} Study Using Distance-Based Neural Networks},
  shorttitle = {Hand Movement Recognition for {{Brazilian Sign Language}}},
  booktitle = {Proc. {{Int}}. {{Joint Conf}}. on {{Neural Networks}} ({{IJCNN}})},
  author = {Dias, D.B. and Madeo, R.C.B. and Rocha, T. and Biscaro, H.H. and Peres, S.M.},
  year = {2009},
  month = jun,
  pages = {697--704},
  doi = {10.1109/IJCNN.2009.5178917},
  abstract = {In this paper, the vision-based hand movement recognition problem is formulated for the universe of discourse of the Brazilian Sign Language. In order to analyze this specific domain we have used the artificial neural networks models based on distance, including neural-fuzzy models. The experiments explored here show the usefulness of these models to extract helpful knowledge about the classes of movements and to support the project of adaptative recognizer modules for Libras-oriented computational tools. Using artificial neural networks architectures - Self Organizing Maps and (Fuzzy) Learning Vector Quantization, it was possible to understand the data space and to build models able to recognize hand movements performed for one or more than one specific Libras users.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dias et al_2009_Hand movement recognition for Brazilian Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/I9T9SKR4/abs_all.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=15;signer=4;sl=Libras;capturedBy=vision,modal=movement,signer=4,sl=Libras,slreco,slreco;modal=movement,vocab<50,vocab=15}
}

@article{diba_temporal_2017,
  title = {Temporal {{3D ConvNets}}: {{New Architecture}} and {{Transfer Learning}} for {{Video Classification}}},
  shorttitle = {Temporal {{3D ConvNets}}},
  author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Karami, Amir Hossein and Arzani, Mohammad Mahdi and Yousefzadeh, Rahman and Van Gool, Luc},
  year = {2017},
  month = nov,
  abstract = {The work in this paper is driven by the question how to exploit the temporal cues available in videos for their accurate classification, and for human action recognition in particular? Thus far, the vision community has focused on spatio-temporal approaches with fixed temporal convolution kernel depths. We introduce a new temporal layer that models variable temporal convolution kernel depths. We embed this new temporal layer in our proposed 3D CNN. We extend the DenseNet architecture - which normally is 2D - with 3D filters and pooling kernels. We name our proposed video convolutional network `Temporal 3D ConvNet'\textasciitilde (T3D) and its new temporal layer `Temporal Transition Layer'\textasciitilde (TTL). Our experiments show that T3D outperforms the current state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets. The other issue in training 3D ConvNets is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in 2D ConvNets is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for 3D CNNs. Thus, by finetuning this network, we beat the performance of generic and recent methods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M, and finetuned on the target datasets, e.g. HMDB51/UCF101. The T3D codes will be released},
  archivePrefix = {arXiv},
  eprint = {1711.08200},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6KKCY2JN/1711.html},
  journal = {arXiv:1711.08200 [cs]},
  primaryClass = {cs}
}

@incollection{dick_visual_2006,
  title = {Visual Hand Posture Recognition in Monocular Image Sequences},
  booktitle = {Pattern {{Recognition}}},
  author = {Dick, Thorsten and Zieren, J{\"o}rg and Kraiss, Karl-Friedrich},
  year = {2006},
  pages = {566--575},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dick et al_2006_Visual hand posture recognition in monocular image sequences.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2U5DTX8X/11861898_57.html}
}

@inproceedings{dilsizian_new_2014,
  title = {A {{New Framework}} for {{Sign Language Recognition}} Based on {{3D Handshape Identification}} and {{Linguistic Modeling}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Dilsizian, Mark and Yanovich, Polina and Wang, Shu and Neidle, Carol and Metaxas, Dimitris},
  year = {2014},
  month = may,
  pages = {1924--1929},
  address = {{Reykjav\'ik, Iceland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dilsizian et al_2014_A New Framework for Sign Language Recognition based on 3D Handshape.pdf}
}

@article{ding_modelling_2009,
  title = {Modelling and Recognition of the Linguistic Components in {{American Sign Language}}},
  author = {Ding, Liya and Martinez, Aleix M.},
  year = {2009},
  month = nov,
  volume = {27},
  pages = {1826--1844},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2009.02.005},
  abstract = {The manual signs in sign languages are generated and interpreted using three basic building blocks: handshape, motion, and place of articulation. When combined, these three components (together with palm orientation) uniquely determine the meaning of the manual sign. This means that the use of pattern recognition techniques that only employ a subset of these components is inappropriate for interpreting the sign or to build automatic recognizers of the language. In this paper, we define an algorithm to model these three basic components form a single video sequence of two-dimensional pictures of a sign. Recognition of these three components are then combined to determine the class of the signs in the videos. Experiments are performed on a database of (isolated) American Sign Language (ASL) signs. The results demonstrate that, using semi-automatic detection, all three components can be reliably recovered from two-dimensional video sequences, allowing for an accurate representation and recognition of the signs. (C) 2009 Elsevier B.V. All rights reserved.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ding_Martinez_2009_Modelling and recognition of the linguistic components in American Sign Language.pdf},
  journal = {Image and Vision Computing},
  keywords = {capturedBy=vision,isolated,isolated;vocab=38;signer=10;sl=ASL;capturedBy=vision,modal=location,modal=movement,modal=shape,signer=10,sl=ASL,slreco,slreco;modal=location;modal=movement;modal=shape,vocab<50,vocab=38},
  language = {English},
  number = {12}
}

@inproceedings{ding_threedimensional_2006,
  title = {Three-Dimensional Shape and Motion Reconstruction for the Analysis of American Sign Language},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Ding, Liya and Martinez, Aleix M.},
  year = {2006},
  pages = {146--146},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ding_Martinez_2006_Three-dimensional shape and motion reconstruction for the analysis of american.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J4R9WKP7/login.html}
}

@inproceedings{dobbriner_dialect_2019,
  title = {Towards a Dialect Classification in {{German}} Speech Samples},
  booktitle = {International {{Conference}} on {{Speech}} and {{Computer}}},
  author = {Dobbriner, Johanna and Jokisch, Oliver},
  year = {2019},
  pages = {64--74},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dobbriner_Jokisch_2019_Towards a dialect classification in German speech samples.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M4QFEA4J/978-3-030-26061-3_7.html}
}

@article{doersch_unsupervised_2015,
  title = {Unsupervised {{Visual Representation Learning}} by {{Context Prediction}}},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  year = {2015},
  month = may,
  abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
  archivePrefix = {arXiv},
  eprint = {1505.05192},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Doersch et al_2015_Unsupervised Visual Representation Learning by Context Prediction.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2NQK8IEF/1505.html},
  journal = {arXiv:1505.05192 [cs]},
  primaryClass = {cs}
}

@article{doetsch_inverted_2017,
  title = {Inverted {{Alignments}} for {{End}}-to-{{End Automatic Speech Recognition}}},
  author = {Doetsch, P. and Hannemann, M. and Schl{\"u}ter, R. and Ney, H.},
  year = {2017},
  volume = {PP},
  pages = {1--1},
  issn = {1932-4553},
  doi = {10.1109/JSTSP.2017.2752691},
  abstract = {In this work, we propose an inverted alignment approach for sequence classification systems like automatic speech recognition (ASR) that naturally incorporates discriminative, artificial neural network based label distributions. Instead of aligning each input frame to a state label as in the standard hidden Markov model (HMM) derivation, we propose to inversely align each element of an HMM state label sequence to a segmentwise encoding of several consecutive input frames. This enables an integrated discriminative model that can be trained end-toend from scratch or starting from an existing alignment path. The approach does not assume the usual decomposition into a separate (generative) acoustic model and a language model, and allows for a variety of model assumptions, including statistical variants of attention. Following our initial paper with proof-ofconcept experiments on handwriting recognition, the focus of this work was the investigation of integrated training and an inverted decoding approach, whereas the acoustic modeling still remains largely similar to standard hybrid modeling. We provide experiments on the CHiME-4 noisy ASR task. Our results show that we can reach competitive results with inverted alignment and decoding strategies.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Doetsch et al_2017_Inverted Alignments for End-to-End Automatic Speech Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NBQKRAAJ/8038027.html},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  number = {99}
}

@inproceedings{donahue_longterm_2015,
  title = {Long-{{Term Recurrent Convolutional Networks}} for {{Visual Recognition}} and {{Description}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
  year = {2015},
  pages = {2625--2634},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Donahue et al_2015_Long-Term Recurrent Convolutional Networks for Visual Recognition and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XG7W6WAM/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.html}
}

@article{donato_classifying_1999,
  title = {Classifying {{Facial Actions}}},
  author = {Donato, Gianluca and Bartlett, Marian Stewart and Hager, Joseph C. and Ekman, Paul and Sejnowski, Terrence J.},
  year = {1999},
  volume = {21},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Donato et al_1999_Classifying Facial Actions.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {10}
}

@inproceedings{dondera_kernel_2011,
  title = {Kernel {{PLS}} Regression for Robust Monocular Pose Estimation},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Dondera, Radu and Davis, Larry},
  year = {2011},
  pages = {24--30},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dondera_Davis_2011_Kernel PLS regression for robust monocular pose estimation.pdf}
}

@inproceedings{dos_santos_deep_2014,
  title = {Deep {{Convolutional Neural Networks}} for {{Sentiment Analysis}} of {{Short Texts}}},
  booktitle = {Proc. {{COLING Int}}. {{Conf}}. on {{Computational Linguistics}}},
  author = {{dos Santos}, Cicero and Gatti, Maira},
  year = {2014},
  month = aug,
  pages = {69--78},
  publisher = {{Dublin City University and Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/dos Santos_Gatti_2014_Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts.pdf}
}

@phdthesis{dreuw_appearancebased_2005,
  title = {Appearance-{{Based Gesture Recognition}}},
  author = {Dreuw, Philippe},
  year = {2005},
  month = jan,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw_2005_Appearance-Based Gesture Recognition.pdf},
  school = {Human Language Technology and Pattern Recognition Group, RWTH Aachen University}
}

@inproceedings{dreuw_automatic_2008,
  title = {Towards {{Automatic Sign Language Annotation}} for the {{ELAN Tool}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Dreuw, Philippe and Ney, Hermann},
  year = {2008},
  month = jun,
  address = {{Marrakech, Morocco}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw_Ney_2008_Towards Automatic Sign Language Annotation for the ELAN Tool.pdf}
}

@inproceedings{dreuw_benchmark_2008,
  title = {Benchmark {{Databases}} for {{Video}}-{{Based Automatic Sign Language Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Dreuw, Philippe and Neidle, Carol and Athitsos, Vassilis and Sclaroff, Stan and Ney, Hermann},
  year = {2008},
  month = may,
  address = {{Marrakech, Morocco}}
}

@inproceedings{dreuw_efficient_2008,
  title = {Efficient {{Approximations}} to {{Model}}-Based {{Joint Tracking}} and {{Recognition}} of {{Continuous Sign Language}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Dreuw, Philippe and Forster, Jens and Deselaers, Thomas and Ney, Hermann},
  year = {2008},
  month = sep,
  pages = {1--6},
  address = {{Amsterdam, The Netherlands}}
}

@inproceedings{dreuw_enhancing_2007,
  title = {Enhancing a {{Sign Language Translation System}} with {{Vision}}-{{Based Features}}},
  booktitle = {International {{Workshop}} on {{Gesture}} in {{Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {Dreuw, Philippe and Stein, Daniel and Ney, Hermann},
  year = {2007},
  month = may,
  pages = {18--20},
  address = {{Lisbon, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw et al_2007_Enhancing a Sign Language Translation System with Vision-Based Features.pdf}
}

@article{dreuw_enhancing_2009,
  title = {Enhancing a {{Sign Language Translation System}} with {{Vision}}-{{Based Features}}},
  author = {Dreuw, Philippe and Stein, Daniel and Ney, Hermann},
  year = {2009},
  month = jan,
  volume = {5085},
  pages = {108--113},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw et al_2009_Enhancing a Sign Language Translation System with Vision-Based Features.pdf},
  journal = {Gesture-Based Human-Computer Interaction and Simulation},
  number = {1},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{dreuw_gesture_2005,
  title = {Gesture {{Recognition Using Image Comparison Methods}}},
  booktitle = {International {{Workshop}} on {{Gesture}} in {{Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {Dreuw, Philippe and Keysers, Daniel and Deselaers, Thomas and Ney, Hermann},
  editor = {Gibet, S. and Courty, N. and Kamp, J. -F},
  year = {2005},
  month = may,
  volume = {3881},
  pages = {124--128},
  address = {{Ile-de-Berder, France}},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{dreuw_modeling_2006,
  title = {Modeling {{Image Variability}} in {{Appearance}}-{{Based Gesture Recognition}}},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Dreuw, Philippe and Deselaers, Thomas and Keysers, Daniel and Ney, Hermann},
  year = {2006},
  month = may,
  pages = {7--18},
  address = {{Graz, Austria}}
}

@phdthesis{dreuw_probabilistic_2012,
  title = {Probabilistic {{Sequence Models}} for {{Image Sequence Processing}} and {{Recognition}}},
  author = {Dreuw, Philippe},
  year = {2012},
  month = apr,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw_2012_Probabilistic Sequence Models for Image Sequence Processing and Recognition.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=head,modal=movement,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=movement;modal=shape;modal=head,vocab<500,vocab=103},
  school = {RWTH Aachen University}
}

@inproceedings{dreuw_signspeak_2009,
  title = {{{SignSpeak}} - {{Bridging}} the {{Gap Between Signers}} and {{Speakers}}},
  booktitle = {Elektronische {{Sprachsignalverarbeitung}}},
  author = {Dreuw, Philippe and Ney, Hermann},
  year = {2009},
  month = sep,
  pages = {278--286},
  address = {{Dresden, Germany}}
}

@inproceedings{dreuw_signspeak_2010,
  title = {The {{SignSpeak Project}} - {{Bridging}} the {{Gap Between Signers}} and {{Speakers}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Dreuw, Philippe and Ney, Hermann and Martinez, Gregorio and Crasborn, Onno and Piater, Justus and Miguel Moya, Jose and Wheatley, Mark},
  year = {2010},
  month = may,
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw et al_2010_The SignSpeak Project - Bridging the Gap Between Signers and Speakers.pdf}
}

@inproceedings{dreuw_signspeak_2010a,
  title = {{{SignSpeak}} - {{Understanding}}, {{Recognition}}, and {{Translation}} of {{Sign Languages}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Dreuw, Philippe and Forster, Jens and Gweth, Yannick and Stein, Daniel and Ney, Hermann and Martinez, Gregorio and Verges Llahi, Jaume and Crasborn, Onno and Ormel, Ellen and Du, Wei and Hoyoux, Thomas and Piater, Justus and Moya Lazaro, Jose Miguel and Wheatley, Mark},
  year = {2010},
  month = may,
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw et al_2010_SignSpeak - Understanding, Recognition, and Translation of Sign Languages.pdf}
}

@inproceedings{dreuw_smoothed_2009,
  title = {Smoothed {{Disparity Maps}} for {{Continuous American Sign Language Recognition}}},
  booktitle = {Proc. {{Iberian Conf}}. on {{Pattern Recognition}} and {{Image Analysis}} ({{IbPRIA}})},
  author = {Dreuw, Philippe and Steingrube, Pascal and Deselaers, Thomas and Ney, Hermann},
  year = {2009},
  month = jun,
  pages = {24--31},
  address = {{P\'ovoa de Varzim, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw et al_2009_Smoothed Disparity Maps for Continuous American Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=movement,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=movement;modal=shape,vocab<500,vocab=103},
  series = {{{LNCS}}}
}

@inproceedings{dreuw_speech_2007,
  title = {Speech {{Recognition Techniques}} for a {{Sign Language Recognition System}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Dreuw, Philippe and Rybach, David and Deselaers, Thomas and Zahedi, Morteza and Ney, Hermann},
  year = {2007},
  month = aug,
  pages = {2513--2516},
  address = {{Antwerp, Belgium}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw et al_2007_Speech Recognition Techniques for a Sign Language Recognition System.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=location,modal=movement,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=location;modal=movement,vocab<500,vocab=103}
}

@article{dreuw_spoken_2008,
  title = {Spoken {{Language Processing Techniques}} for {{Sign Language Recognition}} and {{Translation}}},
  author = {Dreuw, Philippe and Stein, Daniel and Deselaers, Thomas and Rybach, David and Zahedi, Morteza and Bungeroth, Jan and Ney, Hermann},
  editor = {Soede, Dr. Ir. M},
  year = {2008},
  month = jun,
  volume = {20},
  pages = {121--133},
  journal = {Technology and Dissability},
  number = {2}
}

@inproceedings{dreuw_surfface_2009,
  title = {{{SURF}}-{{Face}}: {{Face Recognition Under Viewpoint Consistency Constraints}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Dreuw, Philippe and Steingrube, Pascal and Hanselmann, Harald and Ney, Hermann},
  year = {2009},
  month = sep,
  address = {{London, UK}}
}

@inproceedings{dreuw_tracking_2006,
  title = {Tracking {{Using Dynamic Programming}} for {{Appearance}}-{{Based Sign Language Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Dreuw, Philippe and Deselaers, Thomas and Rybach, David and Keysers, Daniel and Ney, Hermann},
  year = {2006},
  month = apr,
  pages = {293--298},
  address = {{Southampton, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw et al_2006_Tracking Using Dynamic Programming for Appearance-Based Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5A9Z7K3Z/cookiedetectresponse.html}
}

@inproceedings{dreuw_tracking_2010,
  title = {Tracking {{Benchmark Databases}} for {{Video}}-{{Based Sign Language Recognition}}},
  booktitle = {{{ECCV International Workshop}} on {{Sign}}, {{Gesture}}, and {{Activity}}},
  author = {Dreuw, Philippe and Forster, Jens and Ney, Hermann},
  year = {2010},
  month = sep,
  pages = {286--297},
  publisher = {{Springer}},
  address = {{Crete, Greece}}
}

@inproceedings{dreuw_visual_2008,
  title = {Visual {{Modeling}} and {{Feature Adaptation}} in {{Sign Language Recognition}}},
  booktitle = {{{ITG Conference}} on {{Speech Communication}}},
  author = {Dreuw, Philippe and Ney, Hermann},
  year = {2008},
  month = oct,
  pages = {1--4},
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw_Ney_2008_Visual Modeling and Feature Adaptation in Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=104;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=shape,vocab<500,vocab=104}
}

@inproceedings{dreuw_visual_2008a,
  title = {Visual {{Modeling}} and {{Tracking Adaptation}} for {{Automatic Sign Language Recognition}}},
  booktitle = {International {{Computer Vision Summer School}}},
  author = {Dreuw, Philippe},
  year = {2008},
  month = jul,
  address = {{Sicily, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dreuw_2008_Visual Modeling and Tracking Adaptation for Automatic Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=movement,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=movement;modal=shape,vocab<500,vocab=103}
}

@inproceedings{drigas_performance_2006,
  title = {Performance {{Evaluation}} of {{Sign}}-{{Language Videoconference Traffic}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Innovations}} in {{Information Technology}}},
  author = {Drigas, Athanasios and Kouremenos, Stelios and Kouremenos, Dimitris},
  year = {2006},
  pages = {1--6},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Drigas et al_2006_Performance Evaluation of Sign-Language Videoconference Traffic.pdf}
}

@inproceedings{drouard_head_2015,
  title = {Head Pose Estimation via Probabilistic High-Dimensional Regression},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Drouard, V. and Ba, S. and Evangelidis, G. and Deleforge, A. and Horaud, R.},
  year = {2015},
  month = sep,
  pages = {4624--4628},
  doi = {10.1109/ICIP.2015.7351683},
  abstract = {This paper addresses the problem of head pose estimation with three degrees of freedom (pitch, yaw, roll) from a single image. Pose estimation is formulated as a high-dimensional to low-dimensional mixture of linear regression problem. We propose a method that maps HOG-based descriptors, extracted from face bounding boxes, to corresponding head poses. To account for errors in the observed bounding-box position, we learn regression parameters such that a HOG descriptor is mapped onto the union of a head pose and an offset, such that the latter optimally shifts the bounding box towards the actual position of the face in the image. The performance of the proposed method is assessed on publicly available datasets. The experiments that we carried out show that a relatively small number of locally-linear regression functions is sufficient to deal with the non-linear mapping problem at hand. Comparisons with state-of-the-art methods show that our method outperforms several other techniques.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Drouard et al_2015_Head pose estimation via probabilistic high-dimensional regression.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JSHRE5VB/articleDetails.html}
}

@inproceedings{du_hierarchical_2015,
  title = {Hierarchical {{Recurrent Neural Network}} for {{Skeleton Based Action Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Du, Yong and Wang, Wei and Wang, Liang},
  year = {2015},
  pages = {1110--1118},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Du et al_2015_Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WBMVHBZU/Du_Hierarchical_Recurrent_Neural_2015_CVPR_paper.html}
}

@article{duan_multimodality_2016,
  title = {Multi-{{Modality Fusion}} Based on {{Consensus}}-{{Voting}} and {{3D Convolution}} for {{Isolated Gesture Recognition}}},
  author = {Duan, Jiali and Zhou, Shuai and Wan, Jun and Guo, Xiaoyuan and Li, Stan Z.},
  year = {2016},
  month = nov,
  abstract = {Recently, the popularity of depth-sensors such as Kinect has made depth videos easily available while its advantages have not been fully exploited. This paper investigates, for gesture recognition, to explore the spatial and temporal information complementarily embedded in RGB and depth sequences. We propose a convolutional twostream consensus voting network (2SCVN) which explicitly models both the short-term and long-term structure of the RGB sequences. To alleviate distractions from background, a 3d depth-saliency ConvNet stream (3DDSN) is aggregated in parallel to identify subtle motion characteristics. These two components in an unified framework significantly improve the recognition accuracy. On the challenging Chalearn IsoGD benchmark, our proposed method outperforms the first place on the leader-board by a large margin (10.29\%) while also achieving the best result on RGBD-HuDaAct dataset (96.74\%). Both quantitative experiments and qualitative analysis shows the effectiveness of our proposed framework and codes will be released to facilitate future research.},
  archivePrefix = {arXiv},
  eprint = {1611.06689},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Duan et al_2016_Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for Isolated.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EBG756D4/1611.html},
  journal = {arXiv:1611.06689 [cs]},
  primaryClass = {cs}
}

@article{dubot_sign_2014,
  title = {Sign {{Language Gibberish}} for Syntactic Parsing Evaluation},
  author = {Dubot, R{\'e}mi and Collet, Christophe},
  year = {2014},
  month = mar,
  abstract = {Sign Language (SL) automatic processing slowly progresses bottom-up. The field has seen proposition to handle the video signal, to recognize and synthesize sublexical and lexical units. It starts to see the development of supra-lexical processing. But the recognition, at this level, lacks data. The syntax of SL appears very specific as it uses massively the multiplicity of articulators and its access to the spatial dimensions. Therefore new parsing techniques are developed. However these need to be evaluated. The shortage on real data restrains the corpus-based models to small sizes. We propose here a solution to produce data-sets for the evaluation of parsers on the specific properties of SL. The article first describes the general model used to generates dependency grammars and the phrase generation from these lasts. It then discusses the limits of approach. The solution shows to be of particular interest to evaluate the scalability of the techniques on big models.},
  archivePrefix = {arXiv},
  eprint = {1403.4473},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dubot_Collet_2014_Sign Language Gibberish for syntactic parsing evaluation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IEXTCS6U/1403.html},
  journal = {arXiv:1403.4473 [cs]},
  primaryClass = {cs}
}

@book{duda_pattern_2000,
  title = {Pattern {{Classification}}},
  author = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
  year = {2000},
  month = nov,
  edition = {2 edition},
  publisher = {{Wiley-Interscience}},
  address = {{New York}},
  abstract = {The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics. An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Duda et al_2000_Pattern Classification.pdf},
  isbn = {978-0-471-05669-0},
  language = {English}
}

@article{dudley_automatic_1958,
  title = {Automatic {{Recognition}} of {{Phonetic Patterns}} in {{Speech}}},
  author = {Dudley, Homer and Balashek, S.},
  year = {1958},
  month = aug,
  volume = {30},
  pages = {721--732},
  issn = {0001-4966},
  doi = {10.1121/1.1909742},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dudley_Balashek_1958_Automatic Recognition of Phonetic Patterns in Speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JYPDSYMF/1.html},
  journal = {The Journal of the Acoustical Society of America},
  number = {8}
}

@phdthesis{duffner_face_2013,
  title = {Face {{Image Analysis}} with {{Convolutional Neural Networks}}},
  author = {Duffner, Stefan},
  year = {2013},
  month = oct,
  address = {{M\"unchen; Ravensburg}},
  abstract = {Doctoral Thesis / Dissertation from the year 2008 in the subject Computer Science - Applied, grade: 1, University of Freiburg (Lehrstuhl f\"ur Mustererkennung und Bildverarbeitung), language: English, abstract: In this work, we present the problem of automatic appearance-based facial analysis with machine learning techniques and describe common specific sub-problems like face detection, facial feature detection and face recognition which are the crucial parts of many applications in the context of indexation, surveillance, access-control or human-computer interaction. To tackle this problem, we particularly focus on a technique called Convolutional Neural Network (CNN) which is inspired by biological evidence found in the visual cortex of mammalian brains and which has already been applied to many different classi fication problems. Existing CNN-based methods, like the face detection system proposed by Garcia and Delakis, show that this can be a very effective, efficient and robust approach to non-linear image processing tasks. An important step in many automatic facial analysis applications, e.g. face recognition, is face alignment which tries to translate, scale and rotate the face image such that specific facial features are roughly at predefined positions in the image. We propose an efficient approach to this problem using CNNs and experimentally show its very good performance on difficult test images. We further present a CNN-based method for automatic facial feature detection. The proposed system employs a hierarchical procedure which first roughly localizes the eyes, the nose and the mouth and then refines the result by detecting 10 different facial feature points. The detection rate of this method is 96\% for the AR database and 87\% for the BioID database tolerating an error of 10\% of the inter-ocular distance. Finally, we propose a novel face recognition approach based on a specific CNN architecture learning a non-linear mapping of the image space into a lower-dim},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Duffner_2013_Face Image Analysis with Convolutional Neural Networks.pdf},
  language = {English},
  school = {GRIN Verlag}
}

@article{dupont_audiovisual_2000,
  title = {Audio-Visual Speech Modeling for Continuous Speech Recognition},
  author = {Dupont, S. and Luettin, J.},
  year = {2000},
  month = sep,
  volume = {2},
  pages = {141--151},
  issn = {1520-9210},
  doi = {10.1109/6046.865479},
  abstract = {This paper describes a speech recognition system that uses both acoustic and visual speech information to improve recognition performance in noisy environments. The system consists of three components: a visual module; an acoustic module; and a sensor fusion module. The visual module locates and tracks the lip movements of a given speaker and extracts relevant speech features. This task is performed with an appearance-based lip model that is learned from example images. Visual speech features are represented by contour information of the lips and grey-level information of the mouth area. The acoustic module extracts noise-robust features from the audio signal. Finally the sensor fusion module is responsible for the joint temporal modeling of the acoustic and visual feature streams and is realized using multistream hidden Markov models (HMMs). The multistream method allows the definition of different temporal topologies and levels of stream integration and hence enables the modeling of temporal dependencies more accurately than traditional approaches. We present two different methods to learn the asynchrony between the two modalities and how to incorporate them in the multistream models. The superior performance for the proposed system is demonstrated on a large multispeaker database of continuously spoken digits. On a recognition task at 15 dB acoustic signal-to-noise ratio (SNR), acoustic perceptual linear prediction (PLP) features lead to 56\% error rate, noise robust RASTA-PLP (relative spectra) acoustic features to 7.2\% error rate and combined noise robust acoustic features and visual features to 2.5\% error rate},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dupont_Luettin_2000_Audio-visual speech modeling for continuous speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9CDCESJW/865479.html},
  journal = {IEEE Transactions on Multimedia},
  number = {3}
}

@inproceedings{dupont_using_1997,
  title = {Using Multiple Time Scales in a Multi-Stream Speech Recognition System},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Dupont, St{\'e}phane and Bourlard, Herv{\'e}},
  year = {1997},
  month = sep,
  pages = {3--6},
  address = {{Rhodes, Greece}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dupont_Bourlard_1997_Using multiple time scales in a multi-stream speech recognition system.pdf}
}

@article{dwork_exposed_2017,
  title = {Exposed! {{A Survey}} of {{Attacks}} on {{Private Data}}},
  author = {Dwork, Cynthia and Smith, Adam and Steinke, Thomas and Ullman, Jonathan},
  year = {2017},
  volume = {4},
  pages = {61--84},
  doi = {10.1146/annurev-statistics-060116-054123},
  abstract = {Privacy-preserving statistical data analysis addresses the general question of protecting privacy when publicly releasing information about a sensitive dataset. A privacy attack takes seemingly innocuous released information and uses it to discern the private details of individuals, thus demonstrating that such information compromises privacy. For example, re-identification attacks have shown that it is easy to link supposedly de-identified records to the identity of the individual concerned. This survey focuses on attacking aggregate data, such as statistics about how many individuals have a certain disease, genetic trait, or combination thereof. We consider two types of attacks: reconstruction attacks, which approximately determine a sensitive feature of all the individuals covered by the dataset, and tracing attacks, which determine whether or not a target individual's data are included in the dataset. We also discuss techniques from the differential privacy literature for releasing approximate aggregate statistics while provably thwarting any privacy attack.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Dwork et al_2017_Exposed.pdf},
  journal = {Annual Review of Statistics and Its Application},
  number = {1}
}

@incollection{ebbinghaus_german_1994,
  title = {German {{Words}} in {{German Sign Language}}: {{Do They Tell Us Something New About Sign Languages}}?},
  booktitle = {The {{Deaf Way}}: {{Perspectives}} from the {{International Conference}} on {{Deaf Culture}}},
  author = {Ebbinghaus, Horst and Hessmann, Jens},
  editor = {Erting, Carol},
  year = {1994},
  pages = {399--409},
  publisher = {{Gallaudet University Press}},
  abstract = {The Deaf Way documents the vast scholarly and artistic endeavors that took place in July 1989 when more than 6,000 deaf people from around the world met at Gallaudet University to celebrate Deaf culture. More than 150 articles by world-renowned experts examine every aspect of Deaf life in societies crisscrossing the globe. This outstanding volume is divided into ten distinct sections: Deaf Culture Around the World, Deaf History, The Study of Sign Language in Society, Diversity in the Deaf Community, Deaf Clubs and Sports, The Deaf Child in the Family, Education, Deaf/Hearing Interaction, Deaf People and the Arts, and Deaf People and Human Rights Issues.},
  isbn = {978-1-56368-026-7},
  language = {en}
}

@incollection{ebbinghaus_sign_2001,
  title = {Sign {{Language}} as {{Multidimensional Communication}} - or: {{Why Manual Signs}}, {{Mouthings}}, and {{Mouth Gestures Are Three Different Things}}.},
  booktitle = {The {{Hands Are The Head}} of {{The Mouth}}. {{The Mouth}} as {{Articulator}} in {{Sign Languages}}},
  author = {Ebbinghaus, Horst and Hessmann, Jens},
  editor = {Braem, P. Boyes and {Sutton-Spence}, R. L.},
  year = {2001},
  pages = {133--153},
  publisher = {{Signum Press}},
  address = {{Hamburg}},
  language = {English}
}

@article{ebbinghaus_signs_1996,
  title = {Signs and Words: {{Accounting}} for Spoken Language Elements in {{German Sign Language}}},
  shorttitle = {Signs and Words},
  author = {Ebbinghaus, Horst and {He{\textbackslash}s smann}, Jens},
  year = {1996},
  volume = {1},
  pages = {23--56},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ebbinghaus_Hes smann_1996_Signs and words.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7BK6IX9C/books.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/G8TJVQIT/books.html},
  journal = {International review of sign linguistics},
  number = {1}
}

@inproceedings{ebling_bridging_2015,
  title = {Bridging the Gap between Sign Language Machine Translation and Sign Language Animation Using Sequence Classification},
  booktitle = {6th {{Workshop}} on {{Speech}} and {{Language Processing}} for {{Assistive Technologies}}},
  author = {Ebling, Sarah and Huenerfauth, Matt},
  year = {2015},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ebling_Huenerfauth_2015_Bridging the gap between sign language machine translation and sign language.pdf}
}

@inproceedings{ebling_smile_2018,
  title = {{{SMILE Swiss German Sign Language Dataset}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Ebling, Sarah and Camgoz, Necati and Braem, Penny and Tissi, Katja and {Sidler-Miserez}, Sandra and Stoll, Stephanie and Hadfield, Simon and Haug, Tobias and Bowden, Richard and Tornay, Sandrine and Razavi, Marzieh and {Magimai-Doss}, Mathew},
  year = {2018},
  month = feb,
  pages = {4221--4229},
  address = {{Myazaki, Japan}},
  abstract = {Sign language recognition (SLR) involves identifying the form and meaning of isolated signs or sequences of signs. To our knowledge, the combination of SLR and sign language assessment is novel. The goal of an ongoing three-year project in Switzerland is to pioneer an assessment system for lexical signs of Swiss German Sign Language (Deutschschweizerische Geb\"ardensprache, DSGS) that relies on SLR. The assessment system aims to give adult L2 learners of DSGS feedback on the correctness of the manual parameters (handshape, hand position, location, and movement) of isolated signs they produce. In its initial version, the system will include automatic feedback for a subset of a DSGS vocabulary production test consisting of 100 lexical items. To provide the SLR component of the assessment system with sufficient training samples, a large-scale dataset containing videotaped repeated productions of the 100 items of the vocabulary test with associated transcriptions and annotations was created, consisting of data from 11 adult L1 signers and 19 adult L2 learners of DSGS. This paper introduces the dataset, which will be made available to the research community.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ebling et al_2018_SMILE Swiss German Sign Language Dataset.pdf},
  keywords = {capturedBy=depth,capturedBy=vision,dataset=SmileDSGS,isolated,isolated;vocab=100;signer=30;sl=DSGS;capturedBy=vision;capturedBy=depth;dataset=SmileDSGS,signer=30,sl=DSGS,vocab<500,vocab=100}
}

@article{ebling_synthesizing_,
  title = {Synthesizing the Finger Alphabet of {{Swiss German Sign Language}} and Evaluating the Comprehensibility of the Resulting Animations},
  author = {Ebling, Sarah and Wolfe, Rosalee and Schnepp, Jerry and Baowidan, Souad and McDonald, John and Moncrief, Robyn and {Sidler-Miserez}, Sandra and Tissi, Katja},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ebling et al_Synthesizing the finger alphabet of Swiss German Sign Language and evaluating.pdf}
}

@inproceedings{eckart_accelerated_2016,
  title = {Accelerated {{Generative Models}} for {{3D Point Cloud Data}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Eckart, Ben and Kim, Kihwan and Troccoli, Alejandro and Kelly, Alonzo and Kautz, Jan},
  year = {2016},
  pages = {5497--5505},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Eckart et al_2016_Accelerated Generative Models for 3D Point Cloud Data.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FF954HBS/Eckart_Accelerated_Generative_Models_CVPR_2016_paper.html}
}

@inproceedings{edwards_interpreting_1998,
  title = {Interpreting Face Images Using Active Appearance Models},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Edwards, Gareth J. and Taylor, Christopher J. and Cootes, Timothy F.},
  year = {1998},
  month = jun,
  pages = {300--305},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Edwards et al_1998_Interpreting face images using active appearance models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/966WPP3J/abs_all.html}
}

@inproceedings{efthimiou_gslc_2007,
  title = {{{GSLC}}: {{Creation}} and {{Annotation}} of a {{Greek Sign Language Corpus}} for {{HCI}}},
  shorttitle = {{{GSLC}}},
  booktitle = {Universal {{Acess}} in {{Human Computer Interaction}}. {{Coping}} with {{Diversity}}},
  author = {Efthimiou, Eleni and Fotinea, Stavroula-Evita},
  year = {2007},
  month = jul,
  pages = {657--666},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-73279-2_73},
  abstract = {In the framework of a research target that aims at integration of sign language technologies to human-computer interaction applications, creation and annotation of the Greek Sign Language Corpus (GSLC) involve, on the one hand, data and analysis of the phonological structure of morphemes of Greek Sign Language (GSL) and, on the other hand, collection of sentence level language samples and assignment of their respective annotations. GSLC also entails free sign narrations fully annotated at least for sentence segmentation. Simple and complex sign morpheme formation is directly relevant to development of sign recognition prototypes. In this sense, a sign language corpus intended to support sign recognition by exploitation of a language model has to entail sufficient data from simple- to complex- morpheme level. Sentence level annotation, except for sentence boundaries, focuses on phrase boundary marking and grammar information often conveyed by multi-layer markers, as is the case of e.g. topicalisation, nominal phrase formation, temporal indicators, question formation and sentential negation in GSL.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Efthimiou_Fotinea_2007_GSLC.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WKN5B38T/978-3-540-73279-2_73.html},
  isbn = {978-3-540-73278-5 978-3-540-73279-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{efthimiou_sign_2009a,
  title = {Sign Language Recognition, Generation, and Modelling: A Research Effort with Applications in Deaf Communication},
  shorttitle = {Sign Language Recognition, Generation, and Modelling},
  booktitle = {Universal {{Access}} in {{Human}}-{{Computer Interaction}}. {{Addressing Diversity}}},
  author = {Efthimiou, Eleni and Fotinea, Stavroula-Evita and Vogler, Christian and Hanke, Thomas and Glauert, John and Bowden, Richard and Braffort, Annelies and Collet, Christophe and Maragos, Petros and Segouat, J{\'e}r{\'e}mie},
  year = {2009},
  pages = {21--30},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Efthimiou et al_2009_Sign language recognition, generation, and modelling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7DK7J4QV/978-3-642-02707-9_3.html}
}

@inproceedings{efthimiou_sign_2012,
  title = {Sign {{Language}} Technologies and Resources of the {{Dicta}}-{{Sign}} Project},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Efthimiou, Eleni and Fotinea, Stavroula-Evita and Hanke, Thomas and Glauert, John and Bowden, Richard and Braffort, Annelies and Collet, Christophe and Maragos, Petros and {Lefebvre-Albaret}, Fran{\c c}ois},
  year = {2012},
  month = may,
  pages = {23--27},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Efthimiou et al_2012_Sign Language technologies and resources of the Dicta-Sign project.pdf}
}

@article{efthimiou_sign_2019,
  title = {Sign {{Language Technologies}} and the {{Critical Role}} of {{SL Resources}} in {{View}} of {{Future Internet Accessibility Services}}},
  author = {Efthimiou, Eleni and Fotinea, Stavroula-Evita and Goulas, Theodore and Vacalopoulou, Anna and Vasilaki, Kiki and Dimou, Athanasia-Lida},
  year = {2019},
  month = mar,
  volume = {7},
  pages = {18},
  doi = {10.3390/technologies7010018},
  abstract = {In this paper, we touch upon the requirement for accessibility via Sign Language as regards dynamic composition and exchange of new content in the context of natural language-based human interaction, and also the accessibility of web services and electronic content in written text by deaf and hard-of-hearing individuals. In this framework, one key issue remains the option for composition of signed \&ldquo;text\&rdquo;, along with the ability for the reuse of pre-existing signed \&ldquo;text\&rdquo; by exploiting basic editing facilities similar to those available for written text that serve vocal language representation. An equally critical related issue is accessibility of vocal language text by born or early deaf signers, as well as the use of web-based facilities via Sign Language-supported interfaces, taking into account that the majority of native signers present limited reading skills. It is, thus, demonstrated how Sign Language technologies and resources may be integrated in human-centered applications, enabling web services and content accessibility in the education and an everyday communication context, in order to facilitate integration of signer populations in a societal environment that is strongly defined by smart life style conditions. This potential is also demonstrated by end-user-evaluation results.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Efthimiou et al_2019_Sign Language Technologies and the Critical Role of SL Resources in View of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8JMU5ERV/18.html},
  journal = {Technologies},
  language = {en},
  number = {1}
}

@book{ekman_facial_1978,
  title = {Facial {{Action Coding System}}: {{A Technique}} for the {{Measurement}} of {{Facial Movement}}.},
  shorttitle = {Facial {{Action Coding System}}},
  author = {Ekman, P and Friesen, W},
  year = {1978},
  publisher = {{Consulting Psychologists Press}}
}

@article{eleftheriadis_discriminative_2015,
  title = {Discriminative {{Shared Gaussian Processes}} for {{Multiview}} and {{View}}-{{Invariant Facial Expression Recognition}}},
  author = {Eleftheriadis, S. and Rudovic, O. and Pantic, M.},
  year = {2015},
  month = jan,
  volume = {24},
  pages = {189--204},
  issn = {1057-7149},
  doi = {10.1109/TIP.2014.2375634},
  abstract = {Images of facial expressions are often captured from various views as a result of either head movements or variable camera position. Existing methods for multiview and/or view-invariant facial expression recognition typically perform classification of the observed expression using either classifiers learned separately for each view or a single classifier learned for all views. However, these approaches ignore the fact that different views of a facial expression are just different manifestations of the same facial expression. By accounting for this redundancy, we can design more effective classifiers for the target task. To this end, we propose a discriminative shared Gaussian process latent variable model (DS-GPLVM) for multiview and view-invariant classification of facial expressions from multiple views. In this model, we first learn a discriminative manifold shared by multiple views of a facial expression. Subsequently, we perform facial expression classification in the expression manifold. Finally, classification of an observed facial expression is carried out either in the view-invariant manner (using only a single view of the expression) or in the multiview manner (using multiple views of the expression). The proposed model can also be used to perform fusion of different facial features in a principled manner. We validate the proposed DS-GPLVM on both posed and spontaneously displayed facial expressions from three publicly available datasets (MultiPIE, labeled face parts in the wild, and static facial expressions in the wild). We show that this model outperforms the state-of-the-art methods for multiview and view-invariant facial expression classification, and several state-of-the-art methods for multiview learning and feature fusion.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Eleftheriadis et al_2015_Discriminative Shared Gaussian Processes for Multiview and View-Invariant.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2TPEHQ7H/abs_all.html},
  journal = {IEEE Transactions on Image Processing},
  number = {1}
}

@inproceedings{elgammal_nonparametric_2000,
  title = {Non-Parametric {{Model}} for {{Background Subtraction}}},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Elgammal, Ahmed and Harwood, David and Davis, Larry},
  editor = {Vernon, David},
  year = {2000},
  pages = {751--767},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Background subtraction is a method typically used to segment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the background of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detection of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates.},
  copyright = {\textcopyright 2000 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Elgammal et al_2000_Non-parametric Model for Background Subtraction.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FHIPG9D8/3-540-45053-X_48.html},
  isbn = {978-3-540-67686-7 978-3-540-45053-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{elliott_linguistic_2008,
  title = {Linguistic Modelling and Language-Processing Technologies for {{Avatar}}-Based Sign Language Presentation},
  author = {Elliott, Ralph and Glauert, John RW and Kennaway, J. R. and Marshall, Ian and Safar, Eva},
  year = {2008},
  volume = {6},
  pages = {375--391},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Elliott et al_2008_Linguistic modelling and language-processing technologies for Avatar-based sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C7R56GV6/s10209-007-0102-z.html},
  journal = {Universal Access in the Information Society},
  number = {4}
}

@inproceedings{elliott_overview_2004,
  title = {An Overview of the {{SiGML}} Notation and {{SiGML Signing}} Software System},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Elliott, Ralph and Glauert, J. R. W. and Jennings, Vince and Kennaway, J. R.},
  year = {2004},
  pages = {98--104}
}

@phdthesis{elliott_phonological_2013,
  title = {Phonological {{Functions}} of {{Facial Movements}}: {{Evidence}} from Deaf Users of {{German Sign Language}}},
  author = {Elliott, Eeva A.},
  year = {2013},
  address = {{Berlin, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Elliott_2013_Phonological Functions of Facial Movements.pdf},
  school = {Freie Universit\"at}
}

@article{ellis_linear_,
  title = {Linear {{Predictors}} for {{Fast Simultaneous Modeling}} and {{Tracking}}},
  author = {Ellis, L. and Dowson, N. and Matas, J. and Bowden, R.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ellis et al_Linear Predictors for Fast Simultaneous Modeling and Tracking.pdf}
}

@article{ellis_linear_2010,
  title = {Linear {{Regression}} and {{Adaptive Appearance Models}} for~{{Fast~Simultaneous Modelling}} and {{Tracking}}},
  author = {Ellis, Liam and Dowson, Nicholas and Matas, Jiri and Bowden, Richard},
  year = {2010},
  month = jul,
  issn = {0920-5691},
  doi = {10.1007/s11263-010-0364-4},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ellis et al_2010_Linear Regression and Adaptive Appearance Models for Fast Simultaneous.pdf},
  journal = {International Journal of Computer Vision (IJCV)}
}

@article{ellis_linear_2011,
  title = {Linear {{Regression}} and {{Adaptive Appearance Models}} for {{Fast Simultaneous Modelling}} and {{Tracking}}},
  author = {Ellis, Liam and Dowson, Nicholas and Matas, Jiri and Bowden, Richard},
  year = {2011},
  volume = {95},
  pages = {154--179},
  issn = {0920-5691},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ellis et al_2011_Linear Regression and Adaptive Appearance Models for Fast Simultaneous.pdf},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {2}
}

@article{ellis_online_,
  title = {Online {{Learning}} and {{Partitioning}} of {{Linear Displacement Predictors}} for {{Tracking}}},
  author = {Ellis, L. and Matas, J. and Bowden, R.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ellis et al_Online Learning and Partitioning of Linear Displacement Predictors for Tracking.pdf}
}

@inproceedings{elons_facial_2014,
  title = {Facial Expressions Recognition for Arabic Sign Language Translation},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Computer Engineering Systems}} ({{ICCES}})},
  author = {Elons, A.S. and Ahmed, Menna and Shedid, Hwaidaa},
  year = {2014},
  month = dec,
  pages = {330--335},
  doi = {10.1109/ICCES.2014.7030980},
  abstract = {Contrary to the common sense that tells us sign language depends mainly on hands, other factors such as facial expressions, body movements and lips affect dramatically a sign meaning. Arabic Sign Language (ArSL) tends to be a descriptive gesture language, facial expressions are involved in 70\% of total signs. In this paper, a study on an ArSL database is performed to conclude that the 6 main facial expressions are essential to recognize the sign. A developed system used to classify these expressions accomplished 92\% recognition rate on 5 different people. The system employed already existing technical methods such as: Recursive Principle Components (RPCA) for feature extraction and Multi-layer Perceptron (MLP) for classification. The main contribution of this paper is employing the developed module and integrating it with an already existing hand sign recognition system. The proposed system enhanced the hand sign recognition system and raised the recognition rate from 88\% to 98\%. Various people's shapes and capturing angles and distances have also been taken into consideration.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Elons et al_2014_Facial expressions recognition for arabic sign language translation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FWDMWNRM/abs_all.html}
}

@article{elons_proposed_2013,
  title = {A Proposed {{PCNN}} Features Quality Optimization Technique for Pose-Invariant {{3D Arabic}} Sign Language Recognition},
  author = {Elons, A. Samir and {Abull-ela}, Magdy and Tolba, M. F.},
  year = {2013},
  month = apr,
  volume = {13},
  pages = {1646--1660},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2012.11.036},
  abstract = {This paper proposes a novel technique to deal with pose variations in 3D object recognition. This technique uses pulse-coupled neural network (PCNN) for image features generation from two different viewing angles. These signatures qualities are then evaluated, using a proposed fitness function. The features evaluation step is taken before any classification steps are performed. The evaluation results dynamic weighting factors for each camera express the features quality from the current viewing angles. The proposed technique uses the two 2D image features and their corresponding calculated weighting factors to construct optimized quality 3D features. An experiment was conducted in Arabic sign language recognition application which multiple views are necessary to distinguish some signs. The proposed technique obtained a 96\% recognition accuracy for pose-invariant restrictions with a degree of freedom from 0 to 90.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Elons et al_2013_A proposed PCNN features quality optimization technique for pose-invariant 3D.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3JP4PICG/S1568494612005200.html},
  journal = {Applied Soft Computing},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=50;sl=ArSL;capturedBy=vision;capturedBy=depth,modal=shape,sl=ArSL,slreco,slreco;modal=shape,vocab<100,vocab=50},
  number = {4}
}

@book{emmorey_language_2001,
  title = {Language, {{Cognition}}, and the {{Brain}}: {{Insights From Sign Language Research}}},
  shorttitle = {Language, {{Cognition}}, and the {{Brain}}},
  author = {Emmorey, Karen},
  year = {2001},
  month = nov,
  publisher = {{Psychology Press}},
  abstract = {Once signed languages are recognized as natural human languages, a world of exploration opens up. Signed languages provide a powerful tool for investigating the nature of human language and language processing, the relation between cognition and language, and the neural organization of language. The value of sign languages lies in their modality. Specifically, for perception, signed languages depend upon high-level vision and motion processing systems, and for production, they require the integration of motor systems involving the hands and face. These facts raise many questions: What impact does this different biological base have for grammatical systems? For online language processing? For the acquisition of language? How does it affect nonlinguistic cognitive structures and processing? Are the same neural systems involved?  These are some of the questions that this book aims at addressing. The answers provide insight into what constrains grammatical form, language processing, linguistic working memory, and hemispheric specialization for language. The study of signed languages allows researchers to address questions about the nature of linguistic and cognitive systems that otherwise could not be easily addressed.},
  isbn = {978-1-135-66481-7},
  language = {en}
}

@book{emmorey_signs_2000,
  title = {The {{Signs}} of {{Language Revisited}}: {{An Anthology}} to {{Honor Ursula Bellugi}} and {{Edward Klima}}},
  shorttitle = {The {{Signs}} of {{Language Revisited}}},
  author = {Emmorey, Karen and Lane, Harlan L. and Bellugi, Ursula and Klima, Edward S.},
  year = {2000},
  publisher = {{Lawrence Erlbaum Associates}},
  abstract = {The burgeoning of research on signed language during the last two decades has had a major influence on several disciplines concerned with mind and language, including linguistics, neuroscience, cognitive psychology, child language acquisition, sociolinguistics, bilingualism, and deaf education. The genealogy of this research can be traced to a remarkable degree to a single pair of scholars, Ursula Bellugi and Edward Klima, who have conducted their research on signed language and educated scores of scholars in the field since the early 1970s.   The Signs of Language Revisited has three major objectives:  * presenting the latest findings and theories of leading scientists in numerous specialties from language acquisition in children to literacy and deaf people;  * taking stock of the distance scholarship has come in a given field, where we are now, and where we should be headed; and  * acknowledging and articulating the intellectual debt of the authors to Bellugi and Klima--in some cases through personal reminiscences.  Thus, this book is also a document in the sociology and history of science.},
  googlebooks = {SvveqKVshj0C},
  isbn = {978-0-8058-3246-4},
  language = {en}
}

@article{epameinondasantonakos_classification_2014,
  title = {Classification of Extreme Facial Events in Sign Language Videos},
  author = {{Epameinondas Antonakos} and Pitsikalis, Vassilis and Maragos, Petros},
  year = {2014},
  month = mar,
  volume = {2014},
  pages = {14},
  issn = {1687-5281},
  doi = {10.1186/1687-5281-2014-14},
  abstract = {We propose a new approach for Extreme States Classification (ESC) on feature spaces of facial cues in sign language (SL) videos. The method is built upon Active Appearance Model (AAM) face tracking and feature extraction of global and local AAMs. ESC is applied on various facial cues - as, for instance, pose rotations, head movements and eye blinking - leading to the detection of extreme states such as left/right, up/down and open/closed. Given the importance of such facial events in SL analysis, we apply ESC to detect visual events on SL videos, including both American (ASL) and Greek (GSL) corpora, yielding promising qualitative and quantitative results. Further, we show the potential of ESC for assistive annotation tools and demonstrate a link of the detections with indicative higher-level linguistic events. Given the lack of facial annotated data and the fact that manual annotations are highly time-consuming, ESC results indicate that the framework can have significant impact on SL processing and analysis.},
  copyright = {2014 Antonakos et al.; licensee Springer.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Epameinondas Antonakos et al_2014_Classification of extreme facial events in sign language videos.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P6GNJC2D/abstract.html},
  journal = {EURASIP Journal on Image and Video Processing},
  language = {en},
  number = {1}
}

@inproceedings{er-rady_automatic_2017,
  title = {Automatic Sign Language Recognition: {{A}} Survey},
  shorttitle = {Automatic Sign Language Recognition},
  booktitle = {Int. {{Conf}}. on {{Advanced Technologies}} for {{Signal}} and {{Image Processing}} ({{ATSIP}})},
  author = {{Er-Rady}, A. and Faizi, R. and Thami, R. O. H. and Housni, H.},
  year = {2017},
  month = may,
  pages = {1--7},
  doi = {10.1109/ATSIP.2017.8075561},
  abstract = {Sign Language, which is a fully visual language with its own grammar, differs largely from that of spoken languages [21]. After nearly 30 years of research, SL recognition still in its infancy when compared to Automatic Speech Recognition. When producing Sign language (SL), different body parts are involved. Most importantly the hands, but also facial expressions and body movements/postures. The recognition of SL is still one of the most challenging problems in gesture recognition. In this survey, we are going to discuss the advancement of sign language recognition through the last decade. In this paper, we provide a review of the state-of-the-art building blocks of Automatic Sign Language Recognition (ASLR) system, from feature extraction up to sign.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Er-Rady et al_2017_Automatic sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RH74WVZD/8075561.html}
}

@inproceedings{erdem_automatic_2002,
  title = {Automatic Detection of Relevant Head Gestures in {{American Sign Language}} Communication},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Erdem, Ugur Murat and Sclaroff, Stan},
  year = {2002},
  month = aug,
  volume = {1},
  pages = {460--463},
  address = {{Quebec, Canada}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Erdem_Sclaroff_2002_Automatic detection of relevant head gestures in American Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3AZHA6J2/login.html}
}

@inproceedings{erdem_ransacbased_2010,
  title = {{{RANSAC}}-Based Training Data Selection for Emotion Recognition from Spontaneous Speech},
  booktitle = {Proceedings of the 3rd International Workshop on {{Affective}} Interaction in Natural Environments},
  author = {Erdem, C. E. and Bozkurt, E. and Erzin, E. and Erdem, A. T.},
  year = {2010},
  pages = {9--14},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Erdem et al_2010_RANSAC-based training data selection for emotion recognition from spontaneous.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TDN3PH95/citation.html}
}

@inproceedings{erenshteyn_multistage_1996,
  title = {A {{Multi}}-{{Stage Approach To Fingerspelling And Gesture Recognition}}},
  booktitle = {Proceedings of the {{Workshop}} on the {{Integration}} of {{Gesture}} in {{Language}} and {{Speech}}},
  author = {Erenshteyn, Roman and Laskov, Pavel},
  year = {1996},
  pages = {185--194},
  publisher = {{Academic Press, Inc}},
  abstract = {We examine application of a hierarchical (multi-stage) neural network classifier to recognition of fingerspelling. We address the problem of finding a hierarchy of classifiers, optimal with respect to recognition accuracy, given a labeled corpus of signing samples. This problem stems from the necessity to recognize a large number of classes for which a single neural network classifier requires prohibitively large training sets and infeasible training time. We outline the general framework of the problem of finding an optimal hierarchy and explain its relationship to the problems of decision tree inference and object clustering. Three algorithms are presented, their accuracy tested on a subset of figerspelling letters and compared to the accuracy of the expert-devised hierarchy and a single-network classifier. The best algorithm attains significant increase in accuracy and exemplifies crucial distinction between finding an optimal hierarchy and clustering.  INTRODUCTION Knowledge and re...},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Erenshteyn_Laskov_1996_A Multi-Stage Approach To Fingerspelling And Gesture Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PXN9QVEA/summary.html}
}

@inproceedings{erenshteyn_recognition_1996,
  title = {Recognition Approach to Gesture Language Understanding},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Erenshteyn, R. and Laskov, P. and Foulds, R. and Messing, L. and Stern, G.},
  year = {1996},
  month = aug,
  volume = {3},
  pages = {431-435 vol.3},
  doi = {10.1109/ICPR.1996.546984},
  abstract = {We explore recognition implications of understanding gesture communication, having chosen American sign language as an example of a gesture language. An instrumented glove and specially developed software have been used for data collection and labeling. We address the problem of recognizing dynamic signing, i.e. signing performed at natural speed. Two neural network architectures have been used for recognition of different types of finger-spelled sentences. Experimental results are presented suggesting that two features of signing affect recognition accuracy: signing frequency which to a large extent can be accounted for by training a network on the samples of the respective frequency; and coarticulation effect which a network fails to identify. As a possible solution to coarticulation problem two post-processing algorithms for temporal segmentation are proposed and experimentally evaluated},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Erenshteyn et al_1996_Recognition approach to gesture language understanding.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AJXPH4M4/546984.html}
}

@inproceedings{escalera_chalearn_2014,
  title = {Chalearn Looking at People Challenge 2014: {{Dataset}} and Results},
  shorttitle = {Chalearn Looking at People Challenge 2014},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Escalera, Sergio and Bar{\'o}, Xavier and Gonzalez, Jordi and Bautista, Miguel A. and Madadi, Meysam and Reyes, Miguel and {Ponce-L{\'o}pez}, V{\'i}ctor and Escalante, Hugo J. and Shotton, Jamie and Guyon, Isabelle},
  year = {2014},
  month = sep,
  pages = {459--473},
  address = {{Zurich, Switzerland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Escalera et al_2014_Chalearn looking at people challenge 2014.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Q4XGGBWS/978-3-319-16178-5_32.html}
}

@inproceedings{escartin_chasing_2014,
  title = {Chasing the {{Perfect Splitter}}: {{A Comparison}} of {{Different Compound Splitting Tools}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Escart{\'i}n, Carla Parra},
  year = {2014 - May},
  pages = {3340--3347},
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Reykjavik, Iceland}},
  abstract = {This paper reports on the evaluation of two compound splitters for German. Compounding is a very frequent phenomenon in German and thus efficient ways of detecting and correctly splitting compound words are needed for natural language processing applications. This paper presents different strategies for compound splitting, focusing on German. Four compound splitters for German are presented. Two of them were used in Statistical Machine Translation (SMT) experiments, obtaining very similar qualitative scores in terms of BLEU and TER and therefore a thorough evaluation of both has been carried out.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Escartín_2014_Chasing the Perfect Splitter.pdf},
  language = {en}
}

@article{essa_coding_1997,
  title = {Coding, Analysis, Interpretation, and Recognition of Facial Expressions},
  author = {Essa, Irfan A. and Pentland, Alex Paul},
  year = {1997},
  volume = {19},
  pages = {757--763},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Essa_Pentland_1997_Coding, analysis, interpretation, and recognition of facial expressions.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {7}
}

@misc{european_sign_language_center_spreadthesign_2018,
  title = {Spreadthesign},
  author = {{European Sign Language Center}},
  year = {2018},
  howpublished = {https://www.spreadthesign.com}
}

@inproceedings{evangelidis_continuous_2014,
  title = {Continuous Gesture Recognition from Articulated Poses},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Evangelidis, Georgios and Singh, Gurkirt and Horaud, Radu, P.},
  year = {2014},
  month = sep,
  pages = {595--607},
  address = {{Zurich, Switzerland}},
  abstract = {This paper addresses the problem of continuous gesture recognition from articulated poses. Unlike the common isolated recognition scenario, the gesture boundaries are here unknown, and one has to solve two problems: segmentation and recognition. This is cast into a labeling framework, namely every site (frame) must be assigned a label (gesture ID). The inherent constraint for a piece-wise constant labeling is satisfied by solving a global optimization problem with a smoothness term. For efficiency reasons, we suggest a dynamic programming (DP) solver that seeks the optimal path in a recursive manner. To quantify the consistency between the labels and the observations, we build on a recent method that encodes sequences of articulated poses into Fisher vectors using short skeletal descriptors. A sliding window allows to frame-wise build such Fisher vectors that are then classified by a multi-class SVM, whereby each label is assigned to each frame at some cost. The evaluation in the ChalearnLAP-2014 challenge shows that the method outperforms other participants that rely only on skeleton data. We also show that the proposed method competes with the top-ranking methods when colour and skeleton features are jointly used.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Evangelidis et al_2014_Continuous gesture recognition from articulated poses.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4QRC6SEE/hal-01082981.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/T6PJRNTW/hal-01082981.html}
}

@incollection{fagiani_new_2012,
  title = {A {{New Italian Sign Language Database}}},
  booktitle = {Advances in {{Brain Inspired Cognitive Systems}}},
  author = {Fagiani, Marco and Principi, Emanuele and Squartini, Stefano and Piazza, Francesco},
  editor = {Zhang, Huaguang and Hussain, Amir and Liu, Derong and Wang, Zhanshan},
  year = {2012},
  month = jan,
  pages = {164--173},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {In this work a new video database of Italian Sign Language (Lingua Italiana dei Segni - LIS) is proposed. Several other attempts have been made in the literature, but they are typically oriented to international languages (like the American Sign Language - ASL). As in speech, also this kind of language presents different peculiarities strictly depending on the geographical location where it is used. The authors have firstly observed that a specific database for LIS is missing and this shoved them to develop the one here presented. It has been conceived to be used in Automatic Sign Recognition and Synthesis (often referred as Automatic Translation into Sign Languages) applications, which represent an important technological opportunity to augment the social inclusion of people with severe hearing impairments. The Database, namely A3LIS-147, is free and available for download.},
  copyright = {\textcopyright 2012 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fagiani et al_2012_A New Italian Sign Language Database.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HDANDAIP/10.html},
  isbn = {978-3-642-31560-2 978-3-642-31561-9},
  keywords = {capturedBy=vision,isolated,isolated;vocab=147;signer=10;sl=LIS;capturedBy=vision,signer=10,sl=LIS,vocab<500,vocab=147},
  number = {7366},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{fagiani_signer_2015,
  title = {Signer Independent Isolated {{Italian}} Sign Recognition Based on Hidden {{Markov}} Models},
  author = {Fagiani, Marco and Principi, Emanuele and Squartini, Stefano and Piazza, Francesco},
  year = {2015},
  month = may,
  volume = {18},
  pages = {385--402},
  issn = {1433-755X},
  doi = {10.1007/s10044-014-0400-z},
  abstract = {Sign languages represent the most natural way to communicate for deaf and hard of hearing. However, there are often barriers between people using this kind of languages and hearing people, typically oriented to express themselves by means of oral languages. To facilitate the social inclusiveness in everyday life for deaf minorities, technology can play an important role. Indeed many attempts have been recently made by the scientific community to develop automatic translation tools. Unfortunately, not many solutions are actually available for the Italian Sign Language (Lingua Italiana dei Segni\textemdash LIS) case study, specially for what concerns the recognition task. In this paper, the authors want to face such a lack, in particular addressing the signer-independent case study, i.e., when the signers in the testing set are to included in the training set. From this perspective, the proposed algorithm represents the first real attempt in the LIS case. The automatic recognizer is based on Hidden Markov Models (HMMs) and video features have been extracted using the OpenCV open source library. The effectiveness of the HMM system is validated by a comparative evaluation with Support Vector Machine approach. The video material used to train the recognizer and testing its performance consists in a database that the authors have deliberately created by involving 10 signers and 147 isolated-sign videos for each signer. The database is publicly available. Computer simulations have shown the effectiveness of the adopted methodology, with recognition accuracies comparable to those obtained by the automatic tools developed for other sign languages.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fagiani et al_2015_Signer independent isolated Italian sign recognition based on hidden Markov.pdf},
  journal = {Pattern Analysis and Applications},
  keywords = {capturedBy=vision,isolated,isolated;vocab=147;signer=10;sl=LIS;capturedBy=vision,modal=2hlocation,modal=2horientation,modal=2hshape,modal=head,signer=10,sl=LIS,slreco,slreco;modal=2hlocation;modal=2hshape;modal=2horientation;modal=head,vocab<500,vocab=147},
  language = {en},
  number = {2}
}

@inproceedings{fainberg_latticebased_2019,
  title = {Lattice-{{Based Lightly}}-{{Supervised Acoustic Model Training}}},
  booktitle = {Interspeech 2019},
  author = {Fainberg, Joachim and Klejch, Ond{\v r}ej and Renals, Steve and Bell, Peter},
  year = {2019},
  month = sep,
  pages = {1596--1600},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2533},
  abstract = {In the broadcast domain there is an abundance of related text data and partial transcriptions, such as closed captions and subtitles. This text data can be used for lightly supervised training, in which text matching the audio is selected using an existing speech recognition model. Current approaches to light supervision typically filter the data based on matching error rates between the transcriptions and biased decoding hypotheses. In contrast, semi-supervised training does not require matching text data, instead generating a hypothesis using a background language model. State-of-the-art semi-supervised training uses lattice-based supervision with the lattice-free MMI (LF-MMI) objective function. We propose a technique to combine inaccurate transcriptions with the lattices generated for semisupervised training, thus preserving uncertainty in the lattice where appropriate. We demonstrate that this combined approach reduces the expected error rates over the lattices, and reduces the word error rate (WER) on a broadcast task.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fainberg et al_2019_Lattice-Based Lightly-Supervised Acoustic Model Training.pdf},
  language = {en}
}

@inproceedings{fang_deepasl_2017,
  title = {{{DeepASL}}: {{Enabling Ubiquitous}} and {{Non}}-{{Intrusive Word}} and {{Sentence}}-{{Level Sign Language Translation}}},
  shorttitle = {{{DeepASL}}},
  booktitle = {Proceedings of {{SenSys}} 2017},
  author = {Fang, Biyi and Co, Jillian and Zhang, Mi},
  year = {2017},
  month = nov,
  pages = {13},
  address = {{Delft, Netherlands}},
  doi = {10.1145/3131672.3131693},
  abstract = {There is an undeniable communication barrier between deaf people and people with normal hearing ability. Although innovations in sign language translation technology aim to tear down this communication barrier, the majority of existing sign language translation systems are either intrusive or constrained by resolution or ambient lighting conditions. Moreover, these existing systems can only perform single-sign ASL translation rather than sentence-level translation, making them much less useful in daily-life communication scenarios. In this work, we fill this critical gap by presenting DeepASL, a transformative deep learning-based sign language translation technology that enables ubiquitous and non-intrusive American Sign Language (ASL) translation at both word and sentence levels. DeepASL uses infrared light as its sensing mechanism to non-intrusively capture the ASL signs. It incorporates a novel hierarchical bidirectional deep recurrent neural network (HB-RNN) and a probabilistic framework based on Connectionist Temporal Classification (CTC) for word-level and sentence-level ASL translation respectively. To evaluate its performance, we have collected 7, 306 samples from 11 participants, covering 56 commonly used ASL words and 100 ASL sentences. DeepASL achieves an average 94.5\% word-level translation accuracy and an average 8.2\% word error rate on translating unseen ASL sentences. Given its promising performance, we believe DeepASL represents a significant step towards breaking the communication barrier between deaf people and hearing majority, and thus has the significant potential to fundamentally change deaf people's lives.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang et al_2017_DeepASL.pdf},
  keywords = {capturedBy=depth,continuous,continuous;vocab=16;signer=11;sl=ASL;capturedBy=depth,isolated,isolated;vocab=56;signer=11;sl=ASL;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=11,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<100,vocab<50,vocab=16,vocab=56}
}

@inproceedings{fang_dynamic_2019,
  title = {Dynamic {{Gesture Recognition Using Inertial Sensors}}-Based {{Data Gloves}}},
  booktitle = {2019 {{IEEE}} 4th {{International Conference}} on {{Advanced Robotics}} and {{Mechatronics}} ({{ICARM}})},
  author = {Fang, Bin and Lv, Qin and Shan, Jianhua and Sun, Fuchun and Liu, Huaping and Guo, Di and Zhao, Yize},
  year = {2019},
  month = jul,
  pages = {390--395},
  doi = {10.1109/ICARM.2019.8834314},
  abstract = {Gesture recognition systems provide a natural interface of interaction between humans and computational systems. This study proposes a system for dynamic gesture recognition using inertial sensors-based data gloves. The proposed data gloves that consist of thirty-six inertial measurement units capture the motion of two arms and hands. The multimodal dataset included the sign language information of data gloves and skeletons is built. Then the convolutional neural network structure for sign language recognition named SLRNet is designed. It mainly consists of a convolutional layer, a batch normalization layer, and a fully connected layer. Finally dynamical gesture recognition experiments are implemented to prove the effectiveness of the proposed method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang et al_2019_Dynamic Gesture Recognition Using Inertial Sensors-based Data Gloves.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VZE4S6MM/8834314.html},
  keywords = {Assistive technology,batch normalization layer,Cameras,computational systems,convolutional neural nets,convolutional neural network structure,Convolutional neural networks,data gloves,Data gloves,dynamical gesture recognition experiments,Feature extraction,fully connected layer,Gesture recognition,image sensors,inertial sensors-based data gloves,multimodal dataset,Robot sensing systems,sign language recognition,SLRNet,thirty-six inertial measurement units}
}

@article{fang_largevocabulary_2007,
  title = {Large-{{Vocabulary Continuous Sign Language Recognition Based}} on {{Transition}}-{{Movement Models}}},
  author = {Fang, Gaolin and Gao, Wen and Zhao, Debin},
  year = {2007},
  month = jan,
  volume = {37},
  pages = {1--9},
  issn = {1083-4427},
  doi = {10.1109/TSMCA.2006.886347},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang et al_2007_Large-Vocabulary Continuous Sign Language Recognition Based on.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang et al_2007_Large-Vocabulary Continuous Sign Language Recognition Based on2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VNFCT52P/4032919.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=5113;signer=2;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=2,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape,vocab=5113,vocab>=1000},
  number = {1}
}

@inproceedings{fang_novel_2004,
  title = {A Novel Approach to Automatically Extracting Basic Units from {{Chinese}} Sign Language},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Fang, Gaolin and Gao, Xiujuan and Gao, Wen and Chen, Yiqiang},
  year = {2004},
  month = aug,
  volume = {4},
  pages = {454--457},
  doi = {10.1109/ICPR.2004.1333800},
  abstract = {In sign language recognition, using subwords instead of whole signs as basic units scales well with increasing vocabulary size. However, there are no subwords defined in the signs' lexical forms. How to automatically extract subwords is a challenging issue. In this paper, a novel approach is proposed to automatically extract these subwords from Chinese sign language (CSL). Signs can be broken down into several segments using hidden Markov models in which each state represents one segment. Temporal clustering algorithm is presented to extract subwords from these segments. The 238 subwords are automatically extracted from 5113 signs, and they can be used as the basic units for large vocabulary CSL recognition with good performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang et al_2004_A novel approach to automatically extracting basic units from Chinese sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RT8CF2SE/Fang et al. - 2004 - A novel approach to automatically extracting basic.html}
}

@inproceedings{fang_signerindependent_2001,
  title = {Signer-Independent Continuous Sign Language Recognition Based on {{SRN}}/{{HMM}}},
  booktitle = {International {{Gesture Workshop}}},
  author = {Fang, Gaolin and Gao, Wen and Chen, Xilin and Wang, Chunli and Ma, Jiyong},
  year = {2001},
  month = apr,
  pages = {76--85},
  address = {{London, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang et al_2001_Signer-independent continuous sign language recognition based on SRN-HMM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R7VZ6RCG/3-540-47873-6_8.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=208;signer=7;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,signer=5,signer=7,sl=CSL,slreco,slreco;modal=2horientation;modal=2hlocation;modal=2hmovement,vocab<500,vocab=208}
}

@inproceedings{fang_signerindependent_2001a,
  title = {Signer-Independent Sign Language Recognition Based on {{SOFM}}/{{HMM}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Fang, Gaolin and Gao, Wen and Ma, Jiyong},
  year = {2001},
  pages = {90--95},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang et al_2001_Signer-independent sign language recognition based on SOFM-HMM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GUF8Z8MX/938915.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=208;signer=7;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=7,sl=CSL,slreco,slreco;modal=2hshape;modal=2horientation;modal=2hmovement;modal=2hlocation,vocab<500,vocab=208}
}

@inproceedings{fang_srnhmm_2002,
  title = {A {{SRN}}/{{HMM}} System for Signer-Independent Continuous Sign Language Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Fang, Gaolin and Gao, Wen},
  year = {2002},
  pages = {312--317},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fang_Gao_2002_A SRN-HMM system for signer-independent continuous sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IQTTAEXP/1004172.html}
}

@inproceedings{farag_learning_2019,
  title = {Learning {{Motion Disfluencies}} for {{Automatic Sign Language Segmentation}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Farag, I. and Brock, H.},
  year = {2019},
  month = may,
  pages = {7360--7364},
  doi = {10.1109/ICASSP.2019.8683523},
  abstract = {We introduce a novel technique for the automatic detection of word boundaries within continuous sentence expressions in Japanese Sign Language from three-dimensional body joint positions. First, the flow of signed sentence data within a temporal neighborhood is determined utilizing the spatial correlations between line segments of inter-joint pairs. Next, a frame-wise binary random forest classifier is trained to distinguish word and non-word frame content based on the extracted spatio-temporal features. The output of the classifier is used to propose an automatic word synthesis that achieves reliable and accurate sentence segmentation with an average frame-wise F1 score of 0.89. Evaluation with a baseline data set furthermore shows that the proposed approach can easily be adapted to distinguish between motion transitions and motion primitives for a coarse-action domain.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Farag_Brock_2019_Learning Motion Disfluencies for Automatic Sign Language Segmentation.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Farag_Brock_2019_Learning Motion Disfluencies for Automatic Sign Language Segmentation2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/26CHM63Q/8683523.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JZYW35C3/8683523.html},
  keywords = {capturedBy=mocap,continuous,continuous;vocab=12;sl=JSL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,sl=JSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=12}
}

@inproceedings{farhadi_aligning_2006,
  title = {Aligning {{ASL}} for Statistical Translation Using a Discriminative Word Model},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Farhadi, Ali and Forsyth, David},
  year = {2006},
  month = jun,
  volume = {2},
  pages = {1471--1476},
  address = {{New York, NY, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Farhadi_Forsyth_2006_Aligning ASL for statistical translation using a discriminative word model.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/I7A5N823/abs_all.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=21;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=head,sl=ASL,slreco,slreco;modal=2hshape;modal=head;modal=2hmovement;modal=2hlocation,vocab<50,vocab=21}
}

@inproceedings{farhadi_transfer_2007,
  title = {Transfer {{Learning}} in {{Sign}} Language},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Farhadi, A. and Forsyth, D. and White, R.},
  year = {2007},
  pages = {1--8},
  doi = {10.1109/CVPR.2007.383346},
  abstract = {We build word models for American Sign Language (ASL) that transfer between different signers and different aspects. This is advantageous because one could use large amounts of labelled avatar data in combination with a smaller amount of labelled human data to spot a large number of words in human data. Transfer learning is possible because we represent blocks of video with novel intermediate discriminative features based on splits of the data. By constructing the same splits in avatar and human data and clustering appropriately, our features are both discriminative and semantically similar: across signers similar features imply similar words. We demonstrate transfer learning in two scenarios: from avatar to a frontally viewed human signer and from an avatar to human signer in a 3/4 view.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2007/Farhadi et al/Farhadi et al_2007_Transfer Learning in Sign language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Farhadi et al_2007_Transfer Learning in Sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IG2KAUBQ/login.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UPVASRTJ/abs_all.html}
}

@article{fasel_automatic_2003,
  title = {Automatic Facial Expression Analysis: A Survey},
  shorttitle = {Automatic Facial Expression Analysis},
  author = {Fasel, B. and Luettin, Juergen},
  year = {2003},
  month = jan,
  volume = {36},
  pages = {259--275},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(02)00052-3},
  abstract = {Over the last decade, automatic facial expression analysis has become an active research area that finds potential applications in areas such as more engaging human\textendash computer interfaces, talking heads, image retrieval and human emotion analysis. Facial expressions reflect not only emotions, but other mental activities, social interaction and physiological signals. In this survey, we introduce the most prominent automatic facial expression analysis methods and systems presented in the literature. Facial motion and deformation extraction approaches as well as classification methods are discussed with respect to issues such as face normalization, facial expression dynamics and facial expression intensity, but also with regard to their robustness towards environmental changes.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fasel_Luettin_2003_Automatic facial expression analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/H46Q4A79/S0031320302000523.html},
  journal = {Pattern Recognition},
  number = {1}
}

@inproceedings{feichtenhofer_convolutional_2016,
  title = {Convolutional {{Two}}-{{Stream Network Fusion}} for {{Video Action Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  year = {2016},
  pages = {1933--1941},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Feichtenhofer et al_2016_Convolutional Two-Stream Network Fusion for Video Action Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TGGHUPTU/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html}
}

@article{fels_glovetalk_1993,
  title = {Glove-{{Talk}}: A Neural Network Interface between a Data-Glove and a Speech Synthesizer},
  shorttitle = {Glove-{{Talk}}},
  author = {Fels, S. S. and Hinton, G. E.},
  year = {1993},
  month = jan,
  volume = {4},
  pages = {2--8},
  issn = {1045-9227},
  doi = {10.1109/72.182690},
  abstract = {To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single `speaker' in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1\% of the time, and no word is produced about 5\% of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fels_Hinton_1993_Glove-Talk.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4P8SEQSU/182690.html},
  journal = {IEEE Transactions on Neural Networks},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=203;sl=ASL;capturedBy=electronicGlove,modal=orientation,modal=shape,sl=ASL,slreco,slreco;modal=shape;modal=orientation,vocab<500,vocab=203},
  number = {1}
}

@article{felzenszwalb_object_2010,
  title = {Object {{Detection}} with {{Discriminatively Trained Part}}-{{Based Models}}},
  author = {Felzenszwalb, P.F. and Girshick, R.B. and McAllester, D. and Ramanan, D.},
  year = {2010},
  volume = {32},
  pages = {1627--1645},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2009.167},
  abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI\textendash SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Felzenszwalb et al_2010_Object Detection with Discriminatively Trained Part-Based Models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VESNK99F/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {9}
}

@article{Fenlon__2013,
  title = {Variation in Handshape and Orientation in {{British Sign Language}}: {{The}} Case of the `1' Hand Configuration},
  shorttitle = {Variation in Handshape and Orientation in {{British Sign Language}}},
  author = {Fenlon, Jordan and Schembri, Adam and Rentelis, Ramas and Cormier, Kearsy},
  year = {2013},
  month = jan,
  volume = {33},
  pages = {69--91},
  issn = {0271-5309},
  doi = {10.1016/j.langcom.2012.09.001},
  abstract = {This paper investigates phonological variation in British Sign Language (BSL) signs produced with a `1' hand configuration in citation form. Multivariate analyses of 2084 tokens reveals that handshape variation in these signs is constrained by linguistic factors (e.g., the preceding and following phonological environment, grammatical category, indexicality, lexical frequency). The only significant social factor was region. For the subset of signs where orientation was also investigated, only grammatical function was important (the surrounding phonological environment and social factors were not significant). The implications for an understanding of pointing signs in signed languages are discussed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fenlon et al_2013_Variation in handshape and orientation in British Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VQGP2B4Z/S0271530912000390.html},
  journal = {Language \& Communication},
  number = {1}
}

@incollection{fenlon_documentary_2015,
  title = {Documentary and {{Corpus Approaches}} to {{Sign Language Research}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Fenlon, Jordan and Schembri, Adam and Johnston, Trevor and Cormier, Kearsy},
  year = {2015},
  pages = {156--172},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch10},
  abstract = {This chapter discusses some key aspects of the methodology associated with sign language documentation and corpus-based approaches to sign language research. It first introduces the field of sign language corpus linguistics, carefully defining the term ``corpus'' in this context and discussing the emergence of technology that has made this new approach to sign language research possible. The chapter then describes specific details of the methodology involved in corpus-building, such as the recruitment of participants, the selection of language activities for the corpus, and the set-up for filming. Next, it examines annotation for corpora, with a focus on the use of ID glossing. The annotation stage is the most time-consuming and laborious stage in the creation of sign language corpora. The chapter concludes with a brief discussion of online archiving and accessibility.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fenlon et al_2015_Documentary and Corpus Approaches to Sign Language Research.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KRI8UHS2/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{fernando_low_2013,
  title = {Low Cost Approach for Real Time Sign Language Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Industrial}} and {{Information Systems}}},
  author = {Fernando, M. and Wijayanayaka, J.},
  year = {2013},
  month = dec,
  pages = {637--642},
  doi = {10.1109/ICIInfS.2013.6732059},
  abstract = {Sign Language is the language of people who suffer from speech and hearing defects. Still the rest of the world doesn't have a clear idea of sign language. The communication between speech impaired people and other people is very inefficient. To overcome this problem technology can act as an intermediate flexible medium for speech impaired people to communicate amongst themselves and with other individuals as well as to enhance their level of learning / education. The suggested solutions in the literature for sign language recognition are very expensive for day to day use. Therefore, the main objective of this research is to find out a low cost affordable method of sign language interpretation. This paper discusses the possible ways to deal with the sign language postures to identify the signs and convert them into text and speech using appearance based approach with a low cost web camera. Further this approach will be very useful to the sign language learners to practice sign language. During the research available human computer interaction approaches in posture recognition were tested and evaluated. A series of image processing techniques with Hu-moment classification was identified as the best approach. The system is able to recognize selected Sign Language signs with the accuracy of 76\% without a controlled background with small light adjustments.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fernando_Wijayanayaka_2013_Low cost approach for real time sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FJ56JJGG/6732059.html}
}

@inproceedings{ferreira_learning_2019,
  title = {Learning {{Signer}}-{{Invariant Representations}} with {{Adversarial Training}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Vision}} ({{ICMV}})},
  author = {Ferreira, Pedro M. and Pernes, Diogo and Rebelo, Ana and Cardoso, Jaime S.},
  year = {2019},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ferreira et al_2019_Learning Signer-Invariant Representations with Adversarial Training.pdf}
}

@article{filippova_czech_2016,
  title = {Czech {{Sign Language}} in Contemporary {{Czech}} Society},
  author = {Filippov{\'a}, Eva and Hud{\'a}kov{\'a}, Andrea},
  year = {2016},
  volume = {2016},
  pages = {85--103},
  issn = {0165-2516},
  doi = {10.1515/ijsl-2015-0046},
  abstract = {Czech Sign Language (CzSL) is a vital means of communication used in the Czech Deaf community; it constitutes an essential communication tool for a minority population in the Czech sociolinguistic space. The aim of the article is to provide an overview of the usage of CzSL in contemporary Czech society, to sketch a brief history of its formative stages and to list the challenges the Czech Deaf face nowadays. An emphasis is placed on the question of bilingualism of the CzSL users. Our special focus is on the questions of language socialization and social-cognitive development, as the early years set the stage for a Deaf child's and the Deaf community's subsequent advancement. Real world implications for the healthy and uncompromised development resonate throughout the discussion of the Czech education system and the place of the Deaf in it. CzSL represents an irrefutable part of the identity of the Czech Deaf population and, as such, merits a critical consideration in the context of the minority languages in the present day Czech society.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Filippová_Hudáková_2016_Czech Sign Language in contemporary Czech society.pdf},
  journal = {International Journal of the Sociology of Language},
  number = {238}
}

@inproceedings{fillbrandt_extraction_2003,
  title = {Extraction of {{3D}} Hand Shape and Posture from Image Sequences for Sign Language Recognition},
  booktitle = {{{IEEE International Workshop}} on {{Analysis}} and {{Modeling}} of {{Faces}} and {{Gestures}}, 2003. {{AMFG}} 2003},
  author = {Fillbrandt, H. and Akyol, S. and Kraiss, K.-F.},
  year = {2003},
  month = oct,
  pages = {181--186},
  doi = {10.1109/AMFG.2003.1240841},
  abstract = {We propose a novel method for extracting natural hand parameters from monocular image sequences. The purpose is to improve a vision-based sign language recognition system by providing detail information about the finger constellation and the 3D hand posture. Therefore, the hand is modelled by a set of 2D appearance models, each representing a limited variation range of 3D hand shape and posture. The single models are linked to each other according to the natural neighbourhood of the corresponding hand status. During an image sequence, necessary model transitions are executed towards one of the current neighbour models. The natural hand parameters are calculated from the shape and texture parameters of the current model, using a relation estimated by linear regression. The method is robust against large differences between subsequent frames and also against poor image quality. It can be implemented in real-time and offers good properties to handle occlusion and partly missing image information.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2003/Fillbrandt et al/Fillbrandt et al_2003_Extraction of 3D hand shape and posture from image sequences for sign language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fillbrandt et al_2003_Extraction of 3D hand shape and posture from image sequences for sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PQCN3QQB/20100181.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XNK9GWF9/abs_all.html}
}

@misc{finishassociationofthedeaf_suvi_2015,
  title = {Suvi {{Viittomat}}},
  author = {{Finish Association of the Deaf}},
  year = {2015},
  howpublished = {http://suvi.viittomat.net/}
}

@article{finn_automatic_1988,
  title = {Automatic Optically-Based Recognition of Speech},
  author = {Finn, K.E. and Montgomery, A.A.},
  year = {1988},
  volume = {8},
  pages = {159--164},
  abstract = {A system for automatic recognition of speech using only optical images of the talker's lips was investigated. Results were compared with the performance of an acoustic-based ASR device, and a performance estimate was made for one possible combination of acoustic and optical processing. \textcopyright{} 1988.},
  journal = {Pattern Recognition Letters},
  number = {3}
}

@article{fischer_crosslinguistic_2017,
  title = {Crosslinguistic {{Variation}} in {{Sign Language Syntax}}},
  author = {Fischer, Susan D.},
  year = {2017},
  volume = {3},
  pages = {125--147},
  doi = {10.1146/annurev-linguistics-011516-034150},
  abstract = {This review introduces and compares syntactic structures in a variety of sign languages. I first examine ways in which sign languages function like spoken languages, and ways in which they differ. I then briefly discuss what sign languages have in common in the syntactic realm; the rest of the article focuses on how they can differ. Because the level of the simple sentence has been documented extensively, this review emphasizes complex sentences, such as sentential complementation, relative clauses, adverbial clauses, embedded questions, and conditionals.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fischer_2017_Crosslinguistic Variation in Sign Language Syntax.pdf},
  journal = {Annual Review of Linguistics},
  number = {1}
}

@article{fisher_confusions_1968,
  title = {Confusions among Visually Perceived Consonants},
  author = {Fisher, Cletus G.},
  year = {1968},
  volume = {11},
  pages = {796},
  journal = {Journal of Speech, Language and Hearing Research},
  number = {4}
}

@inproceedings{fisher_taking_2014,
  title = {Taking a {{User Centred Design Approach}} for {{Designing}} a {{System}} to {{Teach Sign Language}}},
  author = {Fisher, Julie and Ellis, Kirsten and Willoughby, Louisa and Barca, Jan Carlo},
  year = {2014},
  publisher = {{ACIS}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fisher et al_2014_Taking a User Centred Design Approach for Designing a System to Teach Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8FZ3PKXK/8122.html}
}

@article{flasinski_use_2010,
  title = {On the Use of Graph Parsing for Recognition of Isolated Hand Postures of {{Polish Sign Language}}},
  author = {Flasi{\'n}ski, Mariusz and My{\'s}li{\'n}ski, Szymon},
  year = {2010},
  month = jun,
  volume = {43},
  pages = {2249--2264},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2010.01.004},
  abstract = {A novel method of recognition of hand postures of Polish Sign Language (PSL) is presented in the paper. The method is based on a syntactic pattern recognition paradigm. A construction of algorithms for generating a structural (graph) description of hand postures that can be analyzed with the ETPL ( k ) graph grammar parsing model has been a main objective of the research. A structural description generated with these algorithms is unique and unambiguous that results in good discriminative properties of the method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Flasiński_Myśliński_2010_On the use of graph parsing for recognition of isolated hand postures of Polish.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K87PA4EI/S0031320310000361.html},
  journal = {Pattern Recognition},
  number = {6}
}

@inproceedings{fok_more_2018,
  title = {Towards {{More Robust Speech Interactions}} for {{Deaf}} and {{Hard}} of {{Hearing Users}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Fok, Raymond and Kaur, Harmanpreet and Palani, Skanda and Mott, Martez E. and Lasecki, Walter S.},
  year = {2018},
  pages = {57--67},
  address = {{New York, NY, USA}},
  doi = {10.1145/3234695.3236343},
  abstract = {Mobile, wearable, and other ubiquitous computing devices are increasingly creating a context in which conventional keyboard and screen-based inputs are being replaced in favor of more natural speech-based interactions. Digital personal assistants use speech to control a wide range of functionality, from environmental controls to information access. However, many deaf and hard-of-hearing users have speech patterns that vary from those of hearing users due to incomplete acoustic feedback from their own voices. Because automatic speech recognition (ASR) systems are largely trained using speech from hearing individuals, speech-controlled technologies are typically inaccessible to deaf users. Prior work has focused on providing deaf users access to aural output via real-time captioning or signing, but little has been done to improve users' ability to provide input to these systems' speech-based interfaces. Further, the vocalization patterns of deaf speech often make accurate recognition intractable for both automated systems and human listeners, making traditional approaches to mitigate ASR limitations, such as human captionists, less effective. To bridge this accessibility gap, we investigate the limitations of common speech recognition approaches and techniques---both automatic and human-powered---when applied to deaf speech. We then explore the effectiveness of an iterative crowdsourcing workflow, and characterize the potential for groups to collectively exceed the performance of individuals. This paper contributes a better understanding of the challenges of deaf speech recognition and provides insights for future system development in this space.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fok et al_2018_Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users.pdf},
  isbn = {978-1-4503-5650-3},
  series = {{{ASSETS}} '18}
}

@article{fontana_language_2015,
  title = {Language Research and Language Community Change: {{Italian Sign Language}} 1981\textendash 2013},
  shorttitle = {Language Research and Language Community Change},
  author = {Fontana, Sabina and Corazza, Serena and Braem, Penny Boyes and Volterra, Virginia},
  year = {2015},
  volume = {2015},
  pages = {1--30},
  issn = {1613-3668},
  doi = {10.1515/ijsl-2015-0019},
  abstract = {By providing evidence that sign language is an autonomous language, research has contributed to various changes both within and beyond the signing communities. The aim of this article is to show an example of how sign language change is driven not only by language internal factors but also by changes in language perception, as well as in the changing groups of users and the contexts of use. Drawing from data collected at a sign language research centre in Italy on Italian Sign Language during a time span of over thirty years, the present study will show how language research was a major impetus for a new linguistic awareness and changes in language attitude has influenced new linguistic practices and has forced Italian signers to think about rules governing the use of their language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fontana et al_2015_Language research and language community change.pdf},
  journal = {International Journal of the Sociology of Language},
  number = {236}
}

@article{foo_recognition_2004,
  title = {Recognition of Visual Speech Elements Using Adaptively Boosted Hidden {{Markov}} Models},
  author = {Foo, Say Wei and Lian, Yong and Dong, Liang},
  year = {2004},
  volume = {14},
  pages = {693--705},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Foo et al_2004_Recognition of visual speech elements using adaptively boosted hidden Markov.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N69CAGME/cookiedetectresponse.html},
  journal = {Circuits and Systems for Video Technology, IEEE Transactions on},
  number = {5}
}

@inproceedings{forster_best_2010,
  title = {Best {{Practice}} for {{Sign Language Data Collections Regarding}} the {{Needs}} of {{Data}}-{{Driven Recognition}} and {{Translation}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Forster, Jens and Stein, Daniel and Ormel, Ellen and Crasborn, Onno and Ney, Hermann},
  year = {2010},
  month = may,
  pages = {92--97},
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Forster et al_2010_Best Practice for Sign Language Data Collections Regarding the Needs of.pdf}
}

@inproceedings{forster_extensions_2014,
  title = {Extensions of the {{Sign Language Recognition}} and {{Translation Corpus RWTH}}-{{PHOENIX}}-{{Weather}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Forster, Jens and Schmidt, Christoph and Koller, Oscar and Bellgardt, Martin and Ney, Hermann},
  year = {2014},
  month = may,
  pages = {1911--1916},
  address = {{Reykjavik, Island}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Forster et al_2014_Extensions of the Sign Language Recognition and Translation Corpus.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=1558;signer=9;sl=DGS;capturedBy=vision,continuous;vocab=911;signer=7;sl=DGS;capturedBy=vision,signer=7,signer=9,sl=DGS,vocab<1000,vocab=1558,vocab=911,vocab>=1000}
}

@inproceedings{forster_improving_2013,
  title = {Improving {{Continuous Sign Language Recognition}}: {{Speech Recognition Techniques}} and {{System Design}}},
  booktitle = {Workshop on {{Speech}} and {{Language Processing}} for {{Assistive Technologies}}},
  author = {Forster, Jens and Koller, Oscar and Oberd{\"o}rfer, Christian and Gweth, Yannick and Ney, Hermann},
  year = {2013},
  month = aug,
  pages = {41--46},
  address = {{Grenoble, France}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Forster et al_2013_Improving Continuous Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=266;signer=1;sl=DGS;capturedBy=vision,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Signum,modal=fullframe,modal=head,modal=movement,modal=shape,signer=1,sl=DGS,slreco,slreco;modal=fullframe;modal=shape;modal=movement;modal=head,vocab<500,vocab=266,vocab=455}
}

@phdthesis{forster_integrated_2008,
  title = {An {{Integrated Tracking And Recognition Approach For Video}}},
  author = {Forster, Jens},
  year = {2008},
  month = may,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Forster_2008_An Integrated Tracking And Recognition Approach For Video.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision;dataSet=BU-104,dataSet=BU-104,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=shape,vocab<500,vocab=103},
  school = {RWTH Aachen University}
}

@inproceedings{forster_modality_2013,
  title = {Modality {{Combination Techniques}} for {{Continuous Sign Language Recognition}}},
  booktitle = {Proc. {{Iberian Conf}}. on {{Pattern Recognition}} and {{Image Analysis}} ({{IbPRIA}})},
  author = {Forster, Jens and Oberd{\"o}rfer, Christian and Koller, Oscar and Ney, Hermann},
  year = {2013},
  month = jun,
  pages = {89--99},
  publisher = {{Springer}},
  address = {{Madeira, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Forster et al_2013_Modality Combination Techniques for Continuous Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FPQZZQFS/978-3-642-38628-2_10.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=266;signer=1;sl=DGS;capturedBy=vision,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,continuous;vocab=455;signer=25;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Signum,modal=2hshape,modal=fullframe,modal=head,modal=movement,signer=1,signer=25,sl=DGS,slreco,slreco;modal=fullframe;modal=2hshape;modal=movement;modal=head,vocab<500,vocab=266,vocab=455},
  series = {Lecture {{Notes}} in {{Computer Science}} 7887}
}

@inproceedings{forster_rwthphoenixweather_2012,
  title = {{{RWTH}}-{{PHOENIX}}-{{Weather}}: {{A Large Vocabulary Sign Language Recognition}} and {{Translation Corpus}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Forster, Jens and Schmidt, Christoph and Hoyoux, Thomas and Koller, Oscar and Zelle, Uwe and Piater, Justus and Ney, Hermann},
  year = {2012},
  month = may,
  pages = {3785--3789},
  address = {{Istanbul, Turkey}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Forster et al_2012_RWTH-PHOENIX-Weather.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=266;signer=1;sl=DGS;capturedBy=vision,continuous;vocab=911;signer=7;sl=DGS;capturedBy=vision,signer=1,signer=7,sl=DGS,vocab<1000,vocab<500,vocab=266,vocab=911}
}

@phdthesis{freitag2016:phd,
  title = {Investigations on {{Machine Translation System Combination}}},
  author = {Freitag, Markus},
  year = {2016},
  month = apr,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Freitag_2016_Investigations on Machine Translation System Combination.pdf},
  school = {RWTH Aachen University}
}

@article{french_selfensembling_2018,
  title = {Self-Ensembling for Visual Domain Adaptation},
  author = {French, Geoff and Mackiewicz, Michal and Fisher, Mark},
  year = {2018},
  month = feb,
  abstract = {This paper explores the use of self-ensembling for visual domain adaptation problems. Our technique is derived from the mean teacher variant (Tarvainen et. al 2017) of temporal ensembling (Laine et...},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/French et al_2018_Self-ensembling for visual domain adaptation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/S5N7QL7Y/forum.html}
}

@article{friedman_phonological_1975,
  title = {Phonological {{Processes}} in the {{American Sign Language}}},
  author = {Friedman, Lynn A.},
  year = {1975},
  month = sep,
  volume = {1},
  pages = {147--159},
  issn = {2377-1666},
  doi = {10.3765/bls.v1i0.2341},
  abstract = {Proceedings of the First Annual Meeting of the Berkeley Linguistics
Society (1975), pp. 147-159},
  copyright = {Copyright (c)},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Friedman_1975_Phonological Processes in the American Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JJXPFPEQ/2341.html},
  journal = {Annual Meeting of the Berkeley Linguistics Society},
  language = {en-US},
  number = {0}
}

@inproceedings{fujimura_sign_2006,
  title = {Sign Recognition Using Depth Image Streams},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Fujimura, K. and {Xia Liu}},
  year = {2006},
  month = apr,
  pages = {381--386},
  doi = {10.1109/FGR.2006.101},
  abstract = {A set of techniques is presented for extracting essential shape information from image sequences. Presented methods are (i) human detection, (ii) human body parts detection, and (iii) hand shape analysis, all based on depth image streams. In particular, representative types of hand shapes used in Japanese sign language (JSL) are recognized in a non-intrusive manner with a high recognition rate. An experimental JSL recognition system is built that can recognize over 100 words by using an active sensing hardware to capture a stream of depth images at a video rate. Experimental results are shown to validate our approach and characteristics of our approach are discussed},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Fujimura_Xia Liu_2006_Sign recognition using depth image streams.pdf},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=100;sl=JSL;capturedBy=vision;capturedBy=depth,modal=location,modal=shape,sl=JSL,slreco,slreco;modal=shape;modal=location,vocab<500,vocab=100}
}

@article{gales_maximum_1998,
  title = {Maximum Likelihood Linear Transformations for {{HMM}}-Based Speech Recognition},
  author = {Gales, Mark JF},
  year = {1998},
  volume = {12},
  pages = {75--98},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gales_1998_Maximum likelihood linear transformations for HMM-based speech recognition.pdf},
  journal = {Computer Speech and Language},
  number = {2}
}

@article{gamage_gaussian_2011,
  title = {Gaussian {{Process Dynamical Models}} for Hand Gesture Interpretation in {{Sign Language}}},
  author = {Gamage, Nuwan and Kuang, Ye Chow and Akmeliawati, Rini and Demidenko, Serge},
  year = {2011},
  month = nov,
  volume = {32},
  pages = {2009--2014},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2011.08.015},
  abstract = {Classifying human hand gestures in the context of a Sign Language has been historically dominated by Artificial Neural Networks and Hidden Markov Model with varying degrees of success. The main objective of this paper is to introduce Gaussian Process Dynamical Model as an alternative machine learning method for hand gesture interpretation in Sign Language. In support of this proposition, the paper presents the experimental results for Gaussian Process Dynamical Model against a database of 66 hand gestures from the Malaysian Sign Language. Furthermore, the Gaussian Process Dynamical Model is tested against established Hidden Markov Model for a comparative evaluation. A discussion on why Gaussian Process Dynamical Model is superior over existing methods in Sign Language interpretation task is then presented.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gamage et al_2011_Gaussian Process Dynamical Models for hand gesture interpretation in Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BMRFQEQ2/S0167865511002662.html},
  journal = {Pattern Recognition Letters},
  number = {15}
}

@article{gao_chinese_2004,
  title = {A {{Chinese}} Sign Language Recognition System Based on {{SOFM}}/{{SRN}}/{{HMM}}},
  author = {Gao, Wen and Fang, Gaolin and Zhao, Debin and Chen, Yiqiang},
  year = {2004},
  month = dec,
  volume = {37},
  pages = {2389--2402},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2004.04.008},
  abstract = {In sign language recognition (SLR), the major challenges now are developing methods that solve signer-independent continuous sign problems. In this paper, SOFM/HMM is first presented for modeling signer-independent isolated signs. The proposed method uses the self-organizing feature maps (SOFM) as different signers' feature extractor for continuous hidden Markov models (HMM) so as to transform input signs into significant and low-dimensional representations that can be well modeled by the emission probabilities of HMM. Based on these isolated sign models, a SOFM/SRN/HMM model is then proposed for signer-independent continuous SLR. This model applies the improved simple recurrent network (SRN) to segment continuous sign language in terms of transformed SOFM representations, and the outputs of SRN are taken as the HMM states in which the lattice Viterbi algorithm is employed to search the best matched word sequence. Experimental results demonstrate that the proposed system has better performance compared with conventional HMM system and obtains a word recognition rate of 82.9\% over a 5113-sign vocabulary and an accuracy of 86.3\% for signer-independent continuous SLR.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gao et al_2004_A Chinese sign language recognition system based on SOFM-SRN-HMM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DTT6VPHR/S0031320304001657.html},
  journal = {Pattern Recognition},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=5113;signer=6;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,isolated,isolated;vocab=5113;signer=6;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2horientation,modal=2hshape,signer=6,sl=CSL,slreco,slreco;modal=2hshape;modal=2hlocation;modal=2horientation,vocab=5113,vocab>=1000},
  number = {12}
}

@inproceedings{gao_transition_2004,
  title = {Transition Movement Models for Large Vocabulary Continuous Sign Language Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Gao, Wen and Fang, Gaolin and Zhao, Debin and Chen, Yiqiang},
  year = {2004},
  pages = {553--558},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/and_and_2004_Transition movement models for large vocabulary continuous sign language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gao et al_2004_Transition movement models for large vocabulary continuous sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F5JHYZD5/1301591.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/INW25475/cookiedetectresponse.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=5113;signer=6;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2horientation,modal=2hshape,signer=6,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=2horientation,vocab=5113,vocab>=1000}
}

@article{gao_video_2017,
  title = {Video {{Captioning With Attention}}-{{Based LSTM}} and {{Semantic Consistency}}},
  author = {Gao, L. and Guo, Z. and Zhang, H. and Xu, X. and Shen, H. T.},
  year = {2017},
  month = sep,
  volume = {19},
  pages = {2045--2055},
  issn = {1520-9210},
  doi = {10.1109/TMM.2017.2729019},
  abstract = {Recent progress in using long short-term memory (LSTM) for image captioning has motivated the exploration of their applications for video captioning. By taking a video as a sequence of features, an LSTM model is trained on video-sentence pairs and learns to associate a video to a sentence. However, most existing methods compress an entire video shot or frame into a static representation, without considering attention mechanism which allows for selecting salient features. Furthermore, existing approaches usually model the translating error, but ignore the correlations between sentence semantics and visual content. To tackle these issues, we propose a novel end-to-end framework named aLSTMs, an attention-based LSTM model with semantic consistency, to transfer videos to natural sentences. This framework integrates attention mechanism with LSTM to capture salient structures of video, and explores the correlation between multimodal representations (i.e., words and visual content) for generating sentences with rich semantic content. Specifically, we first propose an attention mechanism that uses the dynamic weighted sum of local two-dimensional convolutional neural network representations. Then, an LSTM decoder takes these visual features at time t and the word-embedding feature at time t-1 to generate important words. Finally, we use multimodal embedding to map the visual and sentence features into a joint space to guarantee the semantic consistence of the sentence description and the video visual content. Experiments on the benchmark datasets demonstrate that our method using single feature can achieve competitive or even better results than the state-of-the-art baselines for video captioning in both BLEU and METEOR.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gao et al_2017_Video Captioning With Attention-Based LSTM and Semantic Consistency.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TF7EBRGG/7984828.html},
  journal = {IEEE Transactions on Multimedia},
  number = {9}
}

@article{garcia_convolutional_2004,
  title = {Convolutional Face Finder: A Neural Architecture for Fast and Robust Face Detection},
  shorttitle = {Convolutional Face Finder},
  author = {Garcia, C. and Delakis, M.},
  year = {2004},
  month = nov,
  volume = {26},
  pages = {1408--1423},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2004.97},
  abstract = {In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to {$\pm$}20 degrees in image plane and turned up to {$\pm$}60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an in-depth sensitivity analysis with respect to the degrees of variability of the face patterns.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Garcia_Delakis_2004_Convolutional face finder.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TART4TXH/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {11}
}

@article{garcia_realtime_,
  title = {Real-Time {{American Sign Language Recognition}} with {{Convolutional Neural Networks}}},
  author = {Garcia, Brandon and Stanford, C. A. and Viesca, Sigberto Alarcon},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Garcia et al_Real-time American Sign Language Recognition with Convolutional Neural Networks.pdf}
}

@article{garcia_transcription_2013,
  title = {Transcription Systems for Sign Languages: A Sketch of the Different Graphical Representations of Sign Language and Their Characteristics},
  shorttitle = {Transcription Systems for Sign Languages},
  author = {Garcia, Brigitte and Sallandre, Marie-Anne and others},
  year = {2013},
  pages = {1125--1338},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Garcia et al_2013_Transcription systems for sign languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8B84NUD3/hal-01043755.html},
  journal = {Handbook'Body-Language-Communication'}
}

@inproceedings{garcia-bautista_mexican_2017,
  title = {Mexican Sign Language Recognition Using Kinect and Data Time Warping Algorithm},
  booktitle = {2017 {{International Conference}} on {{Electronics}}, {{Communications}} and {{Computers}} ({{CONIELECOMP}})},
  author = {{Garc{\'i}a-Bautista}, G. and {Trujillo-Romero}, F. and {Caballero-Morales}, S. O.},
  year = {2017},
  month = feb,
  pages = {1--5},
  doi = {10.1109/CONIELECOMP.2017.7891832},
  abstract = {Sign Language (SL) allows hearing and speech impaired people to communicate each other. Nevertheless, SL is not understood by most of common people. The development of a real-time Mexican Sign Language (MSL) translator could benefit the speech impaired community. In this work we propose a method capable of recognize in real-time a basic list of Mexican Sign Language (MSL) signs of 20 meaningful words and translate them into speech and text. The signs were collected from a group of 35 MSL signers executed in front of a Microsoft Kinect Sensor. The hand gesture recognition system use the RGB-D camera to store depth, color and skeleton tracking information. We propose a method to obtain the representative hand trajectory pat-tern information. A Dynamic Time Warping (DTW) algorithm is used to interpret the hand gestures. Finally, we use K-Fold Cross Validation method for testing stages. Our results achieve a mean accuracy of 99.1\% using N-best strategy (N=5) after find the best-match approaches of our templates data-set. And a mean accuracy of 98.57\% from real-time testing.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/García-Bautista et al_2017_Mexican sign language recognition using kinect and data time warping algorithm.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3VM8V75R/7891832.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=20;signer=35;sl=MexicanSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=bodyjoints,signer=35,sl=MexicanSL,slreco,slreco;modal=2hlocation;modal=bodyjoints,vocab<50,vocab=20}
}

@article{garg_variational_2013,
  title = {A Variational Approach to Video Registration with Subspace Constraints},
  author = {Garg, Ravi and Roussos, Anastasios and Agapito, Lourdes},
  year = {2013},
  volume = {104},
  pages = {286--314},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {3}
}

@inproceedings{garner_automatic_2014,
  title = {Automatic {{Speech Recognition}} and {{Translation}} of a {{Swiss German Dialect}}: {{Walliserdeutsch}}},
  shorttitle = {Automatic {{Speech Recognition}} and {{Translation}} of a {{Swiss German Dialect}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Garner, Philip N. and Imseng, David and Meyer, Thomas},
  year = {2014},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Garner et al_2014_Automatic Speech Recognition and Translation of a Swiss German Dialect.pdf}
}

@inproceedings{gass_constrained_2010,
  title = {Constrained {{Energy Minimization}} for {{Matching}}-{{Based Image Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Gass, T. and Dreuw, P. and Ney, H.},
  year = {2010},
  month = aug,
  pages = {3304--3307},
  address = {{Istanbul, Turkey}},
  doi = {10.1109/ICPR.2010.808},
  abstract = {We propose to use energy minimization in MRFs for matching-based image recognition tasks. To this end, the Tree-Reweighted Message Passing algorithm is modified by geometric constraints and efficiently used by exploiting the guaranteed monotonicity of the lower bound within a nearest-neighbor based classification framework. The constraints allow for a speedup linear to the dimensionality of the reference image, and the lower bound allows to optimally prune the nearest-neighbor search without loosing accuracy, effectively allowing to increase the number of optimization iterations without an effect on runtime. We evaluate our approach on well-known OCR and face recognition tasks and on the latter outperform current state-of-the-art.}
}

@inproceedings{gass_warp_2011,
  title = {Warp That {{Smile}} on Your {{Face}}: {{Optimal}} and {{Smooth Deformations}} for {{Face Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Gass, Tobias and Pishchulin, Leonid and Dreuw, Philippe and Ney, Hermann},
  year = {2011},
  month = mar,
  pages = {456--463},
  address = {{Santa Barbara, CA, USA}}
}

@inproceedings{gattupalli_evaluation_2016,
  title = {Evaluation of Deep Learning Based Pose Estimation for Sign Language Recognition},
  booktitle = {Proc {{Int}}. {{Conf}}. on {{PErvasive Technologies Related}} to {{Assistive Environments}}},
  author = {Gattupalli, Srujana and Ghaderi, Amir and Athitsos, Vassilis},
  year = {2016},
  month = jun,
  pages = {12},
  address = {{Corfu, Greece}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gattupalli et al_2016_Evaluation of Deep Learning based Pose Estimation for Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/D3M4GM35/citation.html}
}

@inproceedings{gauvain_developments_1995,
  title = {Developments in {{Continuous Speech Dictation}} Using the \{\vphantom\}{{DARPA}}\vphantom\{\} \{\vphantom\}{{WSJ}}\vphantom\{\} {{Task}}.},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gauvain, J. L},
  year = {1995},
  address = {{Detroit, USA}}
}

@inproceedings{gauvain_language_2004,
  title = {Language {{Recognition Using Phone Latices}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{ICSLP}})},
  author = {Gauvain, J. L and Messaoudi, A. and Schwenk, H.},
  year = {2004},
  pages = {1215--1218}
}

@inproceedings{gavrilov_detecting_2012,
  title = {Detecting {{Reduplication}} in {{Videos}} of {{American Sign Language}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Gavrilov, Zoya and Sclaroff, Stan and Neidle, Carol and Dickinson, Sven},
  year = {2012},
  address = {{Istanbul, Turkey}},
  abstract = {A framework is proposed for the detection of reduplication in digital videos of American Sign Language (ASL). In ASL, reduplication is used for a variety of linguistic purposes, including overt marking of plurality on nouns, aspectual inflection on verbs, and nominalization of verbal forms. Reduplication involves the repetition, often partial, of the articulation of a sign. In this paper, the apriori algorithm for mining frequent patterns in data streams is adapted for finding reduplication in videos of ASL. The proposed algorithm can account for varying weights on items in the apriori algorithm's input sequence. In addition, the apriori algorithm is extended to allow for inexact matching of similar hand motion subsequences and to provide robustness to noise. The formulation is evaluated on 105 lexical signs produced by two native signers. To demonstrate the formulation, overall hand motion direction and magnitude are considered; however, the formulation should be amenable to combining these features with others, such as hand shape, orientation, and place of articulation.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gavrilov et al_2012_Detecting Reduplication in Videos of American Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8UAMF7PC/summary.html}
}

@article{ge_realtime_2018,
  title = {Real-Time {{3D Hand Pose Estimation}} with {{3D Convolutional Neural Networks}}},
  author = {Ge, L. and Liang, H. and Yuan, J. and Thalmann, D.},
  year = {2018},
  pages = {1--1},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2018.2827052},
  abstract = {In this paper, we present a novel method for real-time 3D hand pose estimation from single depth images using 3D Convolutional Neural Networks (CNNs). Image-based features extracted by 2D CNNs are not directly suitable for 3D hand pose estimation due to the lack of 3D spatial information. Our proposed 3D CNN-based method, taking a 3D volumetric representation of the hand depth image as input and extracting 3D features from the volumetric input, can capture the 3D spatial structure of the hand and accurately regress full 3D hand pose in a single pass. In order to make the 3D CNN robust to variations in hand sizes and global orientations, we perform 3D data augmentation on the training data. To further improve the estimation accuracy, we propose applying the 3D deep network architectures and leveraging the complete hand surface as intermediate supervision for learning 3D hand pose from depth images. Extensive experiments on three challenging datasets demonstrate that our proposed approach outperforms baselines and state-of-the-art methods. A cross-dataset experiment also shows that our method has good generalization ability. Furthermore, our method is fast as our implementation runs at over 91 frames per second on a standard computer with a single GPU.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ge et al_2018_Real-time 3D Hand Pose Estimation with 3D Convolutional Neural Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2F3YHJIV/8338122.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9CQW7Y4G/8338122.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C4FNINK9/8338122.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/S5K83QCN/8338122.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {3D hand pose estimation,Heating systems,Pose estimation}
}

@inproceedings{gebre_automatic_2013,
  title = {Automatic Sign Language Identification},
  booktitle = {2013 {{IEEE International Conference}} on {{Image Processing}}},
  author = {Gebre, Binyam Gebrekidan and Wittenburg, Peter and Heskes, Tom},
  year = {2013},
  month = sep,
  pages = {2626--2630},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2013.6738541},
  abstract = {We propose a Random-Forest based sign language identification system. The system uses low-level visual features and is based on the hypothesis that sign languages have varying distributions of phonemes (hand-shapes, locations and movements). We evaluated the system on two sign languages - British SL and Greek SL, both taken from a publicly available corpus, called Dicta Sign Corpus. Achieved average F1 scores are about 95\% - indicating that sign languages can be identified with high accuracy using only low-level visual features.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gebre et al_2013_Automatic sign language identification.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R8KGQX9P/6738541.html},
  keywords = {hand-shapes,language identification,low-level visual features,phonemes,Random-Forest based sign language identification system,Sign language,sign language identification,sign language recognition}
}

@inproceedings{gebre_unsupervised_2014,
  title = {Unsupervised {{Feature Learning}} for {{Visual Sign Language Identification}}},
  booktitle = {Proceedings of the 52nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Gebre, Binyam Gebrekidan and Crasborn, Onno and Wittenburg, Peter and Drude, Sebastian and Heskes, Tom},
  year = {2014},
  month = jun,
  pages = {370--376},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-2061},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gebre et al_2014_Unsupervised Feature Learning for Visual Sign Language Identification2.pdf}
}

@inproceedings{geetha_m_vision_2013,
  title = {A Vision Based Dynamic Gesture Recognition of {{Indian Sign Language}} on {{Kinect}} Based Depth Images},
  booktitle = {Int. {{Conf}}. on {{Emerging Trends}} in {{Communication}}, {{Control}}, {{Signal Processing}} and {{Computing Applications}} ({{C2SPCA}})},
  author = {{Geetha M} and {Manjusha C} and {Unnikrishnan P} and {Harikrishnan R}},
  year = {2013},
  month = oct,
  pages = {1--7},
  doi = {10.1109/C2SPCA.2013.6749448},
  abstract = {Indian Sign Language (ISL) is a visual-spatial language which provides linguistic information using hands, arms, facial expressions, and head/body postures. Our proposed work aims at recognizing 3D dynamic signs corresponding to ISL words. With the advent of 3D sensors like Microsoft Kinect Cameras, 3D geometric processing of images has received much attention in recent researches. We have captured 3D dynamic gestures of ISL words using Kinect camera and has proposed a novel method for feature extraction of dynamic gestures of ISL words. While languages like the American sign language(ASL) are of huge popularity in the field of research and development, Indian Sign Language on the other hand has been standardized recently and hence its (ISLs) recognition is less explored. The method extracts features from the signs and convert it to the intended textual form. The proposed method integrates both local as well as global information of the dynamic sign. A new trajectory based feature extraction method using the concept of Axis of Least Inertia (ALI) is proposed for global feature extraction. An eigen distance based method using the seven 3D key points- (five corresponding to each finger tips, one corresponding to centre of the palm and another corresponding to lower part of palm), extracted using Kinect is proposed for local feature extraction. Integrating 3D local feature has improved the performance of the system as shown in the result. Apart from serving as an aid to the disabled people, other applications of the system also include serving as a sign language tutor, interpreter and also be of use in electronic systems that take gesture input from the users.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Geetha M et al_2013_A vision based dynamic gesture recognition of Indian Sign Language on Kinect.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AAEK7Q9N/6749448.html}
}

@article{gehring_convolutional_2017,
  title = {Convolutional {{Sequence}} to {{Sequence Learning}}},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  year = {2017},
  month = may,
  abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
  archivePrefix = {arXiv},
  eprint = {1705.03122},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gehring et al_2017_Convolutional Sequence to Sequence Learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GHMJS4SK/1705.html},
  journal = {arXiv:1705.03122 [cs]},
  primaryClass = {cs}
}

@inproceedings{generet_extensions_1995,
  title = {Extensions of Absolute Discounting for Language Modeling},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Generet, M. and Ney, Hermann and Wessel, Frank},
  year = {1995},
  month = sep,
  pages = {1245--1248},
  address = {{Madrid, Spain}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8KLC2AB3/e95_1245.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZPSMCBVB/e95_1245.html}
}

@inproceedings{geng_chinese_2014,
  title = {Chinese Sign Language Recognition with {{3D}} Hand Motion Trajectories and Depth Images},
  booktitle = {Proceeding of the 11th {{World Congress}} on {{Intelligent Control}} and {{Automation}}},
  author = {Geng, Lubo and Ma, Xin and Wang, Haibo and Gu, Jason and Li, Y.},
  year = {2014},
  month = jun,
  pages = {1457--1461},
  doi = {10.1109/WCICA.2014.7052933},
  abstract = {An important part for sign language expression is hand shape, and the 3D hand motion trajectories also contain abundant information to interpret the meaning of sign language. In this paper, a novel feature descriptor is proposed for sign language recognition, the hand shape features extracted from the depth images and spherical coordinate (SPC) feature extracted from the 3D hand motion trajectories combine to make up the final feature representation. The new representation not only incorporates both the spatial and temporal information to depict the kinematic connectivity among hand, wrist and elbow for recognition effectively but also avoids the interference of the illumination change and cluttered background compared with other methods. Meanwhile, our self-built dataset includes 320 instances to evaluate the effectiveness of our combining feature. In experiments with the dataset and different feature representation, the superior performance of Extreme Learning Machine (ELM) is tested, compared with Support Vector Machine (SVM).},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Geng et al_2014_Chinese sign language recognition with 3D hand motion trajectories and depth.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U99W98PR/7052933.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=8;signer=8;sl=CSL;capturedBy=vision;capturedBy=depth,modal=location,modal=movement,modal=shape,signer=8,sl=CSL,slreco,slreco;modal=location;modal=movement;modal=shape,vocab<50,vocab=8}
}

@inproceedings{gers_learning_1999,
  title = {Learning to Forget: Continual Prediction with {{LSTM}}},
  shorttitle = {Learning to Forget},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Artificial Neural Networks}} ({{ICANN}})},
  author = {Gers, F. A. and Schmidhuber, J. and Cummins, F.},
  year = {1999},
  volume = {2},
  pages = {850-855 vol.2},
  doi = {10.1049/cp:19991218},
  abstract = {Long short-term memory (LSTM) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive ``forget gate'' that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative benchmark problem on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve a continual version of that problem. LSTM with forget gates, however, easily solves it in an elegant way}
}

@inproceedings{ghahramani_factorial_1996,
  title = {Factorial Hidden {{Markov}} Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ghahramani, Zoubin and Jordan, Michael I.},
  year = {1996},
  month = dec,
  pages = {472--478},
  address = {{Denver, CO, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ghahramani_Jordan_1996_Factorial hidden Markov models.pdf}
}

@inproceedings{ghahremani_linearly_2016,
  title = {Linearly Augmented Deep Neural Network},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ghahremani, Pegah and Droppo, Jasha and Seltzer, Michael L.},
  year = {2016},
  pages = {5085--5089},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ghahremani et al_2016_Linearly augmented deep neural network.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8HJ29EKB/7472646.html}
}

@inproceedings{gibson_hypothesis_2006,
  title = {Hypothesis Spaces for Minimum Bayes Risk Training in Large Vocabulary Speech Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Gibson, Matthew and Hain, Thomas},
  year = {2006},
  month = sep,
  address = {{Pittsburgh, PA, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gibson_Hain_2006_Hypothesis spaces for minimum bayes risk training in large vocabulary speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VMLYSPKV/i06_1653.html}
}

@article{gilbert_action_2010,
  title = {Action {{Recognition Using Mined Hierarchical Compound Features}}},
  author = {Gilbert, A. and Illingworth, J. and Bowden, R.},
  year = {2010},
  issn = {0162-8828},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Gilbert et al/Gilbert et al_2010_Action Recognition Using Mined Hierarchical Compound Features.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gilbert et al_2010_Action Recognition Using Mined Hierarchical Compound Features.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RST46ZEV/TPAMI.2010.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@inproceedings{gilbert_push_2011,
  title = {Push and {{Pull}}: {{Iterative}} Grouping of Media},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Gilbert, Andrew and Bowden, Richard},
  year = {2011},
  pages = {66.1--66.12},
  publisher = {{BMVA Press}},
  doi = {http://dx.doi.org/10.5244/C.25.66},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gilbert_Bowden_2011_Push and Pull.pdf},
  isbn = {1-901725-43-X}
}

@inproceedings{gillick_statistical_1989,
  title = {Some {{Statistical Issues}} in the {{Comparison}} of {{Speech Recognition Algorithms}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gillick, L. and Cox, S.},
  year = {1989},
  pages = {532--535},
  address = {{Glasgow, Scotland}}
}

@inproceedings{gineke_sign_2009,
  title = {Sign Language Perception Research for Improving Automatic Sign Language Recognition},
  booktitle = {Proceedings of {{SPIE}}},
  author = {Gineke, A. and Arendsen, J. and {de Ridder}, H. and {Koenderink-van Doorn}, A. J. and Reinders, M. J. T. and Hendriks, E. A.},
  year = {2009},
  volume = {7240},
  pages = {72400C},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gineke et al_2009_Sign language perception research for improving automatic sign language.pdf}
}

@inproceedings{girdhar_actionvlad_2017,
  title = {{{ActionVLAD}}: {{Learning Spatio}}-{{Temporal Aggregation}} for {{Action Classification}}},
  shorttitle = {{{ActionVLAD}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Girdhar, Rohit and Ramanan, Deva and Gupta, Abhinav and Sivic, Josef and Russell, Bryan},
  year = {2017},
  pages = {971--980},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Girdhar et al_2017_ActionVLAD.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7FDQSCTU/Girdhar_ActionVLAD_Learning_Spatio-Temporal_CVPR_2017_paper.html}
}

@inproceedings{glaser_doubling_2006,
  title = {Doubling Phenomena in {{Swiss German}} Dialects},
  booktitle = {Workshop {{Syntactic Doubling}} in {{European Dialects}}, {{March}} 16\textendash 18},
  author = {Glaser, Elvira and Frey, Natascha},
  year = {2006},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Glaser_Frey_2006_Doubling phenomena in Swiss German dialects.pdf}
}

@article{glasser_deaf_2017,
  title = {Deaf, {{Hard}} of {{Hearing}}, and {{Hearing Perspectives}} on Using {{Automatic Speech Recognition}} in {{Conversation}}},
  author = {Glasser, Abraham and Kushalnagar, Kesavan and Kushalnagar, Raja},
  year = {2017},
  pages = {427--432},
  doi = {10.1145/3132525.3134781},
  abstract = {Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce costs and interactive friction, supported by the rapid growth in capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78\% word error rate (WER) compared to a hearing speech 18\% WER. Our findings show that current speech-controlled interfaces are not usable by DHH people. Based on our findings, significant advances in speech recognition software or alternative approaches will be needed for deaf use of speech-controlled interfaces. We show that current speech-controlled interfaces are not usable by DHH people.},
  archivePrefix = {arXiv},
  eprint = {1909.01176},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Glasser et al_2017_Deaf, Hard of Hearing, and Hearing Perspectives on using Automatic Speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N82EPPSQ/1909.html},
  journal = {Proc. Int. ACM SIGACCESS Conf. on Computers and Accessibility (ASSETS)}
}

@article{glasser_feasibility_2017,
  title = {Feasibility of {{Using Automatic Speech Recognition}} with {{Voices}} of {{Deaf}} and {{Hard}}-of-{{Hearing Individuals}}},
  author = {Glasser, Abraham and Kushalnagar, Kesavan and Kushalnagar, Raja},
  year = {2017},
  pages = {373--374},
  doi = {10.1145/3132525.3134819},
  abstract = {Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce device costs and interactive friction. This transition has been hastened by the increasing capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78\% word error rate (WER) compared to a hearing speech 18\% WER. Our findings show that current speech-controlled interfaces are not usable by deaf and hard of hearing people. Therefore, it might be wise to pursue other methods for deaf persons to deliver natural commands to computers.},
  archivePrefix = {arXiv},
  eprint = {1909.01167},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Glasser et al_2017_Feasibility of Using Automatic Speech Recognition with Voices of Deaf and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R99SMBHD/1909.html},
  journal = {Proc. Int. ACM SIGACCESS Conf. on Computers and Accessibility (ASSETS)}
}

@inproceedings{glauert_extending_2011,
  title = {Extending the {{SiGML}} Notation\textemdash a Progress Report},
  booktitle = {Second {{International Workshop}} on {{Sign Language Translation}} and {{Avatar Technology}} ({{SLTAT}})},
  author = {Glauert, John and Elliott, Ralph},
  year = {2011},
  volume = {23},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Glauert_Elliott_2011_Extending the SiGML notation—a progress report.pdf}
}

@inproceedings{glorot_understanding_2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = may,
  pages = {249--256},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf}
}

@inproceedings{goldschen_continuous_1994,
  title = {Continuous Optical Automatic Speech Recognition by Lipreading},
  booktitle = {Proc. of {{Asilomar Conf}}. on {{Signals}}, {{Systems}} and {{Computers}}},
  author = {Goldschen, A.J. and Garcia, O.N. and Petajan, E.},
  year = {1994},
  month = oct,
  volume = {1},
  pages = {572-577 vol.1},
  doi = {10.1109/ACSSC.1994.471517},
  abstract = {We describe a continuous optical automatic speech recognizer (OASR) that uses optical information from the oral-cavity shadow of a speaker. The system achieves a 25.3 percent recognition on sentences having a perplexity of 150 without using any syntactic, semantic, acoustic, or contextual guides. We introduce 13, mostly dynamic, oral-cavity features used for optical recognition, present phones that appear optically similar (visemes) for our speaker, and present the recognition results for our hidden Markov models (HMMs) using visemes, trisemes, and generalized trisemes. We conclude that future research is warranted for optical recognition, especially when combined with other input modalities},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Goldschen et al_1994_Continuous optical automatic speech recognition by lipreading.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VIRVFM4F/abs_all.html}
}

@inproceedings{golik_crossentropy_2013,
  title = {Cross-Entropy vs. Squared Error Training: A Theoretical and Experimental Comparison.},
  shorttitle = {Cross-Entropy vs. Squared Error Training},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Golik, Pavel and Doetsch, Patrick and Ney, Hermann},
  year = {2013},
  month = aug,
  pages = {1756--1760},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Golik et al_2013_Cross-entropy vs.pdf}
}

@inproceedings{gollan08:confadapt,
  title = {Confidence {{Scores}} for {{Acoustic Model Adaptation}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gollan, Christian and Bacchiani, Michiel},
  year = {2008},
  month = apr,
  pages = {4289--4292},
  address = {{Las Vegas, NV, USA}},
  booktitlelink = {http://www.icassp2008.org/},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gollan_Bacchiani_2008_Confidence Scores for Acoustic Model Adaptation.pdf}
}

@phdthesis{gollan2014:phd,
  title = {Efficient {{Setup}} of {{Acoustic Models}} for {{Large Vocabulary Continuous Speech Recognition}}},
  author = {Gollan, Christian},
  year = {2014},
  month = aug,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gollan_2014_Efficient Setup of Acoustic Models for Large Vocabulary Continuous Speech.pdf},
  school = {RWTH Aachen University}
}

@article{goodfellow_generative_2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archivePrefix = {arXiv},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Goodfellow et al_2014_Generative Adversarial Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TQP8IVFW/1406.html},
  journal = {arXiv:1406.2661 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{gosztolya_building_2015,
  title = {Building Context-Dependent {{DNN}} Acoustic Models Using {{Kullback}}-{{Leibler}} Divergence-Based State Tying},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Gosztolya, G{\'a}bor and Gr{\'o}sz, Tam{\'a}s and T{\'o}th, L{\'a}szl{\'o} and Imseng, David},
  year = {2015},
  pages = {4570--4574},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gosztolya et al_2015_Building context-dependent DNN acoustic models using Kullback-Leibler.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8X8XJ8A8/7178836.html}
}

@article{gotardo_computing_2011,
  title = {Computing {{Smooth Time Trajectories}} for {{Camera}} and {{Deformable Shape}} in {{Structure}} from {{Motion}} with {{Occlusion}}},
  author = {Gotardo, P. F.U and Martinez, A. M},
  year = {2011},
  month = oct,
  volume = {33},
  pages = {2051--2065},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2011.50},
  abstract = {We address the classical computer vision problems of rigid and nonrigid structure from motion (SFM) with occlusion. We assume that the columns of the input observation matrix W describe smooth 2D point trajectories over time. We then derive a family of efficient methods that estimate the column space of W using compact parameterizations in the Discrete Cosine Transform (DCT) domain. Our methods tolerate high percentages of missing data and incorporate new models for the smooth time trajectories of 2D-points, affine and weak-perspective cameras, and 3D deformable shape. We solve a rigid SFM problem by estimating the smooth time trajectory of a single camera moving around the structure of interest. By considering a weak-perspective camera model from the outset, we directly compute euclidean 3D shape reconstructions without requiring postprocessing steps such as euclidean upgrade and bundle adjustment. Our results on real SFM data sets with high percentages of missing data compared positively to those in the literature. In nonrigid SFM, we propose a novel 3D shape trajectory approach that solves for the deformable structure as the smooth time trajectory of a single point in a linear shape space. A key result shows that, compared to state-of-the-art algorithms, our nonrigid SFM method can better model complex articulated deformation with higher frequency DCT components while still maintaining the low-rank factorization constraint. Finally, we also offer an approach for nonrigid SFM when W is presented with missing data.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gotardo_Martinez_2011_Computing Smooth Time Trajectories for Camera and Deformable Shape in Structure.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {English},
  number = {10}
}

@inproceedings{granger_comparing_2017,
  title = {Comparing {{Hybrid NN}}-{{HMM}} and {{RNN}} for {{Temporal Modeling}} in {{Gesture Recognition}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Granger, Nicolas and el Yacoubi, Moun{\^i}m A.},
  year = {2017},
  month = nov,
  pages = {147--156},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-70096-0_16},
  abstract = {This paper provides an extended comparison of two temporal models for gesture recognition, namely Hybrid Neural Network-Hidden Markov Models (NN-HMM) and Recurrent Neural Networks (RNN) which have lately claimed the state-the-art performances. Experiments were conducted on both models in the same body of work, with similar representation learning capacity and comparable computational costs. For both solutions, we have integrated recent contributions to the model architectures and training techniques. We show that, for this task, Hybrid NN-HMM models remain competitive with Recurrent Neural Networks in a standard setting. For both models, we analyze the influence of the training objective function on the final evaluation metric. We further tested the influence of temporal convolution to improve context modeling, a technique which was recently reported to improve the accuracy of gesture recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Granger_Yacoubi_2017_Comparing Hybrid NN-HMM and RNN for Temporal Modeling in Gesture Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/83KQRW3R/10.html},
  isbn = {978-3-319-70095-3 978-3-319-70096-0},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@phdthesis{granger_deeplearning_2019,
  title = {Deep-Learning for High Dimensional Sequential Observations : Application to Continuous Gesture Recognition},
  shorttitle = {Deep-Learning for High Dimensional Sequential Observations},
  author = {Granger, Nicolas},
  year = {2019},
  month = jan,
  address = {{Paris, France}},
  abstract = {This thesis aims to improve the intuitiveness of human-computer interfaces. In particular, machines should try to replicate human's ability to process streams of information continuously. However, the sub-domain of Machine Learning dedicated to recognition on time series remains barred by numerous challenges. Our studies use gesture recognition as an exemplar application, gestures intermix static body poses and movements in a complex manner using widely different modalities. The first part of our work compares two state-of-the-art temporal models for continuous sequence recognition, namely Hybrid Neural Network--Hidden Markov Models (NN-HMM) and Bidirectional Recurrent Neural Networks (BDRNN) with gated units. To do so, we reimplemented the two within a shared test-bed which is more amenable to a fair comparative work. We propose adjustments to Neural Network training losses and the Hybrid NN-HMM expressions to accommodate for highly imbalanced data classes. Although recent publications tend to prefer BDRNNs, we demonstrate that Hybrid NN-HMM remain competitive. However, the latter rely significantly on their input layers to model short-term patterns. Finally, we show that input representations learned via both approaches are largely inter-compatible. The second part of our work studies one-shot learning, which has received relatively little attention so far, in particular for sequential inputs such as gestures. We propose a model built around a Bidirectional Recurrent Neural Network. Its effectiveness is demonstrated on the recognition of isolated gestures from a sign language lexicon. We propose several improvements over this baseline by drawing inspiration from related works and evaluate their performances, exhibiting different advantages and disadvantages for each},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Granger_2019_Deep-learning for high dimensional sequential observations.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JLKM8W8Z/tel-02012106v1.html},
  language = {en},
  school = {Universit\'e Paris-Saclay}
}

@inproceedings{graves_connectionist_2006,
  title = {Connectionist {{Temporal Classification}}: {{Labelling Unsegmented Sequence Data}} with {{Recurrent Neural Networks}}},
  shorttitle = {Connectionist {{Temporal Classification}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Learning}} ({{ICML}})},
  author = {Graves, Alex and Fern{\'a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2006},
  pages = {369--376},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1143844.1143891},
  abstract = {Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8IKJ3PEB/icml_2006.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LJZBMIAX/icml_2006.pdf},
  isbn = {978-1-59593-383-6},
  series = {{{ICML}} '06}
}

@inproceedings{graves_endtoend_2014,
  title = {Towards {{End}}-{{To}}-{{End Speech Recognition}} with {{Recurrent Neural Networks}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Learning}} ({{ICML}})},
  author = {Graves, Alex and Jaitly, Navdeep},
  year = {2014},
  pages = {1764--1772},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves_Jaitly_2014_Towards End-To-End Speech Recognition with Recurrent Neural Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NFBFXVGH/graves14.html}
}

@article{graves_framewise_2005,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  volume = {18},
  pages = {602--610},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves_Schmidhuber_2005_Framewise phoneme classification with bidirectional LSTM and other neural.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8DP87ZSH/S0893608005001206.html},
  journal = {Neural Networks},
  number = {5}
}

@article{graves_novel_2009,
  title = {A {{Novel Connectionist System}} for {{Unconstrained Handwriting Recognition}}},
  author = {Graves, A. and Liwicki, M. and Fern{\'a}ndez, S. and Bertolami, R. and Bunke, H. and Schmidhuber, J.},
  year = {2009},
  month = may,
  volume = {31},
  pages = {855--868},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2008.137},
  abstract = {Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves et al_2009_A Novel Connectionist System for Unconstrained Handwriting Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DVGX6URG/4531750.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {5}
}

@inproceedings{graves_offline_2008,
  title = {Offline {{Handwriting Recognition}} with {{Multidimensional Recurrent Neural Networks}}.},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Graves, Alex and Schmidhuber, Juergen},
  year = {2008},
  pages = {545--552},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves_Schmidhuber_2008_Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks.pdf}
}

@article{graves_sequence_2012,
  ids = {graves\_sequence\_2012},
  title = {Sequence {{Transduction}} with {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2012},
  month = nov,
  abstract = {Many machine learning tasks can be expressed as the transformation---or \textbackslash emph\{transduction\}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \textbackslash emph\{finding\} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.},
  archivePrefix = {arXiv},
  eprint = {1211.3711},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves_2012_Sequence transduction with recurrent neural networks.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves_2012_Sequence Transduction with Recurrent Neural Networks2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DKMQSHW2/1211.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TSFCCRZH/1211.html},
  journal = {arXiv:1211.3711 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{graves_speech_2013,
  title = {Speech {{Recognition}} with {{Deep Recurrent Neural Networks}}},
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  year = {2013},
  month = mar,
  abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
  archivePrefix = {arXiv},
  eprint = {1303.5778},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves et al_2013_Speech Recognition with Deep Recurrent Neural Networks.pdf},
  journal = {arXiv:1303.5778 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@book{graves_supervised_2012,
  title = {Supervised Sequence Labelling},
  author = {Graves, Alex},
  year = {2012},
  publisher = {{Springer}}
}

@inproceedings{graves_unconstrained_2008,
  title = {Unconstrained On-Line Handwriting Recognition with Recurrent Neural Networks},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Graves, Alex and Liwicki, Marcus and Bunke, Horst and Schmidhuber, J{\"u}rgen and Fern{\'a}ndez, Santiago},
  year = {2008},
  month = dec,
  pages = {577--584},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Graves et al_2008_Unconstrained on-line handwriting recognition with recurrent neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5JS3VKDM/3213-unconstrained-on-line-handwriting-recognition-with.html}
}

@article{greff_lstm_2016,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  author = {Greff, K. and Srivastava, R. K. and Koutn{\'i}k, J. and Steunebrink, B. R. and Schmidhuber, J.},
  year = {2016},
  volume = {PP},
  pages = {1--11},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2016.2582924},
  abstract = {Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs (\${$\approx\$\$$}15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2016/Greff et al/Greff et al_2016_LSTM.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Greff et al_2016_LSTM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J2AM7RU5/1503.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XHUQA88I/7508408.html},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  number = {99}
}

@inproceedings{grezl_probabilistic_2007,
  title = {Probabilistic and {{Bottle}}-{{Neck Features}} for {{LVCSR}} of {{Meetings}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Grezl, F. and Karafiat, M. and Kontar, S. and Cernocky, J.},
  year = {2007},
  month = apr,
  volume = {4},
  pages = {IV-757-IV-760},
  doi = {10.1109/ICASSP.2007.367023},
  abstract = {In recent years, probabilistic features became an integral part of state-of-the-are LVCSR systems. In this work, we are exploring the possibility of obtaining the features directly from neural net without the necessity of converting output probabilities to features suitable for subsequent GMM-HMM system. We experimented with 5-layer MLP with bottle-neck in the middle layer. After training such a neural net, we used outputs of the bottle-neck as features for GMM-HMM recognition system. The benefits are twofold: first, improvement was gained when these features are used instead of the probabilistic features, second, the size of the system was reduced, as only part of the neural net is used. The experiments were performed on meetings recognition task defined in MST RT'05 evaluation.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Grezl et al_2007_Probabilistic and Bottle-Neck Features for LVCSR of Meetings.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZFSCVB64/abs_all.html}
}

@patent{grimes_digital_1983,
  title = {Digital Data Entry Glove Interface Device},
  author = {Grimes, Gary J.},
  year = {1983},
  month = nov,
  abstract = {A man-machine interface is disclosed for translating discrete hand positions into electrical signals representing alpha-numeric characters. The interface comprises a glove having sensors positioned with respect to the hand for detecting the flex of finger joints and sensors for detecting the contact between various portions of the hand. Additional sensors detect the movement of the hand with respect to a gravitational vector and a horizontal plane of reference. Further additional sensors detect the twisting and flexing of the wrist. The additional sensors are associated with prescribed mode signals which determine whether subsequently formed or priorly formed character specifying hand positions are to be entered for transmission. The alpha-numeric characters associated with the formed character specifying hand positions are transmitted only when the appropriate mode signal results. The forming and moving of the hand actuates various combinations of sensors so that electrical signals representing the specified characters are generated and transmitted.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Grimes_1983_Digital data entry glove interface device.pdf},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=26;sl=ASL;capturedBy=electronicGlove,modal=orientation,modal=shape,sl=ASL,slreco,slreco;modal=shape;modal=orientation,vocab<50,vocab=26},
  nationality = {USA},
  number = {US4414537 A}
}

@article{grinblat_classsplitting_2018,
  title = {Class-{{Splitting Generative Adversarial Networks}}},
  author = {Grinblat, Guillermo L. and Uzal, Lucas C. and Granitto, Pablo M.},
  year = {2018},
  month = may,
  abstract = {Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.},
  archivePrefix = {arXiv},
  eprint = {1709.07359},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8SBP9DYU/Grinblat et al. - 2018 - Class-Splitting Generative Adversarial Networks.pdf},
  journal = {arXiv:1709.07359 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{grobel_isolated_1997,
  title = {Isolated Sign Language Recognition Using Hidden {{Markov}} Models},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Grobel, K. and Assan, M.},
  year = {1997},
  month = oct,
  volume = {1},
  pages = {162-167 vol.1},
  doi = {10.1109/ICSMC.1997.625742},
  abstract = {This paper is concerned with the video-based recognition of isolated signs. Concentrating on the manual parameters of sign language, the system aims for the signer dependent recognition of 262 different signs. For hidden Markov modelling a sign is considered a doubly stochastic process, represented by an unobservable state sequence. The observations emitted by the states are regarded as feature vectors, that are extracted from video frames. The system achieves recognition rates up to 94\%},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Grobel_Assan_1997_Isolated sign language recognition using hidden Markov models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MZZ42ZFG/625742.html}
}

@article{gross_generic_2005,
  title = {Generic vs. Person Specific Active Appearance Models},
  author = {Gross, Ralph and Matthews, Iain and Baker, Simon},
  year = {2005},
  volume = {23},
  pages = {1080--1093},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gross et al_2005_Generic vs.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HG9TRD47/S0262885605001113.html},
  journal = {Image and Vision Computing},
  number = {12}
}

@inproceedings{gruber_sign_2018,
  title = {Sign {{Language Numeral Gestures Recognition Using Convolutional Neural Network}}},
  booktitle = {Interactive {{Collaborative Robotics}}},
  author = {Gruber, Ivan and Ryumin, Dmitry and Hr{\'u}z, Marek and Karpov, Alexey},
  editor = {Ronzhin, Andrey and Rigoll, Gerhard and Meshcheryakov, Roman},
  year = {2018},
  month = sep,
  pages = {70--77},
  address = {{Leipzig, Germany}},
  abstract = {This paper presents usage of convolutional neural network for classification of sign language numeral gestures. For requirements of this research, we created a new dataset of these gestures. The dataset was recorded via Kinect v2 device and it consists of recordings of 18 different people. Only depth data-stream was used in our research. For a classification task, there was utilized classic VGG16 architecture and its results were compared with chosen baseline method and other tested architectures. Our experiment on classification showed the great potential of neural networks for this task. We reached recognition accuracy 86.45\%, which is by more than 34\% better result than chosen baseline method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gruber et al_2018_Sign Language Numeral Gestures Recognition Using Convolutional Neural Network.pdf},
  isbn = {978-3-319-99582-3},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=10;signer=18;sl=CzSL;capturedBy=vision;capturedBy=depth,modal=shape,signer=18,sl=CzSL,slreco,slreco;modal=shape,vocab<50,vocab=10},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{grzejszczak_hand_2016,
  title = {Hand Landmarks Detection and Localization in Color Images},
  author = {Grzejszczak, Tomasz and Kawulok, Michal and Galuszka, Adam},
  year = {2016},
  month = dec,
  volume = {75},
  pages = {16363--16387},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-015-2934-5},
  abstract = {This paper introduces a new method for detecting and localizing hand landmarks in 2D color images. Location of the hand landmarks is an important source of information for recognizing hand gestures, effectively exploited in a number of recent methods which operate from the depth maps. However, this problem has not yet been satisfactorily solved for 2D color images. Here, we propose to analyze the skin-presence masks, as well as the directional image of a hand using the distance transform and template matching. This makes it possible to detect the landmarks located both at the contour and inside the hand masks. Moreover, we performed an extensive experimental study to compare the proposed method with a number of state-of-the-art algorithms. The obtained quantitative and qualitative results clearly indicate that our approach outperforms other methods, which may help improve the existing gesture recognition systems.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Grzejszczak et al_2016_Hand landmarks detection and localization in color images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6N69JEC8/s11042-015-2934-5.html},
  journal = {Multimedia Tools and Applications},
  language = {en},
  number = {23}
}

@article{guerra-filho_language_2007,
  title = {A Language for Human Action},
  author = {{Guerra-Filho}, Gutemberg and Aloimonos, Yiannis},
  year = {2007},
  volume = {40},
  pages = {42--51},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Guerra-Filho_Aloimonos_2007_A language for human action.pdf},
  journal = {Computer},
  number = {5}
}

@inproceedings{guilinyao_real_2006,
  title = {Real {{Time Large Vocabulary Continuous Sign Language Recognition Based}} on {{OP}}/{{Viterbi Algorithm}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {{Guilin Yao} and {Hongxun Yao} and {Xin Liu} and {Feng Jiang}},
  year = {2006},
  month = aug,
  volume = {3},
  pages = {312--315},
  doi = {10.1109/ICPR.2006.954},
  abstract = {Up to now, continuous sign language recognition is mainly based on statistical methods, especially hidden Markov models (HMM) and Viterbi-beam searching. However, the recognition speed often gets unacceptable with an increased vocabulary, which could cause a long time delay that is not fit for the real time recognition system. To speed up the recognition process, we present a method using one-pass (OP) pre-searching before Viterbi recognition. The experiments are processed in the large vocabulary database. Results show that the average recognition speed of OP/Viterbi approach can get a notable raise comparing with the single frame's without reducing too much recognition accuracy},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Guilin Yao et al_2006_Real Time Large Vocabulary Continuous Sign Language Recognition Based on.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6KWWC9KD/1699528.html}
}

@inproceedings{gunawan_sign_2018,
  title = {Sign {{Language Recognition Using Modified Convolutional Neural Network Model}}},
  booktitle = {2018 {{Indonesian Association}} for {{Pattern Recognition International Conference}} ({{INAPR}})},
  author = {Gunawan, {and} H. and Thiracitta, N. and Nugroho, A.},
  year = {2018},
  month = sep,
  pages = {1--5},
  doi = {10.1109/INAPR.2018.8627014},
  abstract = {Sign Language is an interesting topic and similar to Action Recognition. Especially along with the great development of Deep Learning. Video-based Sign Language Recognition is our concern because we want to recognize a sign not only by the shape but also by the action the signer does. The problem is sign language is very complex and vary. The variation of sign language is making the system harder to recognize all the words accurately. Many researchers have been researching Sign Language Recognition for a long time. So many methods had been used to find out which one is the best method. Because of similarity between Sign Language Recognition and Action Recognition, we are trying to implement one of the top-tier models in Action Recognition which is i3d inception this model is also a new Action Recognition model with very high accuracy. So we can know is it possible to adopt Action Recognition behavior into Sign Language Recognition. The goal of this paper is to implement the i3d inception model to Sign Language Recognition with transfer learning method. From the test we've been done, we got 100\% accuracy on training with 10 words and 10 signers with 100 classes but the validation accuracy is pretty low. This model is too overfit.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gunawan et al_2018_Sign Language Recognition Using Modified Convolutional Neural Network Model.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BT93FEL2/8627014.html},
  keywords = {capturedBy=coloredGlove,comparation,dataSet=LSA64,I3D,isolated,isolated;vocab=10;signer=10;sl=ArgentinianSL;capturedBy=coloredGlove;dataSet=LSA64,LSA64,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=10,sl=ArgentinianSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=10}
}

@inproceedings{guo_dense_2019,
  title = {Dense Temporal Convolution Network for Sign Language Translation},
  booktitle = {Proc. of the  {{AAAI Conf}}. on {{Artificial Intelligence}}},
  author = {Guo, Dan and Wang, Shuo and Tian, Qi and Wang, Meng},
  year = {2019},
  pages = {744--750},
  publisher = {{AAAI Press}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Guo et al_2019_Dense temporal convolution network for sign language translation.pdf},
  keywords = {augment=?,authorgroup=Hefei Tech,authorgroup=Huawei,capturedBy=vision,cnnresiduals=1,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,dataSet=Phoenix14,loss=CTC,modal=fullframe,nn=1dcnn,nn=3dcnn,nnarch=3d-resnet,nntrain=3dcnn+1dcnn+blstm,paperinfo,paperinfo;authorgroup=Hefei Tech;authorgroup=Huawei;shorttitle=Dense Temporal Conv.,re-align=0,shorttitle=Dense Temporal Conv.,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;dataSet=CSL;nn=3dcnn;nn=1dcnn;nnarch=3d-resnet;cnnresiduals=1;re-align=0;augment=?;nntrain=3dcnn+1dcnn+blstm;loss=CTC,slreco;modal=fullframe;dataSet=Phoenix14;wer=35.9/36.5;nn=3dcnn;nn=1dcnn;nnarch=3d-resnet;cnnresiduals=1;re-align=0;augment=?;nntrain=3dcnn+1dcnn+blstm;loss=CTC,vocab<500,vocab=1080,vocab=178,vocab>=1000,wer=35.9/36.5}
}

@inproceedings{guo_hierarchical_2018,
  title = {Hierarchical {{LSTM}} for {{Sign Language Translation}}},
  booktitle = {Proc. of the  {{AAAI Conf}}. on {{Artificial Intelligence}}},
  author = {Guo, Dan and Zhou, Wengang and Li, Houqiang and Wang, Meng},
  year = {2018},
  month = feb,
  pages = {6845--6852},
  address = {{New Orleans, Louisiana, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Guo et al_2018_Hierarchical LSTM for Sign Language Translation.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=50,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<500,vocab=178}
}

@article{guo_hierarchical_2020,
  title = {Hierarchical {{Recurrent Deep Fusion Using Adaptive Clip Summarization}} for {{Sign Language Translation}}},
  author = {Guo, Dan and Zhou, Wengang and Li, Anyang and Li, Houqiang and Wang, Meng},
  year = {2020},
  volume = {29},
  pages = {1575--1590},
  issn = {1941-0042},
  doi = {10.1109/TIP.2019.2941267},
  abstract = {Vision-based sign language translation (SLT) is a challenging task due to the complicated variations of facial expressions, gestures, and articulated poses involved in sign linguistics. As a weakly supervised sequence-to-sequence learning problem, in SLT there are usually no exact temporal boundaries of actions. To adequately explore temporal hints in videos, we propose a novel framework named Hierarchical deep Recurrent Fusion (HRF). Aiming at modeling discriminative action patterns, in HRF we design an adaptive temporal encoder to capture crucial RGB visemes and skeleton signees. Specifically, RGB visemes and skeleton signees are learned by the same scheme named Adaptive Clip Summarization (ACS), respectively. ACS consists of three key modules, i.e., variable-length clip mining, adaptive temporal pooling, and attention-aware weighting. Besides, based on unaligned action patterns (RGB visemes and skeleton signees), a query-adaptive decoding fusion is proposed to translate the target sentence. Extensive experiments demonstrate the effectiveness of the proposed HRF framework.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YGANDU39/8846585.html},
  journal = {IEEE Transactions on Image Processing}
}

@article{guo_online_2017,
  title = {Online {{Early}}-{{Late Fusion Based}} on {{Adaptive HMM}} for {{Sign Language Recognition}}},
  author = {Guo, Dan and Zhou, Wengang and Li, Houqiang and Wang, Meng},
  year = {2017},
  month = dec,
  volume = {14},
  pages = {8:1--8:18},
  issn = {1551-6857},
  doi = {10.1145/3152121},
  abstract = {In sign language recognition (SLR) with multimodal data, a sign word can be represented by multiply features, for which there exist an intrinsic property and a mutually complementary relationship among them. To fully explore those relationships, we propose an online early-late fusion method based on the adaptive Hidden Markov Model (HMM). In terms of the intrinsic property, we discover that inherent latent change states of each sign are related not only to the number of key gestures and body poses but also to their translation relationships. We propose an adaptive HMM method to obtain the hidden state number of each sign by affinity propagation clustering. For the complementary relationship, we propose an online early-late fusion scheme. The early fusion (feature fusion) is dedicated to preserving useful information to achieve a better complementary score, while the late fusion (score fusion) uncovers the significance of those features and aggregates them in a weighting manner. Different from classical fusion methods, the fusion is query adaptive. For different queries, after feature selection (including the combined feature), the fusion weight is inversely proportional to the area under the curve of the normalized query score list for each selected feature. The whole fusion process is effective and efficient. Experiments verify the effectiveness on the signer-independent SLR with large vocabulary. Compared either on different dataset sizes or to different SLR models, our method demonstrates consistent and promising performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Guo et al_2017_Online Early-Late Fusion Based on Adaptive HMM for Sign Language Recognition.pdf},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=370;signer=5;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hshape,modal=bodyjoints,signer=5,sl=CSL,slreco,slreco;modal=2hshape;modal=2hlocation;modal=bodyjoints,vocab<500,vocab=370},
  number = {1}
}

@article{gupta_anthropometric_2010,
  title = {Anthropometric {{3D}} Face Recognition},
  author = {Gupta, Shalini and Markey, Mia K. and Bovik, Alan C.},
  year = {2010},
  volume = {90},
  pages = {331--349},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gupta et al_2010_Anthropometric 3D face recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6MI8JZNA/s11263-010-0360-8.html},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {3}
}

@article{gupta_data_2019,
  title = {Data Augmentation for Low Resource Sentiment Analysis Using Generative Adversarial Networks},
  author = {Gupta, Rahul},
  year = {2019},
  month = feb,
  abstract = {Sentiment analysis is a task that may suffer from a lack of data in certain cases, as the datasets are often generated and annotated by humans. In cases where data is inadequate for training discriminative models, generate models may aid training via data augmentation. Generative Adversarial Networks (GANs) are one such model that has advanced the state of the art in several tasks, including as image and text generation. In this paper, I train GAN models on low resource datasets, then use them for the purpose of data augmentation towards improving sentiment classifier generalization. Given the constraints of limited data, I explore various techniques to train the GAN models. I also present an analysis of the quality of generated GAN data as more training data for the GAN is made available. In this analysis, the generated data is evaluated as a test set (against a model trained on real data points) as well as a training set to train classification models. Finally, I also conduct a visual analysis by projecting the generated and the real data into a two-dimensional space using the t-Distributed Stochastic Neighbor Embedding (t-SNE) method.},
  archivePrefix = {arXiv},
  eprint = {1902.06818},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gupta_2019_Data augmentation for low resource sentiment analysis using generative.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MA7JLW6T/1902.html},
  journal = {arXiv:1902.06818 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gupta_evaluating_2011,
  title = {Evaluating {{Dimensionality Reduction Techniques}} for {{Visual Category Recognition Using R\'enyi Entropy}}},
  author = {Gupta, A. and Bowden, R.},
  year = {2011},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gupta_Bowden_2011_Evaluating Dimensionality Reduction Techniques for Visual Category Recognition.pdf}
}

@inproceedings{gupta_selecting_2016,
  title = {Towards Selecting Robust Hand Gestures for Automotive Interfaces},
  booktitle = {Intelligent {{Vehicles Symposium}} ({{IV}}), 2016 {{IEEE}}},
  author = {Gupta, Shalini and Molchanov, Pavlo and Yang, Xiaodong and Kim, Kihwan and Tyree, Stephen and Kautz, Jan},
  year = {2016},
  pages = {1350--1357},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gupta et al_2016_Towards selecting robust hand gestures for automotive interfaces.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GH3CD5JD/7535566.html}
}

@inproceedings{gupta_texas_2010,
  title = {Texas {{3D}} Face Recognition Database},
  booktitle = {Image {{Analysis}} \& {{Interpretation}} ({{SSIAI}}), 2010 {{IEEE Southwest Symposium}} On},
  author = {Gupta, Shalini and Castleman, Kenneth R. and Markey, Mia K. and Bovik, Alan C.},
  year = {2010},
  pages = {97--100},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gupta et al_2010_Texas 3D face recognition database.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GPG2W5Z6/5483908.html}
}

@inproceedings{gweth_enhanced_2012,
  title = {Enhanced {{Continuous Sign Language Recognition}} Using {{PCA}} and {{Neural Network Features}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Gweth, Yannick and Plahl, Christian and Ney, Hermann},
  year = {2012},
  month = jun,
  pages = {55--60},
  address = {{Providence, Rhode Island, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Gweth et al_2012_Enhanced Continuous Sign Language Recognition using PCA and Neural Network.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Signum,modal=2hlocation,modal=shape,signer=1,sl=DGS,slreco,slreco;modal=2hlocation;modal=shape,vocab<500,vocab=455}
}

@techreport{hadfield_estimation_2013,
  title = {The Estimation and Use of {{3D}} Information, for Natural Human Action Recognition},
  author = {Hadfield, Simon},
  year = {2013},
  institution = {{University of Surrey}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hadfield_2013_The estimation and use of 3D information, for natural human action recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/A58HMT2P/papers.html}
}

@article{hadfield_kinecting_,
  title = {Kinecting the Dots: {{Particle Based Scene Flow From Depth Sensors}}},
  shorttitle = {Kinecting the Dots},
  author = {Hadfield, S. and Bowden, R.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hadfield_Bowden_Kinecting the dots.pdf}
}

@inproceedings{hadian_endtoend_2018,
  title = {End-to-End {{Speech Recognition Using Lattice}}-Free {{MMI}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Hadian, Hossein and Sameti, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2018},
  doi = {10.21437/Interspeech.2018-1423},
  abstract = {We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models. By end-to-end training, we mean flat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees. We use full biphones to enable context-dependent modeling without trees, and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks. We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hadian et al_2018_End-to-end Speech Recognition Using Lattice-free MMI.pdf}
}

@inproceedings{haeb-umbach_linear_1992,
  title = {Linear Discriminant Analysis for Improved Large Vocabulary Continuous Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Haeb-Umbach}, R. and Ney, H.},
  year = {1992},
  month = mar,
  volume = {1},
  pages = {13--16},
  doi = {10.1109/ICASSP.1992.225984},
  abstract = {The interaction of linear discriminant analysis (LDA) and a modeling approach using continuous Laplacian mixture density HMM is studied experimentally. The largest improvements in speech recognition could be obtained when the classes for the LDA transform were defined to be sub-phone units. On a 12000 word German recognition task with small overlap between training and test vocabulary a reduction in error rate by one-fifth was achieved compared to the case without LDA. On the development set of the DARPA RM1 task the error rate was reduced by one-third. For the DARPA speaker-dependent no-grammar case, the error rate averaged over 12 speakers was 9.9\%. This was achieved with a recognizer using LDA and a set of only 47 Viterbi-trained context-independent phonemes},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Haeb-Umbach_Ney_1992_Linear discriminant analysis for improved large vocabulary continuous speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SQ83IFCE/abs_all.html}
}

@phdthesis{hahn_monotone_2014,
  title = {Monotone {{String}}-to-{{String Translation}} for {{NLU}} and {{ASR Tasks}}},
  author = {Hahn, Stefan},
  year = {2014},
  month = nov,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hahn_2014_Monotone String-to-String Translation for NLU and ASR Tasks.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{haidar_latent_2019,
  title = {Latent {{Code}} and {{Text}}-Based {{Generative Adversarial Networks}} for {{Soft}}-Text {{Generation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Haidar, Md. Akmal and Rezagholizadeh, Mehdi and Omri, Alan Do and Rashid, Ahmad},
  year = {2019},
  pages = {2248--2258},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1234},
  abstract = {Text generation with generative adversarial networks (GANs) can be divided into the textbased and code-based categories according to the type of signals used for discrimination. In this work, we introduce a novel text-based approach called Soft-GAN to effectively exploit GAN setup for text generation. We demonstrate how autoencoders (AEs) can be used for providing a continuous representation of sentences, which we will refer to as softtext. This soft representation will be used in GAN discrimination to synthesize similar softtexts. We also propose hybrid latent code and text-based GAN (LATEXT-GAN ) approaches with one or more discriminators, in which a combination of the latent code and the softtext is used for GAN discriminations. We perform a number of subjective and objective experiments on two well-known datasets (SNLI and Image COCO) to validate our techniques. We discuss the results using several evaluation metrics and show that the proposed techniques outperform the traditional GAN-based text-generation methods.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HD9G5YDK/Haidar et al. - 2019 - Latent Code and Text-based Generative Adversarial .pdf},
  language = {en}
}

@article{haidar_textkdgan_2019,
  title = {{{TextKD}}-{{GAN}}: {{Text Generation}} Using {{KnowledgeDistillation}} and {{Generative Adversarial Networks}}},
  shorttitle = {{{TextKD}}-{{GAN}}},
  author = {Haidar, Md Akmal and Rezagholizadeh, Mehdi},
  year = {2019},
  month = apr,
  abstract = {Text generation is of particular interest in many NLP applications such as machine translation, language modeling, and text summarization. Generative adversarial networks (GANs) achieved a remarkable success in high quality image generation in computer vision, and recently, GANs have gained lots of interest from the NLP community as well. However, achieving similar success in NLP would be more challenging due to the discrete nature of text. In this work, we introduce a method using knowledge distillation to effectively exploit GAN setup for text generation. We demonstrate how autoencoders (AEs) can be used for providing a continuous representation of sentences, which is a smooth representation that assign non-zero probabilities to more than one word. We distill this representation to train the generator to synthesize similar smooth representations. We perform a number of experiments to validate our idea using different datasets and show that our proposed approach yields better performance in terms of the BLEU score and Jensen-Shannon distance (JSD) measure compared to traditional GAN-based text generation approaches without pre-training.},
  archivePrefix = {arXiv},
  eprint = {1905.01976},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P798MR6M/Haidar and Rezagholizadeh - 2019 - TextKD-GAN Text Generation using KnowledgeDistill.pdf},
  journal = {arXiv:1905.01976 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{hall_language_2017,
  title = {Language {{Deprivation Syndrome}}: {{A Possible Neurodevelopmental Disorder}} with {{Sociocultural Origins}}},
  shorttitle = {Language {{Deprivation Syndrome}}},
  author = {Hall, Wyatte C. and Levin, Leonard L. and Anderson, Melissa L.},
  year = {2017},
  month = jun,
  volume = {52},
  pages = {761--776},
  issn = {0933-7954},
  doi = {10.1007/s00127-017-1351-7},
  abstract = {Purpose
There is a need to better understand the epidemiological relationship between language development and psychiatric symptomatology. Language development can be particularly impacted by social factors, as seen in the developmental choices made for deaf children which can create language deprivation. A possible mental health syndrome may be present in deaf patients with severe language deprivation.

Methods
Electronic databases were searched to identify publications focusing on language development and mental health in the deaf population. Screening of relevant publications narrowed the search results to 35 publications.

Results
Although there is very limited empirical evidence, there appears to be suggestions of a mental health syndrome by clinicians working with deaf patients. Possible features include language dysfluency, fund of knowledge deficits, and disruptions in thinking, mood, and/or behavior.

Conclusion
The clinical specialty of deaf mental health appears to be struggling with a clinically observed phenomenon that has yet to be empirically investigated and defined within the DSM-5. Descriptions of patients within the clinical setting suggest a language deprivation syndrome. Language development experiences have an epidemiological relationship with psychiatric outcomes in deaf people. This requires more empirical attention and has implications for other populations with behavioral health disparities as well.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hall et al_2017_Language Deprivation Syndrome.pdf},
  journal = {Social Psychiatry and Psychiatric Epidemiology},
  number = {6},
  pmcid = {PMC5469702},
  pmid = {28204923}
}

@article{hall_what_2017,
  title = {What You Don't Know Can Hurt You: {{The}} Risk of Language Deprivation by Impairing Sign Language Development in Deaf Children},
  shorttitle = {What You Don't Know Can Hurt You},
  author = {Hall, Wyatte C.},
  year = {2017},
  month = may,
  volume = {21},
  pages = {961--965},
  issn = {1092-7875},
  doi = {10.1007/s10995-017-2287-y},
  abstract = {A long-standing belief is that sign language interferes with spoken language development in deaf children. Brain changes associated with language deprivation may be misrepresented as sign language interfering with spoken language outcomes of cochlear implants, which leads to professionals and organizations advocating for preventing sign language exposure before implantation and spreading misinformation. The existence of one \textendash{} time-sensitive \textendash{} language acquisition window means a strong possibility of permanent brain changes when spoken language is not fully accessible to the deaf child and sign language exposure is delayed, as is often standard practice. There is no empirical evidence for the harm of sign language exposure but there is some evidence for its benefits, and there is growing evidence that lack of language access has negative implications. This includes cognitive delays, mental health difficulties, lower quality of life, higher trauma, and limited health literacy. Claims of cochlear implant- and spoken language-only approaches being more effective than sign language-inclusive approaches are not empirically supported. Cochlear implants are an unreliable standalone first-language intervention for deaf children. Priorities of deaf child development should focus on healthy growth of all developmental domains through a fully-accessible first language foundation such as sign language, rather than auditory deprivation and speech skills.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hall_2017_What you don’t know can hurt you.pdf},
  journal = {Maternal and child health journal},
  number = {5},
  pmcid = {PMC5392137},
  pmid = {28185206}
}

@article{han_adversarial_2018,
  title = {Adversarial {{Training}} in {{Affective Computing}} and {{Sentiment Analysis}}: {{Recent Advances}} and {{Perspectives}}},
  shorttitle = {Adversarial {{Training}} in {{Affective Computing}} and {{Sentiment Analysis}}},
  author = {Han, Jing and Zhang, Zixing and Cummins, Nicholas and Schuller, Bj{\"o}rn},
  year = {2018},
  month = sep,
  abstract = {Over the past few years, adversarial training has become an extremely active research topic and has been successfully applied to various Artificial Intelligence (AI) domains. As a potentially crucial technique for the development of the next generation of emotional AI systems, we herein provide a comprehensive overview of the application of adversarial training to affective computing and sentiment analysis. Various representative adversarial training algorithms are explained and discussed accordingly, aimed at tackling diverse challenges associated with emotional AI systems. Further, we highlight a range of potential future research directions. We expect that this overview will help facilitate the development of adversarial training for affective computing and sentiment analysis in both the academic and industrial communities.},
  archivePrefix = {arXiv},
  eprint = {1809.08927},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Han et al_2018_Adversarial Training in Affective Computing and Sentiment Analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7DD3Q5FW/1809.html},
  journal = {arXiv:1809.08927 [cs]},
  keywords = {survey},
  primaryClass = {cs}
}

@article{han_boosted_2013,
  title = {Boosted Subunits: A Framework for Recognising Sign Language from Videos},
  shorttitle = {Boosted Subunits},
  author = {Han, Junwei and Awad, G. and Sutherland, A},
  year = {2013},
  month = feb,
  volume = {7},
  pages = {70--80},
  issn = {1751-9659},
  doi = {10.1049/iet-ipr.2012.0273},
  abstract = {This study addresses the problem of vision-based sign language recognition, which is to translate signs to English. The authors propose a fully automatic system that starts with breaking up signs into manageable subunits. A variety of spatiotemporal descriptors are extracted to form a feature vector for each subunit. Based on the obtained features, subunits are clustered to yield codebooks. A boosting algorithm is then applied to learn a subset of weak classifiers representing discriminative combinations of features and subunits, and to combine them into a strong classifier for each sign. A joint learning strategy is also adopted to share subunits across sign classes, which leads to a more efficient classification. Experimental results on real-world hand gesture videos demonstrate the proposed approach is promising to build an effective and scalable system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Han et al_2013_Boosted subunits.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DGZPZJP2/login.html},
  journal = {IET Image Processing},
  keywords = {capturedBy=vision,isolated,isolated;vocab=20;signer=2;sl=BSL;capturedBy=vision,modal=movement,modal=shape,signer=1,signer=2,sl=BSL,slreco,slreco;modal=shape;modal=movement,vocab<50,vocab=20},
  number = {1}
}

@article{han_modelling_2009,
  title = {Modelling and Segmenting Subunits for Sign Language Recognition Based on Hand Motion Analysis},
  author = {Han, Junwei and Awad, George and Sutherland, Alistair},
  year = {2009},
  month = apr,
  volume = {30},
  pages = {623--633},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2008.12.010},
  abstract = {Modelling and segmenting subunits is one of the important topics in sign language study. Many scholars have proposed the functional definition to subunits from the view of linguistics while the problem of efficiently implementing it using computer vision techniques is a challenge. On the other hand, a number of subunit segmentation work has been investigated for the task of vision-based sign language recognition whereas their subunits either somewhat lack the linguistic support or are improper. In this paper, we attempt to define and segment subunits using computer vision techniques, which also can be basically explained by sign language linguistics. A subunit is firstly defined as one continuous visual hand action in time and space, which comprises a series of interrelated consecutive frames. Then, a simple but efficient solution is developed to detect the subunit boundary using hand motion discontinuity. Finally, temporal clustering by dynamic time warping is adopted to merge similar segments and refine the results. The presented work does not need prior knowledge of the types of signs or number of subunits and is more robust to signer behaviour variation. Furthermore, it correlates highly with the definition of syllables in sign language while sharing characteristics of syllables in spoken languages. A set of comprehensive experiments on real-world signing videos demonstrates the effectiveness of the proposed model.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Han et al_2009_Modelling and segmenting subunits for sign language recognition based on hand.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XMQQJBQB/S0167865509000087.html},
  journal = {Pattern Recognition Letters},
  keywords = {capturedBy=coloredGlove,capturedBy=vision,isolated,isolated;vocab=10;signer=1;sl=IrishSL;capturedBy=vision,isolated;vocab=20;signer=1;sl=BSL;capturedBy=coloredGlove,modal=movement,modal=shape,signer=1,sl=BSL,sl=IrishSL,slreco,slreco;modal=shape;modal=movement,vocab<50,vocab=10,vocab=20},
  number = {6}
}

@article{handhika_gesture_2018,
  title = {Gesture Recognition for {{Indonesian Sign Language}} ({{BISINDO}})},
  author = {Handhika, T. and Zen, R. I. M. and {Murni} and Lestari, D. P. and Sari, I.},
  year = {2018},
  month = jun,
  volume = {1028},
  pages = {012173},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1028/1/012173},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Handhika et al_2018_Gesture recognition for Indonesian Sign Language (BISINDO).pdf},
  journal = {Journal of Physics: Conference Series},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=25;signer=2;sl=IndonesianSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,signer=2,sl=IndonesianSL,slreco,slreco;modal=2hlocation,vocab<50,vocab=25},
  language = {en}
}

@inproceedings{hanke_hamnosys_2004,
  title = {{{HamNoSys}} - Representing Sign Language Data in Lan- Guage Resources and Language Processing Contexts},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Hanke, Thomas},
  year = {2004},
  pages = {1--6},
  address = {{Lisbon, Portugal}}
}

@article{hannun_sequencetosequence_2019,
  title = {Sequence-to-{{Sequence Speech Recognition}} with {{Time}}-{{Depth Separable Convolutions}}},
  author = {Hannun, Awni and Lee, Ann and Xu, Qiantong and Collobert, Ronan},
  year = {2019},
  month = apr,
  abstract = {We propose a fully convolutional sequence-to-sequence encoder architecture with a simple and efficient decoder. Our model improves WER on LibriSpeech while being an order of magnitude more efficient than a strong RNN baseline. Key to our approach is a time-depth separable convolution block which dramatically reduces the number of parameters in the model while keeping the receptive field large. We also give a stable and efficient beam search inference procedure which allows us to effectively integrate a language model. Coupled with a convolutional language model, our time-depth separable convolution architecture improves by more than 22\% relative WER over the best previously reported sequence-to-sequence results on the noisy LibriSpeech test set.},
  archivePrefix = {arXiv},
  eprint = {1904.02619},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hannun et al_2019_Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions.pdf},
  journal = {arXiv:1904.02619 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@book{harte_phonemetoviseme_,
  title = {Phoneme-to-{{Viseme Mappingfor Visual Speech Recognition}}},
  author = {Harte, Naomi},
  abstract = {Phonemes are the standard modelling unit in HMM-based continuous speech recognition systems. Visemes aretheequivalentunitinthevisualdomain,butthereislessagreementonpreciselywhatvisemesare,orhow many to model on the visual side in audio-visual speech recognition systems. This paper compares the use of 5 viseme maps in a continuous speech recognition task. The focus of the study is visual-only recognition to examinethechoiceofvisememap. Allthemapsarebasedonthephoneme-to-visemeapproach,createdeither usingalinguisticmethodoradatadrivenmethod. DCT,PCAandopticalflowareusedasthevisualfeatures. Thebestvisual-onlyrecognitionontheVidTIMITdatabaseisachievedusingalinguisticallymotivatedviseme set. These initial experiments demonstrate that the choice of visual unit requires more careful attention in audio-visualspeech recognition systemdevelopment. 1},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Harte_Phoneme-to-Viseme Mappingfor Visual Speech Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/897FXM23/summary.html}
}

@phdthesis{hasan_triplet_2011,
  title = {Triplet {{Lexicon Models}} for {{Statistical Machine Translation}}},
  author = {Hasan, Sa{\v s}a},
  year = {2011},
  month = nov,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hasan_2011_Triplet Lexicon Models for Statistical Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@article{hashim_kurdish_2018,
  title = {Kurdish {{Sign Language Recognition System}}},
  author = {Hashim, Abdulla D. and Alizadeh, Fattah},
  year = {2018},
  month = jun,
  volume = {2},
  pages = {1--6},
  issn = {2520-7792},
  doi = {10.25079/ukhjse.v2n1y2018.pp1-6},
  copyright = {Copyright (c) 2018 Abdulla D. Hashim, Fattah Alizadeh},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hashim_Alizadeh_2018_Kurdish Sign Language Recognition System.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/H8S55ZKZ/33.html},
  journal = {UKH Journal of Science and Engineering},
  keywords = {capturedBy=vision,isolated,isolated;vocab=12;sl=KurdishSL;capturedBy=vision,modal=shape,sl=KurdishSL,slreco,slreco;modal=shape,vocab<50,vocab=12},
  language = {en},
  number = {1}
}

@article{hassan_multiple_2019,
  title = {Multiple {{Proposals}} for {{Continuous Arabic Sign Language Recognition}}},
  author = {Hassan, Mohamed and Assaleh, Khaled and Shanableh, Tamer},
  year = {2019},
  month = jan,
  volume = {20},
  pages = {4},
  issn = {1557-2072},
  doi = {10.1007/s11220-019-0225-3},
  abstract = {The deaf community relies on sign language as the primary means of communication. For the millions of people around the world who suffer from hearing loss, interaction with hearing people is quite difficult. The main objective of sign language recognition (SLR) is the development of automatic SLR systems to facilitate communication with the deaf community. Arabic SLR (ArSLR) specifically did not receive much attention until recent years. This work presents a comprehensive comparison between two different recognition techniques for continuous ArSLR, namely a Modified k-Nearest Neighbor which is suitable for sequential data and Hidden Markov Models (HMMs) techniques based on two different toolkits. Additionally, in this work, two new ArSL datasets composed of 40 Arabic sentences are collected using Polhemus G4 motion tracker and a camera. An existing glove-based dataset is employed in this work as well. The three datasets are made publicly available to the research community. The advantages and disadvantages of each data acquisition approach and classification technique are discussed in this paper. In the experimental results section, it is shown that classification accuracy for sign sentences acquired using a motion tracker are very similar the classification accuracy for sentences acquired using sensor gloves. The modified KNN solution is inferior to HMMs in terms of the computational time required for classification.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hassan et al_2019_Multiple Proposals for Continuous Arabic Sign Language Recognition2.pdf},
  journal = {Sensing and Imaging},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,capturedBy=vision,continuous,continuous;vocab=80;signer=1;sl=ArSL;capturedBy=electronicGlove;capturedBy=mocap,continuous;vocab=80;signer=1;sl=ArSL;capturedBy=vision,continuous;vocab=80;signer=2;sl=ArSL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,signer=1,signer=2,sl=ArSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<100,vocab=80},
  language = {en},
  number = {1}
}

@article{hatano_contourbased_,
  title = {Contour-Based {{Hand Pose Recognition}} for {{Sign Language Recognition}}},
  author = {Hatano, Mika and Sako, Shinji and Kitamura, Tadashi},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hatano et al_Contour-based Hand Pose Recognition for Sign Language Recognition.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/undefined/Hatano et al/Hatano et al_Contour-based Hand Pose Recognition for Sign Language Recognition.pdf}
}

@inproceedings{hatty_akkubohrhammer_2019,
  title = {Akku\textbullet{{Bohr}}\textbullet{{Hammer}} vs. {{Akku}}\textbullet{{Bohrhammer}}: {{Experiments}} towards the {{Evaluation}} of {{Compound Splitting Tools}} for {{General Language}} and {{Specific Domains}}},
  booktitle = {Proceedings of the 15th {{Conference}} on {{Natural Language Processing}}},
  author = {Hatty, Anna and Heid, Ulrich and Moskvina, Anna and Bettinger, Julia and Dorna, Michael and GmbH, Robert Bosch},
  year = {2019},
  pages = {59--67},
  address = {{Erlangen, Germany}},
  abstract = {We present a comparative evaluation study for splitting German compounds which belong to general language or to a specific domain. For the domain, we focus on DIY (''do-it-yourself''). The study consists of two parts: First, we evaluate three tools for compound splitting in German, one based on lexicons and corpus frequencies and two based on language-independent statistical processing. We introduce the tools, discuss the data and the construction of a gold standard, and show first results for binary and ternary noun compounds, as well as for the handling of non-splittable items. In a second experiment, we post-train one of the splitters with text data from the DIYdomain, and evaluate the splitting performance on domain-specific compounds.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hatty et al_2019_Akku•Bohr•Hammer vs.pdf},
  language = {en}
}

@article{haug_automatic_2018,
  title = {Automatic Sign Language Recognition for Sign Language Assessment},
  author = {Haug, Tobias and Ebling, Sarah and Braem, Penny Boyes and Tissi, Katja and {Sidler-Miserez}, Sandra},
  year = {2018},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Haug et al_2018_Automatic sign language recognition for sign language assessment.pdf}
}

@incollection{hauser_studying_2015,
  title = {Studying {{Sign Language Disorders}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Hauser, Peter C. and Quinto-Pozos, David and Singleton, Jenny L.},
  year = {2015},
  pages = {336--351},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch18},
  abstract = {This chapter provides a clinical neuropsychological assessment approach for assisting with the identification of language disorders in signers. The areas covered include documenting the signers' history and background and testing their general cognitive ability, receptive and expressive language skills. It also include facial processing abilities, vision, visual spatial processing, motor skills of the hands and fingers, executive functioning, visual learning and memory, and social-emotional functioning. The assessment of these domains helps one to rule out a primary language disorder when the language symptoms may be secondary to a different primary disorder. This assessment approach aids the researcher in determining the possible contributing cognitive factors and in describing the symptomology. The chapter examines how to assess developmental language disorders in children, the same neuropsychological assessment approach can be used with adults who may have experienced a disruption to language functioning secondary to a stroke, a tumor, dementia, or a head injury.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hauser et al_2015_Studying Sign Language Disorders.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AWG92NSS/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{hazen_segmentbased_2004,
  title = {A {{Segment}}-Based {{Audio}}-Visual {{Speech Recognizer}}: {{Data Collection}}, {{Development}}, and {{Initial Experiments}}},
  shorttitle = {A {{Segment}}-Based {{Audio}}-Visual {{Speech Recognizer}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Multimodal Interfaces}} ({{ICMI}})},
  author = {Hazen, Timothy J. and Saenko, Kate and La, Chia-Hao and Glass, James R.},
  year = {2004},
  pages = {235--242},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1027933.1027972},
  abstract = {This paper presents the development and evaluation of a speaker-independent audio-visual speech recognition (AVSR) system that utilizes a segment-based modeling strategy. To support this research, we have collected a new video corpus, called Audio-Visual TIMIT (AV-TIMIT), which consists of 4 total hours of read speech collected from 223 different speakers. This new corpus was used to evaluate our new AVSR system which incorporates a novel audio-visual integration scheme using segment-constrained Hidden Markov Models (HMMs). Preliminary experiments have demonstrated improvements in phonetic recognition performance when incorporating visual information into the speech recognition process.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hazen et al_2004_A Segment-based Audio-visual Speech Recognizer.pdf},
  isbn = {1-58113-995-0},
  series = {{{ICMI}} '04}
}

@article{he_convolutional_2014,
  title = {Convolutional {{Neural Networks}} at {{Constrained Time Cost}}},
  author = {He, Kaiming and Sun, Jian},
  year = {2014},
  month = dec,
  abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than "AlexNet" (16.0\% top-5 error, 10-view test).},
  archivePrefix = {arXiv},
  eprint = {1412.1710},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/He_Sun_2014_Convolutional Neural Networks at Constrained Time Cost.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HD5XK9P5/1412.html},
  journal = {arXiv:1412.1710 [cs]},
  primaryClass = {cs}
}

@inproceedings{he_deep_2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  address = {{Las Vegas, NV, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2016/He et al/He et al_2016_Deep Residual Learning for Image Recognition.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/He et al_2016_Deep Residual Learning for Image Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FVXDQ34A/1512.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/W39S9J4S/1512.html}
}

@inproceedings{he_delving_2015,
  title = {Delving Deep into Rectifiers: {{Surpassing}} Human-Level Performance on Imagenet Classification},
  shorttitle = {Delving Deep into Rectifiers},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  pages = {1026--1034},
  address = {{Santiago, Chile}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/He et al/He et al_2015_Delving deep into rectifiers.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/He et al_2015_Delving deep into rectifiers.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FDS7MWUZ/1502.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PGVB4VTV/He_Delving_Deep_into_ICCV_2015_paper.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZV9Q22DG/1502.html}
}

@inproceedings{he_identity_2016,
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/He et al_2016_Identity Mappings in Deep Residual Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X94XFJ68/1603.html}
}

@incollection{he_text_2015,
  title = {Text to {{Head Motion Synthesis}} for {{Chinese Sign Language Avatar}}},
  booktitle = {Image and {{Graphics}}},
  author = {He, Wenjing and Liu, Junfa and Chen, Yiqiang},
  year = {2015},
  pages = {453--459},
  publisher = {{Springer}}
}

@inproceedings{heigold_asynchronous_2014,
  title = {Asynchronous Stochastic Optimization for Sequence Training of Deep Neural Networks},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Heigold, Georg and McDermott, Erik and Vanhoucke, Vincent and Senior, Alan and Bacchiani, Michiel},
  year = {2014},
  pages = {5587--5591},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Heigold et al_2014_Asynchronous stochastic optimization for sequence training of deep neural.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CP3H2DI6/abs_all.html}
}

@inproceedings{heigold_discriminative_2010,
  title = {Discriminative {{HMMS}}, Log-Linear Models, and {{CRFS}}: {{What}} Is the Difference?},
  shorttitle = {Discriminative {{HMMS}}, Log-Linear Models, and {{CRFS}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Heigold, G. and Wiesler, S. and {Nu{\ss}baum-Thom}, M. and Lehnen, P. and Schl{\"u}ter, R. and Ney, H.},
  year = {2010},
  month = mar,
  pages = {5546--5549},
  doi = {10.1109/ICASSP.2010.5495228},
  abstract = {Recently, there have been many papers studying discriminative acoustic modeling techniques like conditional random fields or discriminative training of conventional Gaussian HMMs. This paper will give an overview of the recent work and progress. We will strictly distinguish between the type of acoustic models on the one hand and the training criterion on the other hand. We will address two issues in more detail: the relation between conventional Gaussian HMMs and conditional random fields and the advantages of formulating the training criterion as a convex optimization problem. Experimental results for various speech tasks will be presented to carefully evaluate the different concepts and approaches, including both a digit string and large vocabulary continuous speech recognition tasks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Heigold et al_2010_Discriminative HMMS, log-linear models, and CRFS.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BZ2MZ7UZ/5495228.html}
}

@phdthesis{heigold:phd2010,
  title = {A {{Log}}-{{Linear Discriminative Modeling Framework}} for {{Speech Recognition}}},
  author = {Heigold, Georg},
  year = {2010},
  month = jun,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Heigold_2010_A Log-Linear Discriminative Modeling Framework for Speech Recognition.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{heili_improving_2014,
  title = {Improving Head and Body Pose Estimation through Semi-Supervised Manifold Alignment},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Heili, A. and Varadarajan, J. and Ghanem, B. and Ahuja, N. and Odobez, J. M.},
  year = {2014},
  month = oct,
  pages = {1912--1916},
  doi = {10.1109/ICIP.2014.7025383},
  abstract = {In this paper, we explore the use of a semi-supervised manifold alignment method for domain adaptation in the context of human body and head pose estimation in videos. We build upon an existing state-of-the-art system that leverages on external labelled datasets for the body and head features, and on the unlabelled test data with weak velocity labels to do a coupled estimation of the body and head pose. While this previous approach showed promising results, the learning of the underlying manifold structure of the features in the train and target data and the need to align them were not explored despite the fact that the pose features between two datasets may vary according to the scene, e.g. due to different camera point of view or perspective. In this paper, we propose to use a semi-supervised manifold alignment method to bring the train and target samples closer within the resulting embedded space. To this end, we consider an adaptation set from the target data and rely on (weak) labels, given for example by the velocity direction whenever they are reliable. These labels, along with the training labels are used to bias the manifold distance within each manifold and to establish correspondences for alignment.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Heili et al_2014_Improving head and body pose estimation through semi-supervised manifold.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/238NFZK9/articleDetails.html}
}

@inproceedings{heinzmann_3d_1998,
  title = {3-{{D}} Facial Pose and Gaze Point Estimation Using a Robust Real-Time Tracking Paradigm},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Heinzmann, Jochen and Zelinsky, Alexander},
  year = {1998},
  pages = {142--147},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Heinzmann_Zelinsky_1998_3-D facial pose and gaze point estimation using a robust real-time tracking.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZU99PJCT/login.html}
}

@article{heloir_intuitive_2015,
  title = {Toward an Intuitive Sign Language Animation Authoring System for the Deaf},
  author = {Heloir, Alexis and Nunnari, Fabrizio},
  year = {2015},
  pages = {1--11},
  journal = {Universal Access in the Information Society}
}

@incollection{hemdal_feature_1967,
  title = {A Feature Based Computer Recognition Program for the Modeling of Vowel Perception},
  booktitle = {Models for the {{Perception}} of {{Speech}} and {{Visual Form}}},
  author = {Hemdal, John F. and Hughes, George W.},
  editor = {{Wathen-Dunn}, Weiant},
  year = {1967},
  month = jul,
  pages = {440--453},
  publisher = {{M.I.T. Press}},
  address = {{Cambrigde, MA, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hemdal_Hughes_1967_A feature based computer recognition program for the modeling of vowel.pdf},
  isbn = {978-0-262-23026-1}
}

@article{henner_recent_2018,
  title = {Recent {{Issues}} in the {{Use}} of {{Signed Language Assessments}} for {{Diagnosis}} of {{Language Disorders}} in {{Signing Deaf}} and {{Hard}} of {{Hearing Children}}},
  author = {Henner, Jon and Novogrodsky, Rama and Reis, Jeanne and Hoffmeister, Robert},
  year = {2018},
  month = oct,
  volume = {23},
  pages = {307--316},
  issn = {1081-4159},
  doi = {10.1093/deafed/eny014},
  abstract = {Abstract.  In recent years, normed signed language assessments have become a useful tool for researchers, practitioners, and advocates. Nevertheless, there are},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Henner et al_2018_Recent Issues in the Use of Signed Language Assessments for Diagnosis of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NRV58TXI/4996054.html},
  journal = {The Journal of Deaf Studies and Deaf Education},
  language = {en},
  number = {4}
}

@inproceedings{henrich_determining_2011,
  title = {Determining {{Immediate Constituents}} of {{Compounds}} in {{GermaNet}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Advances}} in {{Natural Language Processing}}},
  author = {Henrich, Verena and Hinrichs, Erhard},
  year = {2011},
  month = sep,
  pages = {420--426},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hissar, Bulgaria}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Henrich_Hinrichs_2011_Determining Immediate Constituents of Compounds in GermaNet.pdf}
}

@inproceedings{heracleous_hmmbased_2009,
  title = {{{HMM}}-Based Vowel and Consonant Automatic Recognition in {{Cued Speech}} for {{French}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Virtual Environments}}, {{Human}}-{{Computer Interfaces}} and {{Measurements Systems}} ({{VECIMS}})},
  author = {Heracleous, P. and Aboutabit, N. and Beautemps, D.},
  year = {2009},
  month = may,
  pages = {33--37},
  doi = {10.1109/VECIMS.2009.5068861},
  abstract = {In this paper, hidden Markov models (HMM)-based vowel and consonant automatic recognition in cued speech for French are presented. Cued speech is a visual communication mode which uses handshapes in different positions and in combination with lip-patterns of speech, makes all the sounds of spoken language clearly understandable to deaf and hearing-impaired people. The aim of cued speech is to overcome the problems of lipreading and thus enable deaf children and adults to fully understand a spoken language. Previously, the authors have reported experimental results on vowel recognition in cued speech for French. This study, further investigates the vowel recognition, and also reports automatic consonant recognition experiments in cued speech for French. In addition, isolated word recognition experiments both in normal-hearing and deaf subject are presented, showing a promising word accuracy of 92\% on average.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Heracleous et al_2009_HMM-based vowel and consonant automatic recognition in Cued Speech for French.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/H6XCDIJ6/abs_all.html}
}

@article{heracleous_lip_2009,
  title = {Lip {{Shape}} and {{Hand Position Fusion}} for {{Automatic Vowel Recognition}} in {{Cued Speech}} for {{French}}},
  author = {Heracleous, P. and Aboutabit, N. and Beautemps, D.},
  year = {2009},
  month = may,
  volume = {16},
  pages = {339--342},
  issn = {1070-9908},
  doi = {10.1109/LSP.2009.2016011},
  abstract = {Cued speech is a visual mode of communication that uses handshapes and placements in combination with the mouth movements of speech to make the phonemes of a spoken language look different from each other and clearly understandable to deaf and hearing-impaired people. The aim of cued speech is to overcome the problems of lip reading and thus enable deaf children and adults to wholly understand spoken language. Cued speech recognition requires hand gesture recognition and lip shape recognition, and also integration of the two components. This article presents hidden Markov model (HMM)-based vowel recognition as used in Cued Speech for French. Based on concatenative feature fusion and multistream HMM decision fusion, lip shape and hand position components were integrated into a single component, and automatic vowel recognition was realized. In the case of multistream HMM decision fusion, the obtained vowel classification accuracy using lip shape and hand position information was 87.6\%, showing absolute improvement of 19.6\% in comparison with a use restricted only to lip parameters.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Heracleous et al_2009_Lip Shape and Hand Position Fusion for Automatic Vowel Recognition in Cued.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/65MDBH6V/abs_all.html},
  journal = {IEEE Signal Processing Letters},
  number = {5}
}

@article{hermansky_multistream_2013,
  title = {Multistream {{Recognition}} of {{Speech}}: {{Dealing With Unknown Unknowns}}},
  shorttitle = {Multistream {{Recognition}} of {{Speech}}},
  author = {Hermansky, H.},
  year = {2013},
  month = may,
  volume = {101},
  pages = {1076--1088},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2012.2236871},
  abstract = {The paper discusses an approach for dealing with unexpected acoustic elements in speech. The approach is motivated by observations of human performance on such problems, which indicate the existence of multiple parallel processing streams in the human speech processing cognitive system, combined with the human ability to know when the correct information is being received. Some earlier relevant engineering approaches in multistream automatic recognition of speech (ASR) that aimed at processing of noisy speech and at dealing with unexpected out-of-vocabulary words are reviewed. The paper also reviews some currently active research in multistream ASR, focusing mainly on feedback-based techniques involving fusion of information between individual processing streams. The difference between the system behavior on its training data and during its operation is proposed as a substitute for the human ability of ``knowing when knowing.'' Most recent results indicate 9\% relative improvement in error rates in phoneme recognition of high signal-to-noise ratio speech and as high as 30\% relative improvements in moderate noise.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hermansky_2013_Multistream Recognition of Speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U32FX6X7/6428587.html},
  journal = {Proceedings of the IEEE},
  number = {5}
}

@article{hermansky_perceptual_1990,
  title = {Perceptual {{Linear Predictive}} (\{\vphantom\}{{PLP}}\vphantom\{\}) {{Analysis}} of {{Speech}}},
  author = {Hermansky, Hynek},
  year = {1990},
  volume = {87},
  pages = {1738--1752},
  journal = {The Journal of the Acoustical Society of America},
  number = {4}
}

@article{hermansky_rastaplp_1992,
  title = {\{\vphantom\}{{Rasta}}-{{PLP}}\vphantom\{\} {{Speech Analysis Technique}}},
  author = {Hermansky, Hynek and Morgan, Nelson and Bayya, Aruna and Kohn, Phil},
  year = {1992},
  pages = {I.121-I.124},
  journal = {Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)}
}

@inproceedings{hermansky_tandem_2000,
  title = {Tandem Connectionist Feature Extraction for Conventional {{HMM}} Systems},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hermansky, Hynek and Ellis, Daniel W. and Sharma, Shantanu},
  year = {2000},
  month = jun,
  volume = {3},
  pages = {1635--1638},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hermansky et al_2000_Tandem connectionist feature extraction for conventional HMM systems.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IHDCT57X/abs_all.html}
}

@inproceedings{hernandez-rebollar_new_2004,
  title = {A {{New Instrumented Approach}} for {{Translating American Sign Language}} into {{Sound}} and {{Text}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {{Hernandez-Rebollar}, J. L. and Kyriakopoulos, N. and Lindeman, R. W.},
  year = {2004},
  month = may,
  pages = {547--552},
  doi = {10.1109/AFGR.2004.1301590},
  abstract = {This work discusses an approach for capturing and translating isolated gestures of American Sign Language into spoken and written words. The instrumented part of the system combines an AcceleGlove and a two-link arm skeleton. Gestures of the American Sign Language are broken down into unique sequences of phonemes called poses and movements, recognized by software modules trained and tested independently on volunteers with different hand sizes and signing ability. Recognition rates of independent modules reached up to 100\% for 42 postures, orientations, 11 locations and 7 movements using linear classification. The overall sign recognizer was tested using a subset of the American Sign Language dictionary comprised by 30 one-handed signs, achieving 98\% accuracy. The system proved to be scalable: when the lexicon was extended to 176 signs and tested without retraining, the accuracy was 95\%. This represents an improvement over classification based on hidden Markov models (HMMs) and neural networks (NNs).},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hernandez-Rebollar et al_2004_A new instrumented approach for translating American Sign Language into sound.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/A59ZVS4G/1301590.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=176;signer=17;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=location,modal=movement,modal=orientation,modal=shape,signer=17,sl=ASL,slreco,slreco;modal=location;modal=movement;modal=shape;modal=orientation,vocab<500,vocab=176}
}

@article{hernandez-vela_probabilitybased_2014,
  title = {Probability-Based Dynamic Time Warping and Bag-of-Visual-and-Depth-Words for Human Gesture Recognition in Rgb-d},
  author = {{Hern{\'a}ndez-Vela}, Antonio and Bautista, Miguel {\'A}ngel and {Perez-Sala}, Xavier and {Ponce-L{\'o}pez}, V{\'i}ctor and Escalera, Sergio and Bar{\'o}, Xavier and Pujol, Oriol and Angulo, Cecilio},
  year = {2014},
  volume = {50},
  pages = {112--121},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hernández-Vela et al_2014_Probability-based dynamic time warping and bag-of-visual-and-depth-words for.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2RZGQDZG/S0167865513003450.html},
  journal = {Pattern Recognition Letters}
}

@inproceedings{herrmannsdoerfer_modellevel_2009,
  title = {Model-Level {{Simulation}} for {{COLA}}},
  booktitle = {Proceedings of the 2009 {{ICSE Workshop}} on {{Modeling}} in {{Software Engineering}}},
  author = {Herrmannsdoerfer, M. and Haberl, W. and Baumgarten, U.},
  year = {2009},
  pages = {38--43},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Herrmannsdoerfer et al_2009_Model-level Simulation for COLA.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JSUAHTKC/Herrmannsdoerfer et al. - 2009 - Model-level Simulation for COLA.html}
}

@inproceedings{hienz_realtime_1996,
  title = {Real-Time Hand-Arm Motion Analysis Using a Single Video Camera},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Hienz, H. and Grobel, K. and Offner, G.},
  year = {1996},
  month = oct,
  pages = {323--327},
  doi = {10.1109/AFGR.1996.557285},
  abstract = {We describe a video-based analysis system for acquisition and classification of hand-arm motion concerning German sign language. These motions are recorded with a single video camera by use of a modular framegrabber system. Data acquisition as well as motion classification are performed in real-time. A colour coded glove and coloured markers at the elbow and shoulder are used. These markers are segmented from the recorded input images as a first step of image processing. Thereafter features of these coloured areas are calculated which are used for determining the 20 positions for each frame and hence the positions of hand and arm. The missing third dimension is derived from a geometric model of the human hand-arm system. The sequence of the position data is converted into a certain representation of motion. Motion is derived from rule-based classification of the performed gesture, which yields a recognition rate of 95\%},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hienz et al_1996_Real-time hand-arm motion analysis using a single video camera.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GVEM3C98/abs_all.html}
}

@inproceedings{hilder_pursuit_2010,
  title = {In Pursuit of Visemes},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Auditory}}-{{Visual Speech Processing}}},
  author = {Hilder, Sarah and Theobald, Barry-John and Harvey, Richard},
  year = {2010},
  pages = {154--159},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hilder et al_2010_In pursuit of visemes.pdf}
}

@phdthesis{Hilger2004,
  title = {Quantile {{Based Histogram Equalization}} for {{Noise Robust Speech Recognition}}},
  author = {Hilger, Florian},
  year = {2004},
  month = dec,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hilger_2004_Quantile Based Histogram Equalization for Noise Robust Speech Recognition.pdf},
  school = {RWTH Aachen University}
}

@incollection{hill_data_2015,
  title = {Data {{Collection}} in {{Sociolinguistics}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Hill, Joseph C.},
  year = {2015},
  pages = {193--205},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch12},
  abstract = {With the Deaf communities, sociolinguistic researchers face special challenges through the different phases of the sociolinguistic projects: recruitment, data collection, and data presentation. This chapter discusses only recruitment and data collection. It addresses the special challenges, and provides a description of data collection methods that are used in the sociolinguistic studies of deaf and hard of hearing people whose primary means of communication are sign languages in their respective communities. In the sociolinguistic literature, a certain set of geographical and social characteristics have been identified as factors of language variation; the factors are typical in variation studies, but they could apply in other areas of sociolinguistics that deal with the use of language in targeted communities. There are two aspects of language ability displayed by language users in general: linguistic competence and linguistic performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hill_2015_Data Collection in Sociolinguistics.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DG8LKPEK/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@book{hochreiter_gradient_2001,
  title = {Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-Term Dependencies},
  shorttitle = {Gradient Flow in Recurrent Nets},
  author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, J{\"u}rgen},
  year = {2001},
  publisher = {{A field guide to dynamical recurrent neural networks. IEEE Press}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hochreiter et al_2001_Gradient flow in recurrent nets.pdf}
}

@article{hochreiter_long_1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  volume = {9},
  pages = {1735--1780},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hochreiter_Schmidhuber_1997_Long short-term memory.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZNZ964WU/neco.1997.9.8.html},
  journal = {Neural Computation},
  number = {8}
}

@inproceedings{hoey_hierarchical_2001,
  title = {Hierarchical Unsupervised Learning of Facial Expression Categories},
  booktitle = {Detection and {{Recognition}} of {{Events}} in {{Video}}, 2001. {{Proceedings}}. {{IEEE Workshop}} On},
  author = {Hoey, Jesse},
  year = {2001},
  pages = {99--106},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2001/Hoey/Hoey_2001_Hierarchical unsupervised learning of facial expression categories.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hoey_2001_Hierarchical unsupervised learning of facial expression categories.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/45NDQH28/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2HW75ERV/abs_all.html}
}

@article{hoffmann-dilloway_dont_2013,
  title = {({{Don}}'t) {{Write My Lips}}: {{Interpretations}} of the {{Relationship}} between {{German Sign Language}} and {{German}} across {{Scales}} of {{SignWriting Practice}}},
  shorttitle = {({{Don}}'t) {{Write My Lips}}},
  author = {{Hoffmann-Dilloway}, Erika},
  year = {2013},
  month = sep,
  volume = {1},
  pages = {243--272},
  issn = {23264489, 23264497},
  doi = {10.1086/672321},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hoffmann-Dilloway_2013_(Don’t) Write My Lips.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JDAJXRT3/_Don_t_Write_My_Lips_Interpretations_of_the_Relationship_between_German_Sign_Language_and_Germa.html},
  journal = {Signs and Society},
  language = {en},
  number = {2}
}

@phdthesis{hoffmeister_bayes_2011,
  title = {Bayes {{Risk Decoding}} and Its {{Application}} to {{System Combination}}},
  author = {Hoffmeister, Bj{\"o}rn},
  year = {2011},
  month = jul,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hoffmeister_2011_Bayes Risk Decoding and its Application to System Combination.pdf},
  school = {RWTH Aachen University}
}

@article{hohenberger_possible_2007,
  title = {The Possible Range of Variation between Sign Languages: {{Universal Grammar}}, Modality, and Typological Aspects},
  shorttitle = {The Possible Range of Variation between Sign Languages},
  author = {Hohenberger, Annette},
  year = {2007},
  volume = {188},
  pages = {341},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hohenberger_2007_The possible range of variation between sign languages.pdf},
  journal = {TRENDS IN LINGUISTICS STUDIES AND MONOGRAPHS}
}

@inproceedings{holden_visual_2000,
  title = {Visual {{Sign Language Recognition}}},
  booktitle = {Multi-{{Image Analysis}}},
  author = {Holden, Eun-Jung and Owens, Robyn},
  editor = {Klette, Reinhard and Gimel'farb, Georgy and Huang, Thomas},
  year = {2000},
  month = mar,
  pages = {270--287},
  address = {{Dagstuhl Castle, Germany}},
  abstract = {Automatic gesture recognition systems generally require two separate processes: a motion sensing process where some motion features are extracted from the visual input; and a classification process where the features are recognised as gestures. We have developed the Hand Motion Understanding (HMU) system that uses the combination of a 3D model-based hand tracker for motion sensing and an adaptive fuzzy expert system for motion classification. The HMU system understands static and dynamic hand signs of the Australian Sign Language (Auslan). This paper presents the hand tracker that extracts 3D hand configuration data with 21 degrees-of-freedom (DOFs) from a 2D image sequence that is captured from a single viewpoint, with the aid of a colour-coded glove. Then the temporal sequence of 3D hand configurations detected by the tracker is recognised as a sign by an adaptive fuzzy expert system. The HMU system was evaluated with 22 static and dynamic signs. Before training the HMU system achieved 91\% recognition, and after training it achieved over 95\% recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Holden_Owens_2001_Visual Sign Language Recognition.pdf},
  isbn = {978-3-540-45134-1},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=22;signer=1;sl=Auslan;capturedBy=coloredGlove,modal=shape,signer=1,sl=Auslan,slreco,slreco;modal=shape,vocab<50,vocab=22},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{hollenstein_compilation_2014,
  title = {Compilation of a {{Swiss German}} Dialect Corpus and Its Application to {{PoS}} Tagging},
  booktitle = {Proceedings of the {{First Workshop}} on {{Applying NLP Tools}} to {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Hollenstein, Nora and Aepli, No{\"e}mi},
  year = {2014},
  pages = {85--94},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hollenstein_Aepli_2014_Compilation of a Swiss German dialect corpus and its application to PoS tagging.pdf}
}

@inproceedings{hollenstein_resource_2015,
  title = {A {{Resource}} for {{Natural Language Processing}} of {{Swiss German Dialects}}.},
  booktitle = {{{GSCL}}},
  author = {Hollenstein, Nora and Aepli, No{\"e}mi},
  year = {2015},
  pages = {108--109},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hollenstein_Aepli_2015_A Resource for Natural Language Processing of Swiss German Dialects.pdf}
}

@article{honnet_machine_2017,
  title = {Machine {{Translation}} of {{Low}}-{{Resource Spoken Dialects}}: {{Strategies}} for {{Normalizing Swiss German}}},
  shorttitle = {Machine {{Translation}} of {{Low}}-{{Resource Spoken Dialects}}},
  author = {Honnet, Pierre-Edouard and {Popescu-Belis}, Andrei and Musat, Claudiu and Baeriswyl, Michael},
  year = {2017},
  month = oct,
  abstract = {The goal of this work is to design a machine translation (MT) system for a low-resource family of dialects, collectively known as Swiss German, which are widely spoken in Switzerland but seldom written. We collected a significant number of parallel written resources to start with, up to a total of about 60k words. Moreover, we identified several other promising data sources for Swiss German. Then, we designed and compared three strategies for normalizing Swiss German input in order to address the regional diversity. We found that character-based neural MT was the best solution for text normalization. In combination with phrase-based statistical MT, our solution reached 36\% BLEU score when translating from the Bernese dialect. This value, however, decreases as the testing data becomes more remote from the training one, geographically and topically. These resources and normalization techniques are a first step towards full MT of Swiss German dialects.},
  archivePrefix = {arXiv},
  eprint = {1710.11035},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Honnet et al_2017_Machine Translation of Low-Resource Spoken Dialects.pdf},
  journal = {arXiv:1710.11035 [cs]},
  primaryClass = {cs}
}

@article{hopkins_choosing_2008,
  title = {Choosing How to Write Sign Language: A Sociolinguistic Perspective},
  shorttitle = {Choosing How to Write Sign Language},
  author = {Hopkins, Jason},
  year = {2008},
  volume = {2008},
  pages = {75--89},
  issn = {1613-3668},
  doi = {10.1515/IJSL.2008.036},
  abstract = {This article introduces the reader to the sociolinguistic issues surrounding the adoption of a writing system for sign languages. Initially, some background on sign language and Deaf culture is presented, followed by a discussion of several alternatives for writing sign languages and how these alternatives have been used and/or adopted. Sign languages in most parts of the world compete with spoken languages (languages that have established written traditions), resulting in diglossia. Though many scholars who work with the deaf community have tried to develop ways to write sign language(s), many Deaf do not feel the need for a writing system, either because they use video media or because they see writing as best done in the dominant language in their diglossic situation.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hopkins_2008_Choosing how to write sign language.pdf},
  journal = {International Journal of the Sociology of Language},
  number = {192}
}

@article{horn_determining_1981,
  title = {Determining Optical Flow},
  author = {Horn, Berthold K. P. and Schunck, Brian G.},
  year = {1981},
  volume = {17},
  pages = {185--203},
  issn = {0004-3702},
  doi = {http://dx.doi.org/10.1016/0004-3702(81)90024-2},
  journal = {Artificial Intelligence},
  number = {1}
}

@inproceedings{hosain_finehand_2020,
  title = {{{FineHand}}: {{Learning Hand Shapes}} for {{American Sign Language Recognition}}},
  shorttitle = {{{FineHand}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Hosain, A. A. and Santhalingam, P. S. and Pathak, P. and Rangwala, H. and Ko{\v s}eck{\'a}, J.},
  year = {2020},
  month = may,
  pages = {397--404},
  address = {{Buenos Aires, Argentina}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hosain et al_2020_FineHand.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5R5HZ43U/1kecIh5NpVC.html}
}

@phdthesis{hoyoux_computer_2017,
  title = {Computer Vision Systems for Automatic Analysis of Face and Eye Images in Specific Applications of Interpretation of Facial Expressions},
  author = {Hoyoux, Thomas},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hoyoux_2017_Computer vision systems for automatic analysis of face and eye images in.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9XDFVHF6/Hoyoux - 2017 - Computer vision systems for automatic analysis of},
  school = {University of Li\`ege,\mbox{} Li\`ege,\mbox\mbox{} Belgium}
}

@inproceedings{hruz_input_2009,
  title = {Input and Output Modalities Used in a Sign-Language-Enabled Information Kiosk},
  booktitle = {Proc. of {{SPECOM}}},
  author = {Hr{\'u}z, Marek and Campr, Pavel and Karpov, Alexey},
  year = {2009},
  volume = {1},
  pages = {113--116},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hrúz et al_2009_Input and output modalities used in a sign-language-enabled information kiosk.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=50;signer=1;sl=CzSL;capturedBy=vision,modal=location,modal=movement,modal=shape,signer=1,sl=CzSL,slreco,slreco;modal=shape;modal=location;modal=movement,vocab<100,vocab=50}
}

@inproceedings{hu_investigating_2018,
  title = {Investigating the {{Use}} of {{Mixed}}-{{Units Based Modeling}} for {{Improving Uyghur Speech Recognition}}},
  booktitle = {The 6th {{Intl}}. {{Workshop}} on {{Spoken Language Technologies}} for {{Under}}-{{Resourced Languages}}},
  author = {Hu, Pengfei and Huang, Shen and Lv, Zhiqiang},
  year = {2018},
  month = aug,
  pages = {210--214},
  publisher = {{ISCA}},
  doi = {10.21437/SLTU.2018-44},
  abstract = {Uyghur is a highly agglutinative language with a large number of words derived from the same root. For such languages the use of subwords in speech recognition becomes a natural choice, which can solve the OOV issues. However, short units in subword modeling will weaken the constraint of linguistic context. Besides, vowel weakening and reduction occur frequently in Uyghur language, which may lead to high deletion errors for short unit sequence recognition. In this paper, we investigate using mixed units in Uyghur speech recognition. Subwords and whole-words are mixed together to build a hybrid lexicon and language models for recognition. We also introduce an interpolated LM to further improve the performance. Experiment results show that the mixed-unit based modeling do outperform word or subword based modeling. About 10\% relative reduction in Word Error Rate and 8\% reduction in Character Error Rate have been achieved for test datasets compared with baseline system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hu et al_2018_Investigating the Use of Mixed-Units Based Modeling for Improving Uyghur Speech.pdf},
  language = {en}
}

@article{hu_sign_2017,
  title = {Sign {{Language Fingerspelling Recognition Using Depth Information}} and {{Deep Belief Networks}}},
  author = {Hu, Yong and Zhao, Hai-Feng and Wang, Zhi-Gang},
  year = {2017},
  month = oct,
  volume = {32},
  pages = {1850018},
  issn = {0218-0014},
  doi = {10.1142/S0218001418500180},
  abstract = {In the sign language fingerspelling scheme, letters in the alphabet are presented by a distinctive finger shape or movement. The presented work is conducted for autokinetic translating fingerspelling signs to text. A recognition framework by using intensity and depth information is proposed and compared with some distinguished works. Histogram of Oriented Gradients (HOG) and Zernike moments are used as discriminative features due to their simplicity and good performance. A Deep Belief Network (DBN) composed of three Restricted Boltzmann Machines (RBMs) is used as a classifier. Experiments are executed on a challenging database, which consists of 120,000 pictures representing 24 alphabet letters over five different users. The proposed approach obtained higher average accuracy, outperforming all other methods. This indicates the effectiveness and the abilities of the proposed framework.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hu et al_2017_Sign Language Fingerspelling Recognition Using Depth Information and Deep.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VUPYXMU5/S0218001418500180.html},
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=24;signer=5;sl=ASL;capturedBy=vision;capturedBy=depth,isolated;vocab=24;signer=9;sl=ASL;capturedBy=vision;capturedBy=depth,modal=shape,signer=5,signer=9,sl=ASL,slreco,slreco;modal=shape,vocab<50,vocab=24},
  number = {06}
}

@article{hu_speakerfollowing_2015,
  title = {Speaker-Following Video Subtitles},
  author = {Hu, Yongtao and Kautz, Jan and Yu, Yizhou and Wang, Wenping},
  year = {2015},
  volume = {11},
  pages = {32},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hu et al_2015_Speaker-following video subtitles.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AG9DUWTG/citation.html},
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)},
  number = {2}
}

@article{huang_attention_2018,
  title = {Attention Based {{3D}}-{{CNNs}} for {{Large}}-{{Vocabulary Sign Language Recognition}}},
  author = {Huang, J. and Zhou, W. and Li, H. and Li, W.},
  year = {2018},
  volume = {early access},
  pages = {1--1},
  issn = {1051-8215},
  doi = {10.1109/TCSVT.2018.2870740},
  abstract = {Sign language recognition (SLR) is an important and challenging research topic in the multimedia field. Conventional techniques for SLR rely on hand-crafted features, which achieve limited success. In this paper, we present an attention based 3D-Convolutional Neural Networks (3D-CNNs) for SLR. The framework has two advantages: 3D convolutional networks learn spatio-temporal features from raw video without prior knowledge, and attention mechanism helps to select the clue. When training 3D-CNN for capturing spatio-temporal features, spatial attention is incorporated into network to focus on the areas of interest. After feature extraction, temporal attention is utilized to select the significant motions for classification. The proposed method is evaluated on two large scale sign language datasets. The first one, collected by ourselves, is a Chinese Sign Language (CSL) dataset that consists of 500 categories. The other is the ChaLearn14 benchmark. The experiment results demonstrate the effectiveness of our approach compared with state-of-the-art algorithms.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang et al_2018_Attention based 3D-CNNs for Large-Vocabulary Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VDAC4AMS/8466903.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WNP7GUB6/8466903.html},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=500;signer=50;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=50,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=bodyjoints,vocab<1000,vocab=500}
}

@inproceedings{huang_connectionist_2016,
  title = {Connectionist Temporal Modeling for Weakly Supervised Action Labeling},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Huang, De-An and {Fei-Fei}, Li and Niebles, Juan Carlos},
  year = {2016},
  pages = {137--153},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang et al_2016_Connectionist temporal modeling for weakly supervised action labeling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IAZ5C2WN/978-3-319-46493-0_9.html}
}

@article{huang_densely_2016,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and {van der Maaten}, Laurens},
  year = {2016},
  month = aug,
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
  archivePrefix = {arXiv},
  eprint = {1608.06993},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang et al_2016_Densely Connected Convolutional Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZSFRXFK7/1608.html},
  journal = {arXiv:1608.06993 [cs]},
  primaryClass = {cs}
}

@inproceedings{huang_detecting_2019,
  title = {Detecting {{Mismatch Between Speech}} and {{Transcription Using Cross}}-{{Modal Attention}}},
  booktitle = {Interspeech 2019},
  author = {Huang, Qiang and Hain, Thomas},
  year = {2019},
  month = sep,
  pages = {584--588},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2125},
  abstract = {In this paper, we propose to detect mismatches between speech and transcriptions using deep neural networks. Although it is generally assumed there are no mismatches in some speech related applications, it is hard to avoid the errors due to one reason or another. Moreover, the use of mismatched data probably leads to performance reduction when training a model. In our work, instead of detecting the errors by computing the distance between manual transcriptions and text strings obtained using a speech recogniser, we view mismatch detection as a classification task and merge speech and transcription features using deep neural networks. To enhance detection ability, we use crossmodal attention mechanism in our approach by learning the relevance between the features obtained from the two modalities. To evaluate the effectiveness of our approach, we test it on Factored WSJCAM0 by randomly setting three kinds of mismatch, word deletion, insertion or substitution. To test its robustness, we train our models using a small number of samples and detect mismatch with different number of words being removed, inserted, and substituted. In our experiments, the results show the use of our approach for mismatch detection is close to 80\% on insertion and deletion and outperforms the baseline.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang_Hain_2019_Detecting Mismatch Between Speech and Transcription Using Cross-Modal Attention.pdf},
  language = {en}
}

@article{huang_historical_2014,
  title = {A Historical Perspective of Speech Recognition},
  author = {Huang, Xuedong and Baker, James and Reddy, Raj},
  year = {2014},
  month = jan,
  volume = {57},
  pages = {94--103},
  issn = {00010782},
  doi = {10.1145/2500887},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X74T5GX6/Huang et al. - 2014 - A historical perspective of speech recognition.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {1}
}

@article{huang_novel_2018,
  title = {A {{Novel Chinese Sign Language Recognition Method Based}} on {{Keyframe}}-{{Centered Clips}}},
  author = {Huang, S. and Mao, C. and Tao, J. and Ye, Z.},
  year = {2018},
  month = mar,
  volume = {25},
  pages = {442--446},
  doi = {10.1109/LSP.2018.2797228},
  abstract = {Isolated sign language recognition (SLR) is a long-standing research problem. The existing methods consider inclusively ambiguous data to represent a sign and ignore the fact that only scarce key information can represent the sign efficiently since most information are redundant. Furthermore, inclusion of redundant information may result in inefficiency and difficulty in modeling the long-term dependency for SLR. This letter delivers a novel sequence-to-sequence learning method based on keyframe centered clips (KCCs) for Chinese SLR. Different from conventional methods, only key information is considered to represent a sign significantly. The frames-to-word task is transformed into a KCCs-to-subwords task successfully, to allow for different attention in the input data. The empirical results of the proposed method outperform significantly the state-of-the-art SLR systems on our dataset containing 310 Chinese sign language words.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang et al_2018_A Novel Chinese Sign Language Recognition Method Based on Keyframe-Centered.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LXJZAIVL/8267225.html},
  journal = {IEEE Signal Processing Letters},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated sign language recognition,isolated;vocab=310;signer=14;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hshape,modal=bodyjoints,modal=fullframemotion,sign language recognition,signer=14,sl=CSL,slreco,slreco;modal=bodyjoints;modal=2hshape;modal=fullframemotion,vocab<500,vocab=310},
  number = {3}
}

@article{huang_semicontinuous_1989,
  title = {Semi-Continuous Hidden {{Markov}} Models for Speech Signals},
  author = {Huang, Xuedong D. and Jack, Mervyn A.},
  year = {1989},
  volume = {3},
  pages = {239--251},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang_Jack_1989_Semi-continuous hidden Markov models for speech signals.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HEI5GJME/088523088990020X.html},
  journal = {Computer Speech and Language},
  number = {3}
}

@article{huang_sign_1998,
  title = {Sign Language Recognition Using Model-Based Tracking and a {{3D Hopfield}} Neural Network},
  author = {Huang, Chung-Lin and Huang, Wen-Yi},
  year = {1998},
  month = apr,
  volume = {10},
  pages = {292--307},
  issn = {1432-1769},
  doi = {10.1007/s001380050080},
  abstract = {. This paper presents a sign language recognition system which consists of three modules: model-based hand tracking, feature extraction, and gesture recognition using a 3D Hopfield neural network (HNN). The first one uses the Hausdorff distance measure to track shape-variant hand motion, the second one applies the scale and rotation-invariant Fourier descriptor to characterize hand figures, and the last one performs a graph matching between the input gesture model and the stored models by using a 3D modified HNN to recognize the gesture. Our system tests 15 different hand gestures. The experimental results show that our system can achieve above 91\% recognition rate, and the recognition process time is about 10 s. The major contribution in this paper is that we propose a 3D modified HNN for gesture recognition which is more reliable than the conventional methods.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang_Huang_1998_Sign language recognition using model-based tracking and a 3D Hopfield neural.pdf},
  journal = {Machine Vision and Applications},
  keywords = {capturedBy=vision,isolated,isolated;vocab=15;sl=TwSL;singlehanded;signerdependent;capturedBy=vision,method=3d-hopfield-NN,modal=location,modal=orientation,modal=shape,signerdependent,singlehanded,sl=TwSL,slreco,slreco;modal=shape;modal=orientation;modal=location,vocab<50,vocab=15},
  language = {en},
  number = {5}
}

@inproceedings{huang_sign_2015,
  title = {Sign {{Language Recognition}} Using {{3D}} Convolutional Neural Networks},
  booktitle = {{{IEEE Int}}. {{Conf}}. on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Huang, Jie and Zhou, Wengang and Li, Houqiang and Li, Weiping},
  year = {2015},
  month = jun,
  pages = {1--6},
  doi = {10.1109/ICME.2015.7177428},
  abstract = {Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang et al_2015_Sign Language Recognition using 3D convolutional neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UTZFMSSM/abs_all.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=25;signer=9;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,modal=bodyjoints,signer=9,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape;modal=bodyjoints,vocab<50,vocab=25}
}

@book{huang_spoken_2001,
  title = {Spoken {{Language Processing}}: {{A Guide}} to {{Theory}}, {{Algorithm}} and {{System Development}}},
  shorttitle = {Spoken Language Processing},
  author = {Huang, Xuedong and Acero, Alex and Hon, Hsiao-Wuen},
  year = {2001},
  publisher = {{Prentice Hall}},
  address = {{Upper Saddle River, NJ}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang et al_2001_Spoken Language Processing.pdf},
  isbn = {0-13-022616-5},
  language = {eng}
}

@article{huang_this_,
  title = {This Thesis, Unless Explicitly Noted, Was Composed Entirely by Myself, and Reports},
  author = {Huang', Xuedong},
  pages = {183},
  language = {en}
}

@inproceedings{huang_videobased_2018,
  title = {Video-Based Sign Language Recognition without Temporal Segmentation},
  booktitle = {Proc. of the  {{AAAI Conf}}. on {{Artificial Intelligence}}},
  author = {Huang, Jie and Zhou, Wengang and Zhang, Qilin and Li, Houqiang and Li, Weiping},
  year = {2018},
  month = feb,
  pages = {2257--2264},
  address = {{New Orleans, Louisiana, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huang et al_2018_Video-based sign language recognition without temporal segmentation.pdf},
  keywords = {augment=?,authorgroup=Here,authorgroup=USTC,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,dataSet=Phoenix14,encoderdecoder,loss=other,lstm,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=fullframe,nn=3dcnn,nn=attention,nn=blstm,nnarch=c3d,nntraining=3dcnn+blstm,paperinfo,paperinfo;authorgroup=USTC;authorgroup=Here;shorttitle=Without Segmentation,realign=0,shorttitle=Without Segmentation,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=2hshape;modal=fullframe;dataSet=CSL;nn=3dcnn;nn=blstm;nn=attention;nnarch=c3d;cnnresiduals=0;realign=0;nntraining=3dcnn+blstm;augment=?;loss=other,slreco;modal=2hshape;modal=fullframe;dataSet=Phoenix14;wer=-/38.3;nn=3dcnn;nn=blstm;nn=attention;nnarch=c3d;cnnresiduals=0;realign=0;nntraining=3dcnn+blstm;augment=?;loss=other,vocab<500,vocab=1080,vocab=178,vocab>=1000,wer=-/38.3}
}

@inproceedings{huenerfauth_best_2015,
  title = {Best Practices for Conducting Evaluations of Sign Language Animation},
  booktitle = {Proc. {{Annual Int}}. {{Technology}} and {{Persons}} with {{Disabilities Conference}} ({{CSUN}})},
  author = {Huenerfauth, Matt and Kacorri, Hernisa},
  year = {2015},
  address = {{San Diego, CA, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huenerfauth_Kacorri_2015_Best practices for conducting evaluations of sign language animation.pdf}
}

@article{huenerfauth_synthesizing_2015,
  title = {Synthesizing and {{Evaluating Animations}} of {{American Sign Language Verbs Modeled}} from {{Motion}}-{{Capture Data}}},
  author = {Huenerfauth, Matt and Lu, Pengfei and Kacorri, Hernisa},
  year = {2015},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Huenerfauth et al_Synthesizing and Evaluating Animations of American Sign Language Verbs Modeled.pdf}
}

@article{hulst_units_1993,
  title = {Units in the Analysis of Signs*},
  author = {van der Hulst, Harry},
  year = {1993},
  month = aug,
  volume = {10},
  pages = {209--241},
  issn = {1469-8188, 0952-6757},
  doi = {10.1017/S095267570000004X},
  abstract = {The assumption that there is a common set of linguistic principles underlying both spoken language and sign language phonology, which forms part of the human language capacity, is shared by most phonologists working on sign language. See Sandler (1993a) for an extensive discussion of these issues. But even though this assumption is reasonable, since both spoken and signed languages are products of the same human brain and fulfil the same function, it is not clear that theories of representation which have been proposed for spoken languages can be directly applied to the structure of sign languages. Such representations have been developed on the basis of the spoken language modality only. They are often so close to the phonetics of spoken languages that we cannot rule out the possibility that non-trivial aspects of them are modality-specific. Therefore, rather than, for example, attempting to test various competing (spoken language-based) theories of syllable structure, we must first investigate the structure of sign language in its own right. This strategy need not be pushed too far, however. In developing a model of signs we can benefit from general principles which have proved successful in the study of spoken languages, especially if these principles do not seem to be directly based on `spoken phonetics'.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hulst_1993_Units in the analysis of signs.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EYG6MJ34/45CB17D5DAF9F3A6B03AF5108486CD99.html},
  journal = {Phonology},
  language = {en},
  number = {2}
}

@article{hung_adversarial_2018,
  title = {Adversarial {{Learning}} for {{Semi}}-{{Supervised Semantic Segmentation}}},
  author = {Hung, Wei-Chih and Tsai, Yi-Hsuan and Liou, Yan-Ting and Lin, Yen-Yu and Yang, Ming-Hsuan},
  year = {2018},
  month = jul,
  abstract = {We propose a method for semi-supervised semantic segmentation using an adversarial network. While most existing discriminators are trained to classify input images as real or fake on the image level, we design a discriminator in a fully convolutional manner to differentiate the predicted probability maps from the ground truth segmentation distribution with the consideration of the spatial resolution. We show that the proposed discriminator can be used to improve semantic segmentation accuracy by coupling the adversarial loss with the standard cross entropy loss of the proposed model. In addition, the fully convolutional discriminator enables semi-supervised learning through discovering the trustworthy regions in predicted results of unlabeled images, thereby providing additional supervisory signals. In contrast to existing methods that utilize weakly-labeled images, our method leverages unlabeled images to enhance the segmentation model. Experimental results on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the effectiveness of the proposed algorithm.},
  archivePrefix = {arXiv},
  eprint = {1802.07934},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Hung et al_2018_Adversarial Learning for Semi-Supervised Semantic Segmentation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QRTPDG9T/1802.html},
  journal = {arXiv:1802.07934 [cs]},
  primaryClass = {cs}
}

@article{ichim_dynamic_2015,
  title = {Dynamic {{3D}} Avatar Creation from Hand-Held Video Input},
  author = {Ichim, Alexandru Eugen and Bouaziz, Sofien and Pauly, Mark},
  year = {2015},
  volume = {34},
  pages = {45},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ichim et al_2015_Dynamic 3D avatar creation from hand-held video input.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8NTBXCTP/citation.html},
  journal = {ACM Transactions on Graphics (TOG)},
  number = {4}
}

@inproceedings{igari_sign_2014,
  title = {Sign Language Word Recognition Using Via-Point Information and Correlation of They Bimanual Movements},
  booktitle = {Proc. {{Int}}. {{Conf}}. of {{Advanced Informatics}}: {{Concept}}, {{Theory}} and {{Application}} ({{ICAICTA}})},
  author = {Igari, S. and Fukumura, N.},
  year = {2014},
  month = aug,
  pages = {75--80},
  doi = {10.1109/ICAICTA.2014.7005918},
  abstract = {We have studied Japanese sign Language (JSL) recognition system. In our previous research, we focus on only JSL words performed in the movement of the dominant arm and proposed a recognition method using via-points extracted from the trajectory data of the dominant arm as feature points based on the minimum jerk model. In this study, in order to recognize JSL words performed in bimanual movements, we investigated an integration method of the matching result of the both arms. We classified JSL movements into three categories as part of sign language recognition system. And we used a correlation coefficient and difference of the path length between the both arm movements as a factor to classify JSL. As a result of recognition experiment, the recognition rate was 98\% or more in 80 words from multiple speakers.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Igari_Fukumura_2014_Sign language word recognition using via-point information and correlation of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/URNZ2JHX/abs_all.html},
  keywords = {capturedBy=mocap,isolated,isolated;vocab=80;signer=3;sl=JSL;capturedBy=mocap,modal=2hmovement,signer=3,sl=JSL,slreco,slreco;modal=2hmovement,vocab<100,vocab=80}
}

@inproceedings{ilse_diva_2019,
  title = {{{DIVA}}: {{Domain}} Invariant Variational Autoencoder},
  shorttitle = {{{DIVA}}},
  booktitle = {2019 {{Deep Generative Models}} for {{Highly Structured Data}}, {{DGS}}@{{ICLR}} 2019 {{Workshop}}},
  author = {Ilse, Maximilian and Tomczak, Jakub M. and Louizos, Christos and Welling, Max},
  year = {2019},
  month = jan,
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PUF7JRKA/diva-domain-invariant-variational-autoencoder.html},
  language = {English}
}

@inproceedings{imagawa_recognition_2000,
  title = {Recognition of Local Features for Camera-Based Sign Language Recognition System},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Imagawa, I. and Matsuo, H. and Taniguchi, R. and Arita, D. and {Shan Lu} and Igi, S.},
  year = {2000},
  month = sep,
  volume = {4},
  pages = {849-853 vol.4},
  doi = {10.1109/ICPR.2000.903050},
  abstract = {A sign language recognition system is required to use information from both global features, such as hand movement and location, and local features, such as hand shape and orientation. We present an adequate local feature recognizer for a sign language recognition system. Our basic approach is to represent the hand images extracted from sign-language images as symbols which correspond to clusters by a clustering technique. The clusters are created from a training set of extracted hand images so that a similar appearance can be classified into the same cluster on an eigenspace. The experimental results indicate that our system can recognize a sign language word even in two-handed and hand-to-hand contact cases.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Imagawa et al_2000_Recognition of local features for camera-based sign language recognition system.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BDB8VZI3/903050.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=33;signer=6;sl=JSL;capturedBy=vision,modal=2hshape,signer=6,sl=JSL,slreco,slreco;modal=2hshape,vocab<50,vocab=33}
}

@inproceedings{imashev_sign_2017,
  title = {Sign {{Language Static Gestures Recognition Tool Prototype}}},
  booktitle = {2017 {{IEEE}} 11th {{International Conference}} on {{Application}} of {{Information}} and {{Communication Technologies}} ({{AICT}})},
  author = {Imashev, Alfarabi},
  year = {2017},
  month = sep,
  pages = {1--4},
  issn = {2472-8586},
  doi = {10.1109/ICAICT.2017.8687032},
  abstract = {This article is aimed at illustrating the approach of detecting and recognizing Kazakh Sign Language static gestures without using expensive depth cameras or sensor gloves. For this research a simple RGB mono camera had been used. The main distinguishing feature of our approach is that, unlike most of the existing approaches, it performs hand detection and gesture recognition jointly, i.e. at the very moment when the shape of a static gesture is detected its specific label is automatically recognized. The approach was tested on a range of datasets, and encouraging results were achieved.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Imashev_2017_Sign Language Static Gestures Recognition Tool Prototype.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YPDIAZRJ/8687032.html},
  keywords = {Assistive technology,cameras,Cameras,Computer vision,expensive depth cameras,gesture recognition,Gesture recognition,hand detection,Histograms,Image edge detection,image motion analysis,machine learning,sensor gloves,Shape,sign language,sign language static gestures recognition tool prototype,simple RGB mono camera,static gesture,Tools}
}

@article{infantino_framework_2007,
  title = {A {{Framework}} for {{Sign Language Sentence Recognition}} by {{Commonsense Context}}},
  author = {Infantino, I. and Rizzo, R. and Gaglio, S.},
  year = {2007},
  month = sep,
  volume = {37},
  pages = {1034--1039},
  issn = {1094-6977},
  doi = {10.1109/TSMCC.2007.900624},
  abstract = {This correspondence proposes a complete framework for sign language recognition that integrates a commonsense engine in order to deal with sentence recognition. The proposed system is based on a multilevel architecture that allows modeling and managing of the knowledge of the recognition process in a simple and robust way. The final abstraction level of this architecture introduces the semantic context and the analysis of the correctness of a sentence given in a sequence of recognized signs. Experimentations are presented using a set of signs from the Italian sign language (LIS) for domotic applications. The implemented system maintains a high recognition rate when the set of signs grows, correcting erroneously recognized single signs using the sentence context.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Infantino et al_2007_A Framework for Sign Language Sentence Recognition by Commonsense Context.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RMHP5UJJ/4292252.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {capturedBy=vision,continuous,continuous;vocab=40;sl=LIS;capturedBy=vision,isolated,isolated;vocab=40;sl=LIS;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=mouth,sl=LIS,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=mouth,vocab<50,vocab=40},
  number = {5}
}

@article{inoue_depth_2015,
  title = {Depth {{Sensor Based Automatic Hand Region Extraction}} by {{Using Time}}-Series {{Curve}} and {{Its Application}} to {{Japanese Finger}}-Spelled {{Sign Language Recognition}}},
  author = {Inoue, Katsufumi and Shiraishi, Takami and Yoshioka, Michifumi and Yanagimoto, Hidekazu},
  year = {2015},
  volume = {60},
  pages = {371--380},
  issn = {18770509},
  doi = {10.1016/j.procs.2015.08.145},
  journal = {Procedia Computer Science},
  language = {en}
}

@article{ioffe_batch_2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archivePrefix = {arXiv},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ioffe_Szegedy_2015_Batch Normalization.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EDDNZB4X/1502.html},
  journal = {arXiv:1502.03167 [cs]},
  primaryClass = {cs}
}

@inproceedings{irie_language_2019,
  title = {Language {{Modeling}} with {{Deep Transformers}}},
  booktitle = {Interspeech 2019},
  author = {Irie, Kazuki and Zeyer, Albert and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2019},
  month = sep,
  pages = {3905--3909},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2225},
  abstract = {We explore deep autoregressive Transformer models in language modeling for speech recognition. We focus on two aspects. First, we revisit Transformer model configurations specifically for language modeling. We show that well configured Transformer models outperform our baseline models based on the shallow stack of LSTM recurrent neural network layers. We carry out experiments on the open-source LibriSpeech 960hr task, for both 200K vocabulary word-level and 10K byte-pair encoding subword-level language modeling. We apply our wordlevel models to conventional hybrid speech recognition by lattice rescoring, and the subword-level models to attention based encoder-decoder models by shallow fusion. Second, we show that deep Transformer language models do not require positional encoding. The positional encoding is an essential augmentation for the self-attention mechanism which is invariant to sequence ordering. However, in autoregressive setup, as is the case for language modeling, the amount of information increases along the position dimension, which is a positional signal by its own. The analysis of attention weights shows that deep autoregressive selfattention models can automatically make use of such positional information. We find that removing the positional encoding even slightly improves the performance of these models.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HCX5HGYY/Irie et al. - 2019 - Language Modeling with Deep Transformers.pdf},
  language = {en}
}

@article{isard_condensationconditional_1998,
  title = {{{CONDENSATION}}---{{Conditional Density Propagation}} for {{Visual Tracking}}},
  author = {Isard, Michael and Blake, Andrew},
  year = {1998},
  volume = {29},
  pages = {5--28},
  issn = {1573-1405},
  doi = {10.1023/A:1008078328650},
  abstract = {The problem of tracking curves in dense visual clutter is challenging. Kalman filtering is inadequate because it is based on Gaussian densities which, being unimo dal, cannot represent simultaneous alternative hypotheses. The Condensation algorithm uses ``factored sampling'', previously applied to the interpretation of static images, in which the probability distribution of possible interpretations is represented by a randomly generated set. Condensation uses learned dynamical models, together with visual observations, to propagate the random set over time. The result is highly robust tracking of agile motion. Notwithstanding the use of stochastic methods, the algorithm runs in near real-time.},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {1}
}

@inproceedings{ishihara_gesture_2004,
  title = {Gesture Recognition Using Auto-Regressive Coefficients of Higher-Order Local Auto-Correlation Features},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Ishihara, Tatsuya and Otsu, Nobuyuki},
  year = {2004},
  pages = {583--588},
  publisher = {{IEEE}}
}

@article{izutov_asl_2020,
  title = {{{ASL Recognition}} with {{Metric}}-{{Learning}} Based {{Lightweight Network}}},
  author = {Izutov, Evgeny},
  year = {2020},
  month = apr,
  abstract = {In the past decades the set of human tasks that are solved by machines was extended dramatically. From simple image classification problems researchers now move towards solving more sophisticated and vital problems, like, autonomous driving and language translation. The case of language translation includes a challenging area of sign language translation that incorporates both image and language processing. We make a step in that direction by proposing a lightweight network for ASL gesture recognition with a performance sufficient for practical applications. The proposed solution demonstrates impressive robustness on MS-ASL dataset and in live mode for continuous sign gesture recognition scenario. Additionally, we describe how to combine action recognition model training with metric-learning to train the network on the database of limited size. The training code is available as part of Intel OpenVINO Training Extensions.},
  archivePrefix = {arXiv},
  eprint = {2004.05054},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Izutov_2020_ASL Recognition with Metric-Learning based Lightweight Network.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J66LYFZD/2004.html},
  journal = {arXiv:2004.05054 [cs]},
  keywords = {capturedBy=vision,Computer Science - Computer Vision and Pattern Recognition,dataSet=MS-ASL,isolated,isolated;vocab=100;signer=189;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=1000;signer=222;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=200;signer=196;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=500;signer=222;sl=ASL;capturedBy=vision;dataSet=MS-ASL,modal=fullframe,signer=189,signer=196,signer=222,sl=ASL,slreco,slreco;modal=fullframe,vocab<1000,vocab<500,vocab=100,vocab=1000,vocab=200,vocab=500,vocab>=1000},
  primaryClass = {cs}
}

@inproceedings{Jain_2016_CVPR,
  title = {Structural-{{RNN}}: {{Deep Learning}} on {{Spatio}}-{{Temporal Graphs}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Jain, Ashesh and Zamir, Amir R. and Savarese, Silvio and Saxena, Ashutosh},
  year = {2016},
  month = jun
}

@article{jain_data_1999,
  title = {Data Clustering: A Review},
  shorttitle = {Data Clustering},
  author = {Jain, Anil K. and Murty, M. Narasimha and Flynn, Patrick J.},
  year = {1999},
  volume = {31},
  pages = {264--323},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jain et al_1999_Data clustering.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/83M2DRR7/citation.html},
  journal = {ACM computing surveys (CSUR)},
  number = {3}
}

@article{jauhiainen_automatic_2018,
  title = {Automatic Language Identification in Texts: {{A}} Survey},
  shorttitle = {Automatic Language Identification in Texts},
  author = {Jauhiainen, Tommi and Lui, Marco and Zampieri, Marcos and Baldwin, Timothy and Lind{\'e}n, Krister},
  year = {2018},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jauhiainen et al_2018_Automatic language identification in texts.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4YBU2V56/1804.html},
  journal = {arXiv preprint arXiv:1804.08186}
}

@inproceedings{jauhiainen_helibased_2018,
  title = {{{HeLI}}-Based {{Experiments}} in {{Discriminating Between Dutch}} and {{Flemish Subtitles}}},
  booktitle = {Proceedings of the {{Fifth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}} ({{VarDial}} 2018)},
  author = {Jauhiainen, Tommi and Jauhiainen, Heidi and Lind{\'e}n, Krister},
  year = {2018},
  month = aug,
  pages = {137--144},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Fe, New Mexico, USA}},
  abstract = {This paper presents the experiments and results obtained by the SUKI team in the Discriminating between Dutch and Flemish in Subtitles shared task of the VarDial 2018 Evaluation Campaign. Our best submission was ranked 8th, obtaining macro F1-score of 0.61. Our best results were produced by a language identifier implementing the HeLI method without any modifications. We describe, in addition to the best method we used, some of the experiments we did with unsupervised clustering.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jauhiainen et al_2018_HeLI-based Experiments in Discriminating Between Dutch and Flemish Subtitles.pdf}
}

@article{jelinek_continuous_1976,
  title = {Continuous Speech Recognition by Statistical Methods},
  author = {Jelinek, Frederick},
  year = {1976},
  volume = {64},
  pages = {532--556},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jelinek_1976_Continuous speech recognition by statistical methods.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QNZU3G86/1454428.html},
  journal = {Proceedings of the IEEE},
  number = {4}
}

@inproceedings{jelinek_dynamic_1991,
  title = {A Dynamic Language Model for Speech Recognition},
  booktitle = {Speech and {{Natural Language}}: {{Proceedings}} of a {{Workshop Held}} at {{Pacific Grove}}, {{California}}, {{February}} 19-22, 1991},
  author = {Jelinek, Frederick and Merialdo, Bernard and Roukos, Salim and Strauss, Martin},
  year = {1991},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jelinek et al_1991_A dynamic language model for speech recognition.pdf}
}

@article{jelinek_fast_1969,
  title = {Fast {{Sequential Decoding Algorithm Using}} a {{Stack}}},
  author = {Jelinek, F.},
  year = {1969},
  month = nov,
  volume = {13},
  pages = {675--685},
  issn = {0018-8646},
  doi = {10.1147/rd.136.0675},
  abstract = {In this paper a new sequential decoding algorithm is introduced that uses stack storage at the receiver. It is much simpler to describe and analyze than the Fano algorithm, and is about six times faster than the latter at transmission rates equal to Rcomp, the rate below which the average number of decoding steps is bounded by a constant. Practical problems connected with implementing the stack algorithm are discussed and a scheme is described that facilitates satisfactory performance even with limited stack storage capacity. Preliminary simulation results estimating the decoding effort and the needed stack size are presented.},
  journal = {IBM Journal of Research and Development},
  number = {6}
}

@inproceedings{jemni_avatar_2007,
  title = {An Avatar Based Approach for Automatic Interpretation of Text to {{Sign}} Language},
  booktitle = {European {{Conf}}. for the {{Advancement}} of the {{Assistive Technologies}} in {{Europe}}},
  author = {Jemni, Mohamed and Elghoul, Oussama},
  year = {2007},
  pages = {3--5}
}

@article{jeni_3d_2012,
  title = {{{3D}} Shape Estimation in Video Sequences Provides High Precision Evaluation of Facial Expressions},
  author = {Jeni, L{\'a}szl{\'o} A. and L{\textbackslash}Horincz, Andr{\'a}s and Nagy, Tam{\'a}s and Palotai, Zsolt and Seb{\textbackslash}Hok, Judit and Szab{\'o}, Zolt{\'a}n and Tak{\'a}cs, D{\'a}niel},
  year = {2012},
  volume = {30},
  pages = {785--795},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jeni et al_2012_3D shape estimation in video sequences provides high precision evaluation of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U3HHHSAE/S0262885612000182.html},
  journal = {Image and Vision Computing},
  number = {10}
}

@inproceedings{jeni_facing_2013,
  title = {Facing {{Imbalanced Data}}\textendash{{Recommendations}} for the {{Use}} of {{Performance Metrics}}},
  booktitle = {Proc. {{Humaine Association Conf}}. on {{Affective Computing}} and {{Intelligent Interaction}} ({{ACII}})},
  author = {Jeni, L.A. and Cohn, J.F. and De La Torre, F.},
  year = {2013},
  month = sep,
  pages = {245--251},
  doi = {10.1109/ACII.2013.47},
  abstract = {Recognizing facial action units (AUs) is important for situation analysis and automated video annotation. Previous work has emphasized face tracking and registration and the choice of features classifiers. Relatively neglected is the effect of imbalanced data for action unit detection. While the machine learning community has become aware of the problem of skewed data for training classifiers, little attention has been paid to how skew may bias performance metrics. To address this question, we conducted experiments using both simulated classifiers and three major databases that differ in size, type of FACS coding, and degree of skew. We evaluated influence of skew on both threshold metrics (Accuracy, F-score, Cohen's kappa, and Krippendorf's alpha) and rank metrics (area under the receiver operating characteristic (ROC) curve and precision-recall curve). With exception of area under the ROC curve, all were attenuated by skewed distributions, in many cases, dramatically so. While ROC was unaffected by skew, precision-recall curves suggest that ROC may mask poor performance. Our findings suggest that skew is a critical factor in evaluating performance metrics. To avoid or minimize skew-biased estimates of performance, we recommend reporting skew-normalized scores along with the obtained ones.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jeni et al_2013_Facing Imbalanced Data–Recommendations for the Use of Performance Metrics.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UC2CTD2W/abs_all.html}
}

@inproceedings{Jeni15FG_ZFace,
  title = {Dense {{3D Face Alignment}} from {{2D Videos}} in {{Real}}-{{Time}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Jeni, L{\'a}szl{\'o} A. and Cohn, Jeffrey F. and Kanade, Takeo},
  year = {2015},
  code = {http://zface.org},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jeni et al_2015_Dense 3D Face Alignment from 2D Videos in Real-Time.pdf},
  shortbooktitle = {FG}
}

@article{ji_3d_2013,
  title = {{{3D Convolutional Neural Networks}} for {{Human Action Recognition}}},
  author = {Ji, S. and Xu, W. and Yang, M. and Yu, K.},
  year = {2013},
  month = jan,
  volume = {35},
  pages = {221--231},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2012.59},
  abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ji et al_2013_3D Convolutional Neural Networks for Human Action Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WICTMS5N/6165309.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {1}
}

@article{ji_realtime_2002,
  title = {Real-Time Eye, Gaze, and Face Pose Tracking for Monitoring Driver Vigilance},
  author = {Ji, Qiang and Yang, Xiaojie},
  year = {2002},
  volume = {8},
  pages = {357--377},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ji_Yang_2002_Real-time eye, gaze, and face pose tracking for monitoring driver vigilance.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8DHB2RPF/S1077201402902792.html},
  journal = {Real-Time Imaging},
  number = {5}
}

@inproceedings{ji_sign_2017,
  title = {Sign {{Language Learning System}} with {{Image Sampling}} and {{Convolutional Neural Network}}},
  booktitle = {2017 {{First IEEE International Conference}} on {{Robotic Computing}} ({{IRC}})},
  author = {Ji, Y. and Kim, S. and Lee, K. B.},
  year = {2017},
  month = apr,
  pages = {371--375},
  doi = {10.1109/IRC.2017.40},
  abstract = {This paper proposes a novel sign language learning system based on 2D image sampling and concatenating to solve the problems of conventional sign recognition. The system constructs the training data by sampling and concatenating from a sign language demonstration video at a certain sampling rate. The learning process is implemented with a well-known network, convolutional neural network. 6 sign language actions are learned in 20 situations and then the system is tested in an uninformed situation. The results show that the system is accurate and robust even if only 2D images obtained with low-cost cameras are used.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ji et al_2017_Sign Language Learning System with Image Sampling and Convolutional Neural.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/D45S2ZKS/7926567.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=6;sl=KSL;capturedBy=vision,modal=2hlocation,modal=2hshape,sl=KSL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<50,vocab=6}
}

@article{jia_caffe_2014,
  title = {Caffe: {{Convolutional Architecture}} for {{Fast Feature Embedding}}},
  author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  year = {2014},
  volume = {abs/1408.5093},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jia et al_2014_Caffe.pdf},
  journal = {CoRR}
}

@article{jia_kind_2013,
  title = {A {{Kind}} of {{Visual Speech Feature}} with the {{Geometric}} and {{Local Inner Texture Description}}},
  author = {Jia, Xibin and Sun, Yanfeng},
  year = {2013},
  volume = {11},
  pages = {877--889},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jia_Sun_2013_A Kind of Visual Speech Feature with the Geometric and Local Inner Texture.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MEW3MCHT/2047.html},
  journal = {TELKOMNIKA Indonesian Journal of Electrical Engineering},
  number = {2}
}

@inproceedings{jiang_similarity_2002,
  title = {Similarity Structure in Perceptual and Physical Measures for Visual {{Consonants}} across Talkers},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jiang, Jintao and Alwan, Abeer and Bernstein, Lyme E. and Auer, Edward T. and Keating, Patricia A.},
  year = {2002},
  month = may,
  volume = {1},
  pages = {I-441-I-444},
  doi = {10.1109/ICASSP.2002.5743749},
  abstract = {This paper investigates the relationship between visual confusion matrices and physical (facial) measures. The similarity structure in perceptual and physical measures for visual consonants was examined across four talkers. Four talkers, spanning a wide range of rated visual intelligibility, were recorded producing 69 Consonant-Vowel (CV) syllables. Audio, video, and 3-D face motion were recorded. Each talker's CV productions were presented for identification in a visual-only condition to six viewers with average or better lipreading ability. The obtained visual confusion matrices demonstrated that phonemic equivalence classes were related to visual intelligibility and were talker and vowel context dependent. Physical measures accounted for about 63\% of the variance of visual consonant perception, with C/u/ syllables yielding higher correlations than C/a/ and C/i/ syllables.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jiang et al_2002_Similarity structure in perceptual and physical measures for visual Consonants.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GBPUGS4U/abs_all.html}
}

@inproceedings{jiang_switching_2015,
  title = {Switching to and Combining Offline-Adapted Cluster Acoustic Models Based on Unsupervised Segment Classification},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Jiang, Jintao and Sawaf, Hassan},
  year = {2015},
  month = apr,
  pages = {4649--4653},
  doi = {10.1109/ICASSP.2015.7178852},
  abstract = {The performance of automatic speech recognition system degrades significantly when the incoming audio differs from training data. Maximum likelihood linear regression has been widely used for unsupervised adaptation, usually in a multiple-pass recognition process. Here we present a novel adaptation framework for which the offline, supervised, high-quality adaptation is applied to clustered channel/speaker conditions that are defined with automatic and manual clustering of the training data. Upon online recognition, each speech segment is classified into one of the training clusters in an unsupervised way, and the corresponding top acoustic models are used for recognition. Recognition lattice outputs are combined. Experiments are performed on the Wall Street Journal data, and a 37.5\% relative reduction of Word Error Rate is reported. The proposed approach is also compared with a general speaker adaptive training approach.}
}

@article{johnson_segmental_2011,
  title = {A {{Segmental Framework}} for {{Representing Signs Phonetically}}},
  author = {Johnson, Robert E. and Liddell, Scott K.},
  year = {2011},
  volume = {11},
  pages = {408--463},
  issn = {0302-1475},
  abstract = {The arguments for dividing the signing stream in signed languages into sequences of phonetic segments are compelling. The visual records of instances of actually occurring signs provide evidence of two basic types of segments: postural segments and trans-forming segments. Postural segments specify an alignment of articulatory features, both manual and nonmanual. In contrast, during trans-forming segments at least some of the articulatory features are changing. Both types of segment are divisible into subcategories based on descriptive details of duration and nature of muscular activity. Features that describe the finer details of the manner in which a trans-forming change is accomplished argue for the specification of trans-forming segments as a part of the phonetic record. We conclude that an adequate phonetic representation of signs must account for both postural and transforming segments. (Contains 33 figures, 1 table and 27 notes.)},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Johnson_Liddell_2011_A Segmental Framework for Representing Signs Phonetically.pdf},
  journal = {Sign Language Studies},
  number = {3}
}

@book{johnston_australian_2007,
  title = {Australian {{Sign Language}} ({{Auslan}}): {{An}} Introduction to Sign Language Linguistics},
  shorttitle = {Australian {{Sign Language}} ({{Auslan}})},
  author = {Johnston, Trevor and Schembri, Adam},
  year = {2007},
  month = jan,
  publisher = {{Cambridge University Press}},
  abstract = {This is first comprehensive introduction to the linguistics of Auslan, the sign language of Australia. Assuming no prior background in language study, it explores each key aspect of the structure of Auslan, providing an accessible overview of its grammar (how sentences are structured), phonology (the building blocks of signs), morphology (the structure of signs), lexicon (vocabulary), semantics (how meaning is created), and discourse (how Auslan is used in context). The authors also discuss a range of myths and misunderstandings about sign languages, provide an insight into the history and development of Auslan, and show how Auslan is related to other sign languages, such as those used in Britain, the USA and New Zealand. Complete with clear illustrations of the signs in use and useful further reading lists, this is an ideal resource for anyone interested in Auslan, as well as those seeking a clear, general introduction to sign language linguistics.},
  googlebooks = {dnxDgCvEnJoC},
  isbn = {978-1-139-45963-1},
  language = {en}
}

@article{johnston_conventionalization_2015,
  title = {On the {{Conventionalization}} of {{Mouth Actions}} in {{Australian Sign Language}}},
  author = {Johnston, Trevor and van Roekel, Jane and Schembri, Adam},
  year = {2015},
  month = feb,
  pages = {0023830915569334},
  issn = {0023-8309, 1756-6053},
  doi = {10.1177/0023830915569334},
  abstract = {This study investigates the conventionalization of mouth actions in Australian Sign Language. Signed languages were once thought of as simply manual languages because the hands produce the signs which individually and in groups are the symbolic units most easily equated with the words, phrases and clauses of spoken languages. However, it has long been acknowledged that non-manual activity, such as movements of the body, head and the face play a very important role. In this context, mouth actions that occur while communicating in signed languages have posed a number of questions for linguists: are the silent mouthings of spoken language words simply borrowings from the respective majority community spoken language(s)? Are those mouth actions that are not silent mouthings of spoken words conventionalized linguistic units proper to each signed language, culturally linked semi-conventional gestural units shared by signers with members of the majority speaking community, or even gestures and expressions common to all humans? We use a corpus-based approach to gather evidence of the extent of the use of mouth actions in naturalistic Australian Sign Language\textendash making comparisons with other signed languages where data is available\textendash and the form/meaning pairings that these mouth actions instantiate.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Johnston et al/Johnston et al_2015_On the Conventionalization of Mouth Actions in Australian Sign Language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Johnston et al_2015_On the Conventionalization of Mouth Actions in Australian Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FJXS23XM/0023830915569334.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N9IA9GBZ/0023830915569334.html},
  journal = {Language and Speech},
  language = {en}
}

@inproceedings{johnston_lexicalization_2010,
  title = {Lexicalization in Signed Languages: {{When}} Is an Idiom Not an Idiom},
  shorttitle = {Lexicalization in Signed Languages},
  booktitle = {Proc. {{UK Cognitive Linguistics Conf}}., {{University}} of {{Hertfordshire}}},
  author = {Johnston, Trevor and Ferrara, Lindsay and others},
  year = {2010},
  pages = {6--8},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Johnston et al_2010_Lexicalization in signed languages.pdf}
}

@article{jongseo_sohn_statistical_1999,
  title = {A Statistical Model-Based Voice Activity Detection},
  author = {Jongseo Sohn and Nam Soo Kim and Wonyong Sung},
  year = {1999},
  month = jan,
  volume = {6},
  pages = {1--3},
  issn = {1558-2361},
  doi = {10.1109/97.736233},
  abstract = {In this letter, we develop a robust voice activity detector (VAD) for the application to variable-rate speech coding. The developed VAD employs the decision-directed parameter estimation method for the likelihood ratio test. In addition, we propose an effective hang-over scheme which considers the previous observations by a first-order Markov process modeling of speech occurrences. According to our simulation results, the proposed VAD shows significantly better performances than the G.729B VAD in low signal-to-noise ratio (SNR) and vehicular noise environments.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jongseo Sohn et al_1999_A statistical model-based voice activity detection.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZKSXHESR/736233.html},
  journal = {IEEE Signal Processing Letters},
  keywords = {decision-directed parameter estimation method,Discrete Fourier transforms,first-order Markov process modeling,Frequency estimation,hang-over scheme,hidden Markov models,Hidden Markov models,HMM,Light rail systems,likelihood ratio test,low signal-to-noise ratio,Maximum likelihood estimation,parameter estimation,Parameter estimation,robust voice activity detector,Signal to noise ratio,simulation results,SNR,speech coding,Speech enhancement,speech occurrences,statistical model-based,Statistics,Testing,VAD,variable rate codes,variable-rate speech coding,vehicular noise environment,voice activity detection},
  number = {1}
}

@inproceedings{jongseo_sohn_voice_1998,
  title = {A Voice Activity Detector Employing Soft Decision Based Noise Spectrum Adaptation},
  booktitle = {Proceedings of the 1998 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}, {{ICASSP}} '98 ({{Cat}}. {{No}}.{{98CH36181}})},
  author = {Jongseo Sohn and Wonyong Sung},
  year = {1998},
  month = may,
  volume = {1},
  pages = {365-368 vol.1},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1998.674443},
  abstract = {In this paper, a voice activity detector (VAD) for variable rate speech coding is decomposed into two parts, a decision rule and a background noise statistic estimator, which are analysed separately by applying a statistical model. A robust decision rule is derived from the generalized likelihood ratio test by assuming that the noise statistics are known a priori. To estimate the time-varying noise statistics, allowing for the occasional presence of the speech signal, a novel noise spectrum adaptation algorithm using the soft decision information of the proposed decision rule is developed. The algorithm is robust, especially for the time-varying noise such as babble noise.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jongseo Sohn_Wonyong Sung_1998_A voice activity detector employing soft decision based noise spectrum.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KBCY54S8/674443.html},
  keywords = {acoustic noise,babble noise,Background noise,background noise statistic estimator,Detectors,generalized likelihood ratio test,Noise robustness,noise spectrum adaptation,robust decision rule,Signal to noise ratio,soft decision information,spectral analysis,Speech analysis,speech coding,Speech coding,Speech enhancement,statistical analysis,Statistical analysis,statistical model,Statistics,Testing,time-varying noise statistics,variable rate codes,variable rate speech coding,voice activity detector}
}

@inproceedings{jordan_hidden_1997,
  title = {Hidden {{Markov}} Decision Trees},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Saul, Lawrence K.},
  year = {1997},
  pages = {501--507},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jordan et al_1997_Hidden Markov decision trees.pdf}
}

@inproceedings{jose_tamil_2019,
  title = {Tamil {{Sign Language Translator}}\textemdash{{An Assistive System}} for {{Hearing}}- and {{Speech}}-{{Impaired People}}},
  booktitle = {Information and {{Communication Technology}} for {{Intelligent Systems}}},
  author = {Jose, Hancy and Julian, Anitha},
  editor = {Satapathy, Suresh Chandra and Joshi, Amit},
  year = {2019},
  month = jul,
  pages = {249--257},
  publisher = {{Springer Singapore}},
  abstract = {In the present society, People With Disability (PWD), specifically hearing- and speech-impaired people, are subject to lot of hassles. This is mainly due to the communication gap between them. The proposed work addresses this issue. It is an assistive system that is used to aid the PWD by translating the gestures made by them into text. The gestures are translated to the native language (Tamil) in order to find a localized solution for this problem. The translator processes 31 alphabets. The Canny Edge Detector is used for edge detection. The Scale-Invariant Feature Transform (SIFT) algorithm is used for gesture recognition and training of system. In the translator system, the gestures made by the PWD are identified accurately and the corresponding Tamil letters are displayed. On experimentation, it has been found that the proposed system has an average accuracy of 97.31\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jose_Julian_2019_Tamil Sign Language Translator—An Assistive System for Hearing- and.pdf},
  isbn = {9789811317477},
  keywords = {capturedBy=vision,isolated,isolated;vocab=31;sl=TamilSL;capturedBy=vision,modal=shape,sl=TamilSL,slreco,slreco;modal=shape,vocab<50,vocab=31},
  language = {en},
  series = {Smart {{Innovation}}, {{Systems}} and {{Technologies}}}
}

@article{joze_msasl_2018,
  title = {{{MS}}-{{ASL}}: {{A Large}}-{{Scale Data Set}} and {{Benchmark}} for {{Understanding American Sign Language}}},
  shorttitle = {{{MS}}-{{ASL}}},
  author = {Joze, Hamid Reza Vaezi and Koller, Oscar},
  year = {2018},
  month = dec,
  abstract = {Computer Vision has been improved significantly in the past few decades. It has enabled machine to do many human tasks. However, the real challenge is in enabling machine to carry out tasks that an average human does not have the skills for. One such challenge that we have tackled in this paper is providing accessibility for deaf individual by providing means of communication with others with the aid of computer vision. Unlike other frequent works focusing on multiple camera, depth camera, electrical glove or visual gloves, we focused on the sole use of RGB which allows everybody to communicate with a deaf individual through their personal devices. This is not a new approach but the lack of realistic large-scale data set prevented recent computer vision trends on video classification in this filed. In this paper, we propose the first large scale ASL data set that covers over 200 signers, signer independent sets, challenging and unconstrained recording conditions and a large class count of 1000 signs. We evaluate baselines from action recognition techniques on the data set. We propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition. We also propose new pre-trained model more appropriate for sign language recognition. Finally, We estimate the effect of number of classes and number of training samples on the recognition accuracy.},
  archivePrefix = {arXiv},
  eprint = {1812.01053},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Joze_Koller_2018_MS-ASL.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Joze_Koller_2018_MS-ASL2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GX43XDCF/1812.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JLT8BH9P/1812.html},
  journal = {arXiv:1812.01053 [cs]},
  primaryClass = {cs}
}

@article{jr_speechreading_1997,
  title = {Speechreading and the Structure of the Lexicon: {{Computationally}} Modeling the Effects of Reduced Phonetic Distinctiveness on Lexical Uniqueness},
  shorttitle = {Speechreading and the Structure of the Lexicon},
  author = {Jr, Edward T. Auer and Bernstein, Lynne E.},
  year = {1997},
  month = dec,
  volume = {102},
  pages = {3704--3710},
  issn = {0001-4966},
  doi = {10.1121/1.420402},
  abstract = {A lexical modeling methodology was employed to examine how the distribution of phonemic patterns in the lexicon constrains lexical equivalence under conditions of reduced phonetic distinctiveness experienced by speechreaders. The technique involved (1) selection of a phonemically transcribed machine-readable lexical database, (2) definition of transcription rules based on measures of phonetic similarity, (3) application of the transcription rules to a lexical database and formation of lexical equivalence classes, and (4) computation of three metrics to examine the transcribed lexicon. The metric percent words unique demonstrated that the distribution of words in the language substantially preserves lexical uniqueness across a wide range in the number of potentially available phonemic distinctions. Expected class size demonstrated that if at least 12 phonemic equivalence classes were available, any given word would be highly similar to only a few other words. Percent information extracted (PIE) [D. Carter, Comput. Speech Lang. 2, 1\textendash 11 (1987)] provided evidence that high-frequency words tend not to reside in the same lexical equivalence classes as other high-frequency words. The steepness of the functions obtained for each metric shows that small increments in the number of visually perceptible phonemic distinctions can result in substantial changes in lexical uniqueness.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jr_Bernstein_1997_Speechreading and the structure of the lexicon.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KWWHR6FQ/1.html},
  journal = {The Journal of the Acoustical Society of America},
  number = {6}
}

@inproceedings{kacorri_demographic_2015,
  title = {Demographic and {{Experiential Factors Influencing Acceptance}} of {{Sign Language Animation}} by {{Deaf Users}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Kacorri, Hernisa and Huenerfauth, Matt and Ebling, Sarah and Patel, Kasmira and Willard, Mackenzie},
  year = {2015},
  pages = {147--154}
}

@article{kacorri_effect_2013,
  title = {Effect of Displaying Human Videos during an Evaluation Study of {{American Sign Language}} Animation},
  author = {Kacorri, Hernisa and Lu, Pengfei and Huenerfauth, Matt},
  year = {2013},
  volume = {5},
  pages = {4},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kacorri et al_2013_Effect of displaying human videos during an evaluation study of American Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZEM228FM/citation.html},
  journal = {ACM Transactions on Accessible Computing (TACCESS)},
  number = {2}
}

@article{kacorri_evaluating_,
  title = {Evaluating a {{Dynamic Time Warping Based Scoring Algorithm}} for {{Facial Expressions}} in {{ASL Animations}}},
  author = {Kacorri, Hernisa and Huenerfauth, Matt},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kacorri_Huenerfauth_Evaluating a Dynamic Time Warping Based Scoring Algorithm for Facial.pdf}
}

@inproceedings{kadir_minimal_2004,
  title = {Minimal {{Training}}, {{Large Lexicon}}, {{Unconstrained Sign Language Recognition}}.},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Kadir, Timor and Bowden, Richard and Ong, Eng-Jon and Zisserman, Andrew},
  year = {2004},
  month = sep,
  pages = {1--10},
  address = {{London, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kadir et al_2004_Minimal Training, Large Lexicon, Unconstrained Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=164;signer=1;sl=BSL;capturedBy=vision,modal=location,modal=movement,modal=orientation,modal=shape,signer=1,sl=BSL,slreco,slreco;modal=shape;modal=location;modal=movement;modal=orientation,vocab<500,vocab=164}
}

@phdthesis{kadous_grasp_1995,
  title = {{{GRASP}}: {{Recognition}} of {{Australian Sign Language Using Instrumented Gloves}}},
  shorttitle = {{{GRASP}}},
  author = {Kadous, Waleed and Taylor, Supervisor Andrew},
  year = {1995},
  month = oct,
  address = {{Sydney, Australia}},
  abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): Instrumented gloves -- gloves equipped with sensors for detecting finger bend, hand position and orientation -- were conceived to allow a more natural interface to computers. However, the extension of their use for recognising sign language, and in this case Auslan (Australian Sign Language), is possible. Several researchers have already explored these possibilities and have successfully achieved finger-spelling recognition with high levels of accuracy, but progress in the recognition of sign language as a whole has been limited.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kadous_Taylor_1995_GRASP.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UBVEVIV3/citations\;jsessionid=D2F1D4D96516F2E05F624C5290C86C70.html},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=95;signer=5;sl=Auslan;capturedBy=electronicGlove,modal=location,modal=movement,modal=orientation,modal=shape,signer=5,sl=Auslan,slreco,slreco;modal=shape;modal=orientation;modal=movement;modal=location,vocab<100,vocab=95},
  school = {University of New South Wales}
}

@inproceedings{kadous_machine_1996,
  title = {Machine Recognition of {{Auslan}} Signs Using {{PowerGloves}}: {{Towards}} Large-Lexicon Recognition of Sign Language},
  shorttitle = {Machine Recognition of {{Auslan}} Signs Using {{PowerGloves}}},
  booktitle = {Proc. of the {{Workshop}} on the {{Integration}} of {{Gesture}} in {{Language}} and {{Speech}}},
  author = {Kadous, Mohammed Waleed},
  year = {1996},
  month = oct,
  pages = {165--174},
  address = {{Wilmington, DE, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kadous_1996_Machine recognition of Auslan signs using PowerGloves.pdf},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=95;signer=5;sl=Auslan;capturedBy=electronicGlove,modal=location,modal=orientation,modal=shape,signer=5,sl=Auslan,slreco,slreco;modal=location;modal=orientation;modal=shape,vocab<100,vocab=95}
}

@article{kafle_artificial_2020,
  title = {Artificial Intelligence Fairness in the Context of Accessibility Research on Intelligent Systems for People Who Are Deaf or Hard of Hearing},
  author = {Kafle, Sushant and Glasser, Abraham and {Al-khazraji}, Sedeeq and Berke, Larwan and Seita, Matthew and Huenerfauth, Matt},
  year = {2020},
  month = mar,
  pages = {4:1},
  issn = {1558-2337},
  doi = {10.1145/3386296.3386300},
  abstract = {We discuss issues of Artificial Intelligence (AI) fairness for people with disabilities, with examples drawn from our research on HCI for AI-based systems for people who are Deaf or Hard of Hearing (DHH). In particular, we discuss the need for inclusion of data from people with disabilities in training sets, the lack of interpretability of AI systems, ethical responsibilities of access technology researchers and companies, the need for appropriate evaluation metrics for AI-based access technologies (to determine if they are ready to be deployed and if they can be trusted by users), and the ways in which AI systems influence human behavior and influence the set of abilities needed by users to successfully interact with computing systems.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kafle et al_2020_Artificial intelligence fairness in the context of accessibility research on.pdf},
  journal = {SIGACCESS Access. Comput.},
  number = {125}
}

@inproceedings{kaiser_novel_2000,
  title = {A Novel Loss Function for the Overall Risk Criterion Based Discriminative Training of {{HMM}} Models},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Kaiser, Janez and Horvat, Bogomir and Kacic, Zdravko},
  year = {2000},
  month = oct,
  address = {{Beijing, China}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kaiser et al_2000_A novel loss function for the overall risk criterion based discriminative.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/S2UPB3K7/i00_2887.html}
}

@inproceedings{kak_purdue_2002,
  title = {Purdue {{RVL}}-{{SLLL ASL Database}} for {{Automatic Recognition}} of {{American Sign Language}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Multimodal Interfaces}} ({{ICMI}})},
  author = {Kak, Avi C.},
  year = {2002},
  month = oct,
  pages = {167--172},
  address = {{Washington, DC, USA}},
  doi = {10.1109/ICMI.2002.1166987},
  abstract = {This article reports on an extensive database of American Sign Language (ASL) motions, handshapes, words and sentences. Research on automatic recognition of ASL requires a suitable database for the training and the testing of algorithms. The databases that are currently available do not allow, for algorithmic development that requires a step-by-step approach to ASL recognition -from the recognition of individual hand-shapes, to the recognition of motion primitives, and, finally, to the recognition of full sentences. We have sought to remove these deficiencies in a new database \textquestiondown{} Purdue RVL-SLLL ASL database.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kak_2002_Purdue RVL-SLLL ASL Database for Automatic Recognition of American Sign Language.pdf},
  isbn = {978-0-7695-1834-3}
}

@article{kamal_technical_2019,
  ids = {kamal\_technical\_2019},
  title = {Technical {{Approaches}} to {{Chinese Sign Language Processing}}: {{A Review}}},
  shorttitle = {Technical {{Approaches}} to {{Chinese Sign Language Processing}}},
  author = {Kamal, S. M. and Chen, Y. and Li, S. and Shi, X. and Zheng, J.},
  year = {2019},
  volume = {7},
  pages = {96926--96935},
  doi = {10.1109/ACCESS.2019.2929174},
  abstract = {As with the huge number of deaf-mute people in China is of concern, there is a growing need to integrate them into mainstream society through the use of efficient sign language processing technologies. Sign language processing entails the systematic recognition and translation of sign language images/videos to text or speech. This survey provides an overview of the most important work on Chinese sign language recognition and translation, discussed its classification, highlights the features explored in sign language recognition research, presents the datasets available, and provides trends for the future research.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kamal et al_2019_Technical Approaches to Chinese Sign Language Processing.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kamal et al_2019_Technical Approaches to Chinese Sign Language Processing2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XJPPUTTG/8764391.html},
  journal = {IEEE Access}
}

@inproceedings{kannan_analysis_2018,
  title = {An {{Analysis}} of {{Incorporating}} an {{External Language Model}} into a {{Sequence}}-to-{{Sequence Model}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kannan, Anjuli and Wu, Yonghui and Nguyen, Patrick and Sainath, Tara N. and Chen, ZhiJeng and Prabhavalkar, Rohit},
  year = {2018},
  month = apr,
  pages = {1--5828},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462682},
  abstract = {Attention-based sequence-to-sequence models for automatic speech recognition jointly train an acoustic model, language model, and alignment mechanism. Thus, the language model component is only trained on transcribed audio-text pairs. This leads to the use of shallow fusion with an external language model at inference time. Shallow fusion refers to log-linear interpolation with a separately trained language model at each step of the beam search. In this work, we investigate the behavior of shallow fusion across a range of conditions: different types of language models, different decoding units, and different tasks. On Google Voice Search, we demonstrate that the use of shallow fusion with an neural LM with wordpieces yields a 9.1\% relative word error rate reduction (WERR) over our competitive attention-based sequence-to-sequence model, obviating the need for second-pass rescoring.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kannan et al_2018_An Analysis of Incorporating an External Language Model into a.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FCB6DKGR/8462682.html},
  keywords = {acoustic model,acoustic signal processing,Acoustics,alignment mechanism,automatic speech recognition,competitive attention-based sequence-to-sequence model,Data models,Decoding,external language model,Google,Google Voice Search,interpolation,Interpolation,language model component,log-linear interpolation,neural LM,speech recognition,Task analysis,Training,transcribed audio-text pairs,word error rate reduction}
}

@article{kannan_largescale_2019,
  title = {Large-{{Scale Multilingual Speech Recognition}} with a {{Streaming End}}-to-{{End Model}}},
  author = {Kannan, Anjuli and Datta, Arindrima and Sainath, Tara N. and Weinstein, Eugene and Ramabhadran, Bhuvana and Wu, Yonghui and Bapna, Ankur and Chen, Zhifeng and Lee, Seungji},
  year = {2019},
  month = sep,
  abstract = {Multilingual end-to-end (E2E) models have shown great promise in expansion of automatic speech recognition (ASR) coverage of the world's languages. They have shown improvement over monolingual systems, and have simplified training and serving by eliminating language-specific acoustic, pronunciation, and language models. This work presents an E2E multilingual system which is equipped to operate in low-latency interactive applications, as well as handle a key challenge of real world data: the imbalance in training data across languages. Using nine Indic languages, we compare a variety of techniques, and find that a combination of conditioning on a language vector and training language-specific adapter layers produces the best model. The resulting E2E multilingual model achieves a lower word error rate (WER) than both monolingual E2E models (eight of nine languages) and monolingual conventional systems (all nine languages).},
  archivePrefix = {arXiv},
  eprint = {1909.05330},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kannan et al_2019_Large-Scale Multilingual Speech Recognition with a Streaming End-to-End Model.pdf},
  journal = {arXiv:1909.05330 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@inproceedings{kanthak_using_2000,
  title = {Using {{SIMD}} Instructions for Fast Likelihood Calculation in {{LVCSR}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kanthak, S. and Schutz, K. and Ney, H.},
  year = {2000},
  volume = {3},
  pages = {1531-1534 vol.3},
  doi = {10.1109/ICASSP.2000.861948},
  abstract = {Most modern processor architectures provide SIMD (single instruction multiple data) instructions to speed up algorithms based on vector or matrix operations. This paper describes the use of SIMD instructions to calculate Gaussian or Laplacian densities in a large vocabulary speech recognition system. We present a simple, robust method based on scalar quantization of the mean and observation vector components without any loss in recognition performance while speeding up the whole system's runtime by a factor of 3. Combining the approach with vector space partitioning techniques accelerated the overall system by a factor of over 7. The experiments show that the approach can be also applied to Viterbi training without any loss of accuracy. All experiments were conducted on a German, 10,000-word, spontaneous speech task using two architectures, namely Intel Pentium III and SUN UltraSPARC},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kanthak et al_2000_Using SIMD instructions for fast likelihood calculation in LVCSR.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HPDDS28P/861948.html}
}

@inproceedings{kapoor_realtime_2001,
  title = {A Real-Time Head Nod and Shake Detector},
  booktitle = {Proceedings of the 2001 Workshop on {{Perceptive}} User Interfaces},
  author = {Kapoor, Ashish and Picard, Rosalind W.},
  year = {2001},
  pages = {1--5},
  publisher = {{ACM}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kapoor_Picard_2001_A real-time head nod and shake detector.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7T32MBJI/citation.html}
}

@incollection{kapuscinski_recognition_2005,
  title = {Recognition of {{Isolated Words}} of the {{Polish Sign Language}}},
  booktitle = {Computer {{Recognition Systems}}},
  author = {Kapuscinski, Tomasz and Wysocki, Marian},
  year = {2005},
  pages = {697--704},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-32390-2_82},
  abstract = {The paper considers recognition of isolated words of the Polish Sign Language using a canonical stereo system that observes the signer from a frontal view. Recognition is based on human skin detection and Hidden Markov Models. Several feature vectors taking into account information about the hand shape and 3D position of the hand with respect to the face are examined. To improve the recognition rate the classifiers are combined by voting or by fuzzy integral. We focus on 101 words that can be used at the doctor's and at the post office.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kapuscinski_Wysocki_2005_Recognition of Isolated Words of the Polish Sign Language.pdf},
  isbn = {978-3-540-25054-8 978-3-540-32390-7},
  keywords = {capturedBy=vision,continuous,continuous;vocab=101;signer=2;sl=PSL;capturedBy=vision,modal=location,modal=shape,signer=2,sl=PSL,slreco,slreco;modal=shape;modal=location,vocab<500,vocab=101},
  language = {en},
  series = {Advances in {{Soft Computing}}}
}

@article{kapuscinski_recognition_2015,
  title = {Recognition of Hand Gestures Observed by Depth Cameras},
  author = {Kapuscinski, Tomasz and Oszust, Mariusz and Wysocki, Marian and Warchol, Dawid},
  year = {2015},
  volume = {12},
  pages = {36},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kapuscinski et al_2015_Recognition of hand gestures observed by depth cameras.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/D883HB9X/60091.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NG82DAAV/60091.html},
  journal = {International Journal of Advanced Robotic Systems},
  number = {4}
}

@inproceedings{karpathy_largescale_2014,
  title = {Large-Scale Video Classification with Convolutional Neural Networks},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and {Fei-Fei}, Li},
  year = {2014},
  pages = {1725--1732},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Karpathy et al_2014_Large-scale video classification with convolutional neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DEFMFDDF/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html}
}

@inproceedings{karppa_slmotion_2014,
  title = {{{SLMotion}} \textendash{} {{An}} Extensible Sign Language Oriented Video Analysis Tool},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Karppa, Matti and Viitaniemi, Ville and Luzardo, Marcos and Laaksonen, Jorma and Jantunen, Tommi},
  year = {2014},
  month = may,
  publisher = {{European Language Resources Association}},
  address = {{Reykjav\'ik, Iceland}}
}

@article{katz_estimation_1987,
  title = {Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer},
  author = {Katz, Slava},
  year = {1987},
  volume = {35},
  pages = {400--401},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Katz_1987_Estimation of probabilities from sparse data for the language model component.pdf},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  number = {3}
}

@inproceedings{kausar_survey_2011,
  title = {A {{Survey}} on {{Sign Language Recognition}}},
  booktitle = {2011 {{Frontiers}} of {{Information Technology}}},
  author = {Kausar, S. and Javed, M. Y.},
  year = {2011},
  month = dec,
  pages = {95--98},
  doi = {10.1109/FIT.2011.25},
  abstract = {Sign Language (SL) recognition is getting more and more attention of the researchers due to its widespread applicability in many fields. This paper is based on the survey of the current research trends in the field of SL recognition to highlight the current status of different research aspects of the area. Paper also critically analyzed the current research to identify the problem areas and challenges faced by the researchers. This identification is aimed at providing guideline for the future advances in the field.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kausar_Javed_2011_A Survey on Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YNACZBZV/6137126.html}
}

@inproceedings{kawai_deafandmute_1984,
  title = {Deaf-{{And}}-{{Mute Sign Language Generation System}}},
  booktitle = {Medical {{Images}} and {{Icons}}},
  author = {Kawai, Hideo and Tamura, Shinichi},
  year = {1984},
  month = aug,
  volume = {0515},
  pages = {209--218},
  doi = {10.1117/12.964758},
  abstract = {We have developed a system which can recognize speech and generate the corresponding animation-like sign language sequence. The system is implemented in a popular personal computer. This has three video-RAM\&apos;s and a voice recognition board which can recognize only registered voice of a specific speaker. Presently, fourty sign language patterns and fifty finger spellings are stored in two floppy disks. Each sign pattern is composed of one to four sub-patterns. That is, if the pattern is composed of one sub-pattern, it is displayed as a still pattern. If not, it is displayed as a motion pattern. This system will help communications between deaf-and-mute persons and healthy persons. In order to display in high speed, almost programs are written in a machine language.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BDGRG4RW/12.964758.html}
}

@article{kawai_deafandmute_1985,
  title = {Deaf-and-Mute Sign Language Generation System},
  author = {Kawai, Hideo and Tamura, Shinichi},
  year = {1985},
  month = jan,
  volume = {18},
  pages = {199--205},
  issn = {0031-3203},
  doi = {10.1016/0031-3203(85)90045-7},
  abstract = {We have developed a system which can recognize speech and generate the corresponding animation-like sign language sequence in real-time. The system is implemented in a popular personal computer. This has three video RAM (VRAM) and a voice recognition board which can recognize only the registered voice of a specific speaker. At present, forty sign language patterns and fifty finger spellings are stored on two floppy disks. Each sign pattern is composed of one to four sub-patterns. That is, if the pattern is composed of one subpattern, it is displayed as a still pattern. If not, it is displayed as a motion pattern. Japanese has fifty Kana characters. Each character has a corresponding finger spelling. Some of them are also displayed as a motion pattern composed of three sub-patterns. When a text is input from a keyboard or voice recognition circuit board (unit), a corresponding pattern sequence composed of a mixture of sign language patterns and finger spelling patterns is generated and displayed on a CRT. The data of each subpattern are read from a floppy disk and transferred to one of the VRAM (640 \texttimes{} 200 \texttimes{} 1 bits). Since the computer has three VRAM, it is possible to read and transfer the data even when a preceding pattern is still being displayed on the CRT. Thus, spoken natural language sentences can be displayed in animation form by switching the VRAM. This system will help communication between deaf-mute and healthy persons. In order to display in high speed, most programs are written in a machine language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kawai_Tamura_1985_Deaf-and-mute sign language generation system.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GWDGS24Q/0031320385900457.html},
  journal = {Pattern Recognition},
  number = {3}
}

@inproceedings{kawulok_fast_2013,
  title = {Fast Propagation-Based Skin Regions Segmentation in Color Images},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Kawulok, M.},
  year = {2013},
  month = apr,
  pages = {1--7},
  doi = {10.1109/FG.2013.6553733},
  abstract = {This paper introduces a new method for skin regions segmentation which consists in spatial analysis of skin probability maps obtained using pixel-wise detectors. There are a number of methods which use various techniques of skin color modeling to classify every individual pixel or transform input color images into skin probability maps, but their performance is limited due to high variance and low specificity of the skin color. Detection precision can be enhanced based on spatial analysis of skin pixels, however this direction has been little explored so far. Our contribution lies in using the distance transform for propagating the ``skinness'' across the image in a combined domain of luminance, hue and skin probability. In the paper we explain theoretical advantages of the proposed method over alternative skin detectors that also perform spatial analysis. Finally, we present results of an extensive experimental study which clearly indicate high competitiveness of the proposed method and its relevance to gesture recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kawulok_2013_Fast propagation-based skin regions segmentation in color images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2DM96B8A/abs_all.html}
}

@article{kawulok_spatialbased_2014,
  title = {Spatial-Based Skin Detection Using Discriminative Skin-Presence Features},
  author = {Kawulok, Michal and Kawulok, Jolanta and Nalepa, Jakub},
  year = {2014},
  month = may,
  volume = {41},
  pages = {3--13},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2013.08.028},
  abstract = {In this paper we propose a new method for skin detection in color images which consists in spatial analysis using the introduced texture-based discriminative skin-presence features. Color-based skin detection has been widely explored and many skin color modeling techniques were developed so far. However, efficacy of the pixel-wise classification is limited due to an overlap between the skin and non-skin pixels reported in many color spaces. To increase the discriminating power of the skin classification schemes, textural and spatial features are often exploited for skin modeling. Our contribution lies in using the proposed discriminative feature space as a domain for spatial analysis of skin pixels. Contrary to existing approaches, we extract the textural features from the skin probability maps rather than from the luminance channel. Presented experimental study confirms that the proposed method outperforms alternative skin detection techniques, which also involve analysis of textural and spatial features.},
  journal = {Pattern Recognition Letters},
  series = {Supervised and {{Unsupervised Classification Techniques}} and Their {{Applications}}}
}

@inproceedings{keane_coarticulation_2011,
  title = {Coarticulation in {{ASL}} Fingerspelling},
  booktitle = {Proceedings of the {{North East Linguistic Society}} ({{NELS}})},
  author = {Keane, Jonathan and Brentari, Diane and Riggle, Jason},
  year = {2011},
  month = oct,
  volume = {42},
  pages = {261--272},
  address = {{New York, NY, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Keane et al_2012_Coarticulation in ASL fingerspelling.pdf},
  language = {en}
}

@inproceedings{kelly_framework_2009,
  title = {A {{Framework}} for {{Continuous Multimodal Sign Language Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Multimodal Interfaces}} ({{ICMI}})},
  author = {Kelly, Daniel and Reilly Delannoy, Jane and Mc Donald, John and Markham, Charles},
  year = {2009},
  month = nov,
  pages = {351--358},
  address = {{Cambridge, MA, USA}},
  doi = {10.1145/1647314.1647387},
  abstract = {We present a multimodal system for the recognition of manual signs and non-manual signals within continuous sign language sentences. In sign language, information is mainly conveyed through hand gestures (Manual Signs). Non-manual signals, such as facial expressions, head movements, body postures and torso movements, are used to express a large part of the grammar and some aspects of the syntax of sign language. In this paper we propose a multichannel HMM based system to recognize manual signs and non-manual signals. We choose a single non-manual signal, head movement, to evaluate our framework when recognizing non-manual signals. Manual signs and non-manual signals are processed independently using continuous multidimensional HMMs and a HMM threshold model. Experiments conducted demonstrate that our system achieved a detection ratio of 0.95 and a reliability measure of 0.93.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kelly et al_2009_A Framework for Continuous Multimodal Sign Language Recognition.pdf},
  isbn = {978-1-60558-772-1},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=8;signer=1;sl=IrishSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,signer=1,sl=IrishSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=8}
}

@inproceedings{kelly_recognizing_2009,
  title = {Recognizing {{Spatiotemporal Gestures}} and {{Movement Epenthesis}} in {{Sign Language}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Vision}} and {{Image Processing}} ({{IMVIP}})},
  author = {Kelly, D. and McDonald, J. and Markham, C.},
  year = {2009},
  month = sep,
  pages = {145--150},
  publisher = {{IEEE}},
  doi = {10.1109/IMVIP.2009.33},
  abstract = {A novel system for the recognition of spatiotemporal hand gestures used in sign language is presented. While recognition of valid sign sequences is an important task in the overall goal of machine recognition of sign language, recognition of movement epenthesis is an important step towards continuous recognition of natural sign language. We propose a framework for recognizing valid sign segments and identifying movement epenthesis. Experiments show our proposed system performs well when classifying eight different signs and identifying 100 different types of movement epenthesis. A ROC analysis of the systems classifications performance showed an area under the curve measurement of 0.949.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kelly et al_2009_Recognizing Spatiotemporal Gestures and Movement Epenthesis in Sign Language.pdf},
  isbn = {978-1-4244-4875-3},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=8;signer=1;sl=IrishSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,signer=1,sl=IrishSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=8},
  language = {English}
}

@article{kelly_weakly_2011,
  title = {Weakly {{Supervised Training}} of a {{Sign Language Recognition System Using Multiple Instance Learning Density Matrices}}},
  author = {Kelly, Daniel and McDonald, J. and Markham, C.},
  year = {2011},
  month = apr,
  volume = {41},
  pages = {526--541},
  issn = {1083-4419},
  doi = {10.1109/TSMCB.2010.2065802},
  abstract = {A system for automatically training and spotting signs from continuous sign language sentences is presented. We propose a novel multiple instance learning density matrix algorithm which automatically extracts isolated signs from full sentences using the weak and noisy supervision of text translations. The automatically extracted isolated samples are then utilized to train our spatiotemporal gesture and hand posture classifiers. The experiments were carried out to evaluate the performance of the automatic sign extraction, hand posture classification, and spatiotemporal gesture spotting systems. We then carry out a full evaluation of our overall sign spotting system which was automatically trained on 30 different signs.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2011/Kelly et al/Kelly et al_2011_Weakly Supervised Training of a Sign Language Recognition System Using Multiple.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kelly et al_2011_Weakly Supervised Training of a Sign Language Recognition System Using Multiple.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CZAC6D6P/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EUKP255T/abs_all.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=8;signer=2;sl=IrishSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,signer=1,signer=2,sl=IrishSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=8},
  number = {2}
}

@inproceedings{kemp_unsupervised_1999,
  title = {Unsupervised {{Training}} of a {{Speech Recognizer}}: {{Recent Experiments}}},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Kemp, T. and Waibel, A.},
  year = {1999},
  volume = {6},
  pages = {2725--2728},
  address = {{Budapest, Hungary}}
}

@incollection{kendon_how_1988,
  title = {How Gestures Can Become like Words},
  booktitle = {Crosscultural {{Perspectives}} in {{Nonverbal Communication}}},
  author = {Kendon, A.},
  year = {1988},
  pages = {131--141},
  publisher = {{C. J. Hogrefe, Publishers}},
  address = {{Toronto}}
}

@inproceedings{kenny_experiments_2004,
  title = {Experiments in {{Speaker Verification Using Factor Analysis Likelihood Ratios}}},
  booktitle = {{{IEEE Odyssey}} 2004: {{The Speaker}} and {{Language Recognition Workshop}}},
  author = {Kenny, P. and Dumouchel, P.},
  year = {2004},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kenny_Dumouchel_2004_Experiments in Speaker Verification Using Factor Analysis Likelihood Ratios.pdf}
}

@inproceedings{keskin_hand_2012,
  title = {Hand {{Pose Estimation}} and {{Hand Shape Classification Using Multi}}-Layered {{Randomized Decision Forests}}},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Keskin, Cem and K{\i}ra{\c c}, Furkan and Kara, Yunus Emre and Akarun, Lale},
  editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  year = {2012},
  month = oct,
  pages = {852--863},
  publisher = {{Springer Berlin Heidelberg}},
  doi = {10.1007/978-3-642-33783-3_61},
  abstract = {Vision based articulated hand pose estimation and hand shape classification are challenging problems. This paper proposes novel algorithms to perform these tasks using depth sensors. In particular, we introduce a novel randomized decision forest (RDF) based hand shape classifier, and use it in a novel multi\textendash layered RDF framework for articulated hand pose estimation. This classifier assigns the input depth pixels to hand shape classes, and directs them to the corresponding hand pose estimators trained specifically for that hand shape. We introduce two novel types of multi\textendash layered RDFs: Global Expert Network (GEN) and Local Expert Network (LEN), which achieve significantly better hand pose estimates than a single\textendash layered skeleton estimator and generalize better to previously unseen hand poses. The novel hand shape classifier is also shown to be accurate and fast. The methods run in real\textendash time on the CPU, and can be ported to the GPU for further increase in speed.},
  copyright = {\textcopyright 2012 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Keskin et al_2012_Hand Pose Estimation and Hand Shape Classification Using Multi-layered.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U44KGVFG/10.html},
  isbn = {978-3-642-33782-6 978-3-642-33783-3},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{keysers_deformation_2007,
  title = {Deformation {{Models}} for {{Image Recognition}}},
  author = {Keysers, D. and Deselaers, T. and Gollan, C. and Ney, H.},
  year = {2007},
  month = aug,
  volume = {29},
  pages = {1422--1435},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2007.1153},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Automated,Biomedical imaging,Computer-Assisted,handwritten digit recognition,image alignment,Image Interpretation,Image matching,image object variability,low-computational complexity,medical image categorization,medical image categorization.,medical images classification,MNIST benchmark,nonlinear image deformation},
  number = {8}
}

@phdthesis{keysers_modeling_2006,
  title = {Modeling of {{Image Variability}} for {{Recognition}}},
  author = {Keysers, Daniel},
  year = {2006},
  month = mar,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Keysers_2006_Modeling of Image Variability for Recognition.pdf},
  school = {Human Language Technology and Pattern Recognition Group, RWTH Aachen University}
}

@phdthesis{khadivi08:scat,
  title = {Statistical {{Computer}}-{{Assisted Translation}}},
  author = {Khadivi, Shahram},
  year = {2008},
  month = jul,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Khadivi_2008_Statistical Computer-Assisted Translation.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{khan_delayed_2011,
  title = {Delayed Absolute Difference ({{DAD}}) Signatures of Dynamic Features for Sign Language Segmentation},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automation}}, {{Robotics}} and {{Applications}} ({{ICARA}})},
  author = {Khan, Shujjat and Bailey, Donald and Sen Gupta, G.},
  year = {2011},
  pages = {109--114},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Khan et al_2011_Delayed absolute difference (DAD) signatures of dynamic features for sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J39TXAUD/login.html}
}

@article{khan_framework_2013,
  title = {Framework for Reliable, Real-Time Facial Expression Recognition for Low Resolution Images},
  author = {Khan, Rizwan Ahmed and Meyer, Alexandre and Konik, Hubert and Bouakaz, Sa{\"i}da},
  year = {2013},
  month = jul,
  volume = {34},
  pages = {1159--1168},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2013.03.022},
  abstract = {Automatic recognition of facial expressions is a challenging problem specially for low spatial resolution facial images. It has many potential applications in human\textendash computer interactions, social robots, deceit detection, interactive video and behavior monitoring. In this study we present a novel framework that can recognize facial expressions very efficiently and with high accuracy even for very low resolution facial images. The proposed framework is memory and time efficient as it extracts texture features in a pyramidal fashion only from the perceptual salient regions of the face. We tested the framework on different databases, which includes Cohn\textendash Kanade (CK+) posed facial expression database, spontaneous expressions of MMI facial expression database and FG-NET facial expressions and emotions database (FEED) and obtained very good results. Moreover, our proposed framework exceeds state-of-the-art methods for expression recognition on low resolution images.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Khan et al_2013_Framework for reliable, real-time facial expression recognition for low.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/92BHX8NW/S0167865513001268.html},
  journal = {Pattern Recognition Letters},
  number = {10}
}

@article{khan_pause_2014,
  title = {Pause Detection in Continuous Sign Language},
  author = {Khan, Shujjat and Bailey, Donald G. and Gupta, Gourab Sen},
  year = {2014},
  volume = {50},
  pages = {75--83},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Khan et al_2014_Pause detection in continuous sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5JSPB34J/1q55354132n06730.html},
  journal = {International Journal of Computer Applications in Technology},
  number = {1}
}

@inproceedings{kim_american_2012,
  title = {American Sign Language Fingerspelling Recognition with Phonological Feature-Based Tandem Models},
  booktitle = {Spoken {{Language Technology Workshop}} ({{SLT}}), 2012 {{IEEE}}},
  author = {Kim, Taehwan and Livescu, Karen and Shakhnarovich, Greg},
  year = {2012},
  pages = {119--124},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kim et al_2012_American sign language fingerspelling recognition with phonological.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VI5B783F/abs_all.html}
}

@inproceedings{kim_bichannel_2008,
  title = {Bi-Channel Sensor Fusion for Automatic Sign Language Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Kim, J. and Wagner, J. and Rehm, M. and Andre, E.},
  year = {2008},
  month = sep,
  pages = {1--6},
  doi = {10.1109/AFGR.2008.4813341},
  abstract = {In this paper, we investigate the mutual-complementary functionality of accelerometer (ACC) and electromyogram (EMG) for recognizing seven word-level sign vocabularies in German sign language (GSL). Results are discussed for the single channels and for feature-level fusion for the bichannel sensor data. For the subject-dependent condition, this fusion method proves to be effective. Most relevant features for all subjects are extracted and their universal effectiveness is proven with a high average accuracy for the single subjects. Additionally, results are given for the subject-independent condition, where subjective differences do not allow for high recognition rates. Finally we discuss a problem of feature-level fusion caused by high disparity between accuracies of each single channel classification.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kim et al_2008_Bi-channel sensor fusion for automatic sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6E7QNE3N/4813341.html},
  keywords = {capturedBy=electronicGlove,capturedBy=emg,isolated,isolated;vocab=7;signer=8;sl=DGS;capturedBy=emg;capturedBy=electronicGlove,modal=movement,modal=shape,signer=8,sl=DGS,slreco,slreco;modal=movement;modal=shape,vocab<50,vocab=7}
}

@article{kim_depth_2015,
  title = {Depth Based {{Sign Language Recognition System Using SVM}}},
  author = {Kim, Kisang and Kim, Su-Kyung and Choi, Hyung-Il},
  year = {2015},
  volume = {10},
  pages = {75--86},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Kim et al/Kim et al_2015_Depth based Sign Language Recognition System Using SVM.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kim et al_2015_Depth based Sign Language Recognition System Using SVM.pdf},
  journal = {International Journal of Multimedia and Ubiquitous Engineering},
  number = {2}
}

@article{kim_dynamic_1996,
  title = {A Dynamic Gesture Recognition System for the {{Korean}} Sign Language ({{KSL}})},
  author = {Kim, Jong-Sung and Jang, Won and Bien, Zeungnam},
  year = {1996},
  month = apr,
  volume = {26},
  pages = {354--359},
  issn = {1083-4419},
  doi = {10.1109/3477.485888},
  abstract = {The sign language is a method of communication for the deaf-mute. Articulated gestures and postures of hands and fingers are commonly used for the sign language. This paper presents a system which recognizes the Korean sign language (KSL) and translates into a normal Korean text. A pair of data-gloves are used as the sensing device for detecting motions of hands and fingers. For efficient recognition of gestures and postures, a technique of efficient classification of motions is proposed and a fuzzy min-max neural network is adopted for on-line pattern recognition},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kim et al_1996_A dynamic gesture recognition system for the Korean sign language (KSL).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U2SI6SPB/abs_all.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=25;sl=KSL;capturedBy=electronicGlove,modal=2hlocation,modal=2horientation,modal=2hshape,sl=KSL,slreco,slreco;modal=2hshape;modal=2hlocation;modal=2horientation,vocab<50,vocab=25},
  number = {2}
}

@article{kim_lexiconfree_2017,
  title = {Lexicon-{{Free Fingerspelling Recognition}} from {{Video}}: {{Data}}, {{Models}}, and {{Signer Adaptation}}},
  author = {Kim, Taehwan and Keane, Jonathan and Wang, Weiran and Tang, Hao and Riggle, Jason and Shakhnarovich, Gregory and Brentari, Diane and Livescu, Karen},
  year = {2017},
  volume = {46},
  pages = {209--232},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kim et al_2017_Lexicon-Free Fingerspelling Recognition from Video.pdf},
  journal = {Computer Speech and Language}
}

@article{kim_signerindependent_2016,
  title = {Signer-Independent {{Fingerspelling Recognition}} with {{Deep Neural Network Adaptation}}},
  author = {Kim, Taehwan and Wang, Weiran and Tang, Hao and Livescu, Karen},
  year = {2016},
  month = feb,
  abstract = {We study the problem of recognition of fingerspelled letter sequences in American Sign Language in a signer-independent setting. Fingerspelled sequences are both challenging and important to recognize, as they are used for many content words such as proper nouns and technical terms. Previous work has shown that it is possible to achieve almost 90\% accuracies on fingerspelling recognition in a signer-dependent setting. However, the more realistic signer-independent setting presents challenges due to significant variations among signers, coupled with the dearth of available training data. We investigate this problem with approaches inspired by automatic speech recognition. We start with the best-performing approaches from prior work, based on tandem models and segmental conditional random fields (SCRFs), with features based on deep neural network (DNN) classifiers of letters and phonological features. Using DNN adaptation, we find that it is possible to bridge a large part of the gap between signer-dependent and signer-independent performance. Using only about 115 transcribed words for adaptation from the target signer, we obtain letter accuracies of up to 82.7\% with frame-level adaptation labels and 69.7\% with only word labels.},
  archivePrefix = {arXiv},
  eprint = {1602.04278},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kim et al_2016_Signer-independent Fingerspelling Recognition with Deep Neural Network.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z4FDENX4/1602.html},
  journal = {arXiv:1602.04278 [cs]},
  primaryClass = {cs}
}

@article{kindiroglu_automatic_2012,
  title = {Automatic Recognition Fingerspelling Gestures in Multiple Languages for a Communication Interface for the Disabled},
  author = {Kindiroglu, A. Alp and Yalcin, H. and Aran, O. and Hr{\'u}z, M. and Campr, P. and Akarun, L. and Karpov, A.},
  year = {2012},
  month = dec,
  volume = {22},
  pages = {527--536},
  issn = {1054-6618, 1555-6212},
  doi = {10.1134/S1054661812040086},
  abstract = {This paper presents the design and evaluation of a multi-lingual fingerspelling recognition module that is designed for an information terminal. Through the use of multimodal input and output methods, the information terminal acts as a communication medium between deaf and blind people. The system converts fingerspelled words to speech and vice versa using fingerspelling recognition, fingerspelling synthesis, speech recognition and speech synthesis in Czech, Russian, and Turkish languages. We describe an adaptive skin color based fingersign recognition system with a close to real-time performance and present recognition results on 88 different letters signed by five different signers, using above four hours of training and test videos.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kindiroglu et al_2012_Automatic recognition fingerspelling gestures in multiple languages for a.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7WIRHER2/S1054661812040086.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AEA7R8TK/S1054661812040086.html},
  journal = {Pattern Recognition and Image Analysis},
  keywords = {capturedBy=vision,isolated,isolated;vocab=88;signer=11;sl=CzSL;sl=RussianSL;sl=TSL;capturedBy=vision,modal=2hshape,signer=11,sl=CzSL,sl=RussianSL,sl=TSL,slreco,slreco;modal=2hshape,vocab<100,vocab=88},
  language = {en},
  number = {4}
}

@article{kindiroglu_multilingual_2011a,
  title = {Multi-Lingual Fingerspelling Recognition for Handicapped Kiosk},
  author = {Kindiroglu, A. A. and Yalcin, H. and Aran, O. and Hruz, M. and Campr, P. and Akarun, L. and Karpov, A.},
  year = {2011},
  month = sep,
  volume = {21},
  pages = {402--406},
  issn = {1054-6618},
  doi = {10.1134/S1054661811020532},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3KICH46C/Kindiroglu et al. - 2011 - Multi-lingual fingerspelling recognition for handi.pdf},
  journal = {Pattern Recognition and Image Analysis},
  number = {3}
}

@inproceedings{kindiroglu_temporal_2019,
  ids = {kindiroglu\_temporal\_2020},
  title = {Temporal {{Accumulative Features}} for {{Sign Language Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision Workshop}} ({{ICCVW}})},
  author = {K{\i}nd{\i}ro{\u g}lu, Ahmet Alp and {\"O}zdemir, O{\u g}ulcan and Akarun, Lale},
  year = {2019},
  month = oct,
  pages = {1288--1297},
  issn = {2473-9944},
  doi = {10.1109/ICCVW.2019.00164},
  abstract = {In this paper, we propose a set of features called temporal accumulative features (TAF) for representing and recognizing isolated sign language gestures. By incorporating sign language specific constructs to better represent the unique linguistic characteristic of sign language videos, we have devised an efficient and fast SLR method for recognizing isolated sign language gestures. The proposed method is an HSV based accumulative video representation where keyframes based on the linguistic movement-hold model are represented by different colors. We also incorporate hand shape information and using a small scale convolutional neural network, demonstrate that sequential modeling of accumulative features for linguistic subunits improves upon baseline classification results.},
  archivePrefix = {arXiv},
  eprint = {2004.01225},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kındıroğlu et al_2019_Temporal Accumulative Features for Sign Language Recognition.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kındıroğlu et al_2020_Temporal Accumulative Features for Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/452N3HWA/9022275.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Y7DSSCMD/2004.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=174;signer=4;sl=TSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hshape,modal=bodyjoints,signer=4,sl=TSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=bodyjoints,vocab<500,vocab=174}
}

@article{kingsbury_robust_1998,
  title = {Robust {{Speech Recognition Using}} the {{Modulation Spectrogram}}},
  author = {Kingsbury, B.E.D and Morgan, N. and Greenberg, S.},
  year = {1998},
  volume = {25},
  pages = {117--132},
  journal = {Speech Communication},
  number = {1-3}
}

@inproceedings{kipp_assessing_2011,
  title = {Assessing the {{Deaf User Perspective}} on {{Sign Language Avatars}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Kipp, Michael and Nguyen, Quan and Heloir, Alexis and Matthes, Silke},
  year = {2011},
  month = oct,
  pages = {107--114},
  address = {{New York, NY, USA}},
  doi = {10.1145/2049536.2049557},
  abstract = {Signing avatars have the potential to become a useful and even cost-effective method to make written content more accessible for Deaf people. However, avatar research is characterized by the fact that most researchers are not members of the Deaf community, and that Deaf people as potential users have little or no knowledge about avatars. Therefore, we suggest two well-known methods, focus groups and online studies, as a two-way information exchange between research and the Deaf community. Our aim was to assess signing avatar acceptability, shortcomings of current avatars and potential use cases. We conducted two focus group interviews (N=8) and, to quantify important issues, created an accessible online user study(N=317). This paper deals with both the methodology used and the elicited opinions and criticism. While we found a positive baseline response to the idea of signing avatars, we also show that there is a statistically significant increase in positive opinion caused by participating in the studies. We argue that inclusion of Deaf people on many levels will foster acceptance as well as provide important feedback regarding key aspects of avatar technology that need to be improved.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kipp et al_2011_Assessing the Deaf User Perspective on Sign Language Avatars.pdf},
  isbn = {978-1-4503-0920-2}
}

@techreport{kipp_machbarkeitsstudie_2011,
  title = {Machbarkeitsstudie Zur {{Absch\"atzung}} Der {{Nutzungsm\"oglichkeiten}} von {{Geb\"ardenavataren}}},
  author = {Kipp, Michael and Nguyen, Quan and Heloir, Alexis},
  year = {2011},
  institution = {{Bundesministerium f\"ur Arbeit und Soziales}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kipp et al_2011_Machbarkeitsstudie zur Abschätzung der Nutzungsmöglichkeiten von.pdf}
}

@inproceedings{kipp_sign_2011,
  title = {Sign Language Avatars: Animation and Comprehensibility},
  shorttitle = {Sign Language Avatars},
  booktitle = {Intelligent {{Virtual Agents}}},
  author = {Kipp, Michael and Heloir, Alexis and Nguyen, Quan},
  year = {2011},
  pages = {113--126},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kipp et al_2011_Sign language avatars.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AKKQJHFQ/978-3-642-23974-8_13.html}
}

@article{kishore_motionlets_2018,
  title = {Motionlets {{Matching With Adaptive Kernels}} for 3-{{D Indian Sign Language Recognition}}},
  author = {Kishore, P. V. V. and Kumar, D. A. and Sastry, A. S. C. S. and Kumar, E. K.},
  year = {2018},
  month = apr,
  volume = {18},
  pages = {3327--3337},
  issn = {1530-437X},
  doi = {10.1109/JSEN.2018.2810449},
  abstract = {Recognizing human gestures in sign language is a complex and challenging task. Human sign language gestures are a combination of independent hand and finger articulations, which are sometimes performed in coordination with the head, face, and body. 3-D motion capture of sign language involves recording 3-D sign videos that are often affected by interobject or self-occlusions, lighting, and background. This paper proposes characterization of sign language gestures articulated at different body parts as 3-D motionlets, which describe the signs with a subset of joint motions. A two-phase fast algorithm identifies 3-D query signs from an adaptively ranked database of 3-D sign language. Phase-I process clusters all human joints into motion joints (MJ) and nonmotion joints (NMJ). The relation between MJ and NMJ is analyzed to categorically segment the database into four motionlet classes. Phase-II process investigates the relation within the motion joints to represent shape information of a sign as 3-D motionlets. The 4-class sign database features three adaptive motionlet kernels. A simple kernel matching algorithm is used to rank the database according to the highest-ranked query sign. The proposed method is sign invariant to temporal misalignment and can characterize sign language based on a 3-D spatiotemporal framework. In this paper, five 500-word Indian sign language data sets were used to evaluate the proposed model. The experimental results reveal that the method proposed here improved recognition compared with the state-of-the-art 3-D action recognition methods.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kishore et al_2018_Motionlets Matching With Adaptive Kernels for 3-D Indian Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AJBWMZGC/8304621.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GSS94A8N/8304621.html},
  journal = {IEEE Sensors Journal},
  keywords = {capturedBy=mocap,isolated,isolated;vocab=500;signer=5;sl=IndianSL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,modal=bodyjoints,modal=head,signer=5,sl=IndianSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape;modal=head;modal=bodyjoints,vocab<1000,vocab=500},
  number = {8}
}

@article{kishore_video_2012,
  title = {A Video Based {{Indian Sign Language Recognition System}} ({{INSLR}}) Using Wavelet Transform and Fuzzy Logic},
  author = {Kishore, P. V. V. and Kumar, P. Rajesh},
  year = {2012},
  volume = {4},
  pages = {537},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kishore_Kumar_2012_A video based Indian Sign Language Recognition System (INSLR) using wavelet.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5FNUV526/openurldocerror.html},
  journal = {Int. Journal of Engineering and Technology},
  keywords = {capturedBy=vision,isolated,isolated;vocab=80;signer=10;sl=IndianSL;capturedBy=vision,modal=2hshape,signer=10,sl=IndianSL,slreco,slreco;modal=2hshape,vocab<100,vocab=80},
  number = {5}
}

@incollection{klapper-rybicka_unsupervised_2001,
  title = {Unsupervised {{Learning}} in {{LSTM Recurrent Neural Networks}}},
  booktitle = {Artificial {{Neural Networks}} \textemdash{} {{ICANN}} 2001},
  author = {{Klapper-Rybicka}, Magdalena and Schraudolph, Nicol N. and Schmidhuber, J{\"u}rgen},
  editor = {Dorffner, Georg and Bischof, Horst and Hornik, Kurt},
  year = {2001},
  month = jan,
  pages = {684--691},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {While much work has been done on unsupervised learning in feedforward neural network architectures, its potential with (theoretically more powerful) recurrent networks and time-varying inputs has rarely been explored. Here we train Long Short-Term Memory (LSTM) recurrent networks to maximize two information-theoretic objectives for unsupervised learning: Binary Information Gain Optimization (BINGO) and Nonparametric Entropy Optimization (NEO). LSTM learns to discriminate different types of temporal sequences and group them according to a variety of features.},
  copyright = {\textcopyright 2001 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Klapper-Rybicka et al_2001_Unsupervised Learning in LSTM Recurrent Neural Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/B5S9AQ5A/3-540-44668-0_95.html},
  isbn = {978-3-540-42486-4 978-3-540-44668-2},
  language = {en},
  number = {2130},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{klaser_spatiotemporal_2008,
  title = {A {{Spatio}}-{{Temporal Descriptor Based}} on {{3D}}-{{Gradients}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Klaser, Alexander and Marszalek, Marcin and Schmid, Cordelia},
  editor = {Everingham, Mark and Needham, Chris and Fraile, Roberto},
  year = {2008},
  month = sep,
  pages = {275:1-10},
  publisher = {{British Machine Vision Association}},
  address = {{Leeds, United Kingdom}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Klaser et al_2008_A Spatio-Temporal Descriptor Based on 3D-Gradients.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/E5ZHPMRX/00005039.html}
}

@book{klima_signs_1979,
  title = {The {{Signs}} of {{Language}}},
  author = {Klima, Edward S.},
  year = {1979},
  publisher = {{Harvard University Press}},
  abstract = {In a book with far-reaching implications, Edward S. Klima and Ursula Bellugi present a full exploration of a language in another mode--a language of the hands and of the eyes. They discuss the origin and development of American Sign Language, the internal structure of its basic units, the grammatical processes it employs, and its heightened use in poetry and wit. The authors draw on research, much of it by and with deaf people, to answer the crucial question of what is fundamental to language as language and what is determined by the mode (vocal or gestural) in which a language is produced.},
  isbn = {978-0-674-80796-9},
  language = {en}
}

@inproceedings{kneser_improved_1995,
  title = {Improved Backing-off for m-Gram Language Modeling},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kneser, Reinhard and Ney, Hermann},
  year = {1995},
  month = may,
  volume = {1},
  pages = {181--184},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kneser_Ney_1995_Improved backing-off for m-gram language modeling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GP9E5ICL/479394.html}
}

@article{ko_neural_2019,
  ids = {ko\_neural\_2018},
  title = {Neural {{Sign Language Translation Based}} on {{Human Keypoint Estimation}}},
  author = {Ko, Sang-Ki and Kim, Chang Jo and Jung, Hyedong and Cho, Choongsang},
  year = {2019},
  month = jan,
  volume = {9},
  pages = {2683},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/app9132683},
  abstract = {We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI (Korea Electronics Technology Institute) sign language dataset, which consists of 14,672 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting point for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from the face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieved 93.28\% (55.28\%, respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compared several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.},
  archivePrefix = {arXiv},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  eprint = {1811.11436},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ko et al_2018_Neural Sign Language Translation based on Human Keypoint Estimation.pdf},
  journal = {Applied Sciences},
  language = {en},
  number = {13}
}

@inproceedings{kobayashi_partlyhidden_1997,
  title = {Partly-Hidden {{Markov}} Model and Its Application to Gesture Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kobayashi, T. and Haruyama, S.},
  year = {1997},
  month = apr,
  volume = {4},
  pages = {3081 -3084 vol.4},
  doi = {10.1109/ICASSP.1997.595443},
  abstract = {A new pattern matching method, the partly-hidden Markov model, is proposed for gesture recognition. The hidden Markov model, which is widely used for the time series pattern recognition, can deal with only piecewise stationary stochastic process. We solved this problem by introducing the modified second order Markov model, in which the first state is hidden and the second one is observable. As shown by the results of 6 sign-language recognition test, the error rate was improved by 73\% compared with normal HMM},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kobayashi_Haruyama_1997_Partly-hidden Markov model and its application to gesture recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V5BDPJC4/Kobayashi and Haruyama - 1997 - Partly-hidden Markov model and its application to .html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=6;signer=20;sl=JSL;capturedBy=vision,modal=head,modal=location,modal=shape,signer=20,sl=JSL,slreco,slreco;modal=shape;modal=location;modal=head,vocab<50,vocab=6}
}

@inproceedings{koehn_empirical_2003,
  title = {Empirical {{Methods}} for {{Compound Splitting}}},
  booktitle = {10th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Koehn, Philipp and Knight, Kevin},
  year = {2003},
  month = apr,
  publisher = {{Association for Computational Linguistics}},
  address = {{Budapest, Hungary}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koehn_Knight_2003_Empirical Methods for Compound Splitting.pdf}
}

@phdthesis{koller_automatic_2010,
  title = {Automatic {{Speech Recognition}} and {{Identification}} of {{African Portuguese}}},
  author = {Koller, Oscar},
  year = {2010},
  month = jun,
  address = {{Berlin, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller_2010_Automatic Speech Recognition and Identification of African Portuguese.pdf},
  school = {Berlin University of Technology}
}

@inproceedings{koller_automatic_2012,
  title = {Automatic {{Speech Recognition}} and {{Identification}} of {{African Portuguese}}},
  booktitle = {{{PROPOR}}'2012 - {{MSc}}/{{PhD Thesis Contest}}},
  author = {Koller, Oscar and Abad, Alberto and Trancoso, Isabel},
  year = {2012},
  month = apr,
  address = {{Coimbra, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2012_Automatic Speech Recognition and Identification of African Portuguese.pdf}
}

@inproceedings{koller_automatic_2016,
  title = {Automatic {{Alignment}} of {{HamNoSys Subunits}} for {{Continuous Sign Language Recognition}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  year = {2016},
  month = may,
  pages = {121--128},
  address = {{Portoro\v{z}, Slovenia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2016_Automatic Alignment of HamNoSys Subunits for Continuous Sign Language.pdf},
  keywords = {augment=flip,augment=randomcrop,authorgroup=RWTH,authorgroup=Surrey,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,dataSet=Phoenix14,loss=CE,modal=2hmovement,modal=eyebrows,modal=eyes,modal=head,modal=location,modal=mouth,modal=orientation,modal=shape,nn=2dcnn,nnarch=inception,nntrain=2dcnn,paperinfo,paperinfo;authorgroup=RWTH;authorgroup=Surrey;shorttitle=Align Hamnosys,re-align=1,shorttitle=Align Hamnosys,signer=9,sl=DGS,slreco,slreco;modal=shape;modal=orientation;modal=location;modal=2hmovement;modal=head;modal=mouth;modal=eyes;modal=eyebrows;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=1;dataSet=Phoenix14;wer=49.6/48.2;augment=randomcrop;augment=flip;nntrain=2dcnn;loss=CE,vocab=1080,vocab>=1000,wer=49.6/48.2}
}

@article{koller_continuous_2015,
  title = {Continuous Sign Language Recognition: {{Towards}} Large Vocabulary Statistical Recognition Systems Handling Multiple Signers},
  shorttitle = {Continuous Sign Language Recognition},
  author = {Koller, Oscar and Forster, Jens and Ney, Hermann},
  year = {2015},
  month = dec,
  volume = {141},
  pages = {108--125},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2015.09.013},
  abstract = {This work presents a statistical recognition approach performing large vocabulary continuous sign language recognition across different signers. Automatic sign language recognition is currently evolving from artificial lab-generated data to `real-life' data. To the best of our knowledge, this is the first time system design on a large data set with true focus on real-life applicability is thoroughly presented. Our contributions are in five areas, namely tracking, features, signer dependency, visual modelling and language modelling. We experimentally show the importance of tracking for sign language recognition with respect to the hands and facial landmarks. We further contribute by explicitly enumerating the impact of multimodal sign language features describing hand shape, hand position and movement, inter-hand-relation and detailed facial parameters, as well as temporal derivatives. In terms of visual modelling we evaluate non-gesture-models, length modelling and universal transition models. Signer-dependency is tackled with CMLLR adaptation and we further improve the recognition by employing class language models. We evaluate on two publicly available large vocabulary databases representing lab-data (SIGNUM database: 25 signers, 455 sign vocabulary, 19k sentences) and unconstrained `real-life' sign language (RWTH-PHOENIX-Weather database: 9 signers, 1081 sign vocabulary, 7k sentences) and achieve up to 10.0\%/16.4\% and respectively up to 34.3\%/53.0\% word error rate for single signer/multi-signer setups. Finally, this work aims at providing a starting point to newcomers into the field.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2015_Continuous sign language recognition.pdf},
  journal = {Computer Vision and Image Understanding (CVIU)},
  keywords = {authorgroup=RWTH,binary,capturedBy=vision,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,dataSet=Phoenix14,modal=2hmovement,modal=eyebrows,modal=eyes,modal=head,modal=location,modal=mouth,modal=shape,paperinfo,paperinfo;authorgroup=RWTH;shorttitle=CSLR,shorttitle=CSLR,signer=9,sl=DGS,slreco,slreco;modal=shape;modal=location;modal=2hmovement;modal=head;modal=mouth;modal=eyes;modal=eyebrows;dataSet=Phoenix14;wer=55.0/53.0,vocab=1080,vocab>=1000,wer=55.0/53.0},
  series = {Pose \& {{Gesture}}}
}

@inproceedings{koller_deep_2016,
  title = {Deep {{Hand}}: {{How}} to {{Train}} a {{CNN}} on 1 {{Million Hand Images When Your Data Is Continuous}} and {{Weakly Labelled}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  year = {2016},
  month = jun,
  pages = {3793--3802},
  address = {{Las Vegas, NV, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2016_Deep Hand.pdf},
  keywords = {augment=flip,augment=randomcrop,authorgroup=RWTH,authorgroup=Surrey,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Phoenix14,dataSet=Signum,loss=CE,modal=2hmovement,modal=eyebrows,modal=eyes,modal=head,modal=location,modal=mouth,modal=shape,nn=2dcnn,nnarch=inception,nntrain=2dcnn,paperinfo,paperinfo;authorgroup=RWTH;authorgroup=Surrey;shorttitle=1 Million Hands,re-align=1,shorttitle=1 Million Hands,signer=1,signer=9,sl=DGS,slreco,slreco;modal=shape;modal=location;modal=2hmovement;modal=head;modal=mouth;modal=eyes;modal=eyebrows;dataSet=Phoenix14;wer=47.1/45.1;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=1;augment=randomcrop;augment=flip;nntrain=2dcnn;loss=CE,slreco;modal=shape;modal=location;modal=2hmovement;modal=head;modal=mouth;modal=eyes;modal=eyebrows;dataSet=Signum;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=1;augment=randomcrop;augment=flip;nntrain=2dcnn;loss=CE,vocab<500,vocab=1080,vocab=455,vocab>=1000,wer=47.1/45.1}
}

@article{koller_deep_2018,
  title = {Deep {{Sign}}: {{Enabling Robust Statistical Continuous Sign Language Recognition}} via {{Hybrid CNN}}-{{HMMs}}},
  author = {Koller, Oscar and Zargaran, Sepehr and Ney, Hermann and Bowden, Richard},
  year = {2018},
  month = dec,
  volume = {126},
  pages = {1311--1325},
  issn = {1573-1405},
  doi = {10.1007/s11263-018-1121-3},
  abstract = {This manuscript introduces the end-to-end embedding of a CNN into a HMM, while interpreting the outputs of the CNN in a Bayesian framework. The hybrid CNN-HMM combines the strong discriminative abilities of CNNs with the sequence modelling capabilities of HMMs. Most current approaches in the field of gesture and sign language recognition disregard the necessity of dealing with sequence data both for training and evaluation. With our presented end-to-end embedding we are able to improve over the state-of-the-art on three challenging benchmark continuous sign language recognition tasks by between 15 and 38\% relative reduction in word error rate and up to 20\% absolute. We analyse the effect of the CNN structure, network pretraining and number of hidden states. We compare the hybrid modelling to a tandem approach and evaluate the gain of model combination.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2018_Deep Sign.pdf},
  journal = {International Journal of Computer Vision (IJCV)},
  keywords = {augment=flip,augment=randomcrop,authorgroup=RWTH,authorgroup=Surrey,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Phoenix14,dataSet=Signum,loss=CE,modal=shape,nn=2dcnn,nnarch=inception,nntrain=2dcnn,paperinfo,paperinfo;authorgroup=RWTH;authorgroup=Surrey;shorttitle=Hybrid CNN-HMMs,re-align=0,shorttitle=Hybrid CNN-HMMs,shorttitle=SLR via Hybrid CNN-HMMs,signer=1,signer=9,sl=DGS,slreco,slreco;modal=shape;dataSet=Phoenix14;wer=31.6/32.5;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=0;augment=randomcrop;augment=flip;nntrain=2dcnn;loss=CE,slreco;modal=shape;dataSet=Signum;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=0;augment=randomcrop;augment=flip;nntrain=2dcnn;loss=CE,vocab<500,vocab=1080,vocab=455,vocab>=1000,wer=31.6/32.5},
  number = {12}
}

@inproceedings{koller_exploiting_2010,
  title = {Exploiting Variety-Dependent {{Phones}} in \{\vphantom\}{{Portuguese}}\vphantom\{\} {{Variety Identification}} Applied to {{Broadcast News Transcription}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Koller, O. and Abad, A. and Trancoso, I. and Viana, C.},
  year = {2010},
  pages = {749--752},
  address = {{Makuhari, Japan}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2010_Exploiting variety-dependent Phones in Portuguese Variety Identification.pdf}
}

@inproceedings{koller_exploiting_2010a,
  title = {Exploiting Variety-Dependent {{Phones}} in \{\vphantom\}{{Portuguese}}\vphantom\{\} {{Variety Identification}}},
  booktitle = {Odyssey 2010: {{The Speaker}} and {{Language Recognition Workshop}}},
  author = {Koller, Oscar and Abad, Alberto and Trancoso, Isabel},
  year = {2010},
  month = jun,
  pages = {279--285},
  address = {{Brno, Czech Republic}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2010_Exploiting variety-dependent Phones in Portuguese Variety Identification.pdf}
}

@inproceedings{koller_may_2013,
  title = {May the {{Force}} Be with You: {{Force}}-{{Aligned SignWriting}} for {{Automatic Subunit Annotation}} of {{Corpora}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  year = {2013},
  month = apr,
  pages = {1--6},
  address = {{Shanghai, PRC}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2013_May the Force be with you.pdf}
}

@article{koller_quantitative_2020,
  title = {Quantitative {{Survey}} of the {{State}} of the {{Art}} in {{Sign Language Recognition}}},
  author = {Koller, Oscar},
  year = {2020},
  month = aug,
  abstract = {This work presents a meta study covering around 300 published sign language recognition papers with over 400 experimental results. It includes most papers between the start of the field in 1983 and 2020. Additionally, it covers a fine-grained analysis on over 25 studies that have compared their recognition approaches on \textbackslash phoenix 2014, the standard benchmark task of the field. Research in the domain of sign language recognition has progressed significantly in the last decade, reaching a point where the task attracts much more attention than ever before. This study compiles the state of the art in a concise way to help advance the field and reveal open questions. Moreover, all of this meta study's source data is made public, easing future work with it and further expansion. The analyzed papers have been manually labeled with a set of categories. The data reveals many insights, such as, among others, shifts in the field from intrusive to non-intrusive capturing, from local to global features and the lack of non-manual parameters included in medium and larger vocabulary recognition systems. Surprisingly, RWTH-PHOENIX-Weather with a vocabulary of 1080 signs represents the only resource for large vocabulary continuous sign language recognition benchmarking world wide.},
  archivePrefix = {arXiv},
  eprint = {2008.09918},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller_2020_Quantitative Survey of the State of the Art in Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZMK2QXJ8/2008.html},
  journal = {arXiv:2008.09918 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{koller_read_2014,
  title = {Read {{My Lips}}: {{Continuous Signer Independent Weakly Supervised Viseme Recognition}}},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  year = {2014},
  month = sep,
  pages = {281--296},
  address = {{Zurich, Switzerland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2014_Read My Lips.pdf}
}

@inproceedings{koller_resign_2017,
  title = {Re-{{Sign}}: {{Re}}-{{Aligned End}}-{{To}}-{{End Sequence Modelling With Deep Recurrent CNN}}-{{HMMs}}},
  shorttitle = {Re-{{Sign}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Koller, Oscar and Zargaran, Sepehr and Ney, Hermann},
  year = {2017},
  month = jul,
  pages = {4297--4305},
  address = {{Honolulu, HI, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2017_Re-Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DYI7L4FT/Koller_Re-Sign_Re-Aligned_End-To-End_CVPR_2017_paper.html},
  keywords = {augment=flip,augment=randomcrop,authorgroup=RWTH,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Phoenix14,dataSet=Signum,loss=CE,modal=fullframe,nn=2dcnn,nn=blstm,nnarch=inception,nntrain=2dcnn+blstm,paperinfo,paperinfo;authorgroup=RWTH;shorttitle=Re-Align,re-align=1,shorttitle=Re-Align,signer=1,signer=9,sl=DGS,slreco,slreco;modal=fullframe;dataSet=Phoenix14;wer=27.1/26.8;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=1;nn=blstm;nntrain=2dcnn+blstm;augment=randomcrop;augment=flip;loss=CE,slreco;modal=fullframe;dataSet=Signum;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=1;nn=blstm;nntrain=2dcnn+blstm;augment=randomcrop;augment=flip;loss=CE,vocab<500,vocab=1080,vocab=455,vocab>=1000,wer=27.1/26.8}
}

@article{koller_weakly_2019,
  title = {Weakly {{Supervised Learning}} with {{Multi}}-{{Stream CNN}}-{{LSTM}}-{{HMMs}} to {{Discover Sequential Parallelism}} in {{Sign Language Videos}}},
  author = {Koller, Oscar and Camgoz, Necati Cihan and Ney, Hermann and Bowden, Richard},
  year = {2019},
  volume = {accepted for publication},
  pages = {15},
  abstract = {In this work we present a new approach to the field of weakly supervised learning in the video domain. Our method is relevant to sequence learning problems which can be split up into sub-problems that occur in parallel. Here, we experiment with sign language data. The approach exploits sequence constraints within each independent stream and combines them by explicitly imposing synchronisation points to make use of parallelism that all sub-problems share. We do this with multi-stream HMMs while adding intermediate synchronisation constraints among the streams. We embed powerful CNN-LSTM models in each HMM stream following the hybrid approach. This allows the discovery of attributes which on their own lack sufficient discriminative power to be identified. We apply the approach to the domain of sign language recognition exploiting the sequential parallelism to learn sign language, mouth shape and hand shape classifiers. We evaluate the classifiers on three publicly available benchmark data sets featuring challenging real-life sign language with over 1000 classes, full sentence based lip-reading and articulated hand shape recognition on a fine-grained hand shape taxonomy featuring over 60 different hand shapes. We clearly outperform the state-of-the-art on all data sets and observe significantly faster convergence using the parallel alignment approach.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2019_Weakly Supervised Learning with Multi-Stream CNN-LSTM-HMMs to Discover.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {augment=flip,augment=randomcrop,authorgroup=RWTH,authorgroup=Surrey,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1066;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,dataSet=Phoenix14,dataSet=Phoenix14T,loss=CE,modal=fullframe,modal=mouth,modal=shape,nn=2dcnn,nn=blstm,nnarch=inception,nntrain=2dcnn+blstm,paperinfo,paperinfo;authorgroup=RWTH;authorgroup=Surrey;shorttitle=Multi-Stream CNN-HMMs,re-align=1,shorttitle=Multi-Stream CNN-HMMs,signer=9,sl=DGS,slreco,slreco;modal=shape;modal=mouth;modal=fullframe;dataSet=Phoenix14;wer=26.0/26.0;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=1;nn=blstm;nntrain=2dcnn+blstm;augment=randomcrop;augment=flip;loss=CE,slreco;modal=shape;modal=mouth;modal=fullframe;dataSet=Phoenix14T;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=1;nn=blstm;nntrain=2dcnn+blstm;augment=randomcrop;augment=flip;loss=CE,vocab=1066,vocab=1080,vocab>=1000,wer=26.0/26.0},
  language = {en}
}

@inproceedings{koller15:mouth,
  title = {Deep {{Learning}} of {{Mouth Shapes}} for {{Sign Language}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  year = {2015},
  month = dec,
  pages = {477--483},
  address = {{Santiago, Chile}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2015_Deep Learning of Mouth Shapes for Sign Language.pdf}
}

@inproceedings{koller16:hybridsign,
  title = {Deep {{Sign}}: {{Hybrid CNN}}-{{HMM}} for {{Continuous Sign Language Recognition}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Koller, Oscar and Zargaran, Sepehr and Ney, Hermann and Bowden, Richard},
  year = {2016},
  month = sep,
  pages = {1--12},
  address = {{York, UK}},
  doi = {10.5244/C.30.136},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2016_Deep Sign.pdf},
  keywords = {augment=flip,augment=randomcrop,authorgroup=RWTH,authorgroup=Surrey,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Phoenix14,dataSet=Signum,loss=CE,modal=shape,nn=2dcnn,nnarch=inception,nntrain=2dcnn,paperinfo,paperinfo;authorgroup=RWTH;authorgroup=Surrey;shorttitle=Deep Sign,re-align=0,shorttitle=Deep Sign,signer=1,signer=9,sl=DGS,slreco,slreco;modal=shape;dataSet=Phoenix14;wer=38.3/38.8;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=0;augment=randomcrop;augment=flip;nntrain=2dcnn;loss=CE,slreco;modal=shape;dataSet=Signum;nn=2dcnn;nnarch=inception;cnnresiduals=0;re-align=0;augment=randomcrop;augment=flip;nntrain=2dcnn;loss=CE,vocab<500,vocab=1080,vocab=455,vocab>=1000,wer=38.3/38.8}
}

@inproceedings{koller2014:mouthings,
  title = {Weakly {{Supervised Automatic Transcription}} of {{Mouthings}} for {{Gloss}}-{{Based Sign Language Corpora}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Koller, Oscar and Ney, Hermann and Bowden, Richard},
  year = {2014},
  month = may,
  pages = {98--94},
  address = {{Reykjavik, Iceland}},
  booktitlelink = {http://www.lrec-conf.org/proceedings/lrec2014/workshops/LREC2014Workshop-SignLanguage%20Proceedings.pdf},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koller et al_2014_Weakly Supervised Automatic Transcription of Mouthings for Gloss-Based Sign.pdf}
}

@inproceedings{kong_3d_2004,
  title = {3-{{D Hand Trajectory Recognition}} for {{Signing Exact English}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Kong, W.W. and Ranganath, S.},
  year = {2004},
  month = may,
  pages = {535--540},
  doi = {10.1109/AFGR.2004.1301588},
  abstract = {This work presents a hieraarchical approach to recogniz isolated 3-D hand gesture trajectories for signing exact English (SEE). SEE hand gestures can be periodic as well as non-periodic. We first differentiate between periodic and non-periodic gestures followed by recognition of individual gestures. After periodicity detection, non-periodic trajectories are classified into 8 classes and periodic trajectories are classified into 4 classes. A Polhemus tracker is used to provide the input data. Periodicity detection is based on Fourier analysis and hand trajectories are recognized by vector quantization principal component analysis (VQPCA). The average periodicity detection accuracy is 95.9\%. The average recognition rates with VQPCA for non-periodic and periodic gestures are 97.3\% and 97.0\% respectively. In comparison, k-means clustering yielded 87.0\% and 85.1\%, respectively.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kong_Ranganath_2004_3-D hand trajectory recognition for signing exact English.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5XN5BZ6T/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VDRNUWXD/login.html}
}

@inproceedings{kong_automatic_2008,
  title = {Automatic {{Hand Trajectory Segmentation}} and {{Phoneme Transcription}} for {{Sign Language}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Kong, W.W. and Ranganath, S.},
  year = {2008},
  month = sep,
  pages = {1--6},
  doi = {10.1109/AFGR.2008.4813462},
  abstract = {This paper presents an automatic approach to segment 3-D hand trajectories and transcribe phonemes based on them, as a step towards recognizing American sign language (ASL).We first apply a segmentation algorithm which detects minimal velocity and maximal change of directional angle to segment the hand motion trajectory of naturally signed sentences. This yields over-segmented trajectories, which are further processed by a trained naive Bayesian detector to identify true segmented points and eliminate false alarms. The above segmentation algorithm yielded 88.5\% true segmented points and 11.8\% false alarms on unseen ASL sentence samples. These segmentation results were refined by a simple majority voting scheme, and the final segments obtained were used to transcribe phonemes for ASL. This was based on clustering PCA-based features extracted from training sentences. We then trained hidden Markov models (HMMs) to recognize the sequence of phonemes in the sentences. On the 25 test sentences containing 157 segments, the average number of errors obtained was 15.6.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kong_Ranganath_2008_Automatic hand trajectory segmentation and phoneme transcription for sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5JPRZXXN/abs_all.html}
}

@article{kong_sign_2010,
  title = {Sign {{Language Phoneme Transcription}} with {{Rule}}-Based {{Hand Trajectory Segmentation}}},
  author = {Kong, W. W. and Ranganath, S.},
  year = {2010},
  volume = {59},
  pages = {211--222},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kong_Ranganath_2010_Sign Language Phoneme Transcription with Rule-based Hand Trajectory Segmentation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5G7SFDTQ/Kong und Ranganath - 2010 - Sign Language Phoneme Transcription with Rule-base.html},
  journal = {Journal of Signal Processing Systems},
  keywords = {capturedBy=mocap,continuous,continuous;vocab=33;signer=1;sl=ASL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,signer=1,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=33},
  number = {2}
}

@article{kong_signing_2008,
  title = {Signing {{Exact English}} ({{SEE}}): {{Modeling}} and Recognition},
  shorttitle = {Signing {{Exact English}} ({{SEE}})},
  author = {Kong, W.W. and Ranganath, Surendra},
  year = {2008},
  month = may,
  volume = {41},
  pages = {1638--1652},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2007.10.016},
  abstract = {We present effective and robust algorithms to recognize isolated signs in Signing Exact English (SEE). The sign-level recognition scheme comprises classifiers for handshape, hand movement and hand location. The SEE gesture data are acquired using CyberGlove \textregistered{} and magnetic trackers. A linear decision tree with Fisher's linear discriminant (FLD) is used to classify 27 SEE handshapes. Hand movement trajectory is classified using vector quantization principal component analysis (VQPCA). Both periodic and non-periodic SEE sign gestures are recognized from isolated 3-D hand trajectories. Experiments yielded average handshape recognition accuracy of 96.1\% on ``unseen'' signers. The average trajectory recognition rate with VQPCA for non-periodic and periodic gestures was 97.3\% and 97.0\%, respectively. These classifiers were combined with a hand location classifier for sign-level recognition, yielding an accuracy of 86.8\% on a 28 sign SEE vocabulary.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kong_Ranganath_2008_Signing Exact English (SEE).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/D2NMPWFF/S0031320307004566.html},
  journal = {Pattern Recognition},
  number = {5}
}

@phdthesis{kong_subject_2011,
  title = {Towards {{Subject Independent Sign Language Recognition}}: {{A Segment}}-{{Based Probabilistic Approach}}},
  author = {Kong, Wei Weon},
  year = {2011},
  address = {{Singapore}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kong_2011_Towards Subject Independent Sign Language Recognition.pdf},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=107;signer=8;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=8,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<500,vocab=107},
  school = {National University of Singapore}
}

@article{kong_subject_2014,
  title = {Towards {{Subject Independent Continuous Sign Language Recognition}}: {{A Segment}} and {{Merge Approach}}},
  shorttitle = {Towards Subject Independent Continuous Sign Language Recognition},
  author = {Kong, W. W. and Ranganath, Surendra},
  year = {2014},
  month = mar,
  volume = {47},
  pages = {1294--1308},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2013.09.014},
  abstract = {This paper presents a segment-based probabilistic approach to robustly recognize continuous sign language sentences. The recognition strategy is based on a two-layer conditional random field (CRF) model, where the lower layer processes the component channels and provides outputs to the upper layer for sign recognition. The continuously signed sentences are first segmented, and the sub-segments are labeled SIGN or ME (movement epenthesis) by a Bayesian network (BN) which fuses the outputs of independent CRF and support vector machine (SVM) classifiers. The sub-segments labeled as ME are discarded and the remaining SIGN sub-segments are merged and recognized by the two-layer CRF classifier; for this we have proposed a new algorithm based on the semi-Markov CRF decoding scheme. With eight signers, we obtained a recall rate of 95.7\% and a precision of 96.6\% for unseen samples from seen signers, and a recall rate of 86.6\% and a precision of 89.9\% for unseen signers.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kong_Ranganath_2014_Towards subject independent continuous sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/26HT4X9B/S0031320313003865.html},
  journal = {Pattern Recognition},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=107;signer=8;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=8,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape,vocab<500,vocab=107},
  number = {3}
}

@inproceedings{konstantinidis_deep_2018,
  title = {A {{Deep Learning Approach}} for {{Analyzing Video}} and {{Skeletal Features}} in {{Sign Language Recognition}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Imaging Systems}} and {{Techniques}} ({{IST}})},
  author = {Konstantinidis, D. and Dimitropoulos, K. and Daras, P.},
  year = {2018},
  month = oct,
  pages = {1--6},
  doi = {10.1109/IST.2018.8577085},
  abstract = {Sign language recognition (SLR) refers to the classification of signs with a specific meaning performed by the deaf and/or hearing-impaired people in their everyday communication. In this work, we propose a deep learning based framework, in which we examine and analyze the contribution of video (image and optical flow) and skeletal (body, hand and face) features in the challenging task of isolated SLR, in which each signed video corresponds to a single word. Moreover, we employ various fusion schemes in order to identify the optimal way to combine the information obtained from the various feature representations and propose a robust SLR methodology. Our experimentation on two sign language datasets and the comparison with state-of-the-art SLR methods reveals the superiority of optimally combining skeletal and video features for SLR tasks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Konstantinidis et al_2018_A Deep Learning Approach for Analyzing Video and Skeletal Features in Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FIURUQ6K/8577085.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YML9J6NQ/8577085.html},
  keywords = {authorgroup=CERTH,capturedBy=coloredGlove,capturedBy=vision,dataSet=LSA64,isolated,isolated;vocab=50;signer=1;sl=DGS;capturedBy=vision,isolated;vocab=64;signer=10;sl=ArgentinianSL;capturedBy=coloredGlove;dataSet=LSA64,LSA64,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,modal=bodyjoints,modal=fullframe,modal=fullframemotion,modal=head,paperinfo,paperinfo;authorgroup=CERTH;shorttitle=Skeletal Features,shorttitle=Skeletal Features,signer=1,signer=10,sl=ArgentinianSL,sl=DGS,slreco,slreco;modal=fullframe;modal=fullframemotion;modal=bodyjoints,vocab<100,vocab=50,vocab=64}
}

@inproceedings{konstantinidis_sign_2018,
  title = {Sign {{Language Recognition Based}} on {{Hand}} and {{Body Skeletal Data}}},
  booktitle = {2018 - {{3DTV}}-{{Conference}}: {{The True Vision}} - {{Capture}}, {{Transmission}} and {{Display}} of {{3D Video}} ({{3DTV}}-{{CON}})},
  author = {Konstantinidis, D. and Dimitropoulos, K. and Daras, P.},
  year = {2018},
  month = jun,
  pages = {1--4},
  doi = {10.1109/3DTV.2018.8478467},
  abstract = {Sign language recognition (SLR) is a challenging, but highly important research field for several computer vision systems that attempt to facilitate the communication among the deaf and hearing impaired people. In this work, we propose an accurate and robust deep learning-based methodology for sign language recognition from video sequences. Our novel method relies on hand and body skeletal features extracted from RGB videos and, therefore, it acquires highly discriminative for gesture recognition skeletal data without the need for any additional equipment, such as data gloves, that may restrict signer's movements. Experimentation on a large publicly available sign language dataset reveals the superiority of our methodology with respect to other state of the art approaches relying solely on RGB features.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Konstantinidis et al_2018_Sign Language Recognition Based on Hand and Body Skeletal Data.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UWCRB52K/8478467.html},
  keywords = {capturedBy=coloredGlove,dataSet=LSA64,isolated,isolated;vocab=64;signer=10;sl=ArgentinianSL;capturedBy=coloredGlove;dataSet=LSA64,LSA64,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,modal=bodyjoints,signer=10,sl=ArgentinianSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=2horientation;modal=bodyjoints,vocab<100,vocab=64}
}

@inproceedings{kontschieder_geof_2013,
  title = {{{GeoF}}: {{Geodesic Forests}} for {{Learning Coupled Predictors}}},
  shorttitle = {{{GeoF}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kontschieder, P. and Kohli, P. and Shotton, J. and Criminisi, A.},
  year = {2013},
  month = jun,
  pages = {65--72},
  doi = {10.1109/CVPR.2013.16},
  abstract = {Conventional decision forest based methods for image labelling tasks like object segmentation make predictions for each variable (pixel) independently [3, 5, 8]. This prevents them from enforcing dependencies between variables and translates into locally inconsistent pixel labellings. Random field models, instead, encourage spatial consistency of labels at increased computational expense. This paper presents a new and efficient forest based model that achieves spatially consistent semantic image segmentation by encoding variable dependencies directly in the feature space the forests operate on. Such correlations are captured via new long-range, soft connectivity features, computed via generalized geodesic distance transforms. Our model can be thought of as a generalization of the successful Semantic Texton Forest, Auto-Context, and Entangled Forest models. A second contribution is to show the connection between the typical Conditional Random Field (CRF) energy and the forest training objective. This analysis yields a new objective for training decision forests that encourages more accurate structured prediction. Our GeoF model is validated quantitatively on the task of semantic image segmentation, on four challenging and very diverse image datasets. GeoF outperforms both state of-the-art forest models and the conventional pair wise CRF.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kontschieder et al_2013_GeoF.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EQT8VPVM/abs_all.html}
}

@article{kosmidou_enhanced_2011,
  title = {Enhanced {{Sign Language Recognition Using Weighted Intrinsic}}-{{Mode Entropy}} and {{Signer}}'s {{Level}} of {{Deafness}}},
  author = {Kosmidou, V.E. and Petrantonakis, P.C. and Hadjileontiadis, L.J.},
  year = {2011},
  month = dec,
  volume = {41},
  pages = {1531--1543},
  issn = {1083-4419},
  doi = {10.1109/TSMCB.2011.2157141},
  abstract = {Sign language (SL) forms an important communication canal for the deaf. In this paper, enhanced SL recognition, by relating the individual way of signing with the signer's level of deafness (LoD) through a novel hybrid adaptive weighting (HAW) process applied to surface electromyogram and 3-D accelerometer data, is proposed. Using a LoD-driven genetic algorithm, HAW optimally weights the intrinsic modes of the acquired signals, preparing them for sample entropy (SampEn) estimation that follows. The resulting feature set, namely, weighted intrinsic-mode entropy (IMEn) (wIMEn), aims at increasing the SL-sign-classification accuracy alone or boosted by signer identification and/or signer's LoD-based group identification. The wIMEn was compared with three other feature sets, i.e., time frequency, SampEn, and IMEn, regarding their discrimination ability (both among signers and SL signs). Data from the dominant hand of nine subjects with various LoD were analyzed for the classification of 61 Greek SL (GSL) signs. Experimental results have shown that the introduced wIMEn feature set exhibited higher performance compared to others, both in signer identification and signer's LoD-based group identification and in GSL sign classification. The findings suggest that LoD could be considered in the construction of a signer-independent SL-classification system toward the enhancement of its performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kosmidou et al_2011_Enhanced Sign Language Recognition Using Weighted Intrinsic-Mode Entropy and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ATA5F8XZ/articleDetails.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {capturedBy=electronicGlove,capturedBy=emg,isolated,isolated;vocab=61;signer=9;sl=GSL;capturedBy=emg;capturedBy=electronicGlove,modal=movement,modal=shape,signer=9,sl=GSL,slreco,slreco;modal=shape;modal=movement,vocab<100,vocab=61},
  number = {6}
}

@inproceedings{koutras_estimation_2015,
  title = {Estimation of Eye Gaze Direction Angles Based on Active Appearance Models},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Koutras, P. and Maragos, P.},
  year = {2015},
  month = sep,
  pages = {2424--2428},
  address = {{Quebec, Canada}},
  doi = {10.1109/ICIP.2015.7351237},
  abstract = {In this paper we demonstrate efficient methods for continuous estimation of eye gaze angles with application to sign language videos. The difficulty of the task lies on the fact that those videos contain images with low face resolution since they are recorded from distance. First, we proceed to the modeling of face and eyes region by training and fitting Global and Local Active Appearance Models (LAAM). Next, we propose a system for eye gaze estimation based on a machine learning approach. In the first stage of our method, we classify gaze into discrete classes using GMMs that are based either on the parameters of the LAAM, or on HOG descriptors for the eyes region. We also propose a method for computing gaze direction angles from GMM log-likelihoods. We qualitatively and quantitatively evaluate our methods on two sign language databases and compare with a state of the art geometric model of the eye based on LAAM landmarks, which provides an estimate in direction angles. Finally, we further evaluate our framework by getting ground truth data from an eye tracking system Our proposed methods, and especially the GMMs using LAAM parameters, demonstrate high accuracy and robustness even in challenging tasks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Koutras_Maragos/Koutras_Maragos_2015_Estimation of eye gaze direction angles based on active appearance models.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Koutras_Maragos_2015_Estimation of eye gaze direction angles based on active appearance models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K2ZAIUS5/articleDetails.html}
}

@inproceedings{krafka_eye_2016,
  title = {Eye {{Tracking}} for {{Everyone}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Krafka, Kyle and Khosla, Aditya and Kellnhofer, Petr and Kannan, Harini and Bhandarkar, Suchendra and Matusik, Wojciech and Torralba, Antonio},
  year = {2016},
  pages = {2176--2184},
  address = {{Las Vegas, NV, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Krafka et al_2016_Eye Tracking for Everyone.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SCAWA26W/Krafka_Eye_Tracking_for_CVPR_2016_paper.html}
}

@inproceedings{krejov_combining_2015,
  title = {Combining Discriminative and Model Based Approaches for Hand Pose Estimation},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Krejov, Philip and Gilbert, Andrew and Bowden, Richard},
  year = {2015},
  pages = {1--7},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Krejov et al_2015_Combining discriminative and model based approaches for hand pose estimation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K8I5CIX3/login.html}
}

@article{krishnan_conditional_2015,
  title = {Conditional Distance Based Matching for One-Shot Gesture Recognition},
  author = {Krishnan, Ravikiran and Sarkar, Sudeep},
  year = {2015},
  volume = {48},
  pages = {1298--1310},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Krishnan_Sarkar_2015_Conditional distance based matching for one-shot gesture recognition.pdf},
  journal = {Pattern Recognition},
  number = {4}
}

@misc{kristoffersen-2016,
  title = {Ordbog over {{Dansk Tegnsprog}} 2008-2016},
  author = {Kristoffersen, Jette H. and Troelsg{\aa}rd, Thomas and Hardell, Anne Skov and Hardell, Bo and Niemel{\"a}, Janne Boye and Sandholt, J{\o}rgen and Toft, Maja},
  year = {2016},
  howpublished = {http://www.tegnsprog.dk/}
}

@inproceedings{krizhevsky_imagenet_2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  month = dec,
  pages = {1106--1114},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf}
}

@inproceedings{kroger_actionbased_2010,
  title = {An {{Action}}-{{Based Concept}} for the {{Phonetic Annotation}} of {{Sign Language Gestures}}},
  booktitle = {Elektronische {{Sprachsignalverarbeitung}}},
  author = {Kr{\"o}ger, Bernd J and Kannampuzha, Jim and Bauer, Dominik and Birkholz, Peter and Dreuw, Philippe and Ney, Hermann},
  year = {2010},
  month = sep,
  address = {{Berlin, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kröger et al_2010_An Action-Based Concept for the Phonetic Annotation of Sign Language Gestures.pdf}
}

@inproceedings{krupka_discriminative_2014,
  title = {Discriminative {{Ferns Ensemble}} for {{Hand Pose Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Krupka, E. and Vinnikov, A. and Klein, B. and Hillel, A.B. and Freedman, D. and Stachniak, S.},
  year = {2014},
  month = jun,
  pages = {3670--3677},
  doi = {10.1109/CVPR.2014.469},
  abstract = {We present the Discriminative Ferns Ensemble (DFE) classifier for efficient visual object recognition. The classifier architecture is designed to optimize both classification speed and accuracy when a large training set is available. Speed is obtained using simple binary features and direct indexing into a set of tables, and accuracy by using a large capacity model and careful discriminative optimization. The proposed framework is applied to the problem of hand pose recognition in depth and infra-red images, using a very large training set. Both the accuracy and the classification time obtained are considerably superior to relevant competing methods, allowing one to reach accuracy targets with run times orders of magnitude faster than the competition. We show empirically that using DFE, we can significantly reduce classification time by increasing training sample size for a fixed target accuracy. Finally a DFE result is shown for the MNIST dataset, showing the method's merit extends beyond depth images.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Krupka et al_2014_Discriminative Ferns Ensemble for Hand Pose Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N23J6PQP/abs_all.html}
}

@article{kuehne_hybrid_2018,
  title = {A {{Hybrid RNN}}-{{HMM Approach}} for {{Weakly Supervised Temporal Action Segmentation}}},
  author = {Kuehne, H. and Richard, A. and Gall, J.},
  year = {2018},
  pages = {1--1},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2018.2884469},
  abstract = {Action recognition has become a rapidly developing research field. But with the increasing demand for large-scale data, the need of hand-annotated data for training becomes more and more impractical. One way to avoid frame-based annotation is the use of action ordering information to learn the respective action classes. We propose a hierarchical approach to address the problem of weakly supervised learning from ordered action labels by structuring recognition in a coarse-to-fine manner. Given a set of videos and an ordered list of occurring actions, we infer the start and end of the related action classes and train the respective action classifiers without any need for hand labeled frame boundaries by combining a framewise RNN model with a coarse probabilistic inference. This combination allows for the temporal alignment of long sequences and thus, for an iterative training of both elements. While this system alone already generates good results, we show that the performance can be improved by approximating the number of subactions to the characteristics of different action classes as well as by the introduction of a regularizing length prior. The proposed system is evaluated on two benchmarks, the Breakfast and the Hollywood extended dataset, showing a competitive performance on temporal action segmentation and alignment.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kuehne et al_2018_A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CJ78NA8V/8585084.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@inproceedings{kumar_gesture_2015,
  title = {Gesture Recognition Using Active Body Parts and Active Difference Signatures},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Kumar, H. and Ptucha, R.},
  year = {2015},
  month = sep,
  pages = {2364--2368},
  doi = {10.1109/ICIP.2015.7351225},
  abstract = {The introduction of low cost depth cameras along with advances in computer vision have spawned an exciting new era in Human Computer Interaction. Real time gesture recognition systems have become commonplace and attention has now turned towards making these systems invariant to within-user and user-to-user variation. Active difference signatures have been used to describe temporal motion as well as static difference from a canonical resting position. Geometric features, such as joint angles, and joint topological distances can be used along with active difference signatures as salient feature descriptors. To achieve robustness to natural gesture variation, this paper introduces active body part recognition along with these features into the Hidden Markov Model framework. The proposed method is bench-marked against other methods, achieving state of the art results on the MSR3D and ChaLearn datasets.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kumar_Ptucha_2015_Gesture recognition using active body parts and active difference signatures.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PRTNA46V/abs_all.html}
}

@article{kumar_position_2018,
  title = {A Position and Rotation Invariant Framework for Sign Language Recognition ({{SLR}}) Using {{Kinect}}},
  author = {Kumar, Pradeep and Saini, Rajkumar and Roy, Partha Pratim and Dogra, Debi Prosad},
  year = {2018},
  month = apr,
  volume = {77},
  pages = {8823--8846},
  issn = {1573-7721},
  doi = {10.1007/s11042-017-4776-9},
  abstract = {Sign language is the only means of communication for speech and hearing impaired people. Using machine translation, Sign Language Recognition (SLR) systems provide medium of communication between speech and hearing impaired and others who have difficulty in understanding such languages. However, most of the SLR systems require the signer to sign in front of the capturing device/sensor. Such systems fail to recognize some gestures when the relative position of the signer is changed or when the body occlusion occurs due to position variations. In this paper, we present a robust position invariant SLR framework. A depth-sensor device (Kinect) has been used to obtain the signer's skeleton information. The framework is capable of recognizing occluded sign gestures and has been tested on a dataset of 2700 gestures. The recognition process has been performed using Hidden Markov Model (HMM) and the results show the efficiency of the proposed framework with an accuracy of 83.77\% on occluded gestures.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kumar et al_2018_A position and rotation invariant framework for sign language recognition (SLR).pdf},
  journal = {Multimedia Tools and Applications},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=30;signer=10;sl=IndianSL;capturedBy=vision;capturedBy=depth,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=10,sl=IndianSL,slreco,slreco;modal=2hmovement;modal=2hshape;modal=bodyjoints,vocab<50,vocab=30},
  language = {en},
  number = {7}
}

@article{kumar_s3drgf_2019,
  title = {{{S3DRGF}}: {{Spatial}} 3-{{D Relational Geometric Features}} for 3-{{D Sign Language Representation}} and {{Recognition}}},
  shorttitle = {{{S3DRGF}}},
  author = {Kumar, D. A. and Sastry, A. S. C. S. and Kishore, P. V. V. and Kumar, E. K. and Kumar, M. T. K.},
  year = {2019},
  month = jan,
  volume = {26},
  pages = {169--173},
  issn = {1070-9908},
  doi = {10.1109/LSP.2018.2883864},
  abstract = {Locations, angles, edges, and surfaces are spatial joint features that were predominantly used for characterizing three-dimensional (3-D) skeletal data in human action recognition. Despite their demonstrated success, features described earlier find difficulty in representing a relational change among joint movements in 3-D space for classifying human actions. To characterize a relation between joints on 3-D skeleton, we propose spatial 3-D relational geometric features (S3DRGFs). S3DRGFs are calculated on a subset of four joints in a chronological order covering all joints on the skeleton. Each of these four joints shape into a polygon, that reshapes spatially and temporally with respect to the sign (action) in the 3-D video. Consequently, we construct the spatio-temporal features (S3DRGF) by computing the area and perimeter of these polygons. Accordingly, query 3-D sign (action) recognition process transforms the joint area and perimeter features (JAF and JPF) into global alignment kernels based on the computed similarity scores with the dataset features. The similarity scores from JAF and JPF kernels are averaged for recognition. The proposed framework has been tested on our own 3-D sign language dataset (BVC3DSL) and three other publicly available datasets: HDM05, CMU, and NTU RGBD skeletal data. The results show higher levels of accuracy in decoding 3-D sign language into text for building a 3-D model based sign language translator.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kumar et al_2019_S3DRGF.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9QYDKJYT/8550661.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HKVK6IG2/8550661.html},
  journal = {IEEE Signal Processing Letters},
  keywords = {capturedBy=mocap,isolated,isolated;vocab=700;signer=10;sl=IndianSL;capturedBy=mocap,modal=bodyjoints,signer=10,sl=IndianSL,slreco,slreco;modal=bodyjoints,vocab<1000,vocab=700},
  number = {1}
}

@article{kumar_threedimensional_2018,
  title = {Three-{{Dimensional Sign Language Recognition With Angular Velocity Maps}} and {{Connived Feature ResNet}}},
  author = {Kumar, E. K. and Kishore, P. V. V. and Kumar, M. T. K. and Kumar, D. A. and Sastry, A. S. C. S.},
  year = {2018},
  month = dec,
  volume = {25},
  pages = {1860--1864},
  issn = {1070-9908},
  doi = {10.1109/LSP.2018.2877891},
  abstract = {The objective of this letter is to design a unique spatio-temporal feature map characterization for three-dimensional (3-D) sign (or action) data. Current maps characterize geometric features, such as joint distances and angles or both, which could not accurately model the relative joint variations in a 3-D sign (or action) location data. Therefore, we propose a new color-coded feature map called joint angular velocity maps to accurately model the 3-D joint motions. Instead of using traditional convolutional neural networks (CNNs), we propose to develop a new ResNet architecture called connived feature ResNet, which has a CNN layer in the feedforward loop of the densely connected standard ResNet architecture. We show that this architecture avoids using dropout in the last layers and achieves the desired goal in less number of iterations compared to other ResNet and CNN based architectures used for sign (action) classification. To test our proposed model, we use our own motion captured 3-D sign language data (BVC3DSL) and other publicly available skeletal action data: CMU, HDM05, and NTU RGBD.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kumar et al_2018_Three-Dimensional Sign Language Recognition With Angular Velocity Maps and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4IZJDFQD/8506429.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9PT2PHSY/8506429.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KQLEE8BK/8506429.html},
  journal = {IEEE Signal Processing Letters},
  keywords = {capturedBy=mocap,isolated,isolated;vocab=500;signer=10;sl=IndianSL;capturedBy=mocap,modal=bodyjoints,signer=10,sl=IndianSL,slreco,slreco;modal=bodyjoints,vocab<1000,vocab=500},
  number = {12}
}

@inproceedings{kumar_time_2018,
  title = {Time {{Series Neural Networks}} for {{Real Time Sign Language Translation}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Kumar, S. S. and Wangyal, T. and Saboo, V. and Srinath, R.},
  year = {2018},
  month = dec,
  pages = {243--248},
  doi = {10.1109/ICMLA.2018.00043},
  abstract = {Sign language is the primary mode of communication for the hearing and speech impaired and there is a need for systems to translate sign languages to spoken languages. Prior research has been focused on providing glove based solutions which are intrusive and expensive. We propose a sign language translation system based solely on visual cues and deep learning for accurate translation. Our system applies Computer Vision and Neural Machine Translation for American Sign Language (ASL) gloss recognition and translation respectively. In this paper, we show that an end to end neural network system is not only capable of recognition of individual ASL glosses but also translation of continuous sign language videos into complete English sentences, making it an effective and practical tool for sign language communication.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kumar et al_2018_Time Series Neural Networks for Real Time Sign Language Translation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DQIU4DFK/8614068.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=51;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<100,vocab=51}
}

@article{kumar_training_2018,
  title = {Training {{CNNs}} for 3-{{D Sign Language Recognition With Color Texture Coded Joint Angular Displacement Maps}}},
  author = {Kumar, E. K. and Kishore, P. V. V. and Sastry, A. S. C. S. and Kumar, M. T. K. and Kumar, D. A.},
  year = {2018},
  month = may,
  volume = {25},
  pages = {645--649},
  issn = {1070-9908},
  doi = {10.1109/LSP.2018.2817179},
  abstract = {Convolutional neural networks (CNNs) can be remarkably effective for recognizing two-dimensional and three-dimensional (3-D) actions. To further explore the potential of CNNs, we applied them in the recognition of 3-D motion-captured sign language (SL). The sign's 3-D spatio-temporal information of each sign was interpreted using joint angular displacement maps (JADMs), which encode the sign as a color texture image; JADMs were calculated for all joint pairs. Multiple CNN layers then capitalized on the differences between these images and identify discriminative spatio-temporal features. We then compared the performance of our proposed model against those of the state-of-the-art baseline models by using our own 3-D SL dataset and two other benchmark action datasets, namely, HDM05 and CMU.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kumar et al_2018_Training CNNs for 3-D Sign Language Recognition With Color Texture Coded Joint.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2KRYT84J/8319435.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3V8RV7RU/8319435.html},
  journal = {IEEE Signal Processing Letters},
  keywords = {capturedBy=mocap,isolated,isolated;vocab=200;signer=10;sl=IndianSL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,modal=bodyjoints,modal=head,signer=10,sl=IndianSL,slreco,slreco;modal=bodyjoints,vocab<500,vocab=200},
  number = {5}
}

@article{kung_dual_2015,
  title = {Dual {{Subspace Nonnegative Graph Embedding}} for {{Identity}}-{{Independent Expression Recognition}}},
  author = {Kung, H. and Tu, Y. and Hsu, C.},
  year = {2015},
  month = mar,
  volume = {10},
  pages = {626--639},
  issn = {1556-6013},
  doi = {10.1109/TIFS.2015.2390138},
  abstract = {Facial expression is one of the intricate biometric traits, where different persons exhibit various appearance changes when posing the same expression. Because facial cues involved in the recognition of facial expression are not fully separate from that of facial identity, this identity-dependent behavior often complicates automatic facial expression recognition. In this paper, to address the identity-independent expression recognition problem, we propose a dual subspace nonnegative graph embedding (DSNGE) to represent expressive images using two subspaces: 1) identity subspace and 2) expression subspace. The identity subspace characterizes identity-dependent appearance variations; whereas the expression subspace characterizes identity-independent expression variations. With DSNGE, we propose to decompose each facial image into an identity part and an expression part represented by their corresponding nonnegative bases. We also address the intra-class variation issue in the expression recognition problem, and further devise a graph-embedding constraint on the expression subspace to tackle this problem. Our experimental results show that the proposed DSNGE outperforms other graph-based nonnegative factorization methods and existing expression recognition methods on CK+, JAFFE, and TFEID databases.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kung et al_2015_Dual Subspace Nonnegative Graph Embedding for Identity-Independent Expression.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5D9AKZT9/articleDetails.html},
  journal = {IEEE Transactions on Information Forensics and Security},
  number = {3}
}

@article{kunz_automatische_2013,
  title = {Automatische {{Wortalignierung}} Zwischen {{Deutschschweizerischer Geb\"ardensprache}} Und {{Deutsch}}},
  author = {Kunz, Beat},
  year = {2013},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kunz_2013_Automatische Wortalignierung zwischen Deutschschweizerischer Gebärdensprache.pdf}
}

@article{kutscher_ikonizitat_2010,
  title = {Ikonizit\"at Und {{Indexikalit\"at}} Im Geb\"ardensprachlichen {{Lexikon}} \textendash{} {{Zur Typologie}} Sprachlicher {{Zeichen}}},
  author = {Kutscher, Silvia},
  year = {2010},
  month = jan,
  volume = {29},
  issn = {0721-9067, 1613-3706},
  doi = {10.1515/zfsw.2010.003},
  journal = {Zeitschrift f\"ur Sprachwissenschaft},
  number = {1}
}

@inproceedings{kuznetsova_realtime_2013,
  title = {Real-{{Time Sign Language Recognition Using}} a {{Consumer Depth Camera}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision Workshops}}},
  author = {Kuznetsova, Alina and {Leal-Taixe}, Laura and Rosenhahn, Bodo},
  year = {2013},
  month = apr,
  pages = {83--90},
  address = {{Sydney, Australia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kuznetsova et al_2013_Real-Time Sign Language Recognition Using a Consumer Depth Camera.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RSZHUWT8/Kuznetsova_Real-Time_Sign_Language_2013_ICCV_paper.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=24;signer=3;sl=ASL;capturedBy=vision;capturedBy=depth,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=shape,vocab<50,vocab=24}
}

@incollection{kyle_research_2015,
  title = {Research {{Methods}} in {{Studying Reading}} and {{Literacy Development}} in {{Deaf Children Who Sign}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Kyle, Fiona E.},
  year = {2015},
  pages = {300--318},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch16},
  abstract = {This chapter discusses research concerning deaf individuals' reading and spelling development, with a focus on collecting literacy data from deaf children and adolescents. It presents an overview of what one knows about reading development in deaf individuals, covering topics such as: What are the reading achievements of deaf children? How do deaf children learn to read? Do they learn to read in a similar way to hearing children? The chapter describes different types of methodologies that have been used to conduct literacy research with deaf children, highlighting important findings. It describes some of the issues involved in researching literacy development in deaf children who sign and potential ways of dealing with these issues. One of the most important issues that researchers need to be aware of when investigating literacy with signing Deaf individuals is that they are assessing reading skills in a language that is not the participant's first language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kyle_2015_Research Methods in Studying Reading and Literacy Development in Deaf Children.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DP9DQ8AD/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{kyriazis_scalable_2014,
  title = {Scalable {{3D Tracking}} of {{Multiple Interacting Objects}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kyriazis, Nikolaos and Argyros, Antonis A},
  year = {2014},
  month = jun,
  pages = {3430--3437},
  publisher = {{IEEE}},
  address = {{Columbus, Ohio, USA}},
  doi = {10.1109/CVPR.2014.438},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Kyriazis_Argyros_2014_Scalable 3D Tracking of Multiple Interacting Objects.pdf}
}

@article{lamel_lightly_2002,
  title = {Lightly {{Supervised}} and {{Unsupervised Acoustic Model Training}}},
  author = {Lamel, Lori and Gauvain, Jean-luc and Adda, Gilles},
  year = {2002},
  volume = {16},
  pages = {115--129},
  journal = {Computer Speech and Language},
  number = {1}
}

@inproceedings{lan_comparing_2009,
  title = {Comparing {{Visual Features}} for {{Lipreading}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Auditory}}-{{Visual Speech Processing}}},
  author = {Lan, Yuxuan and Harvey, Richard and Theobald, Barry-John and Ong, Eng-Jon and Bowden, Richard},
  year = {2009},
  pages = {102--106},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lan et al_2009_Comparing Visual Features for Lipreading.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WGKNF25Z/Lan et al. - 2009 - Comparing visual features for lipreading.html}
}

@inproceedings{lan_improving_2010,
  title = {Improving {{Visual Features}} for {{Lip}}-{{Reading}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Auditory}}-{{Visual Speech Processing}}},
  author = {Lan, Yuxuan and Theobald, Barry-John and Harvey, Richard and Ong, Eng-Jon and Bowden, Richard},
  year = {2010},
  volume = {201},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lan et al_2010_Improving Visual Features for Lip-Reading.pdf}
}

@inproceedings{lan_insights_2012,
  title = {Insights into {{Machine Lip Reading}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lan, Yuxuan and Harvey, Richard and Theobald, Barry-John},
  year = {2012},
  month = mar,
  pages = {4825--4828},
  doi = {10.1109/ICASSP.2012.6288999},
  abstract = {Computer lip-reading is one of the great signal processing challenges. Not only is the signal noisy, it is variable. However it is almost unknown to compare the performance with human lip-readers. Partly this is because of the paucity of human lip-readers and partly because most automatic systems only handle data that are trivial and therefore not representative of human speech. Here we generate a multiview dataset using connected words that can be analysed by an automatic system, based on linear predictive trackers and active appearance models, and human lip-readers. The automatic system we devise has a viseme accuracy of {$\approx$} 46\% which is comparable to poor professional human lip-readers. However, unlike human lip-readers our system is good at guessing its fallibility.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lan et al_2012_Insights into Machine Lip Reading.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R3SM663E/abs_all.html}
}

@inproceedings{lan_view_2012,
  title = {View {{Independent Computer Lip}}-{{Reading}}},
  booktitle = {{{IEEE Int}}. {{Conf}}. on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Lan, Yuxuan and Theobald, B.-J. and Harvey, R.},
  year = {2012},
  month = jul,
  pages = {432--437},
  doi = {10.1109/ICME.2012.192},
  abstract = {Computer lip-reading systems are usually designed to work using a full-frontal view of the face. However, many human experts tend to prefer to lip-read using an angled view. In this paper we consider issues related to the best viewing angle for an automated lip-reading system. In particular, we seek answers to the following questions: (1) Do computers lip-read better using a frontal or a non-frontal view of the face? (2) What is the best viewing angle for a computer lip-reading system? (3) How can a computer lip-reading system be made to work independently of viewing angle? We investigate these issues using a purpose built audio-visual dataset that contains simultaneous recordings of a speaker reciting continuous speech at five angles. We find that the system performs best on a non-frontal view, perhaps because lip gestures, such as lip-protrusion and lip-rounding, are more pronounced when viewing from an angle. We also describe a simple linear mapping that allows us to map any view of the face to the view that we find to be optimal. Hence we present a view-independent lip-reading system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2012/Lan et al/Lan et al_2012_View Independent Computer Lip-Reading.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lan et al_2012_View Independent Computer Lip-Reading.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7VCSZMSX/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MD5JIQEJ/login.html}
}

@article{landis_measurement_1977,
  title = {The {{Measurement}} of {{Observer Agreement}} for {{Categorical Data}}},
  author = {Landis, J. Richard and Koch, Gary G.},
  year = {1977},
  volume = {33},
  pages = {159--174},
  journal = {Biometrics},
  number = {1}
}

@inproceedings{lang_sign_2012,
  title = {Sign {{Language Recognition Using Kinect}}},
  booktitle = {Artificial {{Intelligence}} and {{Soft Computing}}},
  author = {Lang, Simon and Block, Marco and Rojas, Ra{\'u}l},
  year = {2012},
  month = apr,
  pages = {394--402},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-29347-4_46},
  abstract = {An open source framework for general gesture recognition is presented and tested with isolated signs of sign language. Other than common systems for sign language recognition, this framework makes use of Kinect, a depth camera which makes real-time 3D-reconstruction easily applicable. Recognition is done using hidden Markov models with a continuous observation density. The framework also offers an easy way of initializing and training new gestures or signs by performing them several times in front of the camera. First results with a recognition rate of 97\% show that depth cameras are well-suited for sign language recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lang et al_2012_Sign Language Recognition Using Kinect.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IBXMSRBC/978-3-642-29347-4_46.html},
  isbn = {978-3-642-29346-7 978-3-642-29347-4},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=25;sl=DGS;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,sl=DGS,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=25},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{langer_compiling_,
  title = {Compiling a {{Basic Vocabulary}} for {{German Sign Language}} ({{DGS}})\textendash Lexicographic Issues with a Focus on Word Senses},
  author = {Langer, Gabriele and K{\"o}nig, Susanne and Matthes, Silke},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Langer et al_Compiling a Basic Vocabulary for German Sign Language (DGS)–lexicographic.pdf}
}

@article{lanitis_automatic_1997,
  title = {Automatic Interpretation and Coding of Face Images Using Flexible Models},
  author = {Lanitis, A. and Taylor, C.J. and Cootes, T.F.},
  year = {1997},
  month = jul,
  volume = {19},
  pages = {743--756},
  issn = {0162-8828},
  doi = {10.1109/34.598231},
  abstract = {Face images are difficult to interpret because they are highly variable. Sources of variability include individual appearance, 3D pose, facial expression, and lighting. We describe a compact parametrized model of facial appearance which takes into account all these sources of variability. The model represents both shape and gray-level appearance, and is created by performing a statistical analysis over a training set of face images. A robust multiresolution search algorithm is used to fit the model to faces in new images. This allows the main facial features to be located, and a set of shape, and gray-level appearance parameters to be recovered. A good approximation to a given face can be reconstructed using less than 100 of these parameters. This representation can be used for tasks such as image coding, person identification, 3D pose recovery, gender recognition, and expression recognition. Experimental results are presented for a database of 690 face images obtained under widely varying conditions of 3D pose, lighting, and facial expression. The system performs well on all the tasks listed above},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lanitis et al_1997_Automatic interpretation and coding of face images using flexible models.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {7}
}

@inproceedings{laptev_learning_2008,
  title = {Learning Realistic Human Actions from Movies},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Laptev, I. and Marszalek, M. and Schmid, C. and Rozenfeld, B.},
  year = {2008},
  month = jun,
  pages = {1--8},
  doi = {10.1109/CVPR.2008.4587756},
  abstract = {The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8\% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Laptev et al_2008_Learning realistic human actions from movies.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FVXPXIGH/4587756.html}
}

@article{larin_optical_2012,
  title = {Optical Clearing for {{OCT}} Image Enhancement and In-Depth Monitoring of Molecular Diffusion},
  author = {Larin, Kirill V. and Ghosn, Mohamad G. and Bashkatov, Alexey N. and Genina, Elina A. and Trunina, Natalia A. and Tuchin, Valery V.},
  year = {2012},
  volume = {18},
  pages = {1244--1259},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Larin et al_2012_Optical clearing for OCT image enhancement and in-depth monitoring of molecular.pdf},
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  number = {3}
}

@article{latif_arasl_2019,
  title = {{{ArASL}}: {{Arabic Alphabets Sign Language Dataset}}},
  shorttitle = {{{ArASL}}},
  author = {Latif, Ghazanfar and Mohammad, Nazeeruddin and Alghazo, Jaafar and AlKhalaf, Roaa and AlKhalaf, Rawan},
  year = {2019},
  month = apr,
  volume = {23},
  pages = {103777},
  issn = {2352-3409},
  doi = {10.1016/j.dib.2019.103777},
  abstract = {A fully-labelled dataset of Arabic Sign Language (ArSL) images is developed for research related to sign language recognition. The dataset will provide researcher the opportunity to investigate and develop automated systems for the deaf and hard of hearing people using machine learning, computer vision and deep learning algorithms. The contribution is a large fully-labelled dataset for Arabic Sign Language (ArSL) which is made publically available and free for all researchers. The dataset which is named ArSL2018 consists of 54,049 images for the 32 Arabic sign language sign and alphabets collected from 40 participants in different age groups. Different dimensions and different variations were present in images which can be cleared using pre-processing techniques to remove noise, center the image, etc. The dataset is made available publicly at https://data.mendeley.com/datasets/y7pckrw6z2/1.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Latif et al_2019_ArASL.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9UH6SD6C/S2352340919301283.html},
  journal = {Data in Brief},
  keywords = {capturedBy=vision,isolated,isolated;vocab=32;signer=40;sl=ArSL;capturedBy=vision,signer=40,sl=ArSL,vocab<50,vocab=32}
}

@inproceedings{lazebnik_bags_2006,
  title = {Beyond Bags of Features: {{Spatial}} Pyramid Matching for Recognizing Natural Scene Categories},
  shorttitle = {Beyond Bags of Features},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lazebnik, Svetlana and Schmid, Cordelia and Ponce, Jean},
  year = {2006},
  volume = {2},
  pages = {2169--2178},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lazebnik et al_2006_Beyond bags of features.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9MEVNUAF/login.html}
}

@incollection{le_neural_2015,
  title = {Neural {{Networks}} with {{Hidden Markov Models}} in {{Skeleton}}-{{Based Gesture Recognition}}},
  booktitle = {Knowledge and {{Systems Engineering}}},
  author = {Le, Hai-Son and Pham, Ngoc-Quan and Nguyen, Duc-Dung},
  editor = {Nguyen, Viet-Ha and Le, Anh-Cuong and Huynh, Van-Nam},
  year = {2015},
  pages = {299--311},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-11680-8_24},
  abstract = {In Gesture Recognition (GR) tasks, a system with a traditional use of Hidden Markov Models (HMMs) usually serves as a baseline. Their performance is often not so good and therefore somehow overlooked. However, in recent years, especially in Automatic Speech Recognition (ASR), there are advanced methods proposed for this type of model which have been shown to improve significantly recognition results. Among them, the use of Neural Networks (NNs) instead of Gaussian Mixture Models (GMMs) for estimating emission probabilities of HMMs has been considered as one of biggest advances [1,2,3]. This fact implies that the performance of HMM-based models on GR need to be revised. For this reason, in this study, we show that by carefully tailoring NNs to a traditional HMM-based GR system, we can improve significantly the performance, hence, achieving very competitive results on a skeleton-based GR task which is defined by using Microsoft Research Cambridge 12 (MSRC-12) data [4]. It should be pointed out that, it is straightforward to apply our proposed techniques to more complicated GR tasks such as Sign Language Recognition [5], where basically a sequence of sign gestures need to be transcribed.},
  copyright = {\textcopyright 2015 Springer International Publishing Switzerland},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Le et al_2015_Neural Networks with Hidden Markov Models in Skeleton-Based Gesture Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AT6M8TTD/978-3-319-11680-8_24.html},
  isbn = {978-3-319-11679-2 978-3-319-11680-8},
  language = {en},
  number = {326},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}}
}

@inproceedings{lea_temporal_2017,
  title = {Temporal {{Convolutional Networks}} for {{Action Segmentation}} and {{Detection}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lea, Colin and Flynn, Michael D. and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
  year = {2017},
  month = jul,
  pages = {156--165},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/E6ZMKFSB/Lea_Temporal_Convolutional_Networks_CVPR_2017_paper.html}
}

@article{lecun_gradientbased_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  year = {1998},
  volume = {86},
  pages = {2278--2324},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/LeCun et al_1998_Gradient-based learning applied to document recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TP2FD7BM/abs_all.html},
  journal = {Proceedings of the IEEE},
  number = {11}
}

@inproceedings{lee_avicar_2004,
  title = {{{AVICAR}}: {{Audio}}-{{Visual Speech Corpus}} in a {{Car Environment}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Lee, Bowon and {Hasegawa-Johnson}, Mark and Goudeseune, Camille and Kamdar, Suketu and Borys, Sarah and Liu, Ming and Huang, Thomas},
  year = {2004},
  month = oct,
  pages = {2489--2492},
  address = {{Jeju Island, Korea}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lee et al_2004_AVICAR.pdf}
}

@article{lee_hmmbased_1999,
  title = {An {{HMM}}-Based Threshold Model Approach for Gesture Recognition},
  author = {Lee, Hyeon-Kyu and Kim, J. H},
  year = {1999},
  month = oct,
  volume = {21},
  pages = {961--973},
  issn = {0162-8828},
  doi = {10.1109/34.799904},
  abstract = {A new method is developed using the hidden Markov model (HMM) based technique. To handle nongesture patterns, we introduce the concept of a threshold model that calculates the likelihood threshold of an input pattern and provides a confirmation mechanism for the provisionally matched gesture patterns. The threshold model is a weak model for all trained gestures in the sense that its likelihood is smaller than that of the dedicated gesture model for a given gesture. Consequently, the likelihood can be used as an adaptive threshold for selecting proper gesture model. It has, however, a large number of states and needs to be reduced because the threshold model is constructed by collecting the states of all gesture models in the system. To overcome this problem, the states with similar probability distributions are merged, utilizing the relative entropy measure. Experimental results show that the proposed method can successfully extract trained gestures from continuous hand motion with 93.14\% reliability},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lee_Kim_1999_An HMM-based threshold model approach for gesture recognition.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {English},
  number = {10}
}

@inproceedings{lee_realtime_1997,
  title = {Real-Time Recognition System of {{Korean}} Sign Language Based on Elementary Components},
  booktitle = {Proc. {{Int}}. {{Fuzzy Systems Conference}}},
  author = {Lee, Chan-Su and Bien, Zeungnam and Park, Gyu-Tae and Jang, Won and Kim, Jong-Sung and Kim, Sung-Kwon},
  year = {1997},
  month = jul,
  volume = {3},
  pages = {1463--1468},
  doi = {10.1109/FUZZY.1997.619759},
  abstract = {Sign language is a method of communication for deaf persons. In communication using hand gesture, sign words and manual alphabets are used together. In this paper a system is proposed, which recognizes Korean sign Language (KSL). KSL is composed of Korean sign words and Korean manual alphabets continuously. To recognize meanings of continuous gestures which have no token of beginning and end, this system segments current motion states using speed and change of speed in motions and state automata. To understand the meaning of a gesture, basic component classifiers using fuzzy min-max neural network and fuzzy logic are used. Basic elements of meaning used in this system are 14 hand directions, 23 hand postures, and 14 hand orientations. Meaning of signed gesture is interpreted using basic elements which were recognized by 3 classifiers. This system recognizes 31 Korean manual alphabets and 131 Korean signs in real-time with recognition rate 96.7\% for Korean manual alphabets and 94.3\% for Korean sign words, excluding no recognition case},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lee et al_1997_Real-time recognition system of Korean sign language based on elementary.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PXZ8CS9F/619759.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=131;sl=KSL;capturedBy=electronicGlove;capturedBy=mocap,modal=movement,modal=orientation,modal=shape,sl=KSL,slreco,slreco;modal=movement;modal=orientation;modal=shape,vocab<500,vocab=131}
}

@article{lee_smart_2018,
  title = {Smart {{Wearable Hand Device}} for {{Sign Language Interpretation System With Sensors Fusion}}},
  author = {Lee, B. G. and Lee, S. M.},
  year = {2018},
  month = feb,
  volume = {18},
  pages = {1224--1232},
  issn = {1530-437X},
  doi = {10.1109/JSEN.2017.2779466},
  abstract = {Gesturing is an instinctive way of communicating to present a specific meaning or intent. Therefore, research into sign language interpretation using gestures has been explored progressively during recent decades to serve as an auxiliary tool for deaf and mute people to blend into society without barriers. In this paper, a smart sign language interpretation system using a wearable hand device is proposed to meet this purpose. This wearable system utilizes five flex-sensors, two pressure sensors, and a three-axis inertial motion sensor to distinguish the characters in the American sign language alphabet. The entire system mainly consists of three modules: 1) a wearable device with a sensor module; 2) a processing module; and 3) a display unit mobile application module. Sensor data are collected and analyzed using a built-in embedded support vector machine classifier. Subsequently, the recognized alphabet is further transmitted to a mobile device through Bluetooth low energy wireless communication. An Android-based mobile application was developed with a text-to-speech function that converts the received textinto audible voice output. Experiment results indicate that a true sign language recognition accuracy rate of 65.7\% can be achieved on average in the first version without pressure sensors. A second version of the proposed wearable system with the fusion of pressure sensors on the middle finger increased the recognition accuracy rate dramatically to 98.2\%. The proposed wearable system outperforms the existing method, for instance, although background lights, and other factors are crucial to a vision-based processing method, they are not for the proposed system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lee_Lee_2018_Smart Wearable Hand Device for Sign Language Interpretation System With Sensors.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C7NHA9FC/8126796.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QJR92ATC/8126796.html},
  journal = {IEEE Sensors Journal},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=28;sl=ASL;capturedBy=electronicGlove,modal=orientation,modal=shape,sl=ASL,slreco,slreco;modal=shape;modal=orientation,vocab<50,vocab=28},
  number = {3}
}

@incollection{lee_viseme_2002,
  title = {Viseme {{Recognition Experiment Using Context Dependent Hidden Markov Models}}},
  booktitle = {Intelligent {{Data Engineering}} and {{Automated Learning}} \textemdash{} {{IDEAL}} 2002},
  author = {Lee, Soonkyu and Yook, Dongsuk},
  editor = {Yin, Hujun and Allinson, Nigel and Freeman, Richard and Keane, John and Hubbard, Simon},
  year = {2002},
  month = jan,
  pages = {557--561},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Visual images synchronized with audio signals can provide user-friendly interface for man machine interactions. The visual speech can be represented as a sequence of visemes, which are the generic face images corresponding to particular sounds. We use HMMs (hidden Markov models) to convert audio signals to a sequence of visemes. In this paper, we compare two approaches in using HMMs. In the first approach, an HMM is trained for each triviseme which is a viseme with its left and right context, and the audio signals are directly recognized as a sequence of trivisemes. In the second approach, each triphone is modeled with an HMM, and a general triphone recognizer is used to produce a triphone sequence from the audio signals. The triviseme or triphone sequence is then converted to a viseme sequence. The performances of the two viseme recognition systems are evaluated on the TIMIT speech corpus.},
  copyright = {\textcopyright 2002 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lee_Yook_2002_Viseme Recognition Experiment Using Context Dependent Hidden Markov Models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SW9XHAWF/3-540-45675-9_84.html},
  isbn = {978-3-540-44025-3 978-3-540-45675-9},
  language = {en},
  number = {2412},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{lefebvre_blstmrnn_2013,
  title = {{{BLSTM}}-{{RNN}} Based {{3D}} Gesture Classification},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Artificial Neural Networks}} ({{ICANN}})},
  author = {Lefebvre, Gr{\'e}goire and Berlemont, Samuel and Mamalet, Franck and Garcia, Christophe},
  year = {2013},
  pages = {381--388},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lefebvre et al_2013_BLSTM-RNN based 3D gesture classification.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/INK57WWZ/978-3-642-40728-4_48.html}
}

@incollection{lefebvre_inertial_2015,
  title = {Inertial {{Gesture Recognition}} with {{BLSTM}}-{{RNN}}},
  booktitle = {Artificial {{Neural Networks}}},
  author = {Lefebvre, Gr{\'e}goire and Berlemont, Samuel and Mamalet, Franck and Garcia, Christophe},
  year = {2015},
  pages = {393--410},
  publisher = {{Springer}}
}

@inproceedings{lefebvre-albaret_overview_2013,
  title = {Overview of the {{Sign3D Project High}}-Fidelity {{3D}} Recording, Indexing and Editing of {{French Sign Language}} Content},
  booktitle = {Third {{International Symposium}} on {{Sign Language Translation}} and {{Avatar Technology}} ({{SLTAT}}) 2013},
  author = {{Lefebvre-Albaret}, Fran{\c c}ois and Gibet, Sylvie and Turki, Ahmed and Hamon, Ludovic and Brun, R{\'e}mi},
  year = {2013},
  month = oct,
  address = {{Chicago, United States}},
  abstract = {The Sign3D project aims at creating a range of innovative tools to allow the recording and the processing of motion captured French Sign Language (LSF) content. The challenge is to design a complete workflow from the movement capture (including all upper body part articulations, facial expression and gaze direction) to their restitution using a 3D virtual signer. We present the main innovation challenges at each step of the project. As accessibility for Deaf people through Sign Language is one goal of this project, a project overview of the project in SL is accessible at the following address : http://sign3d.websourd.org/sltat},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lefebvre-Albaret et al_2013_Overview of the Sign3D Project High-fidelity 3D recording, indexing and editing.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4K4AM4WQ/hal-00914661.html}
}

@phdthesis{lehnen2017:phd,
  title = {Maximum {{Entropy Models}} for {{Sequences}}: {{Scaling}} up from {{Tagging}} to {{Translation}}},
  author = {Lehnen, Patrick},
  year = {2017},
  month = may,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lehnen_2017_Maximum Entropy Models for Sequences.pdf},
  school = {RWTH Aachen University}
}

@article{lesner_visual_1981,
  title = {Visual Vowel and Diphthong Perception across Speakers},
  author = {Lesner, Sharon A. and Kricos, Patricia B.},
  year = {1981},
  volume = {14},
  pages = {252--258},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lesner_Kricos_1981_Visual vowel and diphthong perception across speakers.pdf},
  journal = {Journal of the Academy of Rehabilitative Audiology}
}

@inproceedings{levenshtein_binary_1966,
  title = {Binary {{Codes Capable}} of {{Correcting Deletions}}, {{Insertions}}, and {{Reversals}}},
  booktitle = {Soviet {{Physics}}-{{Doklady}}},
  author = {Levenshtein, V.},
  year = {1966},
  volume = {10},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Levenshtein_1966_Binary Codes Capable of Correcting Deletions, Insertions, and Reversals.pdf}
}

@article{levinson_introduction_1983,
  title = {An Introduction to the Application of the Theory of Probabilistic Functions of a {{Markov}} Process to Automatic Speech Recognition},
  author = {Levinson, S. E. and Rabiner, L. R. and Sondhi, M. M.},
  year = {1983},
  month = apr,
  volume = {62},
  pages = {1035--1074},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1983.tb03114.x},
  abstract = {In this paper we present several of the salient theoretical and practical issues associated with modeling a speech signal as a probabilistic function of a (hidden) Markov chain. First we give a concise review of the literature with emphasis on the Baum-Welch algorithm. This is followed by a detailed discussion of three issues not treated in the literature: alternatives to the Baum-Welch algorithm; critical facets of the implementation of the algorithms, with emphasis on their numerical properties; and behavior of Markov models on certain artificial but realistic problems. Special attention is given to a particular class of Markov models, which we call ``left-to-right'' models. This class of models is especially appropriate for isolated word recognition. The results of the application of these methods to an isolated word, speaker-independent speech recognition experiment are given in a companion paper.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Levinson et al_1983_An introduction to the application of the theory of probabilistic functions of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/S9JZSGYZ/6768244.html},
  journal = {The Bell System Technical Journal},
  number = {4}
}

@article{lewin_mouth_2011,
  title = {Mouth Gestures in {{British Sign Language}}: {{A}} Case Study of Tongue Protrusion in {{BSL}} Narratives},
  shorttitle = {Mouth Gestures in {{British Sign Language}}},
  author = {Lewin, Donna and Schembri, Adam C.},
  year = {2011},
  month = jan,
  volume = {14},
  pages = {94--114},
  issn = {1387-9316, 1569-996X},
  doi = {10.1075/sll.14.1.06lew},
  abstract = {This article investigates the claim that tongue protrusion (`th') acts as a nonmanual adverbial morpheme in British Sign Language (BSL) (Brennan 1992; Sutton-Spence \&amp; Woll 1999) drawing on narrative data produced by two deaf native signers as part of the European Cultural Heritage Online (ECHO) corpus. Data from ten BSL narratives have been analysed to observe the frequency and form of tongue protrusion. The results from this preliminary investigation indicate tongue protrusion occurs as part of the phonological formation of lexical signs (i.e., `echo phonology', see Woll 2001), as well as a separate meaningful unit that co-occurs (sometimes as part of constructed action) with classifier constructions and lexical verb signs. In the latter cases, the results suggest `th' sometimes appears to function as an adverbial morpheme in BSL, but with a greater variety of meanings than previously suggested in the BSL literature. One use of the adverbial appears similar to a nonmanual signal in American Sign Language described by Liddell (1980), although the form of the mouth gesture in our BSL data differs from what is reported in Liddell's work. Thus, these findings suggest the mouth gesture `th' in BSL has a broad range of functions. Some uses of tongue protrusion, however, remain difficult to categorise and further research with a larger dataset is needed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schembri_2011_Mouth gestures in British Sign Language (BSL).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KHBPHILX/sll.14.1.html},
  journal = {Sign Language \& Linguistics},
  language = {en},
  number = {1}
}

@book{lewis_ethnologue_2009,
  title = {Ethnologue: {{Languages}} of the {{World}}, 16th {{Edition}}},
  shorttitle = {Ethnologue},
  author = {Lewis, M. P.},
  year = {2009},
  month = may,
  edition = {Sixteenth},
  publisher = {{SIL International}},
  isbn = {1-55671-216-2}
}

@inproceedings{li_acoustic_2019,
  title = {Acoustic and {{Lexical Sentiment Analysis}} for {{Customer Service Calls}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Li, B. and Dimitriadis, D. and Stolcke, A.},
  year = {2019},
  month = may,
  pages = {5876--5880},
  doi = {10.1109/ICASSP.2019.8683679},
  abstract = {We describe the development of a sentiment analysis system for customer service calls, starting with the data acquisition and labeling, and proceeding to the algorithmic information extraction and modeling process from both spoken words and their acoustic expression. The proposed system is based on the combination of multiple acoustic and lexical models in a late fusion approach. Acoustic aspects of sentiment are captured by utterance-level features based on aggregated openSMILE and raw cepstral features, and further augmented with an energy contour model. Lexical aspects are captured by back-off n-gram language models. These models are found to combine effectively, showing different strengths as pertains to positive and negative sentiment detection.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li et al_2019_Acoustic and Lexical Sentiment Analysis for Customer Service Calls.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4YLP8TPF/8683679.html}
}

@inproceedings{li_bayesian_2000,
  title = {A {{Bayesian Approach}} to {{Temporal Data Clustering}} Using {{Hidden Markov Models}}.},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Learning}} ({{ICML}})},
  author = {Li, Cen and Biswas, Gautam},
  year = {2000},
  pages = {543--550},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li_Biswas_2000_A Bayesian Approach to Temporal Data Clustering using Hidden Markov Models.pdf}
}

@inproceedings{li_chinese_2017,
  title = {Chinese {{Sign Language Recognition Based}} on {{SHS Descriptor}} and {{Encoder}}-{{Decoder LSTM Model}}},
  booktitle = {Biometric {{Recognition}}},
  author = {Li, Xiaoxu and Mao, Chensi and Huang, Shiliang and Ye, Zhongfu},
  year = {2017},
  month = oct,
  pages = {719--728},
  doi = {10.1007/978-3-319-69923-3_77},
  abstract = {This paper presents a novel approach to recognize isolated Chinese sign language. In order to better distinguish different hand shapes, a new Specific Hand Shape (SHS) descriptor is proposed. Based on the SHS descriptor, an encoder-decoder LSTM model is applied to achieve better sign recognition results. A specific hand shape database and an 80 words isolated Chinese sign language database are constructed using Kinect 2.0 to evaluate the proposed methods. Experimental results show the proposed SHS descriptor is more discriminative than the traditional HOG descriptor and the recognition model is more efficient than the HMM based approach.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li et al_2017_Chinese Sign Language Recognition Based on SHS Descriptor and Encoder-Decoder.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z2A2DM44/978-3-319-69923-3_77.html},
  isbn = {978-3-319-69922-6 978-3-319-69923-3},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=80;signer=10;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=10,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=bodyjoints,vocab<100,vocab=80},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{li_decision_2014,
  title = {Decision Tree Based State Tying for Speech Recognition Using {{DNN}} Derived Embeddings},
  booktitle = {Proc. {{Int}}. {{Symposium}} on {{Chinese Spoken Language Processing}} ({{ISCSLP}})},
  author = {Li, X. and Wu, X.},
  year = {2014},
  month = sep,
  pages = {123--127},
  doi = {10.1109/ISCSLP.2014.6936637},
  abstract = {Recently, context dependent (CD)-deep neural network (DNN)-hidden Markov model (HMM) obtains significant improvements in many automatic speech recognition (ASR) tasks. In the standard training procedure for CD-DNN-HMM, the Gaussian mixture models (GMM) based ASR system has to be firstly built to pre-segment the training data and to define the CD states as the targets for DNN. In this paper, we propose a novel decision tree based state tying procedure, in which, the state embeddings derived from DNN are used and clustered to minimize the sum-of-squared error. Thus, the GMM is not a necessary part to define the targets for CD-DNN. Besides, we introduce a training procedure for CD-DNN-HMM, where, the forward backward algorithm is used for context independent (CI) DNN-HMM training, and the proposed state tying approach is applied to define the CD-DNN targets. Experiments were conducted on a 30-hour Chinese broadcast news speech database and the results demonstrate that the proposed DNN based state tying approach yielded comparable performance to the GMM based one.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li_Wu_2014_Decision tree based state tying for speech recognition using DNN derived.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WCWPBT7E/6936637.html}
}

@article{li_sign_2016,
  title = {Sign {{Transition Modeling}} and a {{Scalable Solution}} to {{Continuous Sign Language Recognition}} for {{Real}}-{{World Applications}}},
  author = {Li, Kehuang and Zhou, Zhengyu and Lee, Chin-Hui},
  year = {2016},
  month = jan,
  volume = {8},
  pages = {7:1--7:23},
  issn = {1936-7228},
  doi = {10.1145/2850421},
  abstract = {We propose a new approach to modeling transition information between signs in continuous Sign Language Recognition (SLR) and address some scalability issues in designing SLR systems. In contrast to Automatic Speech Recognition (ASR) in which the transition between speech sounds is often brief and mainly addressed by the coarticulation effect, the sign transition in continuous SLR is far from being clear and usually not easily and exactly characterized. Leveraging upon hidden Markov modeling techniques from ASR, we proposed a modeling framework for continuous SLR having the following major advantages, namely: (i) the system is easy to scale up to large-vocabulary SLR; (ii) modeling of signs as well as the transitions between signs is robust even for noisy data collected in real-world SLR; and (iii) extensions to training, decoding, and adaptation are directly applicable even with new deep learning algorithms. A pair of low-cost digital gloves affordable for the deaf and hard of hearing community is used to collect a collection of training and testing data for real-world SLR interaction applications. Evaluated on 1,024 testing sentences from five signers, a word accuracy rate of 87.4\% is achieved using a vocabulary of 510 words. The SLR speed is in real time, requiring an average of 0.69s per sentence. The encouraging results indicate that it is feasible to develop real-world SLR applications based on the proposed SLR framework.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li et al_2016_Sign Transition Modeling and a Scalable Solution to Continuous Sign Language.pdf},
  journal = {ACM Transactions on Accessible Computing (TACCESS)},
  keywords = {capturedBy=electronicGlove,continuous,continuous;vocab=510;signer=5;sl=CSL;capturedBy=electronicGlove,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=5,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape,vocab<1000,vocab=510},
  number = {2}
}

@inproceedings{li_transferring_2020,
  title = {Transferring {{Cross}}-{{Domain Knowledge}} for {{Video Sign Language Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Dongxu and Yu, Xin and Xu, Chenchen and Petersson, Lars and Li, Hongdong},
  year = {2020},
  pages = {6205--6214},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li et al_2020_Transferring Cross-Domain Knowledge for Video Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/B5JSU5MJ/Li_Transferring_Cross-Domain_Knowledge_for_Video_Sign_Language_Recognition_CVPR_2020_paper.html},
  keywords = {capturedBy=vision,dataSet=MS-ASL,dataSet=WSASL,isolated,isolated;vocab=100;signer=189;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=100;signer=97;sl=ASL;capturedBy=vision;dataSet=WSASL,isolated;vocab=200;signer=196;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=300;signer=109;sl=ASL;capturedBy=vision;dataSet=WSASL,modal=fullframe,signer=109,signer=189,signer=196,signer=97,sl=ASL,slreco,slreco;modal=fullframe,vocab<500,vocab=100,vocab=200,vocab=300}
}

@article{li_unicodervl_2019,
  title = {Unicoder-{{VL}}: {{A Universal Encoder}} for {{Vision}} and {{Language}} by {{Cross}}-Modal {{Pre}}-Training},
  shorttitle = {Unicoder-{{VL}}},
  author = {Li, Gen and Duan, Nan and Fang, Yuejian and Gong, Ming and Jiang, Daxin and Zhou, Ming},
  year = {2019},
  month = dec,
  abstract = {We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pretrained models, such as XLM (Lample and Conneau 2019) and Unicoder (Huang et al. 2019), both visual and linguistic contents are fed into a multi-layer Transformer (Vaswani et al. 2017) for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling (MLM), Masked Object Classification (MOC) and Visual-linguistic Matching (VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer. We achieve state-of-the-art or comparable results on both two tasks and show the powerful ability of the crossmodal pre-training.},
  archivePrefix = {arXiv},
  eprint = {1908.06066},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SMLTGVUR/Li et al. - 2019 - Unicoder-VL A Universal Encoder for Vision and La.pdf},
  journal = {arXiv:1908.06066 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{li_videolstm_2016,
  title = {{{VideoLSTM Convolves}}, {{Attends}} and {{Flows}} for {{Action Recognition}}},
  author = {Li, Zhenyang and Gavves, Efstratios and Jain, Mihir and Snoek, Cees G. M.},
  year = {2016},
  month = jul,
  abstract = {We present a new architecture for end-to-end sequence learning of actions in video, we call VideoLSTM. Rather than adapting the video to the peculiarities of established recurrent or convolutional architectures, we adapt the architecture to fit the requirements of the video medium. Starting from the soft-Attention LSTM, VideoLSTM makes three novel contributions. First, video has a spatial layout. To exploit the spatial correlation we hardwire convolutions in the soft-Attention LSTM architecture. Second, motion not only informs us about the action content, but also guides better the attention towards the relevant spatio-temporal locations. We introduce motion-based attention. And finally, we demonstrate how the attention from VideoLSTM can be used for action localization by relying on just the action class label. Experiments and comparisons on challenging datasets for action classification and localization support our claims.},
  archivePrefix = {arXiv},
  eprint = {1607.01794},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li et al_2016_VideoLSTM Convolves, Attends and Flows for Action Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9T49DSH6/1607.html},
  journal = {arXiv:1607.01794 [cs]},
  primaryClass = {cs}
}

@article{li_wordlevel_2020,
  ids = {li\_wordlevel\_2020},
  title = {Word-Level {{Deep Sign Language Recognition}} from {{Video}}: {{A New Large}}-Scale {{Dataset}} and {{Methods Comparison}}},
  shorttitle = {Word-Level {{Deep Sign Language Recognition}} from {{Video}}},
  author = {Li, Dongxu and Opazo, Cristian Rodriguez and Yu, Xin and Li, Hongdong},
  year = {2020},
  month = jan,
  abstract = {Vision-based sign language recognition aims at helping deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge, it is by far the largest public ASL dataset to facilitate word-level sign recognition research. Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance-based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that models spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 66\% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at \textbackslash url\{https://dxli94.github.io/WLASL/\}.},
  archivePrefix = {arXiv},
  eprint = {1910.11006},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li et al_2020_Word-level Deep Sign Language Recognition from Video.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Li et al_2020_Word-level Deep Sign Language Recognition from Video2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/72LS3IWR/1910.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N3IZNZMC/1910.html},
  journal = {arXiv:1910.11006 [cs]},
  keywords = {capturedBy=vision,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Multimedia,Computer Science - Neural and Evolutionary Computing,dataSet=WSASL,isolated,isolated;vocab=100;signer=97;sl=ASL;capturedBy=vision;dataSet=WSASL,isolated;vocab=1000;signer=116;sl=ASL;capturedBy=vision;dataSet=WSASL,isolated;vocab=2000;signer=119;sl=ASL;capturedBy=vision;dataSet=WSASL,isolated;vocab=300;signer=109;sl=ASL;capturedBy=vision;dataSet=WSASL,modal=bodyjoints,modal=fullframe,signer=109,signer=116,signer=119,signer=97,sl=ASL,slreco,slreco;modal=bodyjoints,slreco;modal=fullframe,vocab<500,vocab=100,vocab=1000,vocab=2000,vocab=300,vocab>=1000},
  primaryClass = {cs}
}

@inproceedings{liang_multimodal_2018,
  title = {Multimodal {{Language Analysis}} with {{Recurrent Multistage Fusion}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Liang, Paul Pu and Liu, Ziyin and Zadeh, AmirAli Bagher and Morency, Louis-Philippe},
  year = {2018},
  pages = {150--161},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liang et al_2018_Multimodal Language Analysis with Recurrent Multistage Fusion.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K23ZG37Z/D18-1014.html},
  language = {en-us}
}

@inproceedings{liang_realtime_1995,
  title = {A {{Real}}-Time {{Continuous Alphabetic Sign Language}} to {{Speech Conversion VR System}}},
  booktitle = {Computer {{Graphics Forum}}},
  author = {Liang, Rung-Huei and Ouhyoung, Ming},
  year = {1995},
  month = aug,
  volume = {14},
  pages = {67--76},
  publisher = {{Wiley Online Library}},
  abstract = {Many ways of communications are used between human and computer, while using gesture is considered to be one of the most natural way in a virtual reality system. Because of its intuitiveness and its capability of helping the hearing impaired or speaking impaired, we develop a gesture recognition system. Considering the world-wide use of ASL (American Sign Language), this system focuses on the recognition of a continuous flow of alphabets in ASL to spell a word followed by the speech synthesis, and adopts a simple and efficient windowed template matching recognition strategy to achieve the goal of a real-time and continuous recognition. In addition to the abduction and the flex information in a gesture, we introduce a concept of contact-point into our system to solve the intrinsic ambiguities of some gestures in ASL. Five tact switches, served as contact-points and sensed by an analogue to digital board, are sewn on a glove cover to enhance the functions of a traditional data glove.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liang_Ouhyoung_1995_A Real-time Continuous Alphabetic Sign Language to Speech Conversion VR System.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/72J8AXMA/Liang and Ouhyoung - 1995 - A Real-time Continuous Alphabetic Sign Language to},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=26;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=movement,modal=orientation,modal=shape,sl=ASL,slreco,slreco;modal=shape;modal=movement;modal=orientation,vocab<50,vocab=26}
}

@inproceedings{liang_realtime_1998,
  title = {A {{Real}}-{{Time Continuous Gesture Recognition System}} for {{Sign Language}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Liang, Rung-Huei and Ouhyoung, Ming},
  year = {1998},
  month = apr,
  pages = {558--567},
  address = {{Nara, Japan}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liang_Ouhyoung_1998_A Real-Time Continuous Gesture Recognition System for Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ETXKDWZ6/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KJDI52F7/671007.html},
  keywords = {capturedBy=electronicGlove,continuous,continuous;vocab=250;sl=TwSL;capturedBy=electronicGlove,glove,hmm,modal=movement,modal=orientation,modal=shape,sl=TwSL,slreco,slreco;modal=orientation;modal=movement;modal=shape,vocab<500,vocab=250}
}

@article{liao_dynamic_2019,
  ids = {liao\_dynamic\_2019},
  title = {Dynamic {{Sign Language Recognition Based}} on {{Video Sequence With BLSTM}}-{{3D Residual Networks}}},
  author = {Liao, Yanqiu and Xiong, Pengwen and Min, Weidong and Min, Weiqiong and Lu, Jiahao},
  year = {2019},
  volume = {7},
  pages = {38044--38054},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2904749},
  abstract = {Sign language recognition aims to recognize meaningful movements of hand gestures and is a significant solution in intelligent communication between the deaf community and hearing societies. However, until now, the current dynamic sign language recognition methods still have some drawbacks with difficulties of recognizing complex hand gestures, low recognition accuracy for most dynamic sign language recognition, and potential problems in larger video sequence data training. In order to solve these issues, this paper presents a multimodal dynamic sign language recognition method based on a deep 3-dimensional residual ConvNet and bi-directional LSTM networks, which is named as BLSTM-3D residual network (B3D ResNet). This method consists of three main parts. First, the hand object is localized in the video frames in order to reduce the time and space complexity of network calculation. Then, the B3D ResNet automatically extracts the spatiotemporal features from the video sequences and establishes an intermediate score corresponding to each action in the video sequence after feature analysis. Finally, by classifying the video sequences, the dynamic sign language is accurately identified. The experiment is conducted on test datasets, including DEVISIGN\_D dataset and SLR\_Dataset. The results show that the proposed method can obtain state-of-the-art recognition accuracy (89.8\% on the DEVISIGN\_D dataset and 86.9\% on SLR\_Dataset). In addition, the B3D ResNet can effectively recognize complex hand gestures through larger video sequence data, and obtain high recognition accuracy for 500 vocabularies from Chinese hand sign language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liao et al_2019_Dynamic Sign Language Recognition Based on Video Sequence With BLSTM-3D.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liao et al_2019_Dynamic Sign Language Recognition Based on Video Sequence With BLSTM-3D2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3JR7DETA/8667292.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8M8EHG38/8667292.html},
  journal = {IEEE Access},
  keywords = {capturedBy=vision,dataSet=Devisign-D,isolated,isolated;vocab=500;signer=8;sl=CSL;capturedBy=vision;dataSet=Devisign-D,modal=2hshape,signer=8,sl=CSL,slreco,slreco;modal=2hshape,vocab<1000,vocab=500}
}

@inproceedings{lichtenauer_personindependent_2009,
  title = {Person-{{Independent 3D Sign Language Recognition}}},
  booktitle = {Gesture-{{Based Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {Lichtenauer, Jeroen F. and {ten Holt}, Gineke A. and Reinders, Marcel J. T. and Hendriks, Emile A.},
  editor = {Sales Dias, Miguel and Gibet, Sylvie and Wanderley, Marcelo M. and Bastos, Rafael},
  year = {2009},
  month = may,
  pages = {69--80},
  address = {{Lisbon, Portugal}},
  abstract = {In this paper, we present a person independent 3D system for judging the correctness of a sign. The system is camera-based, using computer vision techniques to track the hand and extract features. 3D co-ordinates of the hands and other features are calculated from stereo images. The features are then modeled statistically and automatic feature selection is used to build the classifiers. Each classifier is meant to judge the correctness of one sign. We tested our approach using a 120-sign vocabulary and 75 different signers. Overall, a true positive rate of 96.5\% at a false positive rate of 3.5\% is achieved. The system's performance in a real-world setting largely agreed with human expert judgement.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lichtenauer et al_2009_Person-Independent 3D Sign Language Recognition.pdf},
  isbn = {978-3-540-92865-2},
  keywords = {capturedBy=stereocamera,capturedBy=vision,isolated,isolated;vocab=120;signer=75;sl=NGT;capturedBy=vision;capturedBy=stereocamera,modal=2hlocation,modal=2hmovement,signer=75,sl=NGT,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<500,vocab=120},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{lichtenauer_sign_2008,
  title = {Sign {{Language Recognition}} by {{Combining Statistical DTW}} and {{Independent Classification}}},
  author = {Lichtenauer, J. F. and Hendriks, E. A. and Reinders, M. J. T.},
  year = {2008},
  month = nov,
  volume = {30},
  pages = {2040--2046},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2008.123},
  abstract = {To recognize speech, handwriting, or sign language, many hybrid approaches have been proposed that combine dynamic time warping (DTW) or hidden Markov models (HMMs) with discriminative classifiers. However, all methods rely directly on the likelihood models of DTW/HMM. We hypothesize that time warping and classification should be separated because of conflicting likelihood modeling demands. To overcome these restrictions, we propose using statistical DTW (SDTW) only for time warping, while classifying the warped features with a different method. Two novel statistical classifiers are proposed - combined discriminative feature detectors (CDFDs) and quadratic classification on DF Fisher mapping (Q-DFFM) - both using a selection of discriminative features (DFs), and are shown to outperform HMM and SDTW. However, we have found that combining likelihoods of multiple models in a second classification stage degrades performance of the proposed classifiers, while improving performance with HMM and SDTW. A proof-of-concept experiment, combining DFFM mappings of multiple SDTW models with SDTW likelihoods, shows that, also for model-combining, hybrid classification can provide significant improvement over SDTW. Although recognition is mainly based on 3D hand motion features, these results can be expected to generalize to recognition with more detailed measurements such as hand/body pose and facial expression.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lichtenauer et al_2008_Sign Language Recognition by Combining Statistical DTW and Independent.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/G8B43NJ9/4527247.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {capturedBy=stereocamera,capturedBy=vision,isolated,isolated;vocab=120;signer=75;sl=NGT;capturedBy=vision;capturedBy=stereocamera,modal=2hmovement,signer=75,sl=NGT,slreco,slreco;modal=2hmovement,vocab<500,vocab=120},
  number = {11}
}

@article{liddell_american_1989,
  title = {American {{Sign Language}}: {{The Phonological Base}}.},
  shorttitle = {American {{Sign Language}}},
  author = {Liddell, Scott K. and Johnson, Robert E.},
  year = {1989},
  volume = {1},
  pages = {64:195-277},
  journal = {Sign Language Studies}
}

@incollection{liddell_structures_1990,
  title = {Structures for {{Representing Handshape}} and {{Local Movement}} at the {{Phonemic Level}}},
  booktitle = {Theoretical {{Issues}} in {{Sign Language Research}}, {{Volume}} 1: {{Linguistics}}},
  author = {Liddell, Scott K.},
  editor = {Fischer, Susan D. and Siple, Patricia},
  year = {1990},
  month = nov,
  volume = {1},
  pages = {37--66},
  publisher = {{University of Chicago Press}},
  abstract = {Only recently has linguistic research recognized sign languages as legitimate human languages with properties analogous to those cataloged for French or Navajo, for example. There are many different sign languages, which can be analyzed on a variety of levels\textemdash phonetics, phonology, morphology, syntax, and semantics\textemdash in the same way as spoken languages. Yet the recognition that not all of the principles established for spoken languages hold for sign languages has made sign languages a crucial testing ground for linguistic theory.  Edited by Susan Fischer and Patricia Siple, this collection is divided into four sections, reflecting the traditional core areas of phonology, morphology, syntax, and semantics. Although most of the contributions consider American Sign Language (ASL), five treat sign languages unrelated to ASL, offering valuable perspectives on sign universals. Since some of these languages or systems are only recently established, they provide a window onto the evolution and growth of sign languages.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liddell_1990_Structures for Representing Handshape and Local Movement at the Phonemic Level.pdf},
  googlebooks = {Vl2NFP9deysC},
  isbn = {978-0-226-25150-9},
  language = {en}
}

@article{liddell_think_1984,
  title = {Think and {{Believe}}: {{Sequentiality}} in {{American Sign Language}}},
  author = {Liddell, Scott K.},
  year = {1984},
  volume = {60},
  pages = {pp. 372-399},
  issn = {00978507},
  abstract = {This paper examines the nature of the sublexical structure of signs in American Sign Language, and its relevance to various descriptive and theoretical problems. Data are presented which show that the currently accepted view of a sign as a simultaneous bundle of primes-handshape, movement, location, and orientation-is insufficient in several ways. Timing data are presented which show that the majority of signs are segmentable into MOVEMENTS and HOLDS. A sequential underlying representation of signs is proposed which predicts the behavior of 'non-contacting' contact signs, gives a legitimate status to multiple movement signs, accounts for the three manners of movement (hold, continuous, and restrained), and allows a sequential representation of non-manual signals. It also describes in structural terms the changes which signs undergo in forming compounds, and it allows the incorporation of subject and object agreement markers into verb structure in a straightforward way. This framework allows an interesting parallel with spoken language phonology: both divide the sequential units which make up the words of the language into two major types of units. In spoken language, the units are consonants and vowels; in sign language, they are holds and movements.},
  copyright = {Copyright \textcopyright{} 1984 Linguistic Society of America},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liddell_1984_Think and Believe.pdf},
  journal = {Language},
  language = {English},
  number = {2}
}

@incollection{lieberman_studying_2015,
  title = {Studying {{Sign Language Acquisition}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Lieberman, Amy M. and Mayberry, Rachel I.},
  year = {2015},
  pages = {281--299},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch15},
  abstract = {The study of sign language acquisition has revealed important insights regarding the acquisition of language in the visual modality, the impact of delayed first-language exposure on language ability, and the relationship between language and cognitive processes. Unique challenges arise in studying sign language acquisition due to the low incidence and heterogeneity of the population and the need for inclusion in all aspects of the research of highly skilled native and near-native language users who are deaf. Despite these challenges, a range of methodological approaches have been applied to sign language acquisition, including longitudinal and cross-sectional sampling of the population, case studies, adaptation of assessment instruments, standardized measures, analyses of naturalistic language, and elicited language samples. Through these methods, researchers are able to conduct rigorous studies whose findings have made invaluable contributions to theories of language acquisition and development in a number of sign languages and populations.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lieberman_Mayberry_2015_Studying Sign Language Acquisition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7KXIKMLT/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{lien_automated_1998,
  title = {Automated Facial Expression Recognition Based on {{FACS}} Action Units},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Lien, J.J. and Kanade, T. and Cohn, J.F. and Li, C.-C.},
  year = {1998},
  month = apr,
  pages = {390--395},
  doi = {10.1109/AFGR.1998.670980},
  abstract = {Automated recognition of facial expression is an important addition to computer vision research because of its relevance to the study of psychological phenomena and the development of human-computer interaction (HCI). We developed a computer vision system that automatically recognizes individual action units or action unit combinations in the upper face using hidden Markov models (HMMs). Our approach to facial expression recognition is based an the Facial Action Coding System (FACS), which separates expressions into upper and lower face action. We use three approaches to extract facial expression information: (1) facial feature point tracking; (2) dense flow tracking with principal component analysis (PCA); and (3) high gradient component detection (i.e. furrow detection). The recognition results of the upper face expressions using feature point tracking, dense flow tracking, and high gradient component detection are 85\%, 93\% and 85\%, respectively},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lien et al_1998_Automated facial expression recognition based on FACS action units.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZRHRGRVM/abs_all.html}
}

@inproceedings{lien_subtly_1998,
  title = {Subtly Different Facial Expression Recognition and Expression Intensity Estimation},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Lien, J.J.-J. and Kanade, T. and Cohn, J.F. and Li, C.-C.},
  year = {1998},
  month = jun,
  pages = {853--859},
  doi = {10.1109/CVPR.1998.698704},
  abstract = {We have developed a computer vision system, including both facial feature extraction and recognition, that automatically discriminates among subtly different facial expressions. Expression classification is based on Facial Action Coding System (FACS) action units (AUs), and discrimination is performed using Hidden Markov Models (HMMs). Three methods are developed to extract facial expression information for automatic recognition. The first method is facial feature point tracking using a coarse-to-fine pyramid method. This method is sensitive to subtle feature motion and is capable of handling large displacements with sub-pixel accuracy. The second method is dense flow tracking together with principal component analysis (PCA) where the entire facial motion information per frame is compressed to a low-dimensional weight vector. The third method is high gradient component (i.e., furrow) analysis in the spatio-temporal domain, which exploits the transient variation associated with the facial expression. Upon extraction of the facial information, non-rigid facial expression is separated from the rigid head motion component, and the face images are automatically aligned and normalized using an affine transformation. This system also provides expression intensity estimation, which has significant effect on the actual meaning of the expression},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lien et al_1998_Subtly different facial expression recognition and expression intensity.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KP8UVPET/abs_all.html}
}

@article{lim_feature_2016,
  title = {A Feature Covariance Matrix with Serial Particle Filter for Isolated Sign Language Recognition},
  author = {Lim, Kian Ming and Tan, Alan W. C. and Tan, Shing Chiang},
  year = {2016},
  month = jul,
  volume = {54},
  pages = {208--218},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2016.01.047},
  abstract = {As is widely recognized, sign language recognition is a very challenging visual recognition problem. In this paper, we propose a feature covariance matrix based serial particle filter for isolated sign language recognition. At the preprocessing stage, the fusion of the median and mode filters is employed to extract the foreground and thereby enhances hand detection. We propose to serially track the hands of the signer, as opposed to tracking both hands at the same time, to reduce the misdirection of target objects. Subsequently, the region around the tracked hands is extracted to generate the feature covariance matrix as a compact representation of the tracked hand gesture, and thereby reduce the dimensionality of the features. In addition, the proposed feature covariance matrix is able to adapt to new signs due to its ability to integrate multiple correlated features in a natural way, without any retraining process. The experimental results show that the hand trajectories as obtained through the proposed serial hand tracking are closer to the ground truth. The sign gesture recognition based on the proposed methods yields a 87.33\% recognition rate for the American Sign Language. The proposed hand tracking and feature extraction methodology is an important milestone in the development of expert systems designed for sign language recognition, such as automated sign language translation systems.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lim et al_2016_A feature covariance matrix with serial particle filter for isolated sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/49SD9X8E/S0957417416300112.html},
  journal = {Expert Systems with Applications},
  keywords = {capturedBy=vision,dataSet=BU-50,isolated,isolated;vocab=50;signer=3;sl=ASL;capturedBy=vision;dataSet=BU-50,modal=2hshape,signer=3,sl=ASL,slreco,slreco;modal=2hshape,vocab<100,vocab=50}
}

@inproceedings{lin_ctc_2017,
  title = {{{CTC Network}} with {{Statistical Language Modeling}} for {{Action Sequence Recognition}} in {{Videos}}},
  booktitle = {Proceedings of the on {{Thematic Workshops}} of {{ACM Multimedia}} 2017},
  author = {Lin, Mengxi and Inoue, Nakamasa and Shinoda, Koichi},
  year = {2017},
  pages = {393--401},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3126686.3126755},
  abstract = {We propose a method for recognizing an action sequence in which several actions are concatenated and their boundaries are not given. The proposed method combines Connectionist Temporal Classification (CTC) and a statistical language model. CTC can learn the nature of each element action given no boundary information in an end-to-end manner. The statistical language model can learn the relationship between actions. We evaluate our method on the Breakfast dataset. When we use the trigram as the language model, its accuracy rate is 43.4\%, which is better than the state-of-the-art ECTC method by 6.7 percentage points.},
  isbn = {978-1-4503-5416-5},
  series = {Thematic {{Workshops}} '17}
}

@inproceedings{lin_curve_2014,
  title = {Curve {{Matching}} from the {{View}} of {{Manifold}} for {{Sign Language Recognition}}},
  booktitle = {Asian {{Conf}}. on {{Computer Vision Workshops}} ({{ACCVW}})},
  author = {Lin, Yushun and Chai, Xiujuan and Zhou, Yu and Chen, Xilin},
  year = {2014},
  month = nov,
  pages = {233--246},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-16634-6_18},
  abstract = {Sign language recognition is a challenging task due to the complex action variations and the large vocabulary set. Generally, sign language conveys meaning through multichannel information like trajectory, hand posture and facial expression simultaneously. Obviously, trajectories of sign words play an important role for sign language recognition. Although the multichannel features are helpful for sign representation, this paper only focuses on the trajectory aspect. A method of curve matching based on manifold analysis is proposed to recognize isolated sign language word with 3D trajectory captured by Kinect. From the view of manifold, the main structure of the curve is found by the intrinsic linear segments, which are characterized by some geometric features. Then the matching between curves is transformed into the matching between two sets of sequential linear segments. The performance of the proposed curve matching strategy is evaluated on two different sign language datasets. Our method achieves a top-1 recognition rate of 78.3 \% and 61.4 \% in a 370 daily words dataset and a large dataset containing 1000 vocabularies.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lin et al_2014_Curve Matching from the View of Manifold for Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7QB65U63/978-3-319-16634-6_18.html},
  isbn = {978-3-319-16633-9 978-3-319-16634-6},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{lin_network_2013,
  title = {Network {{In Network}}},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  year = {2013},
  month = dec,
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  archivePrefix = {arXiv},
  eprint = {1312.4400},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lin et al_2013_Network In Network.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BBECFWTR/1312.html},
  journal = {arXiv:1312.4400 [cs]},
  primaryClass = {cs}
}

@inproceedings{lin_study_2009,
  title = {A Study on Multilingual Acoustic Modeling for Large Vocabulary {{ASR}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Lin, H. and Deng, L. and Yu, D. and Gong, Y. and Acero, A. and Lee, C.},
  year = {2009},
  month = apr,
  pages = {4333--4336},
  doi = {10.1109/ICASSP.2009.4960588},
  abstract = {We study key issues related to multilingual acoustic modeling for automatic speech recognition (ASR) through a series of large-scale ASR experiments. Our study explores shared structures embedded in a large collection of speech data spanning over a number of spoken languages in order to establish a common set of universal phone models that can be used for large vocabulary ASR of all the languages seen or unseen during training. Language-universal and language-adaptive models are compared with language-specific models, and the comparison results show that in many cases it is possible to build general-purpose language-universal and language-adaptive acoustic models that outperform language-specific ones if the set of shared units, the structure of shared states, and the shared acoustic-phonetic properties among different languages can be properly utilized. Specifically, our results demonstrate that when the context coverage is poor in language-specific training, we can use one tenth of the adaptation data to achieve equivalent performance in cross-lingual speech recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lin et al_2009_A study on multilingual acoustic modeling for large vocabulary ASR.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HT4UIB6H/4960588.html},
  keywords = {acoustic modeling,acoustic-phonetic property,Adaptation model,automatic speech recognition,Automatic speech recognition,computational linguistics,Context modeling,cross-lingual speech recognition,Impurities,language adaptation,language-adaptive acoustic model,language-specific model,language-universal acoustic model,large vocabulary ASR,Large-scale systems,multilingual acoustic modeling,Multilingualism,Natural languages,speech recognition,Speech recognition,Training data,Uninterruptible power systems,universal phone model,universal phone models,vocabulary,Vocabulary}
}

@article{linde_algorithm_1980,
  title = {An {{Algorithm}} for {{Vector Quantizer Design}}},
  author = {Linde, Y. and Buzo, A. and Gray, R.},
  year = {1980},
  month = jan,
  volume = {28},
  pages = {84--95},
  issn = {0090-6778},
  doi = {10.1109/TCOM.1980.1094577},
  abstract = {An efficient and intuitive algorithm is presented for the design of vector quantizers based either on a known probabilistic model or on a long training sequence of data. The basic properties of the algorithm are discussed and demonstrated by examples. Quite general distortion measures and long blocklengths are allowed, as exemplified by the design of parameter vector quantizers of ten-dimensional vectors arising in Linear Predictive Coded (LPC) speech compression with a complicated distortion measure arising in LPC analysis that does not depend only on the error vector.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Linde et al_1980_An Algorithm for Vector Quantizer Design.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MKPSVF8S/1094577.html},
  journal = {IEEE Transactions on Communications},
  number = {1}
}

@article{lindgren_machine_1965,
  title = {Machine Recognition of Human Language {{Part I}} - {{Automatic}} Speech Recognition},
  author = {Lindgren, N.},
  year = {1965},
  month = mar,
  volume = {2},
  pages = {114--136},
  issn = {0018-9235},
  doi = {10.1109/MSPEC.1965.5531769},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lindgren_1965_Machine recognition of human language Part I - Automatic speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/G6PW6Z8F/5531769.html},
  journal = {IEEE Spectrum},
  number = {3}
}

@article{lipton_critical_2015,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  year = {2015},
  month = may,
  abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
  archivePrefix = {arXiv},
  eprint = {1506.00019},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lipton et al_2015_A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GCPI9EMQ/1506.html},
  journal = {arXiv:1506.00019 [cs]},
  primaryClass = {cs}
}

@inproceedings{liu_3d_2014,
  title = {{{3D Face Tracking}} and {{Multi}}-{{Scale}}, {{Spatio}}-Temporal {{Analysis}} of {{Linguistically Significant Facial Expressions}} and {{Head Positions}} in {{ASL}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Liu, Bo and Liu, Jingjing and Yu, Xiang and Metaxas, Dimitris and Neidle, Carol},
  year = {2014},
  month = may,
  pages = {4512--4518},
  address = {{Reykjav\'ik, Iceland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2014_3D Face Tracking and Multi-Scale, Spatio-temporal Analysis of Linguistically.pdf},
  keywords = {capturedBy=vision,continuous,modal=head,modal=mouth,nonmanual;vocab=5;signer=1;sl=ASL;capturedBy=vision,nonmanualreco;modal=head;modal=mouth;modal=eyeblink,signer=1,sl=ASL,slreco,vocab<50,vocab=5}
}

@inproceedings{liu_automatic_2018,
  title = {Automatic {{Temporal Segmentation}} of {{Hand Movements}} for {{Hand Positions Recognition}} in {{French Cued Speech}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Liu, L. and Feng, G. and Beautemps, D.},
  year = {2018},
  month = apr,
  pages = {3061--3065},
  doi = {10.1109/ICASSP.2018.8462090},
  abstract = {In the context of Cued Speech (CS) recognition, the recognition of lips and hand movements is a key task. As we know, a good temporal segmentation is necessary for the supervised recognition system. However, lips and hand streams cannot share the same temporal segmentation since they are not synchronized. In this work, we propose a hand preceding model to predict temporal segmentations of hand movements automatically by exploring the relationship between hand preceding time and the vowel positions in sentences. To evaluate the performance of the proposed method, we apply the hand preceding model to a multi -speakers database. Hand positions recognition is realized with the multi-Gaussian and Long-Short Term Memory (LSTM). The results show that using the predicted temporal segmentation significantly improves the recognition performance compared with that using the audio based segmentation. To the best of our knowledge, this is the first automatic method to predict the temporal segmentation for hand movements only from the audio based segmentation in CS.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2018_Automatic Temporal Segmentation of Hand Movements for Hand Positions.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5EJXPZGV/8462090.html}
}

@inproceedings{liu_continuous_2017,
  title = {Continuous {{Gesture Recognition With Hand}}-{{Oriented Spatiotemporal Feature}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Liu, Zhipeng and Chai, Xiujuan and Liu, Zhuang and Chen, Xilin},
  year = {2017},
  pages = {3056--3064},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2017_Continuous Gesture Recognition With Hand-Oriented Spatiotemporal Feature.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FIKXAQDQ/Liu_Continuous_Gesture_Recognition_ICCV_2017_paper.html}
}

@inproceedings{liu_model_2004,
  title = {Model Structure Selection \& Training Algorithms for an {{HMM}} Gesture Recognition System},
  booktitle = {Frontiers in {{Handwriting Recognition}}, 2004. {{IWFHR}}-9 2004. {{Ninth International Workshop}} On},
  author = {Liu, Nianjun and Lovell, Brian C. and Kootsookos, Peter J. and Davis, Richard IA},
  year = {2004},
  pages = {100--105},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2004_Model structure selection & training algorithms for an HMM gesture recognition.pdf}
}

@article{liu_modeling_2018,
  title = {Modeling for {{Continuous Cued Speech Recognition}} in {{French}} Using {{Advanced Machine Learning Methods}}},
  author = {Liu, Li},
  year = {2018},
  month = sep,
  abstract = {This PhD thesis deals with the automatic continuous Cued Speech (CS) recognition basedon the images of subjects without marking any artificial landmark. In order to realize thisobjective, we extract high level features of three information flows (lips, hand positions andshapes), and find an optimal approach to merging them for a robust CS recognition system.We first introduce a novel and powerful deep learning method based on the ConvolutionalNeural Networks (CNNs) for extracting the hand shape/lips features from raw images. Theadaptive background mixture models (ABMMs) are also applied to obtain the hand positionfeatures for the first time. Meanwhile, based on an advanced machine learning method Modi-fied Constrained Local Neural Fields (CLNF), we propose the Modified CLNF to extract theinner lips parameters (A and B ), as well as another method named adaptive ellipse model. Allthese methods make significant contributions to the feature extraction in CS. Then, due tothe asynchrony problem of three feature flows (i.e., lips, hand shape and hand position) in CS,the fusion of them is a challenging issue. In order to resolve it, we propose several approachesincluding feature-level and model-level fusion strategies combined with the context-dependentHMM. To achieve the CS recognition, we propose three tandem CNNs-HMM architectureswith different fusion types. All these architectures are evaluated on the corpus without anyartifice, and the CS recognition performance confirms the efficiency of our proposed methods.The result is comparable with the state of the art using the corpus with artifices. In parallel,we investigate a specific study about the temporal organization of hand movements in CS,especially about its temporal segmentation, and the evaluations confirm the superior perfor-mance of our methods. In summary, this PhD thesis applies the advanced machine learningmethods to computer vision, and the deep learning methodologies to CS recognition work,which make a significant step to the general automatic conversion problem of CS to sound.The future work will mainly focus on an end-to-end CNN-RNN system which incorporates alanguage model, and an attention mechanism for the multi-modal fusion.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu_2018_Modeling for Continuous Cued Speech Recognition in French using Advanced.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3XQW2R6H/tel-01960233.html},
  language = {en}
}

@article{liu_nonmanual_2014,
  title = {Non-Manual Grammatical Marker Recognition Based on Multi-Scale, Spatio-Temporal Analysis of Head Pose and Facial Expressions},
  author = {Liu, Jingjing and Liu, Bo and Zhang, Shaoting and Yang, Fei and Yang, Peng and Metaxas, Dimitris N. and Neidle, Carol},
  year = {2014},
  volume = {32},
  pages = {671--681},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2014.02.009},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2014_Non-manual grammatical marker recognition based on multi-scale, spatio-temporal.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FM6A268X/S0262885614000468.html},
  journal = {Image and Vision Computing},
  number = {10}
}

@inproceedings{liu_recognizing_2013,
  title = {Recognizing Eyebrow and Periodic Head Gestures Using {{CRFs}} for Non-Manual Grammatical Marker Detection in {{ASL}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Liu, Jingjing and Liu, Bo and Zhang, Shaoting and Yang, Fei and Yang, Peng and Metaxas, Dimitris N. and Neidle, Carol},
  year = {2013},
  month = apr,
  pages = {1--6},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2013_Recognizing eyebrow and periodic head gestures using CRFs for non-manual.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NSM5IIF9/login.html}
}

@inproceedings{liu_selfboosted_2018,
  title = {Self-Boosted {{Gesture Interactive System}} with {{ST}}-{{Net}}},
  booktitle = {Proceedings of the 26th {{ACM}} International Conference on {{Multimedia}}},
  author = {Liu, Zhengzhe and Qi, Xiaojuan and Pang, Lei},
  year = {2018},
  month = oct,
  pages = {145--153},
  publisher = {{Association for Computing Machinery}},
  address = {{Seoul, Republic of Korea}},
  doi = {10.1145/3240508.3240530},
  abstract = {In this paper, we propose a self-boosted intelligent system for joint sign language recognition and automatic education. A novel Spatial-Temporal Net (ST-Net) is designed to exploit the temporal dynamics of localized hands for sign language recognition. Features from ST-Net can be deployed by our education system to detect failure modes of the learners. Moreover, the education system can help collect a vast amount of data for training ST-Net. Our sign language recognition and education system help improve each other step-by-step.On the one hand, benefited from accurate recognition system, the education system can detect the failure parts of the learner more precisely. On the other hand, with more training data gathered from the education system, the recognition system becomes more robust and accurate. Experiments on Hong Kong sign language dataset containing 227 commonly used words validate the effectiveness of our joint recognition and education system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2018_Self-boosted Gesture Interactive System with ST-Net.pdf},
  isbn = {978-1-4503-5665-7},
  keywords = {capturedBy=depth,capturedBy=vision,convolutional neural networks,interactive system,isolated,isolated;vocab=227;sl=HKSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hshape,modal=fullframe,modal=mouth,sl=HKSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=fullframe;modal=mouth,vocab<500,vocab=227},
  series = {{{MM}} '18}
}

@inproceedings{liu_sign_2016,
  title = {Sign {{Language Recognition}} with {{Long Short}}-{{Term Memory}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Liu, T. and Zhou, W. and Li, H.},
  year = {2016},
  month = sep,
  pages = {2871--2875},
  doi = {10.1109/ICIP.2016.7532884},
  abstract = {Sign Language Recognition (SLR) aims at translating the Sign Language (SL) into speech or text, so as to facilitate the communication between hearing-impaired people and the normal people. This problem has broad social impact, however it is challenging due to the variation for different people and the complexity in sign words. Traditional methods for SLR generally use handcrafted feature and Hidden Markov Models (HMMs) modeling temporal information. But reliable handcrafted features are difficult to design and not able to adapt to the large variations of sign words. To approach this problem, considering that Long Short-Term memory (LSTM) can model the contextual information of temporal sequence well, we propose an end-to-end method for SLR based on LSTM. Our system takes the moving trajectories of 4 skeleton joints as inputs without any prior knowledge and is free of explicit feature design. To evaluate our proposed model, we built a large isolated Chinese sign language vocabulary with Kinect 2.0. Experimental results demonstrate the effectiveness of our approach compared with traditional HMM based methods.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_2016_Sign language recognition with long short-term memory.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZT5T4PCS/7532884.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=100;signer=50;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=500;signer=50;sl=CSL;capturedBy=vision;capturedBy=depth,modal=bodyjoints,signer=50,sl=CSL,slreco,slreco;modal=bodyjoints,vocab<1000,vocab<500,vocab=100,vocab=500}
}

@incollection{liu_survey_2012,
  title = {A {{Survey}} of {{Opinion Mining}} and {{Sentiment Analysis}}},
  booktitle = {Mining {{Text Data}}},
  author = {Liu, Bing and Zhang, Lei},
  editor = {Aggarwal, Charu C. and Zhai, ChengXiang},
  year = {2012},
  pages = {415--463},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4614-3223-4_13},
  abstract = {Sentiment analysis or opinion mining is the computational study of people's opinions, appraisals, attitudes, and emotions toward entities, individuals, issues, events, topics and their attributes. The task is technically challenging and practically very useful. For example, businesses always want to find public or consumer opinions about their products and services. Potential customers also want to know the opinions of existing users before they use a service or purchase a product.With the explosive growth of social media (i.e., reviews, forum discussions, blogs and social networks) on the Web, individuals and organizations are increasingly using public opinions in these media for their decision making. However, finding and monitoring opinion sites on the Web and distilling the information contained in them remains a formidable task because of the proliferation of diverse sites. Each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs. The average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them. Moreover, it is also known that human analysis of text information is subject to considerable biases, e.g., people often pay greater attention to opinions that are consistent with their own preferences. People also have difficulty, owing to their mental and physical limitations, producing consistent results when the amount of information to be processed is large. Automated opinion mining and summarization systems are thus needed, as subjective biases and mental limitations can be overcome with an objective sentiment analysis system.In the past decade, a considerable amount of research has been done in academia [58,76]. There are also numerous commercial companies that provide opinion mining services. In this chapter, we first define the opinion mining problem. From the definition, we will see the key technical issues that need to be addressed. We then describe various key mining tasks that have been studied in the research literature and their representative techniques. After that, we discuss the issue of detecting opinion spam or fake reviews. Finally, we also introduce the research topic of assessing the utility or quality of online reviews.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8CMGV46K/Liu and Zhang - A SURVEY OF OPINION MINING AND SENTIMENT ANALYSIS.pdf},
  isbn = {978-1-4614-3223-4},
  keywords = {survey},
  language = {en}
}

@article{liu_video_,
  title = {Video {{Classification}} via {{Weakly Supervised Sequence Modeling}}},
  author = {Liu, Jingjing and Chen, Chao and Zhu, Yan and Liu, Wei and Metaxas, Dimitris N.},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2015.10.012},
  abstract = {Traditional approaches for video classification treat the entire video clip as one data instance. They extract visual features from video frames which are then quantized (e.g., K-means) and pooled (e.g., average pooling) to produce a single feature vector. Such holistic representations of videos are further used as inputs of a classifier. Despite of efficiency, global and aggregate feature representation unavoidably brings in redundant and noisy information from background and unrelated video frames that sometimes overwhelms targeted visual patterns. Besides, temporal correlations between consecutive video frames are also ignored in both training and testing, which may be the key indicator of an action or event. To this end, we propose Weakly Supervised Sequence Modeling (WSSM), a novel framework that combines Multiple-Instance Learning (MIL) and Conditional Random Field (CRF) model seamlessly. Our model takes each entire video as a bag and one video segment as an instance. In our framework, the salient local patterns for different video categories are explored by MIL, and intrinsic temporal dependencies between instances are explicitly exploited using the powerful chain CRF model. In the training stage, we design a novel conditional likelihood formulation which only requires annotation on videos. Such likelihood can be maximized using an alternating optimization method. The training algorithm is guaranteed to converge and is very efficient. In the testing stage, videos are classified by the learned CRF model. The proposed WSSM algorithm outperforms other MIL-based approaches in both accuracy and efficiency on synthetic data and realistic videos for gesture and action classification.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liu et al_Video Classification via Weakly Supervised Sequence Modeling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z3GCXVWT/S1077314215002325.html},
  journal = {Computer Vision and Image Understanding (CVIU)}
}

@inproceedings{liu_visual_2018,
  title = {Visual {{Recognition}} of {{Continuous Cued Speech Using}} a {{Tandem CNN}}-{{HMM Approach}}.},
  booktitle = {Interspeech},
  author = {Liu, Li and Hueber, Thomas and Feng, Gang and Beautemps, Denis},
  year = {2018},
  pages = {2643--2647}
}

@inproceedings{liwicki_automatic_2009,
  title = {Automatic Recognition of Fingerspelled Words in {{British Sign Language}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Liwicki, S. and Everingham, M.},
  year = {2009},
  month = jun,
  pages = {50--57},
  doi = {10.1109/CVPRW.2009.5204291},
  abstract = {We investigate the problem of recognizing words from video, fingerspelled using the British Sign Language (BSL) fingerspelling alphabet. This is a challenging task since the BSL alphabet involves both hands occluding each other, and contains signs which are ambiguous from the observer's viewpoint. The main contributions of our work include: (i) recognition based on hand shape alone, not requiring motion cues; (ii) robust visual features for hand shape recognition; (iii) scalability to large lexicon recognition with no re-training. We report results on a dataset of 1,000 low quality webcam videos of 100 words. The proposed method achieves a word recognition accuracy of 98.9\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Liwicki_Everingham_2009_Automatic recognition of fingerspelled words in British Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/73PB9VN9/5204291.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8QSIEAKK/abs_all.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=100;sl=BSL;capturedBy=vision,modal=2hshape,sl=BSL,slreco,slreco;modal=2hshape,vocab<500,vocab=100}
}

@inproceedings{liwicki_novel_2007,
  title = {A Novel Approach to On-Line Handwriting Recognition Based on Bidirectional Long Short-Term Memory Networks},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Liwicki, Marcus and Graves, Alex and Bunke, Horst and Schmidhuber, J{\"u}rgen},
  year = {2007},
  month = sep,
  address = {{Washington, DC, USA}},
  abstract = {In this paper we introduce a new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes. The approach uses a bidirectional recurrent neural network with the long short-term memory architecture. We use a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data. Our new system achieves a word recognition rate of 74.0 \%, compared with 65.4 \% using a previously developed HMMbased recognition system. 1.}
}

@article{loeding_progress_2004,
  title = {Progress in Automated Computer Recognition of Sign Language},
  author = {Loeding, B. and Sarkar, S. and Parashar, A. and Karshmer, A.},
  year = {2004},
  volume = {1},
  pages = {624--624},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Loeding et al_2004_Progress in automated computer recognition of sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2J2DG6XE/Loeding et al. - 2004 - Progress in automated computer recognition of sign.html},
  journal = {Computers Helping People with Special Needs}
}

@phdthesis{loefloef2015:phd,
  title = {Investigations on {{Acoustic Model Speaker Adaptation}}},
  author = {L{\"o}{\"o}f, Jonas},
  year = {2015},
  month = jul,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lööf_2015_Investigations on Acoustic Model Speaker Adaptation.pdf},
  school = {RWTH Aachen University}
}

@article{lopez-ludena_automatic_2012,
  title = {Automatic {{Categorization}} for {{Improving Spanish}} into {{Spanish Sign Language Machine Translation}}},
  author = {{L{\'o}pez-Lude{\~n}a}, Ver{\'o}nica and {San-Segundo}, Rub{\'e}n and Montero, Juan Manuel and C{\'o}rdoba, Ricardo and Ferreiros, Javier and Pardo, Jos{\'e} Manuel},
  year = {2012},
  month = jun,
  volume = {26},
  pages = {149--167},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2011.09.003},
  abstract = {This paper describes a preprocessing module for improving the performance of a Spanish into Spanish Sign Language (Lengua de Signos Espanola: LSE) translation system when dealing with sparse training data. This preprocessing module replaces Spanish words with associated tags. The list with Spanish words (vocabulary) and associated tags used by this module is computed automatically considering those signs that show the highest probability of being the translation of every Spanish word. This automatic tag extraction has been compared to a manual strategy achieving almost the same improvement. In this analysis, several alternatives for dealing with non-relevant words have been studied. Non-relevant words are Spanish words not assigned to any sign. The preprocessing module has been incorporated into two well-known statistical translation architectures: a phrase-based system and a Statistical Finite State Transducer (SFST). This system has been developed for a specific application domain: the renewal of Identity Documents and Driver's License. In order to evaluate the system a parallel corpus made up of 4080 Spanish sentences and their LSE translation has been used. The evaluation results revealed a significant performance improvement when including this preprocessing module. In the phrase-based system, the proposed module has given rise to an increase in BLEU (Bilingual Evaluation Understudy) from 73.8\% to 81.0\% and an increase in the human evaluation score from 0.64 to 0.83. In the case of SFST, BLEU increased from 70.6\% to 78.4\% and the human evaluation score from 0.65 to 0.82.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/López-Ludeña et al_2012_Automatic Categorization for Improving Spanish into Spanish Sign Language.pdf},
  journal = {Computer Speech \& Language},
  number = {3}
}

@inproceedings{lowe_object_1999,
  title = {Object {{Recognition}} from {{Local Scale}}-{{Invariant Features}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Lowe, David G.},
  year = {1999},
  month = sep,
  pages = {1150--1157},
  address = {{Kerkyra, Greece}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lowe_1999_Object Recognition from Local Scale-Invariant Features.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/46KZBCT4/abs_all.html},
  keywords = {linguistic}
}

@article{lu_collecting_2014,
  title = {Collecting and Evaluating the {{CUNY ASL}} Corpus for Research on {{American Sign Language}} Animation},
  author = {Lu, Pengfei and Huenerfauth, Matt},
  year = {2014},
  month = may,
  volume = {28},
  pages = {812--831},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2013.10.004},
  abstract = {While there is great potential for sign language animation generation software to improve the accessibility of information for deaf individuals with low written-language literacy, the understandability of current sign language animation systems is limited. Data-driven methodologies using annotated sign language corpora encoding detailed human movement have enabled some researchers to address several key linguistic challenges in ASL generation. This article motivates and describes our current research on collecting a motion-capture corpus of American Sign Language (ASL). As an evaluation of our motion-capture configuration, calibration, and recording protocol, we have conducted several rounds of evaluation studies with native ASL signers, and we have made use of our collected data to synthesize novel animations of ASL, which have also been evaluated in experimental studies with native signers.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lu_Huenerfauth_2014_Collecting and evaluating the CUNY ASL corpus for research on American Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TU95JH9M/S0885230813000879.html},
  journal = {Computer Speech and Language},
  number = {3}
}

@inproceedings{lu_cuny_2012,
  title = {{{CUNY American Sign Language}} Motion-Capture Corpus: First Release},
  shorttitle = {{{CUNY American Sign Language}} Motion-Capture Corpus},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Lu, Pengfei and Huenerfauth, Matt},
  year = {2012},
  month = may,
  pages = {109--116},
  address = {{Istanbul, Turkey}}
}

@article{lu_learning_2013,
  title = {Learning to Track and Identify Players from Broadcast Sports Videos},
  author = {Lu, Wei-Lwun and Ting, J.-A. and Little, James J. and Murphy, Kevin P.},
  year = {2013},
  volume = {35},
  pages = {1704--1716},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lu et al_2013_Learning to track and identify players from broadcast sports videos.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4QATW7K5/login.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {7}
}

@book{lucas_whats_2003,
  title = {What's Your Sign for Pizza?: An Introduction to Variation in {{American Sign Language}}},
  shorttitle = {What's Your Sign for Pizza?},
  author = {Lucas, Ceil and Bayley, Robert and Valli, Clayton},
  year = {2003},
  publisher = {{Gallaudet University Press}},
  address = {{Washington, D.C}},
  abstract = {"What's Your Sign for Pizza derives from an extensive seven-year research project in which more than 200 Deaf ASL users representing different age, gender, and ethnic groups from seven different regions were videotaped sharing their signs for everyday vocabulary. This useful text and its accompanying CD begins with an explanation of the basic concepts of language and the structure of sign language, since sign variation abides by the rules governing all human languages. Each part of the text concludes with questions for discussion, and the final section offers three supplemental readings that provide further information on variation in both spoken and signed languages. What's Your Sign for Pizza also briefly sketches the development of ASL, which explains the relationships between language varieties throughout the country."--BOOK JACKET.},
  isbn = {1-56368-144-7 978-1-56368-144-8},
  language = {English}
}

@article{lucey_aam_2006,
  title = {{{AAM}} Derived Face Representations for Robust Facial Action Recognition},
  author = {Lucey, Simon and Matthews, Iain and Hu, Changbo and Ambadar, Zara and {De la Torre}, Fernando and Cohn, Jeffrey},
  year = {2006},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lucey et al_2006_AAM derived face representations for robust facial action recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HNRJNJFC/139.html}
}

@phdthesis{lucey_audiovisual_2002,
  title = {Audio-Visual Speech Processing},
  author = {Lucey, Simon},
  year = {2002},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lucey_2002_Audio-visual speech processing.pdf},
  school = {Citeseer}
}

@article{lucey_automatically_2011,
  title = {Automatically {{Detecting Pain}} in {{Video Through Facial Action Units}}},
  author = {Lucey, P. and Cohn, J.F. and Matthews, I. and Lucey, S. and Sridharan, S. and Howlett, J. and Prkachin, K.M.},
  year = {2011},
  month = jun,
  volume = {41},
  pages = {664--674},
  issn = {1083-4419},
  doi = {10.1109/TSMCB.2010.2082525},
  abstract = {In a clinical setting, pain is reported either through patient self-report or via an observer. Such measures are problematic as they are: 1) subjective, and 2) give no specific timing information. Coding pain as a series of facial action units (AUs) can avoid these issues as it can be used to gain an objective measure of pain on a frame-by-frame basis. Using video data from patients with shoulder injuries, in this paper, we describe an active appearance model (AAM)-based system that can automatically detect the frames in video in which a patient is in pain. This pain data set highlights the many challenges associated with spontaneous emotion detection, particularly that of expression and head movement due to the patient's reaction to pain. In this paper, we show that the AAM can deal with these movements and can achieve significant improvements in both the AU and pain detection performance compared to the current-state-of-the-art approaches which utilize similarity-normalized appearance features only.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lucey et al_2011_Automatically Detecting Pain in Video Through Facial Action Units.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5D4TMVRV/abs_all.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  number = {3}
}

@inproceedings{lucey_extended_2010,
  title = {The {{Extended Cohn}}-{{Kanade Dataset}} ({{CK}}+): {{A}} Complete Dataset for Action Unit and Emotion-Specified Expression},
  shorttitle = {The {{Extended Cohn}}-{{Kanade Dataset}} ({{CK}}+)},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Lucey, P. and Cohn, J.F. and Kanade, T. and Saragih, J. and Ambadar, Z. and Matthews, I.},
  year = {2010},
  month = jun,
  pages = {94--101},
  doi = {10.1109/CVPRW.2010.5543262},
  abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22\% and the number of subjects by 27\%. The target expression for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Lucey et al/Lucey et al_2010_The Extended Cohn-Kanade Dataset (CK+).pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lucey et al_2010_The Extended Cohn-Kanade Dataset (CK+).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MSN96X2U/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XH8KWZQZ/cookiedetectresponse.html}
}

@inproceedings{luettin_asynchronous_2001,
  title = {Asynchronous Stream Modeling for Large Vocabulary Audio-Visual Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Luettin, Juergen and Potamianos, Gerasimos and Neti, Chalapathy},
  year = {2001},
  volume = {1},
  pages = {169--172},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Luettin et al_2001_Asynchronous stream modeling for large vocabulary audio-visual speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ETR42JFZ/abs_all.html}
}

@article{lugosch_using_2019,
  title = {Using {{Speech Synthesis}} to {{Train End}}-to-{{End Spoken Language Understanding Models}}},
  author = {Lugosch, Loren and Meyer, Brett and Nowrouzezahrai, Derek and Ravanelli, Mirco},
  year = {2019},
  month = oct,
  abstract = {End-to-end models are an attractive new approach to spoken language understanding (SLU) in which the meaning of an utterance is inferred directly from the raw audio without employing the standard pipeline composed of a separately trained speech recognizer and natural language understanding module. The downside of end-to-end SLU is that in-domain speech data must be recorded to train the model. In this paper, we propose a strategy for overcoming this requirement in which speech synthesis is used to generate a large synthetic training dataset from several artificial speakers. Experiments on two open-source SLU datasets confirm the effectiveness of our approach, both as a sole source of training data and as a form of data augmentation.},
  archivePrefix = {arXiv},
  eprint = {1910.09463},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/58MWVLZC/Lugosch et al. - 2019 - Using Speech Synthesis to Train End-to-End Spoken .pdf},
  journal = {arXiv:1910.09463 [eess]},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  language = {en},
  primaryClass = {eess}
}

@article{luo_utterancebased_2018,
  title = {Utterance-{{Based Audio Sentiment Analysis Learned}} by a {{Parallel Combination}} of {{CNN}} and {{LSTM}}},
  author = {Luo, Ziqian and Xu, Hua and Chen, Feiyang},
  year = {2018},
  month = nov,
  abstract = {Audio Sentiment Analysis is a popular research area which extends the conventional text-based sentiment analysis to depend on the effectiveness of acoustic features extracted from speech. However, current progress on audio sentiment analysis mainly focuses on extracting homogeneous acoustic features or doesn't fuse heterogeneous features effectively. In this paper, we propose an utterance-based deep neural network model, which has a parallel combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) based network, to obtain representative features termed Audio Sentiment Vector (ASV), that can maximally reflect sentiment information in an audio. Specifically, our model is trained by utterance-level labels and ASV can be extracted and fused creatively from two branches. In the CNN model branch, spectrum graphs produced by signals are fed as inputs while in the LSTM model branch, inputs include spectral features and cepstrum coefficient extracted from dependent utterances in an audio. Besides, Bidirectional Long Short-Term Memory (BiLSTM) with attention mechanism is used for feature fusion. Extensive experiments have been conducted to show our model can recognize audio sentiment precisely and quickly, and demonstrate our ASV are better than traditional acoustic features or vectors extracted from other deep learning models. Furthermore, experimental results indicate that the proposed model outperforms the state-of-the-art approach by 9.33\% on Multimodal Opinion-level Sentiment Intensity dataset (MOSI) dataset.},
  archivePrefix = {arXiv},
  eprint = {1811.08065},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Luo et al_2018_Utterance-Based Audio Sentiment Analysis Learned by a Parallel Combination of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/38WMZMIX/1811.html},
  journal = {arXiv:1811.08065 [cs, eess]},
  primaryClass = {cs, eess}
}

@incollection{luzardo_head_2013,
  title = {Head Pose Estimation for Sign Language Video},
  booktitle = {Image {{Analysis}}},
  author = {Luzardo, Marcos and Karppa, Matti and Laaksonen, Jorma and Jantunen, Tommi},
  year = {2013},
  pages = {349--360},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Luzardo et al_2013_Head pose estimation for sign language video.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7ZTBQ7GV/978-3-642-38886-6_34.html}
}

@inproceedings{lv_recognition_2006,
  title = {Recognition and Segmentation of 3-d Human Action Using Hmm and Multi-Class Adaboost},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Lv, Fengjun and Nevatia, Ramakant},
  year = {2006},
  pages = {359--372},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lv_Nevatia_2006_Recognition and segmentation of 3-d human action using hmm and multi-class.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KM4QV8X8/11744085_28.html}
}

@article{lyons_automatic_1999,
  title = {Automatic Classification of Single Facial Images},
  author = {Lyons, Michael J. and Budynek, Julien and Akamatsu, Shigeru},
  year = {1999},
  volume = {21},
  pages = {1357--1362},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Lyons et al_1999_Automatic classification of single facial images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RMZ2TEWP/i1357.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {12}
}

@inproceedings{ma_continuous_2000,
  title = {A Continuous {{Chinese}} Sign Language Recognition System},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Ma, Jiyong and Gao, Wen and Wu, Jiangqin and Wang, Chunli},
  year = {2000},
  pages = {428--433},
  doi = {10.1109/AFGR.2000.840670},
  abstract = {We describe a system for recognizing both the isolated and continuous Chinese sign language (CSL) using two cybergloves and two 3SAPCE-position trackers as gesture input devices. To get robust gesture features, each joint-angle collected by cybergloves is normalized. The relative position and orientation of the left hand to those of the right hand are proposed as the signer position-independent features. To speed up the recognition process, fast match and frame prediction techniques are proposed. To tackle the epenthesis movement problem, context-dependent models are obtained by the dynamic programming (DP) technique. HMM are utilized to model basic word units. Then we describe training techniques of the bigram language model and the search algorithm used in our baseline system. The baseline system converts sentence level gestures into synthesis speech and gestures of a 3D virtual human synchronously. Experiments show that these techniques are efficient both in recognition speed and recognition performance},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ma et al_2000_A continuous Chinese sign language recognition system.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U8KMVPD3/840670.html}
}

@incollection{ma_parallel_2000,
  title = {A {{Parallel Multistream Model}} for {{Integration}} of {{Sign Language Recognition}} and {{Lip Motion}}},
  booktitle = {Advances in {{Multimodal Interfaces}} \textemdash{} {{ICMI}} 2000},
  author = {Ma, Jiyong and Gao, Wen and Wang, Rui},
  editor = {Tan, Tieniu and Shi, Yuanchun and Gao, Wen},
  year = {2000},
  pages = {582--589},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The parallel multistream model is proposed for integration sign language recognition and lip motion. The different time scales existing in sign language and lip motion can be tackled well using this approach. Primary experimental results have shown that this approach is efficient for integration of sign language recognition and lip motion. The promising results indicated that parallel multistream model can be a good solution in the framework of multimodal data fusion. An approach to recognize sign language with scalability with the size of vocabulary and a fast approach to locate lip corners are also proposed in this paper.},
  copyright = {\textcopyright 2000 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ma et al_2000_A Parallel Multistream Model for Integration of Sign Language Recognition and.pdf},
  isbn = {978-3-540-41180-2 978-3-540-40063-9},
  keywords = {capturedBy=electronicGlove,capturedBy=vision,isolated,isolated;vocab=5177;sl=CSL;capturedBy=electronicGlove;capturedBy=vision,modal=2hlocation,modal=2horientation,sl=CSL,slreco,slreco;modal=2horientation;modal=2hlocation,vocab=5177,vocab>=1000},
  language = {en},
  number = {1948},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{ma_signfi_2018,
  title = {{{SignFi}}: {{Sign Language Recognition Using WiFi}}},
  shorttitle = {{{SignFi}}},
  author = {Ma, Yongsen and Zhou, Gang and Wang, Shuangquan and Zhao, Hongyang and Jung, Woosub},
  year = {2018},
  month = mar,
  volume = {2},
  pages = {23:1--23:21},
  issn = {2474-9567},
  doi = {10.1145/3191755},
  abstract = {We propose SignFi to recognize sign language gestures using WiFi. SignFi uses Channel State Information (CSI) measured by WiFi packets as the input and a Convolutional Neural Network (CNN) as the classification algorithm. Existing WiFi-based sign gesture recognition technologies are tested on no more than 25 gestures that only involve hand and/or finger gestures. SignFi is able to recognize 276 sign gestures, which involve the head, arm, hand, and finger gestures, with high accuracy. SignFi collects CSI measurements to capture wireless signal characteristics of sign gestures. Raw CSI measurements are pre-processed to remove noises and recover CSI changes over sub-carriers and sampling time. Pre-processed CSI measurements are fed to a 9-layer CNN for sign gesture classification. We collect CSI traces and evaluate SignFi in the lab and home environments. There are 8,280 gesture instances, 5,520 from the lab and 2,760 from the home, for 276 sign gestures in total. For 5-fold cross validation using CSI traces of one user, the average recognition accuracy of SignFi is 98.01\%, 98.91\%, and 94.81\% for the lab, home, and lab+home environment, respectively. We also run tests using CSI traces from 5 different users in the lab environment. The average recognition accuracy of SignFi is 86.66\% for 7,500 instances of 150 sign gestures performed by 5 different users.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ma et al_2018_SignFi.pdf},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  keywords = {capturedBy=wifi,isolated,isolated;vocab=276;signer=5;sl=ASL;capturedBy=wifi,modal=2hlocation,modal=shape,signer=5,sl=ASL,slreco,slreco;modal=2hlocation;modal=shape,vocab<500,vocab=276},
  number = {1}
}

@inproceedings{ma_unsupervised_2008,
  title = {Unsupervised {{Versus Supervised Training}} of {{Acoustic Models}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Ma, J. and Schwartz, R.},
  year = {2008},
  address = {{Brisbane, Australia}}
}

@phdthesis{macherey_discriminative_2010,
  title = {Discriminative {{Training}} and {{Acoustic Modeling}} for {{Automatic Speech Recognition}}},
  author = {Macherey, Wolfgang},
  year = {2010},
  month = mar,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Macherey_2010_Discriminative Training and Acoustic Modeling for Automatic Speech Recognition.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{macherey_languageindependent_2011,
  title = {Language-Independent Compound Splitting with Morphological Operations},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Macherey, Klaus and Dai, Andrew and Talbot, David and Popat, Ashok and Och, Franz},
  year = {2011},
  month = jun,
  pages = {1395--1404},
  publisher = {{Association for Computational Linguistics}},
  address = {{Portland, Oregon, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Macherey et al_2011_Language-independent compound splitting with morphological operations.pdf}
}

@inproceedings{maebatake_sign_2008,
  title = {Sign {{Language Recognition Based}} on {{Position}} and {{Movement Using Multi}}-{{Stream HMM}}},
  booktitle = {2008 {{Second International Symposium}} on {{Universal Communication}}},
  author = {Maebatake, Masaru and Suzuki, Iori and Nishida, Masafumi and Horiuchi, Yasuo and Kuroiwa, Shingo},
  year = {2008},
  month = dec,
  pages = {478--481},
  address = {{Osaka, Japan}},
  doi = {10.1109/ISUC.2008.56},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Maebatake et al_2008_Sign Language Recognition Based on Position and Movement Using Multi-Stream HMM.pdf},
  isbn = {978-0-7695-3433-6},
  keywords = {capturedBy=mocap,continuous,continuous;vocab=183;signer=4;sl=JSL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,signer=4,sl=JSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<500,vocab=183}
}

@inproceedings{mairal_online_2009,
  title = {Online Dictionary Learning for Sparse Coding},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Learning}} ({{ICML}})},
  author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
  year = {2009},
  pages = {689--696},
  publisher = {{ACM}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mairal et al_2009_Online dictionary learning for sparse coding.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JCCTKQEZ/citation.html}
}

@article{malaia_information_2018,
  title = {Information {{Transfer Capacity}} of {{Articulators}} in {{American Sign Language}}},
  author = {Malaia, Evie and Borneman, Joshua D. and Wilbur, Ronnie B.},
  year = {2018},
  month = mar,
  volume = {61},
  pages = {97--112},
  issn = {0023-8309},
  doi = {10.1177/0023830917708461},
  abstract = {The ability to convey information is a fundamental property of communicative signals. For sign languages, which are overtly produced with multiple, completely visible articulators, the question arises as to how the various channels co-ordinate and interact with each other. We analyze motion capture data of American Sign Language (ASL) narratives, and show that the capacity of information throughput, mathematically defined, is highest on the dominant hand (DH). We further demonstrate that information transfer capacity is also significant for the non-dominant hand (NDH), and the head channel too, as compared to control channels (ankles). We discuss both redundancy and independence in articulator motion in sign language, and argue that the NDH and the head articulators contribute to the overall information transfer capacity, indicating that they are neither completely redundant to, nor completely independent of, the DH.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Malaia et al_2018_Information Transfer Capacity of Articulators in American Sign Language.pdf},
  journal = {Language and Speech},
  language = {en},
  number = {1}
}

@inproceedings{malmasi_german_2017,
  title = {German Dialect Identification in Interview Transcriptions},
  booktitle = {Proceedings of the {{Fourth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}} ({{VarDial}})},
  author = {Malmasi, Shervin and Zampieri, Marcos},
  year = {2017},
  pages = {164--169},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3MVUJHQ3/Malmasi and Zampieri - 2017 - German dialect identification in interview transcr.pdf}
}

@phdthesis{mandel_phonotactics_1981,
  title = {Phonotactics and {{Morphophonology}} in {{American Sign Language}}},
  author = {Mandel, Mark},
  year = {1981},
  address = {{Berkeley, CA, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mandel_1981_Phonotactics and Morphophonology in American Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3XAR59GT/90v1j5kx.html},
  language = {en},
  school = {University of California, Berkeley}
}

@inproceedings{manohar_semisupervised_2015,
  title = {Semi-Supervised Maximum Mutual Information Training of Deep Neural Network Acoustic Models.},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Manohar, Vimal and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2015},
  pages = {2630--2634},
  address = {{Dresden, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Manohar et al_2015_Semi-supervised maximum mutual information training of deep neural network.pdf}
}

@inproceedings{manohar_semisupervised_2018,
  title = {Semi-{{Supervised Training}} of {{Acoustic Models Using Lattice}}-{{Free MMI}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Manohar, V. and Hadian, H. and Povey, D. and Khudanpur, S.},
  year = {2018},
  month = apr,
  pages = {4844--4848},
  doi = {10.1109/ICASSP.2018.8462331},
  abstract = {The lattice-free MMI objective (LF-MMI) has been used in supervised training of state-of-the-art neural network acoustic models for automatic speech recognition (ASR). With large amounts of unsupervised data available, extending this approach to the semi-supervised scenario is of significance. Finite-state transducer (FST) based supervision used with LF-MMI provides a natural way to incorporate uncertainties when dealing with unsupervised data. In this paper, we describe various extensions to standard LF-MMI training to allow the use as supervision of lattices obtained via decoding of unsupervised data. The lattices are rescored with a strong LM. We investigate different methods for splitting the lattices and incorporating frame tolerances into the supervision FST. We report results on different subsets of Fisher English, where we achieve WER recovery of 59-64\% using lattice supervision, which is significantly better than using just the best path transcription.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Manohar et al_2018_Semi-Supervised Training of Acoustic Models Using Lattice-Free MMI.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QRN99SQ3/8462331.html},
  keywords = {Acoustics,automatic speech recognition,Automatic speech recognition,Decoding,finite-state transducer,finite-state transducer based supervision,Fisher English,lattice supervision,Lattice-free MMI,lattice-free MMI objective,Lattices,learning (artificial intelligence),natural language processing,neural nets,neural network acoustic models,Neural networks,path transcription,Semi-supervised training,semisupervised scenario,semisupervised training,Sequence training,speech recognition,standard LF-MMI training,Standards,supervised training,supervision FST,Training}
}

@phdthesis{mansour2017:phd,
  title = {Domain {{Adaptation}} for {{Statistical Machine Translation}}},
  author = {Mansour, Saab},
  year = {2017},
  month = may,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mansour_2017_Domain Adaptation for Statistical Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@article{mao_deep_2014,
  title = {Deep Captioning with Multimodal Recurrent Neural Networks (m-Rnn)},
  author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
  year = {2014},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mao et al_2014_Deep captioning with multimodal recurrent neural networks (m-rnn).pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/W7F4U5ZV/1412.html},
  journal = {arXiv preprint arXiv:1412.6632}
}

@article{maqqor_multistream_2013,
  title = {A Multi-Stream Hmm Approach to Offline Handwritten Arabic Word Recognition},
  author = {Maqqor, Ahlam and Halli, Akram and Satori, Khaled},
  year = {2013},
  month = aug,
  volume = {2},
  pages = {21--33},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Maqqor et al_2013_A multi-stream hmm approach to offline handwritten arabic word recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2G57ZKQ2/1309.html},
  journal = {International Journal on Natural Language Computing}
}

@inproceedings{maraqa_recognition_2008,
  title = {Recognition of {{Arabic Sign Language}} ({{ArSL}}) Using Recurrent Neural Networks},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Applications}} of {{Digital Information}} and {{Web Technologies}} ({{ICADIWT}})},
  author = {Maraqa, M. and {Abu-Zaiter}, R.},
  year = {2008},
  month = aug,
  pages = {478--481},
  doi = {10.1109/ICADIWT.2008.4664396},
  abstract = {The objective of this paper is to introduce the use of two different recurrent neural networks in human hand gesture recognition for static images. Because neural networks are a promising tool for many human computer interaction applications, this paper focuses on the ability of neural networks to assist in Arabic Sign Language(ArSL) hand gesture recognition. We have introduced the steps of our proposed system and have presented the Elmanpsilas model as a partially recurrent architecture and a fully connected network with recurrent links that is believed to help the network to converge and gain stability, then we have tested it in an experiment held for this; the results of the experiment have showed that the suggested system with the fully recurrent architecture has had a performance with an accuracy rate 95\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Maraqa_Abu-Zaiter_2008_Recognition of Arabic Sign Language (ArSL) using recurrent neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4WJI75MN/4664396.html},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=30;signer=2;sl=ArSL;capturedBy=coloredGlove,modal=shape,signer=2,sl=ArSL,slreco,slreco;modal=shape,vocab<50,vocab=30}
}

@inproceedings{marcel_hand_2000,
  title = {Hand Gesture Recognition Using Input-Output Hidden Markov Models},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Marcel, S{\'e}bastien and Bernier, Olivier and Viallet, J.-E. and Collobert, Daniel},
  year = {2000},
  month = mar,
  pages = {456--461},
  address = {{Grenoble, France}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Marcel et al_2000_Hand gesture recognition using input-output hidden markov models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VAE9WG6T/840674.html}
}

@inproceedings{mardanbegi_eyebased_2012,
  title = {Eye-Based Head Gestures},
  booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  author = {Mardanbegi, Diako and Hansen, Dan Witzner and Pederson, Thomas},
  year = {2012},
  pages = {139--146},
  publisher = {{ACM}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mardanbegi et al_2012_Eye-based head gestures.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TN2UFX4R/citation.html}
}

@phdthesis{marek_analysis_2006,
  title = {Analysis of {{German Compounds Using Weighted Finite State Transducers}}},
  author = {Marek, Torsten},
  year = {2006},
  address = {{Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Marek_Analysis of German Compounds Using Weighted Finite State Transducers.pdf},
  language = {en},
  school = {Eberhard-Karls-Universit\"at T\"ubingen}
}

@article{marshall_deaf_2015,
  title = {Deaf Children's Non-Verbal Working Memory Is Impacted by Their Language Experience},
  author = {Marshall, Chlo{\"e} and Jones, Anna and Denmark, Tanya and Mason, Kathryn and Atkinson, Joanna and Botting, Nicola and Morgan, Gary},
  year = {2015},
  month = may,
  volume = {6},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00527},
  abstract = {Several recent studies have suggested that deaf children perform more poorly on working memory tasks compared to hearing children, but these studies have not been able to determine whether this poorer performance arises directly from deafness itself or from deaf children's reduced language exposure. The issue remains unresolved because findings come mostly from (1) tasks that are verbal as opposed to non-verbal, and (2) involve deaf children who use spoken communication and therefore may have experienced impoverished input and delayed language acquisition. This is in contrast to deaf children who have been exposed to a sign language since birth from Deaf parents (and who therefore have native language-learning opportunities within a normal developmental timeframe for language acquisition). A more direct, and therefore stronger, test of the hypothesis that the type and quality of language exposure impact working memory is to use measures of non-verbal working memory (NVWM) and to compare hearing children with two groups of deaf signing children: those who have had native exposure to a sign language, and those who have experienced delayed acquisition and reduced quality of language input compared to their native-signing peers. In this study we investigated the relationship between NVWM and language in three groups aged 6\textendash 11 years: hearing children (n = 28), deaf children who were native users of British Sign Language (BSL; n = 8), and deaf children who used BSL but who were not native signers (n = 19). We administered a battery of non-verbal reasoning, NVWM, and language tasks. We examined whether the groups differed on NVWM scores, and whether scores on language tasks predicted scores on NVWM tasks. For the two executive-loaded NVWM tasks included in our battery, the non-native signers performed less accurately than the native signer and hearing groups (who did not differ from one another). Multiple regression analysis revealed that scores on the vocabulary measure predicted scores on those two executive-loaded NVWM tasks (with age and non-verbal reasoning partialled out). Our results suggest that whatever the language modality\textemdash spoken or signed\textemdash rich language experience from birth, and the good language skills that result from this early age of acquisition, play a critical role in the development of NVWM and in performance on NVWM tasks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Marshall et al_2015_Deaf children's non-verbal working memory is impacted by their language.pdf},
  journal = {Frontiers in Psychology},
  pmcid = {PMC4419661},
  pmid = {25999875}
}

@phdthesis{Martin:PhD:2000,
  title = {Statistische {{Auswahl}} von {{Wortabh\"angigkeiten}} in Der Automatischen {{Spracherkennung}}},
  author = {Martin, Sven C.},
  year = {2000},
  month = feb,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Martin_2000_Statistische Auswahl von Wortabhängigkeiten in der automatischen Spracherkennung.pdf},
  ps = {https://www-i6.informatik.rwth-aachen.de/publications/downloader.php?id=414row=ps},
  school = {RWTH Aachen University}
}

@article{martinez_pca_2001,
  title = {{{PCA}} versus {{LDA}}},
  author = {Martinez, Aleix M. and Kak, A.C.},
  year = {2001},
  month = feb,
  volume = {23},
  pages = {228--233},
  issn = {0162-8828},
  doi = {10.1109/34.908974},
  abstract = {In the context of the appearance-based paradigm for object recognition, it is generally believed that algorithms based on LDA (linear discriminant analysis) are superior to those based on PCA (principal components analysis). In this communication, we show that this is not always the case. We present our case first by using intuitively plausible arguments and, then, by showing actual results on a face database. Our overall conclusion is that when the training data set is small, PCA can outperform LDA and, also, that PCA is less sensitive to different training data sets},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Martinez_Kak_2001_PCA versus LDA.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F8NPVJCX/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {2}
}

@inproceedings{martinez-camarena_sign_2015,
  title = {Towards Sign Language Recognition Based on Body Parts Relations},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {{Martinez-Camarena}, M. and Oramas, M. J. and Tuytelaars, T.},
  year = {2015},
  month = sep,
  pages = {2454--2458},
  doi = {10.1109/ICIP.2015.7351243},
  abstract = {Over the years, hand gesture recognition has been mostly addressed considering hand trajectories in isolation. However, in most sign languages, hand gestures are defined on a particular context (body region). We propose a pipeline which models hand movements in the context of other parts of the body captured in the 3D space using the Kinect sensor. In addition, we perform sign recognition based on the different hand postures that occur during a sign. Our experiments show that considering different body parts brings improved performance when compared with methods which only consider global hand trajectories. Finally, we demonstrate that the combination of hand postures features with hand gestures features helps to improve the prediction of a given sign.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Martinez-Camarena et al_2015_Towards sign language recognition based on body parts relations.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4VWEXQM8/abs_all.html}
}

@article{martino_signing_2017,
  title = {Signing Avatars: Making Education More Inclusive},
  shorttitle = {Signing Avatars},
  author = {Martino, Jos{\'e} Mario De and Silva, Ivani Rodrigues and Bolognini, Carmen Zink and Costa, Paula Dornhofer Paro and Kumada, Kate Mamhy Oliveira and Coradine, Luis Cl{\'a}udius and Brito, Patrick Henrique da Silva and do Amaral, Wanessa Machado and Benetti, {\^A}ngelo Brand{\~a}o and Poeta, Enzo Telles and Angare, Leandro Martin Guertzenstein and Ferreira, Carolina Monteiro and Conti, Davi Faria De},
  year = {2017},
  month = aug,
  volume = {16},
  pages = {793--808},
  issn = {1615-5289, 1615-5297},
  doi = {10.1007/s10209-016-0504-x},
  abstract = {In Brazil, there are approximately 9.7 million inhabitants who are deaf or hard of hearing. Moreover, about 30\% of the Brazilian deaf community is illiterate in Brazilian Portuguese due to difficulties to offer deaf children an inclusive environment based on bilingual education. Currently, the prevailing teaching practice depends heavily on verbal language and on written material, making the inclusion of the deaf a challenging task. This paper presents the author's approach for tackling this problem and improving deaf students' accessibility to written material in order to help them master Brazilian Portuguese as a second language. We describe an ongoing project aimed at developing an automatic Brazilian Portuguese-to-Libras translation system that presents the translated content via an animated virtual human, or avatar. The paper describes the methodology adopted to compile a source language corpus having the deaf student needs in central focus. It also describes the construction of a parallel Brazilian Portuguese/Brazilian Sign Language (Libras) corpus based on motion capture technology. The envisioned translation architecture includes the definition of an Intermediate Language to drive the signing avatar. The results of a preliminary assessment of signs intelligibility highlight the application potential.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Martino et al_2017_Signing avatars.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PXX4Z8QF/s10209-016-0504-x.html},
  journal = {Universal Access in the Information Society},
  language = {en},
  number = {3}
}

@article{martins_accessible_2015,
  title = {Accessible {{Options}} for {{Deaf People}} in E-{{Learning Platforms}}: {{Technology Solutions}} for {{Sign Language Translation}}},
  shorttitle = {Accessible {{Options}} for {{Deaf People}} in E-{{Learning Platforms}}},
  author = {Martins, Paulo and Rodrigues, Henrique and Rocha, T{\^a}nia and Francisco, Manuela and Morgado, Leonel},
  year = {2015},
  volume = {67},
  pages = {263--272},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2015.09.270},
  abstract = {This paper presents a study on potential technology solutions for enhancing the communication process for deaf people on e-learning platforms through translation of Sign Language (SL). Considering SL in its global scope as a spatial-visual language not limited to gestures or hand/forearm movement, but also to other non-dexterity markers such as facial expressions, it is necessary to ascertain whether the existing technology solutions can be effective options for the SL integration on e-learning platforms. Thus, we aim to present a list of potential technology options for the recognition, translation and presentation of SL (and potential problems) through the analysis of assistive technologies, methods and techniques, and ultimately to contribute for the development of the state of the art and ensure digital inclusion of the deaf people in e-learning platforms. The analysis show that some interesting technology solutions are under research and development to be available for digital platforms in general, but yet some critical challenges must solved and an effective integration of these technologies in e-learning platforms in particular is still missing.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Martins et al/Martins et al_2015_Accessible Options for Deaf People in e-Learning Platforms.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Martins et al_2015_Accessible Options for Deaf People in e-Learning Platforms.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6GWZ62A6/S1877050915031166.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AWTF5HN2/S1877050915031166.html},
  journal = {Procedia Computer Science},
  series = {Proc. {{Int}}.  {{Conf}}. on {{Software Development}} and {{Technologies}} for {{Enhancing Accessibility}} and {{Fighting Info}}-Exclusion}
}

@phdthesis{martins_dynamic_2008,
  title = {Dynamic {{Language Modeling}} for \{\vphantom\}{{European Portuguese}}\vphantom\{\}},
  author = {Martins, C. A},
  year = {2008},
  school = {University of Aveiro, Portugal}
}

@article{mase_recognition_1991,
  title = {Recognition of {{Facial Expression}} from {{Optical Flow}}},
  author = {Mase, Kenji},
  year = {1991},
  month = oct,
  volume = {E74-D},
  pages = {3474--3483},
  issn = {, 0916-8532},
  abstract = {We present a method that uses optical flow to estimate facial muscle actions which can then be recognized as facial expressions. Facial expressions are the result of facial muscle actions which are triggered by the nerve impulses generated by emotions. The muscle actions cause the movement and deformation of facial skin and facial features such as eyes, mouth and nose. Since facial skin has the texture of a fine-grained organ, which helps in extracting the optical flow, we can extract muscle actions from external appearance. We are thus able to construct a facial expression recognition system based on optical flow data. We investigate the recogniton method in two ways. First, the optical-flow fields of skin movement is evaluated in muscle winsows, each of which defines one primary direction of muscle contraction to correctly extract muscle movement. Second, a fifteen dimensional feature vector is used to represent the most active points in terms of the flow variance through time and local spatial areas. The expression recognition system uses the feature vector to categorize the image sequences into several classes of facial expression. Preliminary experiments indicate an accuracy of approximately 80\% when recognizing four types expressions: happiness, anger, disgust, and surprise.},
  journal = {IEICE Transactions on Information and Systems},
  number = {10}
}

@inproceedings{matejka_brno_2006,
  title = {Brno {{University}} of {{Technology System}} for \{\vphantom\}{{NIST}}\vphantom\{\} 2005 {{Language Recognition Evaluation}}},
  booktitle = {{{IEEE Odyssey}} 2006: {{The Speaker}} and {{Language Recognition Workshop}}},
  author = {Mat{\v e}jka, P. and Burget, L. and Schwarz, P. and {\v C}ernocky, J.},
  year = {2006},
  pages = {1--7}
}

@inproceedings{mathur_sign_2018,
  title = {Sign {{Language Gesture Recognition}} Using {{Zernike Moments}} and {{DTW}}},
  booktitle = {2018 5th {{International Conference}} on {{Signal Processing}} and {{Integrated Networks}} ({{SPIN}})},
  author = {Mathur, S. and Sharma, P.},
  year = {2018},
  month = feb,
  pages = {586--591},
  doi = {10.1109/SPIN.2018.8474179},
  abstract = {Since the last few decades, a dominant area of research in the vision community has been the gesture recognition, mainly for the purpose of Human Computer Interaction (HCI) and recognition of sign language. In this paper, we are using Zernike Moments as shape descriptors. The proposed system for recognizing sign language mainly consists of following five modules: (1) gesture segmentation based on motion detection analysis, (2) real time detection of both hand regions and face region, (3) key frame extraction for removing redundant frames, (4) the feature extraction phase consists of tracking the hands trajectory in terms of orientation, tracking distance of hands from the centre of the face and determining the hand posture using rotation invariant Zernike Moments and finally (5) gesture recognition based on these extracted features using Dynamic Time Warping (DTW) methodology.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mathur_Sharma_2018_Sign Language Gesture Recognition using Zernike Moments and DTW.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BIAJ88HG/8474179.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XCLFDS4F/8474179.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=32;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=2horientation;modal=2hmovement,vocab<50,vocab=32}
}

@inproceedings{matsuo_recognition_1997,
  title = {The Recognition Algorithm with Non-Contact for {{Japanese}} Sign Language Using Morphological Analysis},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Matsuo, Hideaki and Igi, Seiji and Lu, Shan and Nagashima, Yuji and Takata, Yuji and Teshima, Terutaka},
  editor = {Wachsmuth, Ipke and Fr{\"o}hlich, Martin},
  year = {1997},
  month = sep,
  pages = {273--284},
  address = {{Bielefeld, Germany}},
  abstract = {This paper documents the recognition method of deciphering Japanese sign language(JSL) using projected images. The goal of the movement recognition is to foster communication between hearing impaired and people capable of normal speech. We uses a stereo camera for recording three-dimensional movements, a image processing board for tracking movements, and a personal computer for an image processor charting the recognition of JSL patterns. This system works by formalizing the space area of the signers according to the characteristics of the human body, determining components such as location and movements, and then recognizing sign language patterns.The system is able to recognize JSL by determining the extent of similarities in the sign field, and does so even when vibrations in hand movements occur and when there are differences in body build. We obtained useful results from recognition experiments in 38 different JSL in two signers.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Matsuo et al_1998_The recognition algorithm with non-contact for Japanese sign language using.pdf},
  isbn = {978-3-540-69782-4},
  keywords = {capturedBy=stereocamera,capturedBy=vision,isolated,isolated;vocab=38;signer=1;sl=JSL;capturedBy=vision;capturedBy=stereocamera,modal=2hlocation,modal=2hmovement,signer=1,sl=JSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=38},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{matthes_dictasignbuilding_2012,
  title = {Dicta-{{Sign}}\textendash Building a Multilingual Sign Language Corpus},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Matthes, Silke and Hanke, Thomas and Regen, Anja and Storz, Jakob and Worseck, Satu and Efthimiou, Eleni and Dimou, Athanasia-Lida and Braffort, Annelies and Glauert, John and Safar, Eva},
  year = {2012},
  month = may,
  pages = {117--122},
  publisher = {{Istanbul, Turkey}}
}

@article{matthews_active_2004,
  title = {Active Appearance Models Revisited},
  author = {Matthews, Iain and Baker, Simon},
  year = {2004},
  volume = {60},
  pages = {135--164},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Matthews_Baker_2004_Active appearance models revisited.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9UV4IUB4/BVISI.0000029666.37597.html},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {2}
}

@article{matthews_extraction_2002,
  title = {Extraction of Visual Features for Lipreading},
  author = {Matthews, Iain and Cootes, Timothy F. and Bangham, J. Andrew and Cox, Stephen and Harvey, Richard},
  year = {2002},
  volume = {24},
  pages = {198--213},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Matthews et al_2002_Extraction of visual features for lipreading.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7TND5RNS/login.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {2}
}

@phdthesis{matthews_features_1998,
  title = {Features for Audio-Visual Speech Recognition},
  author = {Matthews, Iain},
  year = {1998},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Matthews_1998_Features for audio-visual speech recognition.pdf},
  school = {University of East Anglia}
}

@article{mattheyses_audiovisual_2015,
  title = {Audiovisual Speech Synthesis: {{An}} Overview of the State-of-the-Art},
  shorttitle = {Audiovisual Speech Synthesis},
  author = {Mattheyses, Wesley and Verhelst, Werner},
  year = {2015},
  month = feb,
  volume = {66},
  pages = {182--217},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2014.11.001},
  abstract = {We live in a world where there are countless interactions with computer systems in every-day situations. In the most ideal case, this interaction feels as familiar and as natural as the communication we experience with other humans. To this end, an ideal means of communication between a user and a computer system consists of audiovisual speech signals. Audiovisual text-to-speech technology allows the computer system to utter any spoken message towards its users. Over the last decades, a wide range of techniques for performing audiovisual speech synthesis has been developed. This paper gives a comprehensive overview on these approaches using a categorization of the systems based on multiple important aspects that determine the properties of the synthesized speech signals. The paper makes a clear distinction between the techniques that are used to model the virtual speaker and the techniques that are used to generate the appropriate speech gestures. In addition, the paper discusses the evaluation of audiovisual speech synthesizers, it elaborates on the hardware requirements for performing visual speech synthesis and it describes some important future directions that should stimulate the use of audiovisual speech synthesis technology in real-life applications.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mattheyses_Verhelst_2015_Audiovisual speech synthesis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/329IS7SQ/S0167639314000818.html},
  journal = {Speech Communication}
}

@article{mattheyses_comprehensive_2013,
  title = {Comprehensive Many-to-Many Phoneme-to-Viseme Mapping and Its Application for Concatenative Visual Speech Synthesis},
  author = {Mattheyses, Wesley and Latacz, Lukas and Verhelst, Werner},
  year = {2013},
  month = sep,
  volume = {55},
  pages = {857--876},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.02.005},
  abstract = {The use of visemes as atomic speech units in visual speech analysis and synthesis systems is well-established. Viseme labels are determined using a many-to-one phoneme-to-viseme mapping. However, due to visual coarticulation effects, an accurate mapping from phonemes to visemes should define a many-to-many mapping scheme instead. In this research it was found that neither the use of standardized nor speaker-dependent many-to-one viseme labels could satisfy the quality requirements of concatenative visual speech synthesis. Therefore, a novel technique to define a many-to-many phoneme-to-viseme mapping scheme is introduced, which makes use of both tree-based and k-means clustering approaches. We show that these many-to-many viseme labels more accurately describe the visual speech information as compared to both phoneme-based and many-to-one viseme-based speech labels. In addition, we found that the use of these many-to-many visemes improves the precision of the segment selection phase in concatenative visual speech synthesis using limited speech databases. Furthermore, the resulting synthetic visual speech was both objectively and subjectively found to be of higher quality when the many-to-many visemes are used to describe the speech database and the synthesis targets.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mattheyses et al_2013_Comprehensive many-to-many phoneme-to-viseme mapping and its application for.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SAU59T7C/S0167639313000319.html},
  journal = {Speech Communication},
  number = {7\textendash 8}
}

@article{mattys_stimulusbased_2002,
  title = {Stimulus-Based Lexical Distinctiveness as a General Word-Recognition Mechanism},
  author = {Mattys, Sven L. and Bernstein, Lynne E. and Auer, Edward T.},
  year = {2002},
  month = may,
  volume = {64},
  pages = {667--679},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03194734},
  abstract = {Word recognition is generally assumed to be achieved via competition in the mental lexicon between phonetically similar word forms. However, this process has so far been examined only in the context of auditory phonetic similarity. In the present study, we investigated whether the influence of wordform similarity on word recognition holds in the visual modality and with the patterns of visual phonetic similarity. Deaf and hearing participants identified isolated spoken words presented visually on a video monitor. On the basis of computational modeling of the lexicon from visual confusion matrices of visual speech syllables, words were chosen to vary in visual phonetic distinctiveness, ranging from visually unambiguous (lexical equivalence class [LEC] size of 1) to highly confusable (LEC size greater than 10). Identification accuracy was found to be highly related to the word LEC size and frequency of occurrence in English. Deaf and hearing participants did not differ in their sensitivity to word LEC size and frequency. The results indicate that visual spoken word recognition shows strong similarities with its auditory counterpart in that the same dependencies on lexical similarity and word frequency are found to influence visual speech recognition accuracy. In particular, the results suggest that stimulus-based lexical distinctiveness is a valid construct to describe the underlying machinery of both visual and auditory spoken word recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mattys et al_2002_Stimulus-based lexical distinctiveness as a general word-recognition mechanism.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XNSHE7MS/BF03194734.html},
  journal = {Perception \& Psychophysics},
  language = {en},
  number = {4}
}

@phdthesis{matusov09:phd,
  title = {Combining {{Natural Language Processing Systems}} to {{Improve Machine Translation}} of {{Speech}}},
  author = {Matusov, Evgeny},
  year = {2009},
  month = dec,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Matusov_2009_Combining Natural Language Processing Systems to Improve Machine Translation of.pdf},
  school = {RWTH Aachen University}
}

@article{mayberry_reading_2011,
  title = {Reading {{Achievement}} in {{Relation}} to {{Phonological Coding}} and {{Awareness}} in {{Deaf Readers}}: {{A Meta}}-Analysis},
  shorttitle = {Reading {{Achievement}} in {{Relation}} to {{Phonological Coding}} and {{Awareness}} in {{Deaf Readers}}},
  author = {Mayberry, Rachel I. and {del Giudice}, Alex A. and Lieberman, Amy M.},
  year = {2011},
  month = mar,
  volume = {16},
  pages = {164--188},
  issn = {1081-4159},
  doi = {10.1093/deafed/enq049},
  abstract = {Abstract.  The relation between reading ability and phonological coding and awareness (PCA) skills in individuals who are severely and profoundly deaf was inves},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mayberry et al_2011_Reading Achievement in Relation to Phonological Coding and Awareness in Deaf.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BDD62J66/364821.html},
  journal = {The Journal of Deaf Studies and Deaf Education},
  language = {en},
  number = {2}
}

@inproceedings{mccartney_gesture_2015,
  title = {Gesture Recognition with the Leap Motion Controller},
  booktitle = {Proc. {{Int}}. {{Conf}}.  on {{Image Processing}}, {{Computer Vision}}, and {{Pattern Recognition}} ({{IPCV}})},
  author = {McCartney, Robert and Yuan, Jie and Bischof, Hans-Peter},
  year = {2015},
  pages = {3},
  publisher = {{The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp)}}
}

@book{mccaskill_hidden_2011,
  title = {The {{Hidden Treasure}} of {{Black ASL}}: {{Its History}} and {{Structure}}},
  shorttitle = {The {{Hidden Treasure}} of {{Black ASL}}},
  author = {McCaskill, Carolyn and Lucas, Ceil and Bayley, Robert and Hill, Joseph},
  year = {2011},
  publisher = {{Gallaudet University Press}},
  abstract = {People who first encounter sign language often ask if deaf people around the world sign the same language. Frequently, they are surprised to learn that there are different sign languages in different nations worldwide, as well as variations of these languages. These variations depend on social factors such as region, age, gender, socioeconomic status, and race. One variation, Black ASL, has been recognized for years as a distinct form of sign language but only through anecdotal reports. This volume and its accompanying DVD present the first empirical study that begins to fill in the linguistic gaps about Black ASL.The powerful cast of contributors to The Hidden Treasure of Black ASL considered three questions in their study. First, what was the sociohistorical reality that made a separate variety of ASL possible? Second, what are the features of the variety of ASL that people call Black ASL? Third, can the same kind of unique features that have been identified in African American English be identified in Black ASL? This groundbreaking book and its companion DVD go far in answering these questions while also showing the true treasures of Black ASL.}
}

@article{mcdonald_automated_2015,
  title = {An Automated Technique for Real-Time Production of Lifelike Animations of {{American Sign Language}}},
  author = {McDonald, John and Wolfe, Rosalee and Schnepp, Jerry and Hochgesang, Julie and Jamrozik, Diana Gorman and Stumbo, Marie and Berke, Larwan and Bialek, Melissa and Thomas, Farah},
  year = {2015},
  month = may,
  volume = {1},
  pages = {1--16},
  issn = {1615-5289, 1615-5297},
  doi = {10.1007/s10209-015-0407-2},
  abstract = {Generating sentences from a library of signs implemented through a sparse set of key frames derived from the segmental structure of a phonetic model of ASL has the advantage of flexibility and efficiency, but lacks the lifelike detail of motion capture. These difficulties are compounded when faced with real-time generation and display. This paper describes a technique for automatically adding realism without the expense of manually animating the requisite detail. The new technique layers transparently over and modifies the primary motions dictated by the segmental model and does so with very little computational cost, enabling real-time production and display. The paper also discusses avatar optimizations that can lower the rendering overhead in real-time displays.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/McDonald et al_2015_An automated technique for real-time production of lifelike animations of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2R285W7A/s10209-015-0407-2.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5BF2KNHQ/s10209-015-0407-2.html},
  journal = {Universal Access in the Information Society},
  language = {en}
}

@inproceedings{mcguire_oneway_2004,
  title = {Towards a One-Way {{American}} Sign Language Translator},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {McGuire, R. M. and {Hernandez-Rebollar}, J. and Starner, T. and Henderson, V. and Brashear, H. and Ross, D. S.},
  year = {2004},
  month = may,
  pages = {620--625},
  doi = {10.1109/AFGR.2004.1301602},
  abstract = {Inspired by the Defense Advanced Research Projects Agency's (DARPA) previous successes in speech recognition, we introduce a new task for sign language recognition research: a mobile one-way American sign language translator. We argue that such a device should be feasible in the next few years, may provide immediate practical benefits for the deaf community, and leads to a sustainable program of research comparable to early speech recognition efforts. We ground our efforts in a particular scenario, that of a deaf individual seeking an apartment and discuss the system requirements and our interface for this scenario. Finally, we describe initial recognition results of 94\% accuracy on a 141 sign vocabulary signed in phrases of fours signs using a one-handed glove-based system and hidden Markov models (HMMs).},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/McGuire et al_2004_Towards a one-way American sign language translator.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FTG3CPDI/1301602.html},
  keywords = {capturedBy=electronicGlove,continuous,continuous;vocab=141;signer=1;sl=ASL;capturedBy=electronicGlove,modal=2hlocation,modal=2hshape,signer=1,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<500,vocab=141}
}

@misc{mckee_online_2015,
  title = {The {{Online Dictionary}} of  {{New Zealand Sign Language}}},
  author = {McKee, David and McKee, Rachel and Alexander, Sara Pivac and Pivac, Lynette},
  year = {2015},
  howpublished = {http://nzsl.vuw.ac.nz/}
}

@article{medhat_sentiment_2014,
  title = {Sentiment Analysis Algorithms and Applications: {{A}} Survey},
  shorttitle = {Sentiment Analysis Algorithms and Applications},
  author = {Medhat, Walaa and Hassan, Ahmed and Korashy, Hoda},
  year = {2014},
  month = dec,
  volume = {5},
  pages = {1093--1113},
  issn = {2090-4479},
  doi = {10.1016/j.asej.2014.04.011},
  abstract = {Sentiment Analysis (SA) is an ongoing field of research in text mining field. SA is the computational treatment of opinions, sentiments and subjectivity of text. This survey paper tackles a comprehensive overview of the last update in this field. Many recently proposed algorithms' enhancements and various SA applications are investigated and presented briefly in this survey. These articles are categorized according to their contributions in the various SA techniques. The related fields to SA (transfer learning, emotion detection, and building resources) that attracted researchers recently are discussed. The main target of this survey is to give nearly full image of SA techniques and the related fields with brief details. The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Medhat et al_2014_Sentiment analysis algorithms and applications.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/73NL2LPT/S2090447914000550.html},
  journal = {Ain Shams Engineering Journal},
  keywords = {survey},
  number = {4}
}

@incollection{meinedo_audimusmedia_2003,
  title = {\{\vphantom\}{{AUDIMUS}}.Media:\vphantom\{\} {{A Broadcast News Speech Recognition System}} for the \{\vphantom\}{{European Portuguese}}\vphantom\{\} {{Language}}},
  shorttitle = {{{AUDIMUS}}.{{MEDIA}}},
  booktitle = {International {{Workshop}} on {{Computational Processing}} of the {{Portuguese Language}}},
  author = {Meinedo, H. and Caseiro, D. and Neto, J. and Trancoso, I.},
  year = {2003},
  pages = {196},
  publisher = {{Springer}},
  address = {{Faro, Portugal}}
}

@phdthesis{meinedo_audio_2008,
  title = {Audio {{Pre}}-{{Processing}} and {{Speech Recognition}} for {{Broadcast News}}},
  author = {Meinedo, H.},
  year = {2008},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Meinedo_2008_Audio Pre-Processing and Speech Recognition for Broadcast News.pdf},
  school = {Universidade T\'ecnica de Lisboa}
}

@inproceedings{meinedo_combination_2000,
  title = {Combination of {{Acoustic Models}} in {{Continuous Speech Recognition Hybrid Systems}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{ICSLP}})},
  author = {Meinedo, H. and Neto, J.  P.},
  year = {2000},
  volume = {2},
  pages = {931--934},
  address = {{Beijing, China}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Meinedo_Neto_2000_Combination of Acoustic Models in Continuous Speech Recognition Hybrid Systems.pdf}
}

@inproceedings{meinedo_evaluation_2008,
  title = {Evaluation of a {{Live Broadcast News Subtitling System}} for \{\vphantom\}{{Portuguese}}\vphantom\{\}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Meinedo, H. and Viveiros, M. and Neto, J.},
  year = {2008},
  address = {{Brisbane, Australia}}
}

@inproceedings{mekala_realtime_2011,
  title = {Real-Time Sign Language Recognition Based on Neural Network Architecture},
  booktitle = {{{IEEE Southeastern Symposium}} on {{System Theory}} ({{SSST}})},
  author = {Mekala, Priyanka and Gao, Ying and Fan, Jeffrey and Davari, Asad},
  year = {2011},
  month = mar,
  pages = {195--199},
  address = {{Auburn, AL, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mekala et al_2011_Real-time sign language recognition based on neural network architecture.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ER7TM576/login.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=26;sl=ASL;capturedBy=vision,modal=movement,modal=shape,sl=ASL,slreco,slreco;modal=shape;modal=movement,vocab<50,vocab=26}
}

@inproceedings{melax_dynamics_2013,
  title = {Dynamics {{Based 3D Skeletal Hand Tracking}}},
  booktitle = {Proceedings of {{Graphics Interface}} 2013},
  author = {Melax, Stan and Keselman, Leonid and Orsten, Sterling},
  year = {2013},
  pages = {63--70},
  publisher = {{Canadian Information Processing Society}},
  address = {{Toronto, Ont., Canada, Canada}},
  abstract = {Tracking the full skeletal pose of the hands and fingers is a challenging problem that has a plethora of applications for user interaction. Existing techniques either require wearable hardware, add restrictions to user pose, or require significant computation resources. This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor's samples, the system generates constraints that limit motion orthogonal to the rigid body model's surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions. Such an approach enables real-time, robust, and accurate 3D skeletal tracking of a user's hand on a variety of depth cameras, while only utilizing a single x86 CPU core for processing.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Melax et al_2013_Dynamics Based 3D Skeletal Hand Tracking.pdf},
  isbn = {978-1-4822-1680-6},
  series = {{{GI}} '13}
}

@phdthesis{mercier_modelisation_2007,
  title = {Mod\'elisation et Suivi Des D\'eformations Faciales: {{Applications}} \`a La Description Des Expressions Du Visage Dans Le Contexte de La Langue Des Signes},
  shorttitle = {Mod\'elisation et Suivi Des D\'eformations Faciales},
  author = {Mercier, Hugo},
  year = {2007},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mercier_2007_Modélisation et suivi des déformations faciales.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PU4XC7TF/tel-00185084.html},
  school = {Universit\'e Paul Sabatier-Toulouse III}
}

@article{merkin_community_1995,
  title = {A Community Based Model Providing Services for Deaf and Deaf-Blind Victims of Sexual Assault and Domestic Violence},
  author = {Merkin, Lewis and Smith, Marilyn J.},
  year = {1995},
  month = jun,
  volume = {13},
  pages = {97--106},
  issn = {0146-1044, 1573-6717},
  doi = {10.1007/BF02590059},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Merkin_Smith_1995_A community based model providing services for deaf and deaf-blind victims of.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C6ETFVC3/BF02590059.html},
  journal = {Sexuality and Disability},
  language = {en},
  number = {2}
}

@inproceedings{meyer_robust_2015,
  title = {Robust {{Model}}-{{Based 3D Head Pose Estimation}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Meyer, Gregory P. and Gupta, Shalini and Frosio, Iuri and Reddy, Dikpal and Kautz, Jan},
  year = {2015},
  pages = {3649--3657},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Meyer et al/Meyer et al_2015_Robust Model-Based 3D Head Pose Estimation.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Meyer et al_2015_Robust Model-Based 3D Head Pose Estimation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/A7M3Q5AU/Meyer_Robust_Model-Based_3D_ICCV_2015_paper.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NZ5JIPGS/Meyer_Robust_Model-Based_3D_ICCV_2015_paper.html}
}

@inproceedings{michael_computerbased_2010,
  title = {Computer-Based Recognition of Facial Expressions in {{ASL}}: From Face Tracking to Linguistic Interpretation},
  shorttitle = {Computer-Based Recognition of Facial Expressions in {{ASL}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Michael, Nicholas and Neidle, Carol and Metaxas, Dimitris},
  year = {2010},
  month = may,
  pages = {164--167},
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Michael et al_2010_Computer-based recognition of facial expressions in ASL.pdf},
  keywords = {capturedBy=vision,isolated,modal=eyebrows,modal=eyes,modal=head,nonmanual;vocab=3;signer=3;sl=ASL;capturedBy=vision,nonmanualreco;modal=head;modal=eyebrows;modal=eyes,signer=3,sl=ASL,slreco,vocab<50,vocab=3}
}

@phdthesis{mikolov_statistical_2012,
  title = {Statistical Language Models Based on Neural Networks},
  author = {Mikolov, Tom{\'a}{\v s}},
  year = {2012},
  address = {{Brno, Czech Republic}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mikolov_2012_Statistical language models based on neural networks2.pdf},
  school = {Brno University of Technology}
}

@inproceedings{miller_faces_2004,
  title = {Faces and Names in the News},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Miller, Tamara and Berg, Alexander C. and Edwards, Jaety and Maire, Michael and White, Ryan and Teh, Yee-Whye and {Learned-Miller}, Erik and Forsyth, D. A.},
  year = {2004},
  address = {{Washington, DC, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Miller et al_2004_Faces and names in the news.pdf}
}

@inproceedings{ming_representations_2002,
  title = {Representations for Facial Expressions},
  booktitle = {Proc. {{Int}}. {{Conf}}. {{Control}}, {{Automation}}, {{Robotics}} and {{Vision}}},
  author = {Ming, Khoo Wei and Ranganath, Surendra},
  year = {2002},
  volume = {2},
  pages = {716--721},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ming_Ranganath_2002_Representations for facial expressions.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KRV5ZJA9/abs_all.html}
}

@inproceedings{minnen_robust_2011,
  title = {Towards Robust Cross-User Hand Tracking and Shape Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Minnen, D. and Zafrulla, Z.},
  year = {2011},
  month = nov,
  pages = {1235--1241},
  doi = {10.1109/ICCVW.2011.6130392},
  abstract = {Low-cost depth cameras create new opportunities for robust and ubiquitous vision-based interfaces. While much research has focused on full-body pose estimation and the interpretation of gross body movement, this work investigates skeleton-free hand detection, tracking, and shape classification. Our goal is to build a rich and reliable gestural interface by developing methods that recognize a broad set of hand shapes and which maintain high accuracy rates across a wide range of users. In this paper, we describe our approach to real-time hand detection and tracking using depth data from the Microsoft Kinect. We present quantitative shape recognition results for eight hand shapes collected from 16 users and then discuss physical configuration and interface design issues that help boost reliability and overall user experience.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Minnen_Zafrulla_2011_Towards robust cross-user hand tracking and shape recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CVAMNH83/abs_all.html}
}

@article{missaoui_multistream_2013,
  title = {Multi-Stream Continuous Hidden {{Markov}} Models with Application to Landmine Detection},
  author = {Missaoui, Oualid and Frigui, Hichem and Gader, Paul},
  year = {2013},
  month = dec,
  volume = {2013},
  pages = {40},
  issn = {1687-6180},
  doi = {10.1186/1687-6180-2013-40},
  abstract = {We propose a multi-stream continuous hidden Markov model (MSCHMM) framework that can learn from multiple modalities. We assume that the feature space is partitioned into subspaces generated by different sources of information. In order to fuse the different modalities, the proposed MSCHMM introduces stream relevance weights. First, we modify the probability density function (pdf) that characterizes the standard continuous HMM to include state and component dependent stream relevance weights. The resulting pdf approximate is a linear combination of pdfs characterizing multiple modalities. Second, we formulate the CHMM objective function to allow for the simultaneous optimization of all model parameters including the relevance weights. Third, we generalize the maximum likelihood based Baum-Welch algorithm and the minimum classification error/gradient probabilistic descent (MCE/GPD) learning algorithms to include stream relevance weights. We propose two versions of the MSCHMM. The first one introduces the relevance weights at the state level while the second one introduces the weights at the component level. We illustrate the performance of the proposed MSCHMM structures using synthetic data sets. We also apply them to the problem of landmine detection using ground penetrating radar. We show that when the multiple sources of information are equally relevant across all training data, the performance of the proposed MSCHMM is comparable to the baseline CHMM. However, when the relevance of the sources varies, the MSCHMM outperforms the baseline CHMM because it can learn the optimal relevance weights. We also show that our approach outperforms existing multi-stream HMM because the latter one cannot optimize all model parameters simultaneously.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Missaoui et al_2013_Multi-stream continuous hidden Markov models with application to landmine.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z63F3BNE/1687-6180-2013-40.html},
  journal = {EURASIP Journal on Advances in Signal Processing},
  language = {en},
  number = {1}
}

@article{mitchell_how_2006,
  title = {How {{Many People Use ASL}} in the {{United States}}? {{Why Estimates Need Updating}}},
  shorttitle = {How {{Many People Use ASL}} in the {{United States}}?},
  author = {Mitchell, Ross E and Young, Travas A and Bachleda, Bellamie and Karchmer, Michael A},
  year = {2006},
  volume = {6},
  pages = {306--335},
  issn = {1533-6263},
  doi = {10.1353/sls.2006.0019},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C5WPG6GY/Mitchell et al. - 2006 - How Many People Use ASL in the United States Why .pdf},
  journal = {Sign Language Studies},
  language = {en},
  number = {3}
}

@article{mittal_modifiedlstm_2019,
  title = {A {{Modified}}-{{LSTM Model}} for {{Continuous Sign Language Recognition}} Using {{Leap}} Motion},
  author = {Mittal, A. and Kumar, P. and Roy, P. P. and Balasubramanian, R. and Chaudhuri, B. B.},
  year = {2019},
  month = apr,
  volume = {early access},
  pages = {1--1},
  issn = {1530-437X},
  doi = {10.1109/JSEN.2019.2909837},
  abstract = {Sign language facilitates communication between hearing impaired peoples and the rest of the society. A number of Sign Language Recognition (SLR) systems have been developed by researchers but they are limited to isolated sign gestures only. In this paper, we propose a modified LSTM model for continuous sequences of gestures or continuous SLR that recognizes a sequence of connected gestures. It is based on splitting of continuous signs into sub-units and modeling them with neural networks. Thus, the consideration of different combination of sub-units is not required during training. The proposed system has been tested with 942 signed sentences of Indian Sign Language (ISL). These sign sentences are recognized using 35 different sign words. The average accuracy of 72.3\% and 89.5\% have been recorded on signed sentences and isolated sign words, respectively.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mittal et al_2019_A Modified-LSTM Model for Continuous Sign Language Recognition using Leap motion.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/B649B9RL/8684245.html},
  journal = {IEEE Sensors Journal},
  keywords = {capturedBy=depth,continuous,continuous;vocab=35;sl=ISL;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,sl=ISL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=2horientation,vocab<50,vocab=35}
}

@article{moghadam_clustering_,
  title = {Toward {{Clustering Persian Vowel Viseme}}: {{A New Clustering Approach}} Based on {{HMM}}},
  shorttitle = {Toward {{Clustering Persian Vowel Viseme}}},
  author = {Moghadam, Maedeh and Bastanfard, Azam and Dehshibi, Mohammad Mahdi},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Moghadam et al_Toward Clustering Persian Vowel Viseme.pdf}
}

@inproceedings{mohandes_arabic_2007,
  title = {Arabic {{Sign Language Recognition}} an {{Image}}-{{Based Approach}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Advanced Information Networking}} and {{Applications Workshops}} ({{AINAW}})},
  author = {Mohandes, M. and Quadri, S. I. and Deriche, M.},
  year = {2007},
  month = may,
  volume = {1},
  pages = {272--276},
  doi = {10.1109/AINAW.2007.98},
  abstract = {In this paper we propose an image based system for Arabic sign language recognition. A Gaussian skin color model is used to detect the signer's face. The centroid of the detected face is then used as a reference to track the hands movement using region growing from the sequence of images comprising the signs. A number of features are then selected from the detected hand regions across the sequence of images. The recognition stage is performed using a hidden Markov model. The proposed system achieved a recognition accuracy of about 93\% for a data set of 300 signs with leave one out method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mohandes et al_2007_Arabic Sign Language Recognition an Image-Based Approach.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BFYXBTM8/4221072.html},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=300;signer=1;sl=ArSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hshape,signer=1,sl=ArSL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<500,vocab=300}
}

@inproceedings{mohandes_arabic_2013,
  title = {Arabic Sign Language Recognition by Decisions Fusion Using {{Dempster}}-{{Shafer}} Theory of Evidence},
  booktitle = {Computing, {{Communications}} and {{IT Applications Conf}}. ({{ComComAp}})},
  author = {Mohandes, M. and Deriche, M.},
  year = {2013},
  month = apr,
  pages = {90--94},
  doi = {10.1109/ComComAp.2013.6533615},
  abstract = {Most sign language recognition systems that use gloves and hand trackers combine the data from both devices at the sensor level. In this paper we propose a new approach by combining information acquired from the gloves and the hand tracking systems at the decision level using the Dempster-Shafer theory of evidence. The results using the Dempster-Shafer on the recognition of 100 two-handed signs show enhanced performance compared to the individual systems and to classification based on combined features. A recognition accuracy of 84.7\%, and 91.3\% are achieved when attempting to recognize the signs from the hand tracker only, and the glove data, respectively. When the sensor data from the gloves and hand tracking systems are combined, a recognition accuracy of 96.2\% was achieved while a recognition accuracy of 98.1\% was achieved when the fusion is performed at the decision level using Dempster-Shafer theory of evidence.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mohandes_Deriche_2013_Arabic sign language recognition by decisions fusion using Dempster-Shafer.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QRCWYZ7Q/6533615.html},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=100;signer=1;sl=ArSL;capturedBy=electronicGlove,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=1,signer=2,sl=ArSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape,vocab<500,vocab=100}
}

@inproceedings{mohandes_arabic_2014,
  title = {Arabic Sign Language Recognition Using the Leap Motion Controller},
  booktitle = {Proc. {{IEEE Int}}. {{Symposium}} on {{Industrial Electronics}} ({{ISIE}})},
  author = {Mohandes, M. and Aliyu, S. and Deriche, M.},
  year = {2014},
  month = jun,
  pages = {960--965},
  doi = {10.1109/ISIE.2014.6864742},
  abstract = {Sign language is important for facilitating communication between hearing impaired and the rest of society. Two approaches have traditionally been used in the literature: image-based and sensor-based systems. Sensor-based systems require the user to wear electronic gloves while performing the signs. The glove includes a number of sensors detecting different hand and finger articulations. Image-based systems use camera(s) to acquire a sequence of images of the hand. Each of the two approaches has its own disadvantages. The sensor-based method is not natural as the user must wear a cumbersome instrument while the imagebased system requires specific background and environmental conditions to achieve high accuracy. In this paper, we propose a new approach for Arabic Sign Language Recognition (ArSLR) which involves the use of the recently introduced Leap Motion Controller (LMC). This device detects and tracks the hand and fingers to provide position and motion information. We propose to use the LMC as a backbone of the ArSLR system. In addition to data acquisition, the system includes a preprocessing stage, a feature extraction stage, and a classification stage. We compare the performance of Multilayer Perceptron (MLP) neural networks with the Nave Bayes classifier. Using the proposed system on the Arabic sign alphabets gives 98\% classification accuracy with the Nave Bayes classifier and more than 99\% using the MLP.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014/Mohandes et al/Mohandes et al_2014_Arabic sign language recognition using the leap motion controller.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mohandes et al_2014_Arabic sign language recognition using the leap motion controller.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IHKR6BPE/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PR65WPKK/abs_all.html},
  keywords = {capturedBy=depth,isolated,isolated;vocab=38;signer=10;sl=ArSL;capturedBy=depth,modal=orientation,modal=shape,signer=10,sl=ArSL,slreco,slreco;modal=shape;modal=orientation,vocab<50,vocab=38}
}

@article{mohandes_imagebased_2014,
  title = {Image-{{Based}} and {{Sensor}}-{{Based Approaches}} to {{Arabic Sign Language Recognition}}},
  author = {Mohandes, M. and Deriche, M. and Liu, J.},
  year = {2014},
  month = aug,
  volume = {44},
  pages = {551--557},
  issn = {2168-2291},
  doi = {10.1109/THMS.2014.2318280},
  abstract = {Sign language continues to be the preferred method of communication among the deaf and the hearing-impaired. Advances in information technology have prompted the development of systems that can facilitate automatic translation between sign language and spoken language. More recently, systems translating between Arabic sign and spoken language have become popular. This paper reviews systems and methods for the automatic recognition of Arabic sign language. Additionally, this paper highlights the main challenges characterizing Arabic sign language as well as potential future research directions.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mohandes et al_2014_Image-Based and Sensor-Based Approaches to Arabic Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VQJ9FV25/articleDetails.html},
  journal = {IEEE Transactions on Human-Machine Systems},
  number = {4}
}

@article{mohandes_recognition_2013,
  title = {Recognition of {{Two}}-{{Handed Arabic Signs Using}} the {{CyberGlove}}},
  author = {Mohandes, Mohamed A.},
  year = {2013},
  month = mar,
  volume = {38},
  pages = {669--677},
  issn = {2191-4281},
  doi = {10.1007/s13369-012-0378-z},
  abstract = {Sign language maps letters, words, and expressions of a certain language to a set of hand gestures enabling an individual to communicate by using hands and gestures rather than by speaking. Systems capable of recognizing sign-language symbols can be used for communication with the hearing-impaired. This paper represents the first attempt to recognize two-handed signs from the Unified Arabic Sign Language Dictionary using the CyberGlove and support vector machines (SVMs). 20 samples from each of 100 two-handed signs were collected from two adult signers. Because the signs are of different lengths, time division is used to standardize sign length. The duration of every sign is divided into a specific number of segments, and the mean and standard deviation of each segment are used to represent the signal in the segment. After pre-processing, principal component analysis is used for feature extraction. For recognition, a SVM is trained on 15 samples from each sign. The performance is obtained by testing the trained SVM on the remaining five samples from each sign. A recognition rate of 99.6\% on the testing data is obtained.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mohandes_2013_Recognition of Two-Handed Arabic Signs Using the CyberGlove.pdf},
  journal = {Arabian Journal for Science and Engineering},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=100;signer=2;sl=ArSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=2,sl=ArSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape,vocab<500,vocab=100},
  language = {en},
  number = {3}
}

@article{mohandes_signerindependent_2012,
  title = {A Signer-Independent {{Arabic Sign Language}} Recognition System Using Face Detection, Geometric Features, and a {{Hidden Markov Model}}},
  author = {Mohandes, M. and Deriche, M. and Johar, U. and Ilyas, S.},
  year = {2012},
  month = mar,
  volume = {38},
  pages = {422--433},
  issn = {0045-7906},
  doi = {10.1016/j.compeleceng.2011.10.013},
  abstract = {In this paper, we propose an image-based system for Arabic Sign Language (ArSL) recognition. The algorithm starts by detecting the face of the signer using a Gaussian skin color model. The centroid of the detected face is then used as a reference point for tracking the hands' movements. The hands regions are segmented using a region growing algorithm assuming the signer wears a yellow and an orange colored gloves. From the segmented hands regions, an optimal set of features is extracted. To represent the time varying feature patterns, a Hidden Markov Model (HMM) is then used. Before using HMM in testing, the number of states and the number of Gaussian mixtures are optimized. The proposed system was implemented for both signer dependent and signer independent conditions. The experimental results show that an accuracy of more than 95\% can be achieved with a large database of 300 signs. The results outperform previous work on ArSL mainly restricted to small vocabulary size.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mohandes et al_2012_A signer-independent Arabic Sign Language recognition system using face.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZZGMBQE9/S0045790611001637.html},
  journal = {Computers and Electrical Engineering},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=300;signer=3;sl=ArSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hshape,signer=3,sl=ArSL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<500,vocab=300},
  number = {2}
}

@inproceedings{mohandes_survey_2014,
  title = {A Survey of Image-Based {{Arabic}} Sign Language Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Multi}}-{{Conf}}. on {{Systems}}, {{Signals Devices}} ({{SSD}})},
  author = {Mohandes, M. and {Junzhao Liu} and Deriche, M.},
  year = {2014},
  month = feb,
  pages = {1--4},
  doi = {10.1109/SSD.2014.6808906},
  abstract = {Sign language is the native language of deaf and hearing impaired people which they prefer to use on their daily life. Few interpreters are available to facilitate communication between deaf and vocal people. However, this is neither practical nor possible for all situations. Advances in information technology encouraged the development of systems that can facilitate the automatic translation between sign language and spoken language, and thus removing barriers facing the integration of deaf people in the society. A lot of research has been carried on the development of systems that translate sign languages into spoken words and the reverse. However, only recently systems translating between Arabic sign language and spoken language have been developed. Many signs of the Arabic sign language are reflection of the environment (White color in Arabic sign language is a finger pointing to the chest of the signer as the tradition for male is to wear white color dress). Several review papers have been published on the automatic recognition of other sign languages. This paper represents the first attempt to review systems and methods for the image based automatic recognition of the Arabic sign language. It reviews most published papers and discusses a variety of recognition methods. Additionally, the paper highlights the main challenges characterizing the Arabic sign language as well as potential future research directions in this area.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mohandes et al_2014_A survey of image-based Arabic sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NCLVAGBD/6808906.html}
}

@incollection{mohr_chapter_2014,
  title = {Chapter 2. {{The Linguistic Setup}} of {{Sign Languages}} \textendash{} {{The Case}} of {{Irish Sign Language}} ({{ISL}})},
  booktitle = {Mouth {{Actions}} in {{Sign LanguagesAn Empirical Study}} of {{Irish Sign Language}}},
  author = {Mohr, Susanne},
  year = {2014},
  publisher = {{De Gruyter Mouton}},
  address = {{Berlin, Boston}},
  doi = {10.1515/9781614514978.4},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014_Chapter 2.pdf},
  isbn = {978-1-61451-497-8}
}

@incollection{mohr_chapter_2014a,
  title = {Chapter 3. {{Non}}-Manuals in {{Sign Languages}} \textendash{} {{Theoretical Background}}},
  booktitle = {Mouth {{Actions}} in {{Sign LanguagesAn Empirical Study}} of {{Irish Sign Language}}},
  author = {Mohr, Susanne},
  year = {2014},
  publisher = {{De Gruyter Mouton}},
  address = {{Berlin, Boston}},
  doi = {10.1515/9781614514978.31},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014_Chapter 3.pdf},
  isbn = {978-1-61451-497-8}
}

@incollection{mohr_chapter_2014b,
  title = {Chapter 4. {{Mouth Actions}} in {{ISL}} \textendash{} {{A Typology}} of {{Mouthings}} and {{Mouth Gestures}}},
  booktitle = {Mouth {{Actions}} in {{Sign LanguagesAn Empirical Study}} of {{Irish Sign Language}}},
  author = {Mohr, Susanne},
  year = {2014},
  publisher = {{De Gruyter Mouton}},
  address = {{Berlin, Boston}},
  doi = {10.1515/9781614514978.64},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014_Chapter 4.pdf},
  isbn = {978-1-61451-497-8}
}

@incollection{mohr_chapter_2014c,
  title = {Chapter 5. {{Mouth Actions}} in {{ISL}} \textendash{} {{Sociolinguistic Factors}}},
  booktitle = {Mouth {{Actions}} in {{Sign LanguagesAn Empirical Study}} of {{Irish Sign Language}}},
  author = {Mohr, Susanne},
  year = {2014},
  publisher = {{De Gruyter Mouton}},
  address = {{Berlin, Boston}},
  doi = {10.1515/9781614514978.94},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014_Chapter 5.pdf},
  isbn = {978-1-61451-497-8}
}

@incollection{mohr_chapter_2014d,
  title = {Chapter 6. {{Mouth Actions}} in {{ISL}} \textendash{} {{Linguistic Factors}}},
  booktitle = {Mouth {{Actions}} in {{Sign LanguagesAn Empirical Study}} of {{Irish Sign Language}}},
  author = {Mohr, Susanne},
  year = {2014},
  publisher = {{De Gruyter Mouton}},
  address = {{Berlin, Boston}},
  doi = {10.1515/9781614514978.114},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014_Chapter 6.pdf},
  isbn = {978-1-61451-497-8}
}

@incollection{mohr_chapter_2014e,
  title = {Chapter 7. {{Conclusion}}},
  booktitle = {Mouth {{Actions}} in {{Sign LanguagesAn Empirical Study}} of {{Irish Sign Language}}},
  author = {Mohr, Susanne},
  year = {2014},
  publisher = {{De Gruyter Mouton}},
  address = {{Berlin, Boston}},
  doi = {10.1515/9781614514978.162},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014_Chapter 7.pdf},
  isbn = {978-1-61451-497-8}
}

@book{mohr_mouth_2014,
  title = {Mouth {{Actions}} in {{Sign Languages}}, {{An Empirical Study}} of {{Irish Sign Language}}},
  author = {Mohr, Susanne},
  year = {2014},
  publisher = {{De Gruyter Mouton}},
  address = {{Berlin, Boston}},
  doi = {10.1515/9781614514978},
  isbn = {978-1-61451-497-8},
  language = {ENGL}
}

@article{mohri_weighted_2002,
  title = {Weighted {{Finite}}-{{State Transducers}} in {{Speech Recognition}}},
  author = {Mohri, M. and Pereira, F. and Riley, M.},
  year = {2002},
  volume = {16},
  pages = {69--88},
  journal = {Computer Speech and Language},
  number = {1}
}

@phdthesis{molau_normalization_2003,
  title = {Normalization in the {{Acoustic Feature Space}} for {{Improved Speech Recognition}}},
  author = {Molau, S.},
  year = {2003},
  month = feb,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Molau_2003_Normalization in the Acoustic Feature Space for Improved Speech Recognition.pdf},
  school = {RWTH Aachen}
}

@inproceedings{molchanov_hand_2015,
  title = {Hand {{Gesture Recognition}} with {{3D Convolutional Neural Networks}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Molchanov, Pavlo and Gupta, Shalini and Kim, Kihwan and Kautz, Jan},
  year = {2015},
  month = jun,
  pages = {1--7},
  address = {{Boston, Ma, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Molchanov et al/Molchanov et al_2015_Hand Gesture Recognition with 3D Convolutional Neural Networks.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Molchanov et al_2015_Hand Gesture Recognition with 3D Convolutional Neural Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P83S8JRE/Molchanov_Hand_Gesture_Recognition_2015_CVPR_paper.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RJ6G7IIJ/Molchanov_Hand_Gesture_Recognition_2015_CVPR_paper.html}
}

@inproceedings{molchanov_online_2016,
  title = {Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3d Convolutional Neural Network},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Molchanov, Pavlo and Yang, Xiaodong and Gupta, Shalini and Kim, Kihwan and Tyree, Stephen and Kautz, Jan},
  year = {2016},
  pages = {4207--4215},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Molchanov et al_2016_Online detection and classification of dynamic hand gestures with recurrent 3d.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/59TQ9762/Molchanov_Online_Detection_and_CVPR_2016_paper.html}
}

@inproceedings{molchanov_shortrange_2015,
  title = {Short-Range {{FMCW}} Monopulse Radar for Hand-Gesture Sensing},
  booktitle = {Radar {{Conference}} ({{RadarCon}}), 2015 {{IEEE}}},
  author = {Molchanov, Pavlo and Gupta, Shalini and Kim, Kihwan and Pulli, Kari},
  year = {2015},
  pages = {1491--1496},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Molchanov et al_2015_Short-range FMCW monopulse radar for hand-gesture sensing.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HHWS9E8P/7131232.html}
}

@misc{monschein_empirical_2011,
  title = {Empirical {{Research}} on {{Mouth Patterns}}  Considering {{Sociolinguistic Factors}}: {{A Comparison}} between the {{Use}} of {{Mouth Patterns}} of  {{Deaf L1}}- and {{Hearing L2}}-{{Users}} of {{German Sign Language}} ({{DGS}})},
  author = {Monschein, Lisa},
  year = {2011},
  month = aug
}

@inproceedings{monteiro_detecting_2016,
  title = {Detecting and {{Identifying Sign Languages}} through {{Visual Features}}},
  booktitle = {2016 {{IEEE International Symposium}} on {{Multimedia}} ({{ISM}})},
  author = {Monteiro, Caio D. D. and Mathew, Christy Maria and {Gutierrez-Osuna}, Ricardo and Shipman, Frank},
  year = {2016},
  month = dec,
  pages = {287--290},
  doi = {10.1109/ISM.2016.0063},
  abstract = {The popularity of video sharing sites has encouraged the creation and distribution of sign language (SL) content. Unfortunately, locating SL videos on a desired topic is not a straightforward task. Retrieval depends on the existence and correctness of metadata to indicate that the video contains SL. This problem gets worse when considering a particular type of sign language (e.g. American Sign Language - ASL, British Sign Language - BSL, French Sign Language - LSF, etc.), where metadata needs to be even more specific. To address this problem, we have expanded a previous SL classifier to distinguish videos in different SLs. The new classifier achieves an F1 score of 98\% when discriminating between BSL and LSF videos with static backgrounds, and a 70\% F1 score when distinguishing between ASL and BSL videos found on popular video sharing sites. Such accuracy with visual features alone is possible when comparing languages with one-handed and two-handed manual alphabets.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WSAPTS8W/7823630.html},
  keywords = {ASL videos,Assistive technology,BSL videos,Detectors,F1 score,Face,Gesture recognition,image retrieval,language identification,LSF videos,Manuals,meta data,metadata,Metadata,sign language content creation,sign language content distribution,sign language detection,sign language recognition,static backgrounds,video retrieval,video sharing sites,Visualization,Web sites}
}

@article{monteiro_tradeoffs_2019,
  title = {Tradeoffs in the {{Efficient Detection}} of {{Sign Language Content}} in {{Video Sharing Sites}}},
  author = {Monteiro, Caio D. D. and Shipman, Frank M. and Duggina, Satyakiran and {Gutierrez-Osuna}, Ricardo},
  year = {2019},
  month = jun,
  volume = {12},
  pages = {5:1--5:16},
  issn = {1936-7228},
  doi = {10.1145/3325863},
  abstract = {Video sharing sites have become keepers of de-facto digital libraries of sign language content, being used to store videos including the experiences, knowledge, and opinions of many in the deaf or hard of hearing community. Due to limitations of term-based search over metadata, these videos can be difficult to find, reducing their value to the community. Another result is that community members frequently engage in a push-style delivery of content (e.g., emailing or posting links to videos for others in the sign language community) rather than having access be based on the information needs of community members. In prior work, we have shown the potential to detect sign language content using features derived from the video content rather than relying on metadata. Our prior technique was developed with a focus on accuracy of results and are quite computationally expensive, making it unrealistic to apply them on a corpus the size of YouTube or other large video sharing sites. Here, we describe and examine the performance of optimizations that reduce the cost of face detection and the length of video segments processed. We show that optimizations can reduce the computation time required by 96\%, while losing only 1\% in F1 score. Further, a keyframe-based approach is examined that removes the need to process continuous video. This approach achieves comparable recall but lower precision than the above techniques. Merging the advantages of the optimizations, we also present a staged classifier, where the keyframe approach is used to reduce the number of non-sign language videos fully processed. An analysis of the staged classifier shows a further reduction in average computation time per video while achieving similar quality of results.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Monteiro et al_2019_Tradeoffs in the Efficient Detection of Sign Language Content in Video Sharing.pdf},
  journal = {ACM Transactions on Accessible Computing},
  keywords = {ASL,metadata extraction,Sign language,video analysis,video sharing},
  number = {2}
}

@inproceedings{montero-asenjo_exploring_2006,
  title = {Exploring \{\vphantom\}{{PPRLM}}\vphantom\{\} {{Performance}} for \{\vphantom\}{{NIST}}\vphantom\{\} 2005 {{Language Recognition Evaluation}}},
  booktitle = {{{IEEE Odyssey}} 2006: {{The Speaker}} and {{Language Recognition Workshop}}},
  author = {{Montero-Asenjo}, A. and Toledano, D. T. and {Gonzalez-Dominguez}, J. and {Gonzalez-Rodriguez}, J. and {Ortega-Garcia}, J.},
  year = {2006},
  pages = {1--6}
}

@article{moore_automatic_,
  title = {Automatic {{Facial Expression Recognition}} Using {{Boosted Discriminatory Classifiers}}},
  author = {Moore, S. and Bowden, R.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Moore_Bowden_Automatic Facial Expression Recognition using Boosted Discriminatory Classifiers.pdf}
}

@article{morade_lip_2014,
  title = {Lip {{Reading}} by {{Using}} 3-{{D Discrete Wavelet Transform}} with {{Dmey Wavelet}}},
  author = {Morade, Sunil S. and Patnaik, Suprava},
  year = {2014},
  volume = {8},
  pages = {384},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Morade_Patnaik_2014_Lip Reading by Using 3-D Discrete Wavelet Transform with Dmey Wavelet.pdf},
  journal = {International Journal of Image Processing (IJIP)},
  number = {5}
}

@incollection{morford_research_2015,
  title = {Research {{Methods}} in {{Psycholinguistic Investigations}} of {{Sign Language Processing}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Morford, Jill P. and Nicodemus, Brenda and Wilkinson, Erin},
  year = {2015},
  pages = {207--249},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch13},
  abstract = {This chapter provides an overview of research methods used to investigate the comprehension and production of signed languages. In a methodological review of 61 published studies, it was found that psycholinguistic studies have been carried out on a very restricted range of signed languages. The majority of investigations were conducted in laboratory settings. The populations studied consisted primarily of proficient adult deaf signers, with some inclusion of hearing signers. The methods used were best suited for testing specific hypotheses about signed language processing, rather than for documenting or discovering norms and preferences for language use within social contexts. Three types of tasks have been used to date in sign perception studies: identification and discrimination tasks; phonological similarity judgment tasks; and monitoring tasks. The chapter concludes the review with a discussion of the challenges researchers face when conducting psycholinguistic investigations of signed language processing.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Morford et al_2015_Research Methods in Psycholinguistic Investigations of Sign Language Processing.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EX8QGDXX/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@book{morgan_directions_2002,
  title = {Directions in {{Sign Language Acquisition}}},
  author = {Morgan, Gary and Woll, Bencie},
  year = {2002},
  month = jun,
  publisher = {{John Benjamins Publishing}},
  abstract = {As the first book of its kind, this volume with contributions from many well known scholars brings together some of the most recent original work on sign language acquisition in children learning a variety of different signed languages (i.e., Brazilian Sign Language, American SL, SL of the Netherlands, British SL, SL of Nicaragua, and Italian SL). In addition, the volume addresses methodological and theoretical issues in both sign language research and child language development in general. The book includes both overview chapters addressing matters of general concern in the study of sign language acquisition and chapters related to more specific topics such as sign language phonology, complex sentence structure and verb phrase development. This book will be of interest to sign language researchers, child language specialists and communication disorders professionals alike. The material is presented in such a way that also novices to the area of sign language study will find the text accessible.},
  isbn = {978-90-272-9724-2},
  language = {en}
}

@inproceedings{morrissey_building_2010,
  title = {Building a Sign Language Corpus for Use in Machine Translation},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Morrissey, Sara and Somers, Harold and Smith, Robert and Gilchrist, Shane and Dandapat, Sandipan},
  year = {2010},
  month = may,
  pages = {172--177},
  address = {{Valletta, Malta}},
  abstract = {In recent years data-driven methods of machine translation (MT) have overtaken rule-based approaches as the predominant means of automatically translating between languages. A pre-requisite for such an approach is a parallel corpus of the source and target languages. Technological developments in sign language (SL) capturing, analysis and processing tools now mean that SL corpora are
becoming increasingly available. With transcription and language analysis tools being mainly designed and used for linguistic purposes, we describe the process of creating a multimedia parallel corpus specifically for the purposes of English to Irish Sign Language (ISL) MT. As part of our larger project on localisation, our research is focussed on developing assistive technology for patients with limited English in the domain of healthcare. Focussing on the first point of contact a patient has with a GP's office, the
medical secretary, we sought to develop a corpus from the dialogue between the two parties when scheduling an appointment. Throughout the development process we have created one parallel corpus in six different modalities from this initial dialogue. In this paper we discuss the multi-stage process of the development of this parallel corpus as individual and interdependent entities, both for
our own MT purposes and their usefulness in the wider MT and SL research domains.},
  copyright = {This item is licensed under a Creative Commons Attribution-NonCommercial-Share Alike 3.0 License.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Morrissey et al_2010_Building a sign language corpus for use in machine translation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FEQHR7T3/16040.html},
  language = {en}
}

@inproceedings{morrissey_hybrid_2007,
  title = {Towards a {{Hybrid Data}}-{{Driven MT System}} for {{Sign Languages}}},
  booktitle = {Machine {{Translation Summit}}},
  author = {Morrissey, Sara and Way, Andy and Stein, Daniel and Bungeroth, Jan and Ney, Hermann},
  year = {2007},
  month = sep,
  pages = {329--335},
  address = {{Copenhagen, Denmark}}
}

@inproceedings{mukushev_evaluation_2020,
  title = {Evaluation of {{Manual}} and {{Non}}-Manual {{Components}} for {{Sign Language Recognition}}},
  booktitle = {Proceedings of {{The}} 12th {{Language Resources}} and {{Evaluation Conference}}},
  author = {Mukushev, Medet and Sabyrov, Arman and Imashev, Alfarabi and Koishybay, Kenessary and Kimmelman, Vadim and Sandygulova, Anara},
  year = {2020},
  month = may,
  pages = {6073--6078},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  abstract = {The motivation behind this work lies in the need to differentiate between similar signs that differ in non-manual components present in any sign. To this end, we recorded full sentences signed by five native signers and extracted 5200 isolated sign samples of twenty frequently used signs in Kazakh-Russian Sign Language (K-RSL), which have similar manual components but differ in non-manual components (i.e. facial expressions, eyebrow height, mouth, and head orientation). We conducted a series of evaluations in order to investigate whether non-manual components would improve sign's recognition accuracy. Among standard machine learning approaches, Logistic Regression produced the best results, 78.2\% of accuracy for dataset with 20 signs and 77.9\% of accuracy for dataset with 2 classes (statement vs question). Dataset can be downloaded from the following website: https://krslproject.github.io/krsl20/},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Mukushev et al_2020_Evaluation of Manual and Non-manual Components for Sign Language Recognition.pdf},
  isbn = {979-10-95546-34-4},
  keywords = {capturedBy=vision,isolated,isolated;vocab=20;signer=5;sl=krsl;capturedBy=vision,modal=bodyjoints,modal=eyebrows,modal=eyes,modal=head,modal=mouth,signer=5,sl=krsl,slreco,slreco;modal=bodyjoints;modal=head;modal=eyebrows;modal=eyes;modal=mouth,vocab<50,vocab=20},
  language = {English}
}

@techreport{muller_health_2004,
  title = {Health, {{Well}}-Being and {{Personal Safty}} of {{Women}} in {{Germany}}: {{A Representative Study}} of {{Violence Against Women}} in {{Germany}}; {{Summary}} of the {{Central Research Results}}},
  shorttitle = {Health, {{Well}}-Being and {{Personal Safty}} of {{Women}} in {{Germany}}},
  author = {M{\"u}ller, Ursula and Schr{\"o}ttle, Monika},
  year = {2004},
  institution = {{Federal Ministry for Family Affairs, Senior Citizens, Women and Youth}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Müller_Schröttle_2004_Health, Well-being and Personal Safty of Women in Germany.pdf}
}

@inproceedings{murakami_gesture_1991,
  title = {Gesture Recognition Using Recurrent Neural Networks},
  booktitle = {Proc. of the {{SIGCHI Conf}}. on {{Human Factors}} in {{Computing Systems}}},
  author = {Murakami, Kouichi and Taguchi, Hitomi},
  year = {1991},
  month = apr,
  pages = {237--242},
  publisher = {{ACM}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Murakami_Taguchi_1991_Gesture recognition using recurrent neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XGPS8G43/citation.html},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=10;signer=1;sl=JSL;capturedBy=electronicGlove,modal=location,modal=orientation,modal=shape,signer=1,sl=JSL,slreco,slreco;modal=shape;modal=orientation;modal=location,vocab<50,vocab=10}
}

@article{myers_level_1981,
  title = {A Level Building Dynamic Time Warping Algorithm for Connected Word Recognition},
  author = {Myers, C. and Rabiner, L.},
  year = {1981},
  month = apr,
  volume = {29},
  pages = {284--297},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1981.1163527},
  abstract = {Dynamic time warping has been shown to be an effective method of handling variations in the time scale of polysyllabic words spoken in isolation. This class of techniques has recently been applied to connected word recognition with high degrees of success. In this paper a level building technique is proposed for optimally time aligning a sequence of connected words with a sequence of isolated word reference patterns. The resulting algorithm, which has been found to be a special case of an algorithm previously described by Bahl and Jelinek, is shown to be significantly more efficient than the one recently proposed by Sakoe for connected word recognition, while maintaining the same accuracy in estimating the best possible matching string. An analysis of the level building method shows that it can be obtained as a modification to the Sakoe method by reversing the order of minimizations in the two-pass technique with some subsequent processing. This level building algorithm has a number of implementation parameters that can be used to control the efficiency of the method, as well as its accuracy. The nature of these parameters is discussed in this paper. In a companion paper we discuss the application of this level building time warping method to a connected digit recognition problem.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Myers_Rabiner_1981_A level building dynamic time warping algorithm for connected word recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IBEHSM9F/1163527.html},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  number = {2}
}

@article{nagendraswamy_lbpv_2016,
  title = {{{LBPV}} for {{Recognition}} of {{Sign Language}} at {{Sentence Level}}: {{An Approach Based}} on {{Symbolic Representation}}},
  shorttitle = {{{LBPV}} for {{Recognition}} of {{Sign Language}} at {{Sentence Level}}},
  author = {Nagendraswamy, H. S. and Kumara, B. M.},
  year = {2016},
  month = apr,
  volume = {26},
  pages = {371--385},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nagendraswamy_Kumara_LBPV for Recognition of Sign Language at Sentence Level.pdf},
  journal = {Journal of Intelligent Systems},
  keywords = {capturedBy=vision,continuous,continuous;vocab=26;signer=4;sl=IndianSL;capturedBy=vision,modal=fullframe,signer=4,sl=IndianSL,slreco,slreco;modal=fullframe,vocab<50,vocab=26},
  number = {2}
}

@article{nagendraswamy_symbolic_2015,
  title = {Symbolic {{Representation}} of {{Sign Language}} at {{Sentence Level}}},
  author = {Nagendraswamy, H. S. and Guru, D. S. and Naresh, Y. G.},
  year = {2015},
  volume = {7},
  pages = {49--60},
  issn = {2074-9082},
  doi = {10.5815/ijigsp},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nagendraswamy et al_2015_Symbolic Representation of Sign Language at Sentence Level.pdf},
  journal = {International Journal of Image, Graphics and Signal Processing(IJIGSP)},
  keywords = {capturedBy=vision,isolated,isolated;vocab=15;signer=4;sl=IndianSL;capturedBy=vision,modal=2hlocation,signer=4,sl=IndianSL,slreco,slreco;modal=2hlocation,vocab<50,vocab=15},
  number = {9}
}

@inproceedings{nair_rectified_2010,
  title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Learning}} ({{ICML}})},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  year = {2010},
  month = jun,
  pages = {807--814},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nair_Hinton_2010_Rectified linear units improve restricted boltzmann machines.pdf}
}

@inproceedings{nakamura_development_2013,
  title = {Development of Nodding Detection System Based on {{Active Appearance Model}}},
  booktitle = {2013 {{IEEE}}/{{SICE International Symposium}} on {{System Integration}} ({{SII}})},
  author = {Nakamura, K. and Watanabe, T. and Jindai, M.},
  year = {2013},
  month = dec,
  pages = {400--405},
  doi = {10.1109/SII.2013.6776678},
  abstract = {In face-to-face conversation, embodied rhythms between speech and body movements such as nodding are mutually synchronized between talkers. This synchrony is called entrainment in communication, and it generates the sharing of embodiment in human interaction. Embodied communication, which is closely related to behavioral and physiological entrainment, is an essential form of communication that forms the basis of interaction between talkers through mutual embodiment. In particular, nodding has an important role as a regulator in embodied interaction and communication. The detection of nodding is useful for an estimation of the activity of conversation. In this paper, we propose a nodding detection model based on the analysis of the head movement involved in nodding and introduce a system that uses this model. In this system, nodding is detected from the rotational movement of a head, which is estimated by face tracking using Active Appearance Model. Furthermore, the effectiveness of the system is demonstrated in comparison with the nodding detection by visual inspection.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nakamura et al_2013_Development of nodding detection system based on Active Appearance Model.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JC64CZNN/login.html}
}

@inproceedings{nakamura_multimodal_2002,
  title = {Multi-Modal Temporal Asynchronicity Modeling by Product {{HMMs}} for Robust Audio-Visual Speech Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Multimodal Interfaces}} ({{ICMI}})},
  author = {Nakamura, Satoshi and Kumatani, Ken'ichi and Tamura, Satoshi},
  year = {2002},
  pages = {305--309},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nakamura et al_2002_Multi-modal temporal asynchronicity modeling by product HMMs for robust.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZBEIWGIF/abs_all.html}
}

@article{napier_its_2011,
  title = {``{{It}}'s Not What They Say but the Way They Say It''. {{A}} Content Analysis of Interpreter and Consumer Perceptions towards Signed Language Interpreting in {{Australia}}},
  author = {Napier, Jemina},
  year = {2011},
  volume = {2011},
  pages = {59--87},
  issn = {1613-3668},
  doi = {10.1515/ijsl.2011.003},
  abstract = {This paper presents findings of an innovative study, which involved the thematic and content analyses of discussions held by deaf people, hearing people and interpreters about signed language interpreting in Australia. Six focus groups yielded eight hours of data, which was analyzed to identify themes that emerged about participants' perceptions about interpreters and interpreting. Examples are given to compare how participants view the signed language interpreting profession, and to discuss the expectations of all parties of signed language interpreter-mediated encounters. The focus of analysis is on key themes that were evident from the most frequently used words/signs. The findings provide a clearer understanding of the relationship between consumers and interpreters, and attitudes towards signed language interpreters and interpreting in Australia.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Napier_2011_“It's not what they say but the way they say it”.pdf},
  journal = {International Journal of the Sociology of Language},
  number = {207}
}

@inproceedings{nasr_comprehensive_2019,
  title = {Comprehensive {{Privacy Analysis}} of {{Deep Learning}}: {{Passive}} and {{Active White}}-Box {{Inference Attacks}} against {{Centralized}} and {{Federated Learning}}},
  shorttitle = {Comprehensive {{Privacy Analysis}} of {{Deep Learning}}},
  booktitle = {2019 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  year = {2019},
  month = may,
  pages = {739--753},
  issn = {2375-1207},
  doi = {10.1109/SP.2019.00065},
  abstract = {Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nasr et al_2019_Comprehensive Privacy Analysis of Deep Learning2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/29VEVPFA/8835245.html},
  keywords = {active membership inference attacks,active white-box inference attacks,black-box attacks,centralized learning,CIFAR dataset,comprehensive privacy analysis,Computational modeling,Data models,data privacy,Deep learning,deep learning models,deep neural networks,Deep-Learning,federated learning setting,Federated-Learning,gradient methods,Inference algorithms,inference mechanisms,Inference-Attacks,learning (artificial intelligence),Membership-Inference,neural nets,Privacy,privacy leakage,stochastic gradient descent algorithm,stochastic processes,Stochastic-Gradient-Descent,Training,Training data,white-box membership inference attacks}
}

@inproceedings{natarajan_coupled_2007,
  title = {Coupled Hidden Semi Markov Models for Activity Recognition},
  booktitle = {Motion and {{Video Computing}}, 2007. {{WMVC}}'07. {{IEEE Workshop}} On},
  author = {Natarajan, Pradeep and Nevatia, Ramakant},
  year = {2007},
  pages = {10--10},
  publisher = {{IEEE}}
}

@inproceedings{nayak_automated_2009,
  title = {Automated Extraction of Signs from Continuous Sign Language Sentences Using Iterated Conditional Modes},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nayak, Sunita and Sarkar, Sudeep and Loeding, Barbara},
  year = {2009},
  month = jun,
  pages = {2583--2590},
  address = {{Miami, FL, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nayak et al_2009_Automated extraction of signs from continuous sign language sentences using.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZD6NVZ27/login.html}
}

@article{nayak_finding_2012,
  title = {Finding {{Recurrent Patterns}} from {{Continuous Sign Language Sentences}} for {{Automated Extraction}} of {{Signs}}},
  author = {Nayak, Sunita and Duncan, Kester and Sarkar, Sudeep and Loeding, Barbara},
  year = {2012},
  month = sep,
  volume = {13},
  pages = {2589-2615},
  abstract = {We present a probabilistic framework to automatically learn models of recurring signs from multiple sign language video sequences containing the vocabulary of interest. We extract the parts of the signs that are present in most occurrences of the sign in context and are robust to the variations produced by adjacent signs. Each sentence video is first transformed into a multidimensional time series representation, capturing the motion and shape aspects of the sign. Skin color blobs are extracted from frames of color video sequences, and a probabilistic relational distribution is formed for each frame using the contour and edge pixels from the skin blobs. Each sentence is represented as a trajectory in a low dimensional space called the space of relational distributions. Given these time series trajectories, we extract signemes from multiple sentences concurrently using iterated conditional modes (ICM). We show results by learning single signs from a collection of sentences with one common pervading sign, multiple signs from a collection of sentences with more than one common sign, and single signs from a mixed collection of sentences. The extracted signemes demonstrate that our approach is robust to some extent to the variations produced within a sign due to different contexts. We also show results whereby these learned sign models are used for spotting signs in test sequences.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2012/Nayak et al/Nayak et al_2012_Finding Recurrent Patterns from Continuous Sign Language Sentences for.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nayak et al_2012_Finding Recurrent Patterns from Continuous Sign Language Sentences for.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PW9ZVGV7/citation.html},
  journal = {Journal of Machine Learning Research}
}

@inproceedings{nayak_unsupervised_2005,
  title = {Unsupervised Modeling of Signs Embedded in Continuous Sentences},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Nayak, Sunita and Sarkar, Sudeep and Loeding, Barbara},
  year = {2005},
  pages = {81--81},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nayak et al_2005_Unsupervised modeling of signs embedded in continuous sentences.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3S567I3E/1565388.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=18;signer=1;sl=ASL;capturedBy=vision,modal=movement,signer=1,sl=ASL,slreco,slreco;modal=movement,vocab<50,vocab=18}
}

@inproceedings{nefian_coupled_2002,
  title = {A Coupled {{HMM}} for Audio-Visual Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Nefian, Ara V. and Liang, Luhong and Pi, Xiaobo and Xiaoxiang, Liu and Mao, Crusoe and Murphy, Kevin},
  year = {2002},
  volume = {2},
  pages = {II--2013},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nefian et al_2002_A coupled HMM for audio-visual speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N6PVSNZC/abs_all.html}
}

@article{nefian_dynamic_2002,
  title = {Dynamic {{Bayesian Networks}} for {{Audio}}-{{Visual Speech Recognition}}},
  author = {Nefian, Ara V. and Liang, Luhong and Pi, Xiaobo and Liu, Xiaoxing and Murphy, Kevin},
  year = {2002},
  volume = {11},
  pages = {1274--1288},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nefian et al_2002_Dynamic Bayesian Networks for Audio-Visual Speech Recognition.pdf},
  journal = {EURASIP Journal on Applied Signal Processing}
}

@article{negin_praxis_2018,
  title = {{{PRAXIS}}: {{Towards}} Automatic Cognitive Assessment Using Gesture Recognition},
  shorttitle = {{{PRAXIS}}},
  author = {Negin, Farhood and Rodriguez, Pau and Koperski, Michal and Kerboua, Adlen and Gonz{\`a}lez, Jordi and Bourgeois, Jeremy and Chapoulie, Emmanuelle and Robert, Philippe and Bremond, Francois},
  year = {2018},
  volume = {106},
  pages = {21--35},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Negin et al_2018_PRAXIS.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Negin et al_2018_PRAXIS2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JRLCCDJC/S0957417418302203.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WCKUH3J7/S0957417418302203.html},
  journal = {Expert Systems with Applications},
  keywords = {Computer assisted diagnosis,Cybercare industry applications,Medical services,Patient monitoring}
}

@inproceedings{neidle_challenges_2012,
  title = {Challenges in Development of the American Sign Language Lexicon Video Dataset (Asllvd) Corpus},
  booktitle = {Proc. 5th {{Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}: {{Interactions}} between {{Corpus}} and {{Lexicon}}},
  author = {Neidle, Carol and Thangali, Ashwin and Sclaroff, Stan},
  year = {2012},
  month = may,
  pages = {143--150},
  address = {{Istanbul, Turkey}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Neidle et al_2012_Challenges in development of the american sign language lexicon video dataset.pdf}
}

@inproceedings{neidle_new_2012,
  title = {A New Web Interface to Facilitate Access to Corpora: Development of the {{ASLLRP}} Data Access Interface},
  shorttitle = {A New Web Interface to Facilitate Access to Corpora},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Neidle, Carol and Vogler, Christian},
  year = {2012},
  pages = {137--142},
  publisher = {{ELRA}},
  address = {{Istanbul, Turkey}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Neidle_Vogler_2012_A new web interface to facilitate access to corpora.pdf}
}

@techreport{neti_audiovisual_2000,
  title = {Audio-Visual Speech Recognition},
  author = {Neti, Chalapathy and Potamianos, Gerasimos and Luettin, Juergen and Matthews, Iain and Glotin, Herve and Vergyri, Dimitra and Sison, June and Mashari, Azad and Zhou, Jie},
  year = {2000},
  month = oct,
  pages = {1--86},
  address = {{Martigny, Switzerland}},
  institution = {{IDIAP}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Neti et al_2000_Audio-visual speech recognition.pdf},
  language = {en},
  number = {Idiap-RR-35-2000},
  type = {Techreport}
}

@inproceedings{neti_neuromorphic_1994,
  title = {Neuromorphic Speech Processing for Noisy Environments},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Neural Networks}}},
  author = {Neti, C.},
  year = {1994},
  month = jun,
  volume = {7},
  pages = {4425-4430 vol.7},
  doi = {10.1109/ICNN.1994.374982},
  abstract = {Current speech recognition systems perform very poorly in the presence of background noise, particularly for signal-to-noise ratios (SNR) below 10 dB and for certain noise conditions such as cafeteria noise. In this study we investigate the use of acoustic processing based on cochlear models and neural-like processing as a means of arriving at noise robust acoustic representation of speech. However, unlike previous work based on cochlear models that used cochlear filter parameters based on neurophysiological data, we optimize cochlear filter shape and thresholds to reduce the noise contribution in the resulting acoustic representations. Results suggest that average SNR improvements of the order of 5-10 dB can be obtained for noise corrupted signals with SNRs near 0-6 dB for realistic noise such as cafeteria noise. Furthermore, using a neural network to include context and arrive at a lower dimensional representation can lead to further improvements in SNR},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Neti_1994_Neuromorphic speech processing for noisy environments.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/T3MJBSEZ/abs_all.html}
}

@inproceedings{neto_development_1997,
  title = {The {{Development}} of a {{Speaker Independent Continuous Speech Recognizer}} for \{\vphantom\}{{Portuguese}}\vphantom\{\}},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Neto, J. P and Martins, C. A and Almeida, L. B},
  year = {1997},
  address = {{Rhodes, Greece}}
}

@inproceedings{neto_extreme_2015,
  title = {Extreme {{Learning Machine}} for {{Real Time Recognition}} of {{Brazilian Sign Language}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Neto, F. M. d P. and Cambuim, L. F. and Macieira, R. M. and Ludermir, T. B. and Zanchettin, C. and Barros, E. N.},
  year = {2015},
  month = oct,
  pages = {1464--1469},
  doi = {10.1109/SMC.2015.259},
  abstract = {The quantity of computing application that interacts with users through gesture or body motion has been growing. Among these applications is the sign language recognizer used to help hearing impaired people. This work proposes an architecture able to recognize Brazilian sign language (LIBRAS) in an embedded platform. The system focuses on a simple feature from 'finger spelling expressions' represented by a series of hands gestural images, and uses the Extreme Learning Machine network to classify them. The proposed structure uses camera images only and does not need any gloves or sensors. The obtained results are 5 times faster and 16 times better than classical approaches.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Neto et al/Neto et al_2015_Extreme Learning Machine for Real Time Recognition of Brazilian Sign Language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Neto et al_2015_Extreme Learning Machine for Real Time Recognition of Brazilian Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MKUE4V4W/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TC7WHKNG/abs_all.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=18;sl=Libras;capturedBy=vision,modal=shape,sl=Libras,slreco,slreco;modal=shape,vocab<50,vocab=18}
}

@inproceedings{neverova_multiscale_2014,
  title = {Multi-Scale {{Deep Learning}} for {{Gesture Detection}} and {{Localization}}},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Neverova, Natalia and Wolf, Christian and Taylor, Graham W. and Nebout, Florian},
  editor = {Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten},
  year = {2014},
  month = sep,
  pages = {474--490},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-319-16178-5_33},
  abstract = {We present a method for gesture detection and localization based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at two temporal scales. Key to our technique is a training strategy which exploits i) careful initialization of individual modalities; and ii) gradual fusion of modalities from strongest to weakest cross-modality structure. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams.},
  copyright = {\textcopyright 2015 Springer International Publishing Switzerland},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Neverova et al_2014_Multi-scale Deep Learning for Gesture Detection and Localization.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4ZCFVBJW/978-3-319-16178-5_33.html},
  isbn = {978-3-319-16177-8 978-3-319-16178-5},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{ney_acoustic_1990,
  title = {Acoustic Modeling of Phoneme Units for Continuous Speech Recognition},
  booktitle = {Proc. {{European Signal Processing Conference}}},
  author = {Ney, Hermann},
  year = {1990},
  month = sep,
  pages = {65--72},
  address = {{Barcelona, Spain}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ney_1990_Acoustic modeling of phoneme units for continuous speech recognition.pdf}
}

@misc{ney_image_2010,
  title = {Image Recognition and 2d Warping, {{Lecture Notes}}},
  author = {Ney, Hermann and Dreuw, Philippe and Gass, Tobias and Pishchulin, L.},
  year = {2010},
  publisher = {{RWTH Aachen University}}
}

@article{ney_progress_2000,
  title = {Progress in Dynamic Programming Search for {{LVCSR}}},
  author = {Ney, Hermann and Ortmanns, Stefan},
  year = {2000},
  volume = {88},
  pages = {1224--1240},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ney_Ortmanns_2000_Progress in dynamic programming search for LVCSR.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TDZ8PPMU/880081.html},
  journal = {Proceedings of the IEEE},
  number = {8}
}

@article{ney_use_1984,
  title = {The Use of a One-Stage Dynamic Programming Algorithm for Connected Word Recognition},
  author = {Ney, H.},
  year = {1984},
  month = apr,
  volume = {32},
  pages = {263--271},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1984.1164320},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ney_1984_The use of a one-stage dynamic programming algorithm for connected word.pdf},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  number = {2}
}

@article{ng_prosodic_2010,
  title = {Prosodic {{Attribute Model}} for {{Spoken Language Identification}}},
  author = {Ng, Raymond W. M. and Leung, Cheung-Chi and Lee, Tan and Ma, Bin},
  year = {2010},
  pages = {5022--5025},
  journal = {Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP)}
}

@article{ng_realtime_2002,
  title = {Real-Time Gesture Recognition System and Application},
  author = {Ng, Chan Wah and Ranganath, Surendra},
  year = {2002},
  volume = {20},
  pages = {993--1007},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ng_Ranganath_2002_Real-time gesture recognition system and application.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7S34PHR7/S0262885602001130.html},
  journal = {Image and Vision computing},
  number = {13}
}

@inproceedings{ng_short_2015,
  title = {Beyond {{Short Snippets}}: {{Deep Networks}} for {{Video Classification}}},
  shorttitle = {Beyond {{Short Snippets}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ng, Joe Yue-Hei and Hausknecht, Matthew and Vijayanarasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat and Toderici, George},
  year = {2015},
  pages = {4694--4702},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ng et al_2015_Beyond Short Snippets.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UNSCVZQ2/Ng_Beyond_Short_Snippets_2015_CVPR_paper.html}
}

@article{nguyen_facial_2012,
  title = {Facial Expressions in {{American}} Sign Language: {{Tracking}} and Recognition},
  shorttitle = {Facial Expressions in {{American}} Sign Language},
  author = {Nguyen, Tan Dat and Ranganath, Surendra},
  year = {2012},
  month = may,
  volume = {45},
  pages = {1877--1891},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2011.10.026},
  abstract = {This paper presents work towards recognizing facial expressions that are used in sign language communication. Facial features are tracked to effectively capture temporal visual cues on the signers' face during signing. Face shape constraints are used for robust tracking within a Bayesian framework. The constraints are specified through a set of face shape subspaces learned by Probabilistic Principal Component Analysis (PPCA). An update scheme is also used to adapt to persons with different face shapes. Two tracking algorithms are presented, which differ in the way the face shape constraints are enforced. The results show that the proposed trackers can track facial features with large head motions, substantial facial deformations, and temporary facial occlusions by hand. The tracked results are input to a recognition system comprising Hidden Markov Models (HMM) and a support vector machine (SVM) to recognize six isolated facial expressions representing grammatical markers in American sign language (ASL). Tracking error of less than four pixels (on 640\texttimes 480 videos) was obtained with probability greater than 90\%; in comparison the KLT tracker yielded this accuracy with 76\% probability. Recognition accuracy obtained for ASL facial expressions was 91.76\% in person dependent tests and 87.71\% in person independent tests.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nguyen_Ranganath_2012_Facial expressions in American sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WVZJI5FS/S0031320311004559.html},
  journal = {Pattern Recognition},
  number = {5}
}

@inproceedings{nguyen_tracking_2008,
  title = {Tracking Facial Features under Occlusions and Recognizing Facial Expressions in Sign Language},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Nguyen, Tan Dat and Ranganath, Surendra},
  year = {2008},
  month = sep,
  pages = {1--7},
  doi = {10.1109/AFGR.2008.4813464},
  abstract = {This paper presents work towards recognizing facial expressions that are used in sign language recognition. Facial features are tracked to effectively capture temporal visual cues on the signer's face during signing. A Bayesian framework is proposed as a feedback mechanism to the Kanade-Lucas-Tomasi (KLT) tracker for reliably tracking facial features in the presence of head motions and temporary occlusions by hand. This mechanism relies on a set of face shape subspaces learned by probabilistic principal component analysis with an update scheme to adapt to persons with different face shapes. The results show that the proposed tracker can track facial features with large head motions, substantial facial deformations, and temporary facial occlusions by hand. The tracked results were input to a recognition system comprising HMMs and a NN to recognize four common American sign language facial expressions.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nguyen_Ranganath_2008_Tracking facial features under occlusions and recognizing facial expressions in.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QEPW46BH/abs_all.html}
}

@phdthesis{niessen:smtdiss02,
  title = {Improving {{Statistical Machine Translation}} Using {{Morpho}}-Syntactic {{Information}}},
  author = {Nie{\ss}en, S.},
  year = {2002},
  month = dec,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nießen_2002_Improving Statistical Machine Translation using Morpho-syntactic Information.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{niewiadomski_greta_2009,
  title = {Greta: An Interactive Expressive {{ECA}} System},
  shorttitle = {Greta},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Autonomous Agents}} and {{Multiagent Systems}}},
  author = {Niewiadomski, Radoslaw and Bevacqua, Elisabetta and Mancini, Maurizio and Pelachaud, Catherine},
  year = {2009},
  volume = {2},
  pages = {1399--1400},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Niewiadomski et al_2009_Greta.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PCHBWESR/citation.html}
}

@incollection{NIPS2014_5353,
  title = {Two-{{Stream Convolutional Networks}} for {{Action Recognition}} in {{Videos}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {568--576},
  publisher = {{Curran Associates, Inc.}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Simonyan_Zisserman_2014_Two-Stream Convolutional Networks for Action Recognition in Videos.pdf}
}

@incollection{NIPS2015_5847,
  title = {Attention-{{Based Models}} for {{Speech Recognition}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chorowski, Jan K and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {577--585},
  publisher = {{Curran Associates, Inc.}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chorowski et al_2015_Attention-Based Models for Speech Recognition.pdf}
}

@incollection{NIPS2015_5955,
  title = {Convolutional {{LSTM Network}}: {{A Machine Learning Approach}} for {{Precipitation Nowcasting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {SHI, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and WOO, Wang-chun},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {802--810},
  publisher = {{Curran Associates, Inc.}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/SHI et al_2015_Convolutional LSTM Network.pdf}
}

@inproceedings{nishida_multimodal_2015,
  title = {Multimodal {{Gesture Recognition Using Multi}}-Stream {{Recurrent Neural Network}}},
  booktitle = {Image and {{Video Technology}}},
  author = {Nishida, Noriki and Nakayama, Hideki},
  year = {2015},
  month = nov,
  pages = {682--694},
  publisher = {{Springer, Cham}},
  doi = {10.1007/978-3-319-29451-3_54},
  abstract = {In this paper, we present a novel method for multimodal gesture recognition based on neural networks. Our multi-stream recurrent neural network (MRNN) is a completely data-driven model that can be trained from end to end without domain-specific hand engineering. The MRNN extends recurrent neural networks with Long Short-Term Memory cells (LSTM-RNNs) that facilitate the handling of variable-length gestures. We propose a recurrent approach for fusing multiple temporal modalities using multiple streams of LSTM-RNNs. In addition, we propose alternative fusion architectures and empirically evaluate the performance and robustness of these fusion strategies. Experimental results demonstrate that the proposed MRNN outperforms other state-of-the-art methods in the Sheffield Kinect Gesture (SKIG) dataset, and has significantly high robustness to noisy inputs.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Nishida_Nakayama/Nishida_Nakayama_2015_Multimodal Gesture Recognition Using Multi-stream Recurrent Neural Network2.pdf},
  language = {en}
}

@inproceedings{nishio_elicitation_2010,
  title = {Elicitation Methods in the {{DGS}} ({{German Sign Language}}) {{Corpus Project}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Nishio, Rie and Hong, Sung-Eun and K{\"o}nig, Susanne and Konrad, Reiner and Langer, Gabriele and Hanke, Thomas and Rathmann, Christian},
  year = {2010},
  month = may,
  pages = {178--185},
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nishio et al_2010_Elicitation methods in the DGS (German Sign Language) Corpus Project.pdf}
}

@misc{nist_lre_2009,
  title = {{{LRE}} Progress in {{LID}} Performance.},
  author = {NIST},
  year = {2009},
  month = apr
}

@article{nock_modelling_2002,
  title = {Modelling Asynchrony in Automatic Speech Recognition Using Loosely Coupled Hidden {{Markov}} Models},
  author = {Nock, Harriet J. and Young, Steve J.},
  year = {2002},
  volume = {26},
  pages = {283--301},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nock_Young_2002_Modelling asynchrony in automatic speech recognition using loosely coupled.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BR279VTM/S036402130200071X.html},
  journal = {Cognitive Science},
  number = {3}
}

@phdthesis{nolden_progress_2017,
  title = {Progress in {{Decoding}} for {{Large Vocabulary Continuous Speech Recognition}}},
  author = {Nolden, David},
  year = {2017},
  month = apr,
  doi = {10.13140/RG.2.2.21241.98405},
  abstract = {The subject of this thesis is the search problem in automatic speech recognition. The search is responsible for matching an incoming acoustic speech signal with statistical speech models, in order to find the word sequence which is most most likely to have been spoken. In principle, it is necessary to enumerate all possible word sequences, to compute a likelihood for each word sequence according to the models, and to select the best one. When the vocabulary is large, then such a straightforward approach is not feasible, due to the huge number of possible word sequences; instead, state-of-the-art approaches transform the models into compact search network structures, match the input signal time-synchronously against the search network, and exploit recombination and pruning to limit the search effort. 
In this work, we analyze existing search strategies, combine them, and introduce novel extensions which further improve their efficiency and precision. We give a holistic overview of the ingredients required for efficient search.
We investigate how the search network should be structured, and how the search space can be managed most efficiently. Normally, the search space depends on the language model; we introduce a novel search space management algorithm, which partially decouples the search effort from the language model's order.
We introduce a novel framework which explains why pruning is possible, and which helps motivating and finding effective pruning methods; it establishes a direct relationship between pruning and recombination. Then we analyze common pruning methods regarding effectiveness and motivation, introduce novel pruning methods, and propose improved look-ahead techniques which make the pruning more effective.
Pruning induces a certain amount of search errors, and usually a specific trade-off between precision and efficiency needs to be selected manually. In a last step, we show how search errors can be detected, and derive a search algorithm which allows efficient search without search errors.
All methods are evaluated experimentally on a variety of state-of-the-art speech recognition tasks. On all tasks, a considerable reduction of the search space is achieved using the new methods, and overall, a speedup of the core search by a factor of more than 10 is achieved in comparison to the baseline method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nolden_2017_Progress in Decoding for Large Vocabulary Continuous Speech Recognition.pdf}
}

@inproceedings{nowozin_fgan_2016,
  title = {F-{{GAN}}: {{Training}} Generative Neural Samplers Using Variational Divergence Minimization},
  shorttitle = {F-{{GAN}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
  year = {2016},
  pages = {271--279},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nowozin et al_2016_f-GAN.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/478AS5TJ/6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization.html}
}

@inproceedings{nussbaum-thom_compound_2011,
  title = {Compound {{Word Recombination}} for {{German LVCSR}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {{Nussbaum-Thom}, Markus and Mousa, Amr El-Desoky and Schluter, Ralf and Ney, Hermann},
  year = {2011},
  month = aug,
  pages = {1449--1452},
  address = {{Florence, Italy}},
  abstract = {Compound words are a difficulty for German speech recognition systems since they cause high out-of-vocabulary and word error rates. State of the art approaches augment the language model by the fragments of compounds in order to increase lexical coverage, lower the perplexity and out-of-vocabulary rate. The fragments are tagged in order to concatenate subsequent equally tagged fragments in the recognition result, but this does not guarantee the recombination of proper words. Such recombination techniques neglect the large vocabulary of the language model training data for recombination although most compounds are covered by it. In this paper, we investigate the use of this vocabulary for the recombination of compound words from the recognition result. The approach is tested on two large vocabulary tasks on top of full-word and fragment based language models and achieves good improvements of 3\textendash 7\% relative over the baseline compound-sensitive word error rate.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Nussbaum-Thom et al_2011_Compound Word Recombination for German LVCSR.pdf},
  language = {en}
}

@incollection{nyst_sign_2015,
  title = {Sign {{Language Fieldwork}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Nyst, Victoria},
  year = {2015},
  pages = {105--122},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch7},
  abstract = {This chapter focuses on fieldwork in undocumented sign languages in the former type of setting. It discusses fieldwork issues for descriptive or documentary purposes. More recently the process of data collection has developed into an independent field, in the form of documentary linguistics. The chapter provides an overview of some of the human, practical, and technical challenges one may face when doing sign language fieldwork, highlighting differences from the existing canon of data collection methods for sign language; and differences from fieldwork on undocumented spoken languages. One of the reasons why sign languages remain undocumented for a long time is their coexistence with a larger or more prestigious sign language, which leads to negative perception of the local sign language and to a subsequent tendency to not overtly identify with its use and/or to shift to the prestigious sign language. Such unbalanced unimodal bilingualism affects the fieldwork in various ways.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8BTL7K88/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@mastersthesis{oberdorfer_investigations_2012,
  title = {Investigations on {{Decoding}} of {{Parallel Input Streams}} in {{Sign Language}}},
  author = {Oberd{\"o}rfer, Christian and Oberd{\"o}rfer, Christian and Leibe, B. and Forster, J.},
  year = {2012},
  month = mar,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oberdörfer_2012_Investigations on Decoding of Parallel Input Streams in Sign Language.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=455;signer=1;sl=DGS;capturedBy=vision;dataSet=Signum,continuous;vocab=455;signer=25;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Signum,modal=2hlocation,modal=2hshape,modal=head,signer=1,signer=25,sl=DGS,slreco,slreco;modal=2hlocation;modal=2hshape;modal=head,vocab<500,vocab=455},
  language = {en},
  school = {RWTH Aachen University}
}

@article{oberweger_hands_2015,
  title = {Hands {{Deep}} in {{Deep Learning}} for {{Hand Pose Estimation}}},
  author = {Oberweger, Markus and Wohlhart, Paul and Lepetit, Vincent},
  year = {2015},
  month = feb,
  abstract = {We introduce and evaluate several architectures for Convolutional Neural Networks to predict the 3D joint locations of a hand given a depth map. We first show that a prior on the 3D pose can be easily introduced and significantly improves the accuracy and reliability of the predictions. We also show how to use context efficiently to deal with ambiguities between fingers. These two contributions allow us to significantly outperform the state-of-the-art on several challenging benchmarks, both in terms of accuracy and computation times.},
  archivePrefix = {arXiv},
  eprint = {1502.06807},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oberweger et al_2015_Hands Deep in Deep Learning for Hand Pose Estimation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZKM7ABI8/1502.html},
  journal = {arXiv:1502.06807 [cs]},
  primaryClass = {cs}
}

@article{och_systematic_2003,
  title = {A {{Systematic Comparison}} of {{Various Statistical Alignment Models}}},
  author = {Och, Franz Josef and Ney, Hermann},
  year = {2003},
  month = mar,
  volume = {29},
  pages = {19--51},
  issn = {0891-2017},
  doi = {10.1162/089120103321337421},
  abstract = {We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Och_Ney_2003_A Systematic Comparison of Various Statistical Alignment Models.pdf},
  journal = {Computational Linguistics},
  number = {1}
}

@phdthesis{och:smtdiss02,
  title = {Statistical {{Machine Translation}}: {{From Single}}-{{Word Models}} to {{Alignment Templates}}},
  author = {Och, F. J.},
  year = {2002},
  month = oct,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Och_2002_Statistical Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@article{oconnor_language_2017,
  title = {The {{Language}} of {{Glove}}: {{Wireless}} Gesture Decoder with Low-Power and Stretchable Hybrid Electronics},
  shorttitle = {The {{Language}} of {{Glove}}},
  author = {O'Connor, Timothy F. and Fach, Matthew E. and Miller, Rachel and Root, Samuel E. and Mercier, Patrick P. and Lipomi, Darren J.},
  year = {2017},
  month = jul,
  volume = {12},
  pages = {e0179766},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0179766},
  abstract = {This communication describes a glove capable of wirelessly translating the American Sign Language (ASL) alphabet into text displayable on a computer or smartphone. The key components of the device are strain sensors comprising a piezoresistive composite of carbon particles embedded in a fluoroelastomer. These sensors are integrated with a wearable electronic module consisting of digitizers, a microcontroller, and a Bluetooth radio. Finite-element analysis predicts a peak strain on the sensors of 5\% when the knuckles are fully bent. Fatigue studies suggest that the sensors successfully detect the articulation of the knuckles even when bent to their maximal degree 1,000 times. In concert with an accelerometer and pressure sensors, the glove is able to translate all 26 letters of the ASL alphabet. Lastly, data taken from the glove are used to control a virtual hand; this application suggests new ways in which stretchable and wearable electronics can enable humans to interface with virtual environments. Critically, this system was constructed of components costing less than \$100 and did not require chemical synthesis or access to a cleanroom. It can thus be used as a test bed for materials scientists to evaluate the performance of new materials and flexible and stretchable hybrid electronics.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/O’Connor et al_2017_The Language of Glove.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IEVWSHEB/article.html},
  journal = {PLOS ONE},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=26;sl=ASL;capturedBy=electronicGlove,modal=movement,modal=orientation,modal=shape,sl=ASL,slreco,slreco;modal=shape;modal=orientation;modal=movement,vocab<50,vocab=26},
  language = {en},
  number = {7}
}

@incollection{oh_avatar-based_2014,
  title = {An {{Avatar}}-{{Based Weather Forecast Sign Language System}} for the {{Hearing}}-{{Impaired}}},
  booktitle = {Artificial {{Intelligence Applications}} and {{Innovations}}},
  author = {Oh, Juhyun and Jeon, Seonggyu and Kim, Minho and Kwon, Hyukchul and Kim, Iktae},
  year = {2014},
  pages = {519--527},
  publisher = {{Springer}}
}

@inproceedings{okada_recognizing_2017,
  title = {Recognizing {{Words}} from {{Gestures}}: {{Discovering Gesture Descriptors Associated}} with {{Spoken Utterances}}},
  shorttitle = {Recognizing {{Words}} from {{Gestures}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Okada, S. and Otsuka, K.},
  year = {2017},
  month = may,
  pages = {430--437},
  doi = {10.1109/FG.2017.60},
  abstract = {This study investigates a new challenge: modelling the relationship between gestures and spoken words during continuous hand motions, speech and language data. The problem setting and modelling is defined as ``gesture association'' (associating gestures with words). We present a framework to associate spoken words with hand motion data observed from a optical motion-capture system. The framework identifies pairs of hand motions (gesture) and uttered words as training samples by autonomously aligning the speech and the co-occurring gestures. Using the samples, a supervised learning approach is undertaken to learn a model to discriminate between gestures that co-occur with a spoken word (w) and gestures when the word w is not being spoken. To detect gestures and associate them with spoken words, we extract (1) a trajectory signal feature set, (2) features of various gesture phases from hand motion data and (3) primitive gesture patterns learned by Sift Invariant Sparse Coding (SISC). Then Hidden Markov Model (HMM) and Support Vector Machine (SVM) classifiers were trained with these three-feature sets. In experiments, the classification accuracy achieved for 80 words was above 60 \% (the maximum accuracy achieved was 71 \%) using the proposed framework. In particular, SVMs trained with features (2) and (3) outperform an HMM trained with feature (1) (prepared as a baseline model). The results show that the gesture phase and primitive patterns trained by SISC are effective gesture features for recognizing the words accompanying gestures.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Okada_Otsuka_2017_Recognizing Words from Gestures.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K7AI3T38/7961773.html}
}

@article{okombo_language_2009,
  title = {Language Convergence and Wave Phenomena in the Growth of a National Sign Language in {{Kenya}}},
  author = {OKOMBO, D. O. and AKACH, P. O.},
  year = {2009},
  volume = {125},
  pages = {131--144},
  issn = {1613-3668},
  doi = {10.1515/ijsl.1997.125.131},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/OKOMBO_AKACH_2009_Language convergence and wave phenomena in the growth of a national sign.pdf},
  journal = {International Journal of the Sociology of Language},
  number = {1}
}

@article{okwechime_mimic_2011,
  title = {{{MIMiC}}: {{Multimodal Interactive Motion Controller}}},
  author = {Okwechime, Dumebi and Ong, Eng-Jon and Bowden, Richard},
  year = {2011},
  volume = {13},
  pages = {255--265},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Okwechime et al_2011_MIMiC.pdf},
  journal = {IEEE Transactions on Multimedia},
  number = {2}
}

@inproceedings{okwechime_visualisation_2011,
  title = {Visualisation and Prediction of Conversation Interest through Mined Social Signals},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Okwechime, Dumebi and Ong, Eng-Jon and Gilbert, Andrew and Bowden, Richard Richard},
  year = {2011},
  pages = {951--956},
  publisher = {{IEEE}},
  doi = {http://dx.doi.org/10.1109/FG.2011.5771380},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Okwechime et al_2011_Visualisation and prediction of conversation interest through mined social.pdf},
  isbn = {978-1-4244-9140-7}
}

@article{oliva_modeling_2001,
  title = {Modeling the Shape of the Scene: {{A}} Holistic Representation of the Spatial Envelope},
  shorttitle = {Modeling the Shape of the Scene},
  author = {Oliva, Aude and Torralba, Antonio},
  year = {2001},
  volume = {42},
  pages = {145--175},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {3}
}

@article{ong_automatic_2005,
  title = {Automatic Sign Language Analysis: {{A}} Survey and the Future beyond Lexical Meaning},
  shorttitle = {Automatic Sign Language Analysis},
  author = {Ong, Sylvie CW and Ranganath, Surendra},
  year = {2005},
  volume = {27},
  pages = {873--891},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ong_Ranganath_2005_Automatic sign language analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3N3EN6FX/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {6}
}

@inproceedings{ong_learning_2011,
  title = {Learning {{Sequential Patterns}} for {{Lipreading}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Ong, Eng-Jon and Bowden, Richard},
  year = {2011},
  pages = {55.1--55.10},
  publisher = {{BMVA Press}},
  doi = {http://dx.doi.org/10.5244/C.25.55},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ong_Bowden_2011_Learning Sequential Patterns for Lipreading.pdf},
  isbn = {1-901725-43-X}
}

@article{ong_robust_,
  title = {Robust {{Lip}}-{{Tracking}} Using {{Rigid Flocks}} of {{Selected Linear Predictors}}},
  author = {Ong, E. J and Bowden, R. and GU27XH, G.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ong et al_Robust Lip-Tracking using Rigid Flocks of Selected Linear Predictors.pdf}
}

@inproceedings{ong_robust_2009,
  title = {Robust Facial Feature Tracking Using Selected Multi-Resolution Linear Predictors},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Ong, E. J and Lan, Y. and Theobald, B. and Harvey, R. and Bowden, R.},
  year = {2009},
  pages = {1483--1490},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ong et al_2010_Robust facial feature tracking using selected multi-resolution linear predictors.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M9U5ZR8G/down.html}
}

@inproceedings{ong_sign_2012,
  title = {Sign {{Language Recognition}} Using {{Sequential Pattern Trees}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ong, Eng-Jon and Cooper, H. and Pugeault, N. and Bowden, R.},
  year = {2012},
  month = jun,
  pages = {2200--2207},
  doi = {10.1109/CVPR.2012.6247928},
  abstract = {This paper presents a novel, discriminative, multi-class classifier based on Sequential Pattern Trees. It is efficient to learn, compared to other Sequential Pattern methods, and scalable for use with large classifier banks. For these reasons it is well suited to Sign Language Recognition. Using deterministic robust features based on hand trajectories, sign level classifiers are built from sub-units. Results are presented both on a large lexicon single signer data set and a multi-signer Kinect\texttrademark{} data set. In both cases it is shown to out perform the non-discriminative Markov model approach and be equivalent to previous, more costly, Sequential Pattern (SP) techniques.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2012/Ong et al/Ong et al_2012_Sign Language Recognition using Sequential Pattern Trees.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ong et al_2012_Sign Language Recognition using Sequential Pattern Trees.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F4TMIS5N/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZXMUPRJP/articleDetails.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=40;signer=14;sl=DGS;capturedBy=vision;capturedBy=depth,isolated;vocab=982;signer=1;sl=GSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=bodyjoints,modal=shape,signer=1,signer=14,sl=DGS,sl=GSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=shape;modal=bodyjoints,vocab<1000,vocab<50,vocab=40,vocab=982}
}

@inproceedings{ong_sign_2014,
  title = {Sign {{Spotting}} Using {{Hierarchical Sequential Patterns}} with {{Temporal Intervals}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ong, Eng-Jon and Koller, Oscar and Pugeault, Nicolas and Bowden, Richard},
  year = {2014},
  month = jun,
  pages = {1931--1938},
  address = {{Columbus, OH, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ong et al_2014_Sign Spotting using Hierarchical Sequential Patterns with Temporal Intervals.pdf},
  keywords = {capturedBy=depth,capturedBy=vision,continuous,continuous;vocab=48;signer=3;sl=ASL;capturedBy=vision,isolated,isolated;vocab=40;signer=14;sl=DGS;capturedBy=vision;capturedBy=depth,isolated;vocab=981;signer=1;sl=GSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=1,signer=14,signer=3,sl=ASL,sl=DGS,sl=GSL,slreco,slreco;modal=2hmovement;modal=2hlocation;modal=2hshape;modal=bodyjoints;sl=DGS,slreco;modal=2hmovement;modal=2hlocation;modal=2hshape;modal=bodyjoints;sl=GSL,slreco;modal=2hmovement;modal=2hshape;sl=ASL,vocab<1000,vocab<50,vocab=40,vocab=48,vocab=981}
}

@article{ong_understanding_2006,
  title = {Understanding Gestures with Systematic Variations in Movement Dynamics},
  author = {Ong, Sylvie C.W. and Ranganath, Surendra and Venkatesh, Y.V.},
  year = {2006},
  month = sep,
  volume = {39},
  pages = {1633--1648},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2006.02.010},
  abstract = {Sign language communication includes not only lexical sign gestures but also grammatical processes which represent inflections through systematic variations in sign appearance. We present a new approach to analyse these inflections by modelling the systematic variations as parallel channels of information with independent feature sets. A Bayesian network framework is used to combine the channel outputs and infer both the basic lexical meaning and inflection categories. Experiments using a simulated vocabulary of six basic signs and five different inflections (a total of 20 distinct gestures) obtained from multiple test subjects yielded 85.0\% recognition accuracy. We also propose an adaptation scheme to extend a trained system to recognize gestures from a new person by using only a small set of data from the new person. This scheme yielded 88.5\% recognition accuracy for the new person while the unadapted system yielded only 52.6\% accuracy.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ong et al_2006_Understanding gestures with systematic variations in movement dynamics.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JTVT96SA/S0031320306000549.html},
  journal = {Pattern Recognition},
  number = {9}
}

@article{oord_representation_2018,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  month = jul,
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archivePrefix = {arXiv},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oord et al_2018_Representation Learning with Contrastive Predictive Coding.pdf},
  journal = {arXiv:1807.03748 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{orbay_neural_2020,
  title = {Neural {{Sign Language Translation}} by {{Learning Tokenization}}},
  author = {Orbay, Alptekin and Akarun, Lale},
  year = {2020},
  month = feb,
  abstract = {Sign Language Translation has attained considerable success recently, raising hopes for improved communication with the Deaf. A pre-processing step called tokenization improves the success of translations. Tokens can be learned from sign videos if supervised data is available. However, data annotation at the gloss level is costly, and annotated data is scarce. The paper utilizes Adversarial, Multitask, Transfer Learning to search for semi-supervised tokenization approaches without burden of additional labeling. It provides extensive experiments to compare all the methods in different settings to conduct a deeper analysis. In the case of no additional target annotation besides sentences, the proposed methodology attains 13.25 BLUE-4 and 36.28 ROUGE scores which improves the current state-of-the-art by 4 points in BLUE-4 and 5 points in ROUGE.},
  archivePrefix = {arXiv},
  eprint = {2002.00479},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Orbay_Akarun_2020_Neural Sign Language Translation by Learning Tokenization.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N53BBJ63/2002.html},
  journal = {arXiv:2002.00479 [cs]},
  keywords = {translation;vocab=2887;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T},
  primaryClass = {cs}
}

@inproceedings{ordelman_compound_2003,
  title = {Compound {{Decomposition}} in {{Dutch Large Vocabulary Speech Recognition}}},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Ordelman, Roeland},
  year = {2003},
  pages = {225--228},
  address = {{Geneva, Switzerland}},
  abstract = {This paper addresses compound splitting for Dutch in the context of broadcast news transcription. Language models were created using original text versions and text versions that were decomposed using a data-driven compound splitting algorithm. Language model performances were compared in terms of outof-vocabulary rates and word error rates in a real-world broadcast news transcription task. It was concluded that compound splitting does improve ASR performance. Best results were obtained when frequent compounds were not decomposed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ordelman_2003_Compound Decomposition in Dutch Large Vocabulary Speech Recognition.pdf},
  language = {en}
}

@incollection{orfanidou_introduction_2015,
  title = {Introduction},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Orfanidou, Eleni and Woll, Bencie and Morgan, Gary},
  year = {2015},
  pages = {1--3},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch0},
  abstract = {This is the introductory chapter of this book, which summarizes all the relevant methodologies in sign language research and provides a detailed synopsis of how to conduct social, linguistic, psycholinguistic, and neurolinguistic research. The book is divided into five sections. First, it sets the scene for research in this field by addressing the ethical and political issues that arise in working with a Deaf community that can be considered both a minority linguistic group and a group with a disability. The second section covers the methodologies required to capture sign language data. The third section covers the main methods used in the various fields of sign linguistics research. The fourth section deals with the methods used in developmental, psycholinguistic, and educational research. The final section of the book considers methodology used in neuroscience and neuropsychological studies of sign languages.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Orfanidou et al_2015_Introduction.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MX65DAYW/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@article{ormel_coarticulation_2013,
  title = {Coarticulation of Hand Height in {{Sign Language}} of the {{Netherlands}} Is Affected by Contact Type},
  author = {Ormel, Ellen and Crasborn, Onno and {van der Kooij}, Els},
  year = {2013},
  month = may,
  volume = {41},
  pages = {156--171},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2013.01.001},
  abstract = {Recently, several studies found coarticulation effects for hand location in American Sign Language. In the present study, we established similar effects for Sign Language of the Netherlands (NGT). Moreover, we examined whether the degree of coarticulation of location is sensitive to phonological distinctions. We investigated whether types of major locations in the lexicon (i.e., `weak hand', `torso', and `neutral space') showed different degrees of sensitivity to the surrounding signs. Additionally, we investigated whether different types of movement (i.e., initial contact or final contact with the body) influenced the susceptibility of a sign to change its location under the influence of neighbouring signs. Five deaf adult signers participated in our study. The data were collected using CyberGlove and Flock of Bird, respectively tracking the bending of the fingers with 22 resistant bend-sensors and the X, Y, and Z coordinates. Location of hand height was measured in sign series, comparing the effect of surrounding signs at high and low locations. Results showed that height of the neighbouring signs influenced location height of the target sign. Moreover, coarticulation of location was sensitive to phonological distinctions in the lexicon. Strongest effects of coarticulation were observed at the location `weak hand', and were heavily influenced by contact type.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ormel et al_2013_Coarticulation of hand height in Sign Language of the Netherlands is affected.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GBSR4PIM/S0095447013000041.html},
  journal = {Journal of Phonetics},
  number = {3\textendash 4}
}

@inproceedings{ormel_glossing_2010,
  title = {Glossing a Multi-Purpose Sign Language Corpus},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Ormel, Ellen and Crasborn, Onno and van der Kooij, Els and van Dijken, Lianne and Nauta, Ellen Yassine and Forster, Jens and Stein, Daniel},
  year = {2010},
  month = may,
  address = {{Valletta, Malta}}
}

@phdthesis{ortmanns_effiziente_1998,
  title = {Effiziente {{Suchverfahren}} Zur {{Erkennung}} Kontinuierlich Gesprochener {{Sprache}}},
  author = {Ortmanns, S.},
  year = {1998},
  month = nov,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ortmanns_1998_Effiziente Suchverfahren zur Erkennung kontinuierlich gesprochener Sprache.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{oshin_capturing_2011,
  title = {Capturing the Relative Distribution of Features for Action Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Oshin, Olusegun and Gilbert, Andrew and Bowden, Richard},
  year = {2011},
  pages = {111--116},
  doi = {http://dx.doi.org/10.1109/FG.2011.5771382},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oshin et al_2011_Capturing the relative distribution of features for action recognition.pdf}
}

@inproceedings{oshin_spatiotemporal_2008,
  title = {Spatio-Temporal {{Feature Recogntion}} Using {{Randomised Ferns}}},
  booktitle = {The 1st {{International Workshop}} on {{Machine Learning}} for {{Vision}}-Based {{Motion Analysis}} - {{MLVMA}}'08},
  author = {Oshin, Olusegun and Gilbert, Andrew and Illingworth, John and Bowden, Richard},
  year = {2008},
  address = {{Marseille, France}},
  abstract = {In this paper we present a generic classifier for detecting spatio-temporal feature points within video. The premise being that given a feature detector, we can learn a classifier that duplicates its functionality which is both accurate and computationally efficient. This means that feature point detection can be achieved independent of the complexity of the original interest point formulation.We extend the naive Bayesian classifier of Ferns to the spatio-temporal domain and learn classifiers that duplicate the functionality of common spatio-temporal interest point detectors. Results demonstrate accurate reproduction of results with a classifier that can be applied exhaustively to video at frame-rate, without optimisation, in a scanning window approach.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oshin et al_2008_Spatio-temporal Feature Recogntion using Randomised Ferns.pdf}
}

@incollection{oshin_there_2011,
  title = {There {{Is More Than One Way}} to {{Get Out}} of a {{Car}}: {{Automatic Mode Finding}} for {{Action Recognition}} in the {{Wild}}},
  booktitle = {Pattern {{Recognition}} and {{Image Analysis}}},
  author = {Oshin, Olusegun and Gilbert, Andrew and Bowden, Richard},
  editor = {Vitri{\`a}, Jordi and Sanches, Jo{\~a}o and Hern{\'a}ndez, Mario},
  year = {2011},
  volume = {6669},
  pages = {41--48},
  publisher = {{Springer Berlin / Heidelberg}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oshin et al_2011_There Is More Than One Way to Get Out of a Car.pdf},
  isbn = {978-3-642-21256-7},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@incollection{oszust_determining_2010,
  title = {Determining {{Subunits}} for {{Sign Language Recognition}} by {{Evolutionary Cluster}}-{{Based Segmentation}} of {{Time Series}}},
  booktitle = {Artifical {{Intelligence}} and {{Soft Computing}}},
  author = {Oszust, Mariusz and Wysocki, Marian},
  editor = {Rutkowski, Leszek and Scherer, Rafal and Tadeusiewicz, Ryszard and Zadeh, Lotfi and Zurada, Jacek},
  year = {2010},
  volume = {6114},
  pages = {189--196},
  publisher = {{Springer Berlin Heidelberg}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oszust_Wysocki_2010_Determining Subunits for Sign Language Recognition by Evolutionary.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4PV38TQE/abstract.html},
  isbn = {978-3-642-13231-5},
  keywords = {capturedBy=vision,continuous,continuous;vocab=10;sl=PSL;capturedBy=vision,modal=location,modal=movement,sl=PSL,slreco,slreco;modal=movement;modal=location,vocab<50,vocab=10},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{oszust_polish_2013,
  title = {Polish Sign Language Words Recognition with {{Kinect}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Human System Interactions}} ({{HSI}})},
  author = {Oszust, M. and Wysocki, M.},
  year = {2013},
  month = jun,
  pages = {219--226},
  doi = {10.1109/HSI.2013.6577826},
  abstract = {The paper considers Polish sign language (PSL) words recognition with sensor Kinect. The nearest neighbour classifier with dynamic time warping technique was examined. The classifier was using two sets of features, the first produced by Kinect (in a form of 3D positions of most important joints of observed person body - a skeletal image or a skeleton) and the second describing hands, which were tracked as skin colour regions in the images acquired by Kinect. Obtained feature vectors representing PSL words are clustered in order to discover natural data divisions among them. This step is helpful in revealing possible problems which could occur during recognition and provides additional information about processed data. Results of ten-fold cross-validation tests for the classifier with two feature sets are given. The classifier with data provided by Kinect yielded promising recognition accuracy (89.33\%) but in order to obtain better results (98.33\%) adding new features responsible for hand shape description should be considered.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oszust_Wysocki_2013_Polish sign language words recognition with Kinect.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EU7IGIU3/6577826.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=30;signer=1;sl=PSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=1,sl=PSL,slreco,slreco;modal=2hshape;modal=2hlocation;modal=2hmovement;modal=bodyjoints,vocab<50,vocab=30}
}

@inproceedings{oszust_recognition_2013,
  title = {Recognition of Signed Expressions Observed by {{Kinect Sensor}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Advanced Video}} and {{Signal Based Surveillance}}},
  author = {Oszust, M. and Wysocki, M.},
  year = {2013},
  month = aug,
  pages = {220--225},
  doi = {10.1109/AVSS.2013.6636643},
  abstract = {In this paper we present an approach to recognition of signed expressions based on visual sequences obtained with Kinect sensor. Two variants of time series representing the expressions are considered: the first based on skeletal images of the body, and the second describing shape and position of hands extracted as skin coloured regions. Time series characterising isolated Polish sign language words are examined using three clustering algorithms and popular clustering quality indices which reveal natural gesture data division and indicate gesture samples difficult in further recognition. Ten-fold cross-validation recognition tests for the k-nearest neighbour classifier with dynamic time warping technique are shown. Recognition rate obtained with the skeletal image based features were improved from 89\% to 95\% by changing gesture representation from time series to a vector containing pairwise distances between gesture samples. The approach with skin colour based features involving utilisation of depth information of each pixel obtained by Kinect yielded 98\% recognition rate.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oszust_Wysocki_2013_Recognition of signed expressions observed by Kinect Sensor.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N5DGAT7Q/6636643.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=30;signer=1;sl=PSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hshape,modal=bodyjoints,signer=1,sl=PSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=bodyjoints,vocab<50,vocab=30}
}

@phdthesis{oszust_using_2013,
  title = {Using {{Clustering}} of {{Time Series}} for {{Video}}-{{Based Recognition}} of {{Signed Expressions}}},
  author = {Oszust, Mariusz},
  year = {2013},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oszust_Using Clustering of Time Series for Video-Based Recognition of Signed.pdf}
}

@inproceedings{othman_englishasl_2012,
  title = {English-Asl Gloss Parallel Corpus 2012: {{Aslg}}-Pc12},
  shorttitle = {English-Asl Gloss Parallel Corpus 2012},
  booktitle = {5th {{Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}: {{Interactions}} between {{Corpus}} and {{Lexicon LREC}}},
  author = {Othman, Achraf and Jemni, Mohamed},
  year = {2012},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Othman_Jemni_2012_English-asl gloss parallel corpus 2012.pdf},
  keywords = {translation;vocab=21600;sl=ASL;capturedBy=text}
}

@inproceedings{ouhyoung_sign_1996,
  title = {A Sign Language Recognition System Using Hidden Markov Model and Context Sensitive Search},
  booktitle = {Proceedings of the {{ACM}} Symposium on Virtual Reality Software and Technology},
  author = {Ouhyoung, M. and Liang, R. H.},
  year = {1996},
  pages = {59--66},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ouhyoung_Liang_1996_A sign language recognition system using hidden markov model and context.pdf},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=71;sl=TwSL;capturedBy=electronicGlove;capturedBy=mocap,modal=location,modal=orientation,modal=shape,sl=TwSL,slreco,slreco;modal=location;modal=shape;modal=orientation,vocab<100,vocab=71}
}

@article{oviatt_usercentered_2003,
  title = {User-Centered Modeling and Evaluation of Multimodal Interfaces},
  author = {Oviatt, S.},
  year = {2003},
  volume = {91},
  pages = {1457--1468},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2003.817127},
  abstract = {Historically, the development of computer interfaces has been a technology-driven phenomenon. However, new multimodal interfaces are composed of recognition-based technologies that must interpret human speech, gesture, gaze, movement patterns, and other complex natural behaviors, which involve highly automatized skills that are not under full conscious control. As a result, it now is widely acknowledged that multimodal interface design requires modeling of the modality-centered behavior and integration patterns upon which multimodal systems aim to build. This paper summarizes research on the cognitive science foundations of multimodal interaction, and on the essential role that user-centered modeling has played in prototyping, guiding, and evaluating the design of next-generation multimodal interfaces. In particular, it discusses the properties of different modalities and the information content they carry, the unique features of multimodal language and its processability, as well as when users are likely to interact multimodally and how their multimodal input is integrated and synchronized. It also reviews research on typical performance and linguistic efficiencies associated with multimodal interaction, and on the user-centered reasons why multimodal interaction minimizes errors and expedites error handling. In addition, this paper describes the important role that selective methodologies and evaluation metrics have played in shaping next-generation multimodal systems, and it concludes by highlighting future directions for designing a new class of adaptive multimodal-multisensor interfaces.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oviatt_2003_User-centered modeling and evaluation of multimodal interfaces.pdf},
  journal = {Proc. of the IEEE},
  number = {9}
}

@article{oz_american_2011,
  title = {American {{Sign Language}} Word Recognition with a Sensory Glove Using Artificial Neural Networks},
  author = {Oz, Cemil and Leu, Ming C.},
  year = {2011},
  month = oct,
  volume = {24},
  pages = {1204--1213},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2011.06.015},
  abstract = {An American Sign Language (ASL) recognition system is being developed using artificial neural networks (ANNs) to translate ASL words into English. The system uses a sensory glove called the Cyberglove\texttrademark{} and a Flock of Birds\textregistered{} 3-D motion tracker to extract the gesture features. The data regarding finger joint angles obtained from strain gauges in the sensory glove define the hand shape, while the data from the tracker describe the trajectory of hand movements. The data from these devices are processed by a velocity network with noise reduction and feature extraction and by a word recognition network. Some global and local features are extracted for each ASL word. A neural network is used as a classifier of this feature vector. Our goal is to continuously recognize ASL signs using these devices in real time. We trained and tested the ANN model for 50 ASL words with a different number of samples for every word. The test results show that our feature vector extraction method and neural networks can be used successfully for isolated word recognition. This system is flexible and open for future extension.},
  journal = {Engineering Applications of Artificial Intelligence},
  number = {7},
  series = {Infrastructures and {{Tools}} for {{Multiagent Systems}}}
}

@inproceedings{oz_linguistic_2005,
  title = {Linguistic {{Properties Based}} on {{American Sign Language Recognition}} with {{Artificial Neural Networks Using}} a {{Sensory Glove}} and {{Motion Tracker}}},
  booktitle = {Computational {{Intelligence}} and {{Bioinspired Systems}}},
  author = {Oz, Cemil and Leu, Ming C.},
  year = {2005},
  month = jun,
  pages = {1197--1205},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/11494669_147},
  abstract = {Sign language, which is a highly visual-spatial, linguistically complete and natural language, is the main mode of communication among deaf people. In this paper, an American Sign Language (ASL) word recognition system is being developed using artificial neural networks (ANN) to translate the ASL words into English. The system uses a sensory glove CybergloveTMand a Flock of Birds\textregistered{} 3- D motion tracker to extract the gesture features. The finger joint angle data obtained from strain gauges in the sensory glove define the hand-shape while the data from the tracker describe the trajectory of hand movement. The trajectory of hand is normalized for increase of the signer position flexibility. The data from these devices are processed by two neural networks, a velocity network and a word recognition network. The velocity network uses hand speed to determine the duration of words. To convey the meaning of a sign, signs are defined by feature vectors such as hand shape, hand location, orientation, movement, bounding box, and distance. The second network is used as a classifier to convert ASL signs into words based on features. We trained and tested our ANN model for 60 ASL words for different number of samples. Our test results show that the accuracy of recognition is 92\% .},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Oz_Leu_2005_Linguistic Properties Based on American Sign Language Recognition with.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6SGRKB68/11494669_147.html},
  isbn = {978-3-540-26208-4 978-3-540-32106-4},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=60;signer=6;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=location,modal=movement,modal=orientation,modal=shape,signer=6,sl=ASL,slreco,slreco;modal=shape;modal=orientation;modal=movement;modal=location,vocab<100,vocab=60},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{ozdemir_bosphorussign22k_2020,
  title = {{{BosphorusSign22k Sign Language Recognition Dataset}}},
  author = {{\"O}zdemir, O{\u g}ulcan and K{\i}nd{\i}ro{\u g}lu, Ahmet Alp and Camg{\"o}z, Necati Cihan and Akarun, Lale},
  year = {2020},
  month = apr,
  abstract = {Sign Language Recognition is a challenging research domain. It has recently seen several advancements with the increased availability of data. In this paper, we introduce the BosphorusSign22k, a publicly available large scale sign language dataset aimed at computer vision, video recognition and deep learning research communities. The primary objective of this dataset is to serve as a new benchmark in Turkish Sign Language Recognition for its vast lexicon, the high number of repetitions by native signers, high recording quality, and the unique syntactic properties of the signs it encompasses. We also provide state-of-the-art human pose estimates to encourage other tasks such as Sign Language Production. We survey other publicly available datasets and expand on how BosphorusSign22k can contribute to future research that is being made possible through the widespread availability of similar Sign Language resources. We have conducted extensive experiments and present baseline results to underpin future research on our dataset.},
  archivePrefix = {arXiv},
  eprint = {2004.01283},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Özdemir et al_2020_BosphorusSign22k Sign Language Recognition Dataset.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N5VSMWIW/2004.html},
  journal = {arXiv:2004.01283 [cs]},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=744;signer=6;sl=TSL;capturedBy=vision;capturedBy=depth,modal=fullframe,modal=fullframemotion,signer=6,sl=TSL,slreco,slreco;modal=fullframe;modal=fullframemotion,vocab<1000,vocab=744},
  primaryClass = {cs}
}

@article{padden_american_1987,
  title = {American {{Sign Language}} and the {{Architecture}} of {{Phonological Theory}}},
  author = {Padden, Carol A. and Perlmutter, David M.},
  year = {1987},
  volume = {5},
  pages = {335--375},
  issn = {0167-806X},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Padden_Perlmutter_1987_American Sign Language and the Architecture of Phonological Theory.pdf},
  journal = {Natural Language \& Linguistic Theory},
  number = {3}
}

@article{padden_how_2003,
  title = {How the Alphabet Came to Be Used in a Sign Language},
  author = {Padden, Carol and Gunsauls, Darline Clark},
  year = {2003},
  volume = {4},
  pages = {10--33},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2003/Padden_Gunsauls/Padden_Gunsauls_2003_How the alphabet came to be used in a sign language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Padden_Gunsauls_2003_How the alphabet came to be used in a sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M42DAK6G/Padden and Gunsauls - 2003 - How the alphabet came to be used in a sign languag.html},
  journal = {Sign Language Studies},
  number = {1}
}

@incollection{padden_methods_2015,
  title = {Methods of {{Research}} on {{Sign Language Grammars}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Padden, Carol A.},
  year = {2015},
  pages = {141--155},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch9},
  abstract = {This chapter discusses research methods for investigating four areas of sign language grammar: basic vocabulary; lexicon studies; verb classification and grammatical roles; sentence types and discourse structure. It gives examples of methods designed for comparative sign language work. Using translation and paradigm studies can be difficult in communities where there are few signers and they are unaccustomed to outsider interest in their language. Such sign languages have long been overlooked in favor of sign languages with long institutional histories and large communities of signers. Some studies of sign language word order and grammatical roles use video clips to elicit basic sentence structure with intransitive and transitive actions. Research materials can anticipate flexibility \textendash{} a feature that allows them to be used with signers and non-signers, with signers of established and signers of small sign languages, and with hearing non-signers who speak different languages and belong to different cultures.},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@incollection{palfreyman_methods_2015,
  title = {Methods in {{Carrying}} out {{Language Typological Research}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Palfreyman, Nick and Sagara, Keiko and Zeshan, Ulrike},
  year = {2015},
  pages = {173--192},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch11},
  abstract = {Language typologists are concerned with mapping the diversity of languages and with discovering patterns across them through comparative research in various domains. The aims of sign language typology are threefold: to document individual sign languages; to compare structures, systems and constructions across different sign languages; and to determine the extent to which patterns of variation are modality-specific. This chapter presents an overview of methodological issues in sign language typology, including a discussion of some of the key decisions that must be made when conducting a cross-linguistic typological study. There are two different stages of analysis for typology studies. In the first stage \textendash the initial analysis \textendash{} data are analyzed according to the parameters of investigation by the individuals or groups of linguists who collected the data and have direct knowledge of the sign language through fieldwork. The second stage \textendash comparative typological analysis \textendash{} involves analyzing target structures across the sample.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Palfreyman et al_2015_Methods in Carrying out Language Typological Research.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TQ4W36QC/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{pallet_tools_3,
  title = {Tools for the Analysis of Benchmark Speech Recognition Tests},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Pallet, D. S and Fisher, W. M and Fiscus, J. G},
  year = {3},
  pages = {97 -100 vol.1},
  doi = {10.1109/ICASSP.1990.115546}
}

@article{palmer-brown_snapdrift_2011,
  title = {Snap\textendash Drift Neural Network for Self-Organisation and Sequence Learning},
  author = {{Palmer-Brown}, Dominic and Jayne, Chrisina},
  year = {2011},
  month = oct,
  volume = {24},
  pages = {897--905},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2011.05.007},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Palmer-Brown_Jayne_2011_Snap–drift neural network for self-organisation and sequence learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JSXUDTCX/S0893608011001493.html},
  journal = {Neural Networks},
  number = {8},
  series = {Artificial {{Neural Networks}}: {{Selected Papers}} from {{ICANN}} 2010}
}

@inproceedings{panayotov_librispeech_2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  shorttitle = {Librispeech},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Panayotov, V. and Chen, G. and Povey, D. and Khudanpur, S.},
  year = {2015},
  month = apr,
  pages = {5206--5210},
  doi = {10.1109/ICASSP.2015.7178964},
  abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Panayotov et al_2015_Librispeech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/27UAJFIY/7178964.html}
}

@article{pantic_automatic_2000,
  title = {Automatic Analysis of Facial Expressions: The State of the Art},
  shorttitle = {Automatic Analysis of Facial Expressions},
  author = {Pantic, M. and Rothkrantz, L.J.M.},
  year = {2000},
  month = dec,
  volume = {22},
  pages = {1424--1445},
  issn = {0162-8828},
  doi = {10.1109/34.895976},
  abstract = {Humans detect and interpret faces and facial expressions in a scene with little or no effort. Still, development of an automated system that accomplishes this task is rather difficult. There are several related problems: detection of an image segment as a face, extraction of the facial expression information, and classification of the expression (e.g., in emotion categories). A system that performs these operations accurately and in real time would form a big step in achieving a human-like interaction between man and machine. The paper surveys the past work in solving these problems. The capability of the human visual system with respect to these problems is discussed, too. It is meant to serve as an ultimate goal and a guide for determining recommendations for development of an automatic facial expression analyzer.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pantic_Rothkrantz_2000_Automatic analysis of facial expressions.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WH7IMRC9/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {12}
}

@inproceedings{papadimitriou_endtoend_2019,
  title = {End-to-{{End Convolutional Sequence Learning}} for {{ASL Fingerspelling Recognition}}},
  booktitle = {Interspeech 2019},
  author = {Papadimitriou, Katerina and Potamianos, Gerasimos},
  year = {2019},
  month = sep,
  pages = {2315--2319},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2422},
  abstract = {Although fingerspelling is an often overlooked component of sign languages, it has great practical value in the communication of important context words that lack dedicated signs. In this paper we consider the problem of fingerspelling recognition in videos, introducing an end-to-end lexicon-free model that consists of a deep auto-encoder image feature learner followed by an attention-based encoder-decoder for prediction. The feature extractor is a vanilla auto-encoder variant, employing a quadratic activation function. The learned features are subsequently fed into the attention-based encoder-decoder. The latter deviates from traditional recurrent neural network architectures, being a fully convolutional attention-based encoder-decoder that is equipped with a multi-step attention mechanism relying on a quadratic alignment function and gated linear units over the convolution output. The introduced model is evaluated on the TTIC/UChicago fingerspelling video dataset, where it outperforms previous approaches in letter accuracy under all three, signer-dependent, -adapted, and -independent, experimental paradigms.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3YZ82WJ3/Papadimitriou and Potamianos - 2019 - End-to-End Convolutional Sequence Learning for ASL.pdf},
  language = {en}
}

@article{papandreou_weakly_2015,
  title = {Weakly- and {{Semi}}-{{Supervised Learning}} of a {{DCNN}} for {{Semantic Image Segmentation}}},
  author = {Papandreou, George and Chen, Liang-Chieh and Murphy, Kevin and Yuille, Alan L.},
  year = {2015},
  month = feb,
  abstract = {Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.},
  archivePrefix = {arXiv},
  eprint = {1502.02734},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Papandreou et al_2015_Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HAFI737T/1502.html},
  journal = {arXiv:1502.02734 [cs]},
  primaryClass = {cs}
}

@article{papastratis_continuous_2020,
  title = {Continuous {{Sign Language Recognition Through Cross}}-{{Modal Alignment}} of {{Video}} and {{Text Embeddings}} in a {{Joint}}-{{Latent Space}}},
  author = {Papastratis, Ilias and Dimitropoulos, Kosmas and Konstantinidis, Dimitrios and Daras, Petros},
  year = {2020},
  volume = {8},
  pages = {91170--91180},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2993650},
  abstract = {Continuous Sign Language Recognition (CSLR) refers to the challenging problem of recognizing sign language glosses and their temporal boundaries from weakly annotated video sequences. Previous methods focus mostly on visual feature extraction neglecting text information and failing to effectively model the intra-gloss dependencies. In this work, a cross-modal learning approach that leverages text information to improve vision-based CSLR is proposed. To this end, two powerful encoding networks are initially used to produce video and text embeddings prior to their mapping and alignment into a joint latent representation. The purpose of the proposed cross-modal alignment is the modelling of intra-gloss dependencies and the creation of more descriptive video-based latent representations for CSLR. The proposed method is trained jointly with video and text latent representations. Finally, the aligned video latent representations are classified using a jointly trained decoder. Extensive experiments on three well-known sign language recognition datasets and comparison with state-of-the-art approaches demonstrate the great potential of the proposed approach.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Papastratis et al_2020_Continuous Sign Language Recognition Through Cross-Modal Alignment of Video and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FIAV6IN4/9090828.html},
  journal = {IEEE Access},
  keywords = {augment=brightness,augment=contrast,augment=hue,augment=randomcrop,augment=randomframedrop,augment=saturation,authorgroup=CERTH,capturedBy=vision,cnnresiduals=0,continuous,continuous sign language recognition,continuous;vocab=1066;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=23.9/24.0,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,dataSet=Phoenix14,dataSet=Phoenix14T,loss=CE,loss=CTC,loss=other,modal=fullframe,nn=1dcnn,nn=2dcnn,nn=blstm,nnarch=vgg-11,paperinfo,paperinfo;authorgroup=CERTH;shorttitle=Cross-Modal Alignment,re-align=1,shorttitle=Cross-Modal Alignment,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;nn=2dcnn;nn=1dcnn;nn=blstm;nnarch=vgg-11;cnnresiduals=0;re-align=1;augment=randomcrop;augment=randomframedrop;augment=brightness;augment=contrast;augment=saturation;augment=hue;loss=CTC;loss=other;loss=CE,vocab<500,vocab=1066,vocab=1080,vocab=178,vocab>=1000,wer=23.9/24.0}
}

@article{parashar_representation_2003,
  title = {Representation and Interpretation of Manual and Non-Manual Information for Automated {{American Sign Language}} Recognition},
  author = {Parashar, Ayush S.},
  year = {2003},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Parashar_2003_Representation and interpretation of manual and non-manual information for.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BCIUKUHF/1445.html}
}

@inproceedings{parraescartin_german_2014,
  title = {German {{Compounds}} and {{Statistical Machine Translation}}. {{Can}} They Get Along?},
  booktitle = {Proceedings of the 10th {{Workshop}} on {{Multiword Expressions}} ({{MWE}})},
  author = {Parra Escart{\'i}n, Carla and Peitz, Stephan and Ney, Hermann},
  year = {2014},
  month = apr,
  pages = {48--56},
  publisher = {{Association for Computational Linguistics}},
  address = {{Gothenburg, Sweden}},
  doi = {10.3115/v1/W14-0808},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Parra Escartín et al_2014_German Compounds and Statistical Machine Translation.pdf}
}

@article{parton_sign_2006,
  title = {Sign Language Recognition and Translation: {{A}} Multidisciplined Approach from the Field of Artificial Intelligence},
  shorttitle = {Sign Language Recognition and Translation},
  author = {Parton, Becky Sue},
  year = {2006},
  volume = {11},
  pages = {94--101},
  publisher = {{Oxford University Press}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Parton_2006_Sign language recognition and translation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DLLU5PFY/410770.html},
  journal = {Journal of deaf studies and deaf education},
  number = {1}
}

@article{pathak_fully_2014,
  title = {Fully Convolutional Multi-Class Multiple Instance Learning},
  author = {Pathak, Deepak and Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
  year = {2014},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pathak et al_2014_Fully convolutional multi-class multiple instance learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MX8SU94T/1412.html},
  journal = {arXiv preprint arXiv:1412.7144}
}

@article{patwardhan_hand_2007,
  title = {Hand Gesture Modelling and Recognition Involving Changing Shapes and Trajectories, Using a {{Predictive EigenTracker}}},
  author = {Patwardhan, Kaustubh Srikrishna and Dutta Roy, Sumantra},
  year = {2007},
  month = feb,
  volume = {28},
  pages = {329--334},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2006.04.002},
  abstract = {We present a novel eigenspace-based framework to model a dynamic hand gesture that incorporates both hand shape as well as trajectory information. We address the problem of choosing a gesture set that models an upper bound on gesture recognition efficiency. We show encouraging experimental results on a such a representative set.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Patwardhan_Dutta Roy_2007_Hand gesture modelling and recognition involving changing shapes and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WJVIAVDU/S0167865506000833.html},
  journal = {Pattern Recognition Letters},
  number = {3}
}

@inproceedings{paudyal_learn2sign_2019,
  title = {{Learn2Sign: Explainable AI for sign language learning}},
  booktitle = {{CEUR Workshop Proceedings}},
  author = {Paudyal, Prajwal and Lee, Junghyo and Kamzin, Azamat and Soudki, Mohamad and Banerjee, Ayan and Gupta, Sandeep},
  year = {2019},
  month = jan,
  volume = {2327},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Paudyal et al_2019_Learn2Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BVB54EP6/learn2sign-explainable-ai-for-sign-language-learning.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=25;signer=100;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=100,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=25},
  language = {English (US)}
}

@inproceedings{paul_algorithms_1991,
  title = {Algorithms for an Optimal {{A}}* Search and Linearizing the Search in the Stack Decoder},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Paul, D. B.},
  year = {1991},
  month = apr,
  pages = {693-696 vol. 1},
  doi = {10.1109/ICASSP.1991.150434},
  abstract = {Two algorithms are presented for accelerating the operation of a stack decoder. The first is a method for computing the true least upper bound so that an optimal admissible A* search can be performed. The second is a set of methods for linearizing the computation required by a stack decoder. The A* search has been implemented in a continuous speech recognizer simulator and has demonstrated a significant speedup. The linearizing algorithm has been partially implemented in the simulator and has also shown significant computational savings.{$<<$}ETX{$>>$}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Paul_1991_Algorithms for an optimal A search and linearizing the search in the stack.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FY7QPR54/150434.html}
}

@inproceedings{paulraj_extraction_2008,
  title = {Extraction of Head and Hand Gesture Features for Recognition of Sign Language},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Electronic Design}}},
  author = {Paulraj, M. P. and Yaacob, S. and Desa, H. and Hema, C. R. and Ridzuan, W. M. and Majid, W. A.},
  year = {2008},
  month = dec,
  pages = {1--6},
  doi = {10.1109/ICED.2008.4786633},
  abstract = {Sign language is the primary communication method that impaired hearing people used in their daily life. Sign language recognition has gained a lot of attention recently by researchers in computer vision. Sign language recognition systems in general require the knowledge of the hand's position, shape, motion, orientation and facial expression. In this paper we present a simple method for converting sign language into voice signals using features obtained from head and hand gestures which can be used by hearing impaired person to communicate with an ordinary person. A simple feature extraction method based on the area of the objects in a binary image and Discrete Cosine Transform (DCT) is proposed for extracting the features from the video sign language. A simple neural network models is developed for the recognition of gestures using the features computed from the video stream. An audio system is installed to play the particular word corresponding to the gestures. Experimental results demonstrate that the recognition rate of the proposed neural network models is about 91\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Paulraj et al_2008_Extraction of head and hand gesture features for recognition of sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F7E6QCZE/4786633.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=32;sl=MalaySL;capturedBy=vision,modal=2hshape,sl=MalaySL,slreco,slreco;modal=2hshape,vocab<50,vocab=32}
}

@inproceedings{paulraj_phoneme_2010,
  title = {A Phoneme Based Sign Language Recognition System Using Skin Color Segmentation},
  booktitle = {Int. {{Colloquium}} on {{Signal Processing}} and {{Its Applications}} ({{CSPA}})},
  author = {Paulraj, Murugesa Pandiyan and Yaacob, Sazali and Azalan, Bin Zanar and Shuhanaz, Mohd and Palaniappan, Rajkumar},
  year = {2010},
  month = may,
  pages = {1--5},
  address = {{Malacca City, Malaysia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Paulraj et al_2010_A phoneme based sign language recognition system using skin color segmentation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QWM8XQKN/login.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=9;sl=MalaySL;capturedBy=vision,modal=2hshape,sl=MalaySL,slreco,slreco;modal=2hshape,vocab<50,vocab=9}
}

@inproceedings{pei_continuous_2019,
  title = {Continuous {{Sign Language Recognition Based}} on {{Pseudo}}-Supervised {{Learning}}},
  booktitle = {Proceedings of the {{2Nd Workshop}} on {{Multimedia}} for {{Accessible Human Computer Interfaces}}},
  author = {Pei, Xiankun and Guo, Dan and Zhao, Ye},
  year = {2019},
  month = oct,
  pages = {33--39},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3347319.3356837},
  abstract = {Continuous sign language recognition task is challenging for the reason that the ordered words have no exact temporal locations in the video. Aiming at this problem, we propose a method based on pseudo-supervised learning. First, we use a 3D residual convolutional network (3D-ResNet) pre-trained on the UCF101 dataset to extract visual features. Second, we employ a sequence model with connectionist temporal classification (CTC) loss for learning the mapping between the visual features and sentence-level labels, which can be used to generate clip-level pseudo-labels. Since the CTC objective function has limited effects on visual features extracted from early 3D-ResNet, we fine-tune the 3D-ResNet by feeding the clip-level pseudo-labels and video clips to obtain better feature representation. The feature extractor and the sequence model are optimized alternately with CTC loss. The effectiveness of the proposed method is verified on the large datasets RWTH-PHOENIX-Weather-2014.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pei et al_2019_Continuous Sign Language Recognition Based on Pseudo-supervised Learning.pdf},
  isbn = {978-1-4503-6912-1},
  keywords = {augment=?,authorgroup=Hefei Tech,capturedBy=vision,cnnresiduals=1,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,dataSet=Phoenix14,loss=CTC,modal=fullframe,nn=3dcnn,nn=bgru,nnarch=c3d-resnet,nntrain=3dcnn+bgru,paperinfo,paperinfo;authorgroup=Hefei Tech;shorttitle=Pseudo Supervised Learning,re-align=1,shorttitle=Pseudo Supervised Learning,signer=9,sl=DGS,slreco,slreco;modal=fullframe;dataSet=Phoenix14;wer=40.9/40.6;nn=3dcnn;nn=bgru;nnarch=c3d-resnet;cnnresiduals=1;re-align=1;nntrain=3dcnn+bgru;loss=CTC;augment=?;loss=CTC,vocab=1080,vocab>=1000,wer=40.9/40.6},
  series = {{{MAHCI}} '19}
}

@phdthesis{peitz2017:phd,
  title = {Generative {{Training}} and {{Smoothing}} of {{Hierarchical Phrase}}-{{Based Translation Models}}},
  author = {Peitz, Stephan},
  year = {2017},
  month = mar,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Peitz_2017_Generative Training and Smoothing of Hierarchical Phrase-Based Translation.pdf},
  school = {RWTH Aachen University}
}

@article{pellegrini_automatic_2009,
  title = {Automatic Word Decompounding for Asr in a Morphologically Rich Language: Application to Amharic},
  shorttitle = {Automatic Word Decompounding for Asr in a Morphologically Rich Language},
  author = {Pellegrini, Thomas and Lamel, Lori},
  year = {2009},
  volume = {17},
  pages = {863--873},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pellegrini_Lamel_2009_Automatic word decompounding for asr in a morphologically rich language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EUNK5CJ4/5075777.html},
  journal = {IEEE transactions on audio, speech, and language processing},
  number = {5}
}

@inproceedings{pellegrini_investigating_2006,
  title = {Investigating Automatic Decomposition for {{ASR}} in Less Represented Languages},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Pellegrini, Thomas and Lamel, Lori},
  year = {2006},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pellegrini_Lamel_2006_Investigating automatic decomposition for ASR in less represented languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2GIXYH9J/i06_1776.html}
}

@article{pellegrini_using_,
  title = {Using {{Phonetic Features}} in {{Unsupervised Word Decompounding}} for {{ASR}} with {{Application}} to a {{Less}}-{{Represented Language}}},
  author = {Pellegrini, Thomas and Lamel, Lori},
  pages = {5},
  abstract = {In this paper, a data-driven word decompounding algorithm is described and applied to a broadcast news corpus in Amharic. The baseline algorithm has been enhanced in order to address the problem of increased phonetic confusability arising from word decompounding by incorporating phonetic properties and some constraints on recognition units derived from prior forced alignment experiments. Speech recognition experiments have been carried out to validate the approach. Out of vocabulary (OOV) words rates can be reduced by 30\% to 40\% and an absolute Word Error Rate (WER) reduction of 0.4\% has been achieved. The algorithm is relatively language independent and requires minimal adaptation to be applied to other languages.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PBZMRESV/Pellegrini and Lamel - Using Phonetic Features in Unsupervised Word Decom.pdf},
  language = {en}
}

@inproceedings{pellegrini_using_2007,
  title = {Using Phonetic Features in Unsupervised Word Decompounding for {{ASR}} with Application to a Less-Represented Language},
  booktitle = {{{INTERSPEECH}}},
  author = {Pellegrini, Thomas and Lamel, Lori},
  year = {2007},
  abstract = {In this paper, a data-driven word decompounding algorithm is described and applied to a broadcast news corpus in Amharic. The baseline algorithm has been enhanced in order to address the problem of increased phonetic confusability arising from word decompounding by incorporating phonetic properties and some constraints on recognition units derived from prior forced alignment experiments. Speech recognition experiments have been carried out to validate the approach. Out of vocabulary (OOV) words rates can be reduced by 30\% to 40\% and an absolute Word Error Rate (WER) reduction of 0.4\% has been achieved. The algorithm is relatively language independent and requires minimal adaptation to be applied to other languages. Index Terms: automatic speech recognition, unsupervised word decompounding, less-represented languages},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pellegrini_Lamel_2007_Using phonetic features in unsupervised word decompounding for ASR with.pdf}
}

@article{perlmutter_sonority_1992,
  title = {Sonority and {{Syllable Structure}} in {{American Sign Language}}},
  author = {Perlmutter, David M.},
  year = {1992},
  pages = {407--442},
  journal = {Linguistic Inquiry}
}

@article{perniss_cant_2007,
  title = {Can't You See the Difference? {{Sources}} of Variation in Sign Language Structure},
  shorttitle = {Can't You See the Difference?},
  author = {Perniss, Pamela and Pfau, Roland and Steinbach, Markus},
  year = {2007},
  volume = {188},
  pages = {1},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Perniss et al_2007_Can't you see the difference.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3Z4HG3SP/books.html},
  journal = {TRENDS IN LINGUISTICS STUDIES AND MONOGRAPHS}
}

@incollection{perniss_collecting_2015,
  title = {Collecting and {{Analyzing Sign Language Data}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Perniss, Pamela},
  year = {2015},
  pages = {53--73},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch4},
  abstract = {This chapter provides researchers with a resource guide to making and managing video recordings of sign language data and to undertaking linguistic annotation with available software. It focuses on how to obtain video data that are good and usable in terms of quality of the recording and appropriate in terms of answering specific research questions. With respect to annotation, the chapter provides guidelines on devising a coding scheme for specific research questions and on setting up a workflow in order to implement the coding scheme in the annotation software for successful and effective use. Software that allows annotations to be directly linked to and time-aligned with the video stream has been a boon to working with sign language data. Primary examples are ELAN and Anvil. The chapter concludes by delineating its contribution from what has been published on the topic of sign and spoken language video data collection and analysis.},
  copyright = {Copyright \textcopyright{} 2015 John Wiley \& Sons, Inc.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Perniss_2015_Collecting and Analyzing Sign Language Data.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Perniss_2015_Collecting and Analyzing Sign Language Data2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VBUFQJ8N/9781118346013.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3F2KPVLQ/9781118346013.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KXDDU2WA/9781118346013.html},
  isbn = {978-1-118-34601-3},
  keywords = {annotation software,coding scheme,sign language data,video data analysis,video data collection},
  language = {en}
}

@phdthesis{petajan_automatic_1984,
  title = {Automatic {{Lipreading}} to {{Enhance Speech Recognition}} ({{Speech Reading}})},
  author = {Petajan, Eric David},
  year = {1984},
  address = {{Champaign, IL, USA}},
  school = {University of Illinois at Urbana-Champaign}
}

@inproceedings{petajan_improved_1988,
  title = {An {{Improved Automatic Lipreading System}} to {{Enhance Speech Recognition}}},
  booktitle = {Proc. of the {{SIGCHI Conf}}. on {{Human Factors}} in {{Computing Systems}}},
  author = {Petajan, E. and Bischoff, B. and Bodoff, D. and Brooke, N. M.},
  year = {1988},
  pages = {19--25},
  address = {{New York, NY, USA}},
  doi = {10.1145/57167.57170},
  abstract = {Current acoustic speech recognition technology performs well with very small vocabularies in noise or with large vocabularies in very low noise. Accurate acoustic speech recognition in noise with vocabularies over 100 words has yet to be achieved. Humans frequently lipread the visible facial speech articulations to enhance speech recognition, especially when the acoustic signal is degraded by noise or hearing impairment. Automatic lipreading has been found to improve significantly acoustic speech recognition and could be advantageous in noisy environments such as offices, aircraft and factories.
An improved version of a previously described automatic lipreading system has been developed which uses vector quantization, dynamic time warping, and a new heuristic distance measure. This paper presents visual speech recognition results from multiple speakers under optimal conditions. Results from combined acoustic and visual speech recognition are also presented which show significantly improved performance compared to the acoustic recognition system alone.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Petajan et al_1988_An Improved Automatic Lipreading System to Enhance Speech Recognition.pdf},
  isbn = {0-201-14237-6},
  series = {{{CHI}} '88}
}

@article{peterson_control_1952,
  title = {Control {{Methods Used}} in a {{Study}} of the {{Vowels}}},
  author = {Peterson, Gordon E. and Barney, Harold L.},
  year = {1952},
  month = mar,
  volume = {24},
  pages = {175--184},
  issn = {0001-4966},
  doi = {10.1121/1.1906875},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Peterson_Barney_1952_Control Methods Used in a Study of the Vowels.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3MSUB3HZ/1.html},
  journal = {The Journal of the Acoustical Society of America},
  number = {2}
}

@inproceedings{petridis_endtoend_2017,
  title = {End-to-{{End Multi}}-{{View Lipreading}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Petridis, Stavros and Wang, Yujiang and Li, Zuwei and Pantic, Maja},
  year = {2017},
  month = sep,
  pages = {1--12},
  address = {{London, UK}},
  abstract = {Non-frontal lip views contain useful information which can be used to enhance the performance of frontal view lipreading. However, the vast majority of recent lipreading works, including the deep learning approaches which significantly outperform traditional approaches, have focused on frontal mouth images. As a consequence, research on joint learning of visual features and speech classification from multiple views is limited. In this work, we present an end-to-end multi-view lipreading system based on Bidirectional Long-Short Memory (BLSTM) networks. To the best of our knowledge, this is the first model which simultaneously learns to extract features directly from the pixels and performs visual speech classification from multiple views and also achieves state-of-the-art performance. The model consists of multiple identical streams, one for each view, which extract features directly from different poses of mouth images. The temporal dynamics in each stream/view are modelled by a BLSTM and the fusion of multiple streams/views takes place via another BLSTM. An absolute average improvement of 3\% and 3.8\% over the frontal view performance is reported on the OuluVS2 database when the best two (frontal and profile) and three views (frontal, profile, 45) are combined, respectively. The best three-view model results in a 10.5\% absolute improvement over the current multi-view state-of-the-art performance on OuluVS2, without using external databases for training, achieving a maximum classification accuracy of 96.9\%.},
  archivePrefix = {arXiv},
  eprint = {1709.00443},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Petridis et al_2017_End-to-End Multi-View Lipreading.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UWJM9VVC/1709.html}
}

@inproceedings{pezzuoli_development_2017,
  title = {Development of a {{Wearable Device}} for {{Sign Language Translation}}},
  booktitle = {Human {{Friendly Robotics}}},
  author = {Pezzuoli, Francesco and Corona, Dario and Corradini, Maria Letizia and Cristofaro, Andrea},
  editor = {Ficuciello, Fanny and Ruggiero, Fabio and Finzi, Alberto},
  year = {2017},
  month = nov,
  pages = {115--126},
  abstract = {A wearable device for sign language translation, called Talking Hands, is presented. It is composed by a custom data glove, which is designed to optimize the data acquisition, and a smartphone application, which offers user personalizations. Although Talking Hands can not translate a whole sign language, it offers an effective communication to deaf and mute people with everyone through a scenario-based translation. The different challenges of a gesture recognition system have been overcame with simple solutions, since the main goal of this work is an user-based product.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pezzuoli et al_2019_Development of a Wearable Device for Sign Language Translation.pdf},
  isbn = {978-3-319-89327-3},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=40;signer=1;sl=LIS;capturedBy=electronicGlove,modal=orientation,modal=shape,signer=1,sl=LIS,slreco,slreco;modal=shape;modal=orientation,vocab<50,vocab=40},
  language = {en},
  series = {Springer {{Proceedings}} in {{Advanced Robotics}}}
}

@article{pfau_backward_2005,
  title = {Backward and Sideward Reduplication in {{German Sign Language}}},
  author = {Pfau, Roland and Steinbach, Markus},
  year = {2005}
}

@book{pfau_modalityindependent_2006,
  title = {Modality-Independent and Modality-Specific Aspects of Grammaticalization in Sign Languages},
  author = {Pfau, Roland and Steinbach, Markus},
  year = {2006},
  publisher = {{Univ.-Verl}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfau_Steinbach_2006_Modality-independent and modality-specific aspects of grammaticalization in.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X4I6BA7T/1005.html}
}

@incollection{pfau_nonmanuals_2010,
  title = {Nonmanuals: Their Grammatical and Prosodic Roles},
  booktitle = {Sign {{Languages}}},
  author = {Pfau, Roland and Quer, Josep},
  editor = {Brentari, Diane},
  year = {2010},
  month = may,
  pages = {381--402},
  publisher = {{Cambridge University Press}},
  address = {{Cambrigde, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfau_Quer_2010_Nonmanuals.pdf}
}

@article{pfau_plural_2005,
  title = {Plural Formation in {{German}} Sign Language: Constraints and Strategies},
  shorttitle = {Plural Formation in {{German}} Sign Language},
  author = {Pfau, Roland and Steinbach, Markus and others},
  year = {2005},
  volume = {13},
  pages = {111--144},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfau et al_2005_Plural formation in German sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4VTTURUI/245483.html},
  journal = {Linguistische Berichte. Sonderheft}
}

@article{pfau_pluralization_2006,
  title = {Pluralization in Sign and in Speech: {{A}} Cross-Modal Typological Study},
  shorttitle = {Pluralization in Sign and in Speech},
  author = {Pfau, Roland and Steinbach, Markus},
  year = {2006},
  volume = {10},
  pages = {135--182},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfau_Steinbach_2006_Pluralization in sign and in speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7FHQGBU8/lingty.2006.006.html},
  journal = {Linguistic Typology},
  number = {2}
}

@inproceedings{pfister_automatic_2012,
  title = {Automatic and {{Efficient Long Term Arm}} and {{Hand Tracking}} for {{Continuous Sign Language TV Broadcasts}}.},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Pfister, Tomas and Charles, James and Everingham, Mark and Zisserman, Andrew},
  year = {2012},
  pages = {1--11},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfister et al_2012_Automatic and Efficient Long Term Arm and Hand Tracking for Continuous Sign.pdf}
}

@inproceedings{pfister_domainadaptive_2014,
  title = {Domain-Adaptive Discriminative One-Shot Learning of Gestures},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
  year = {2014},
  month = sep,
  pages = {814--829},
  publisher = {{Springer}},
  address = {{Zurich, Switzerland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfister et al_2014_Domain-adaptive discriminative one-shot learning of gestures.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KWWCS2CZ/978-3-319-10599-4_52.html}
}

@inproceedings{pfister_flowing_2015,
  title = {Flowing {{ConvNets}} for {{Human Pose Estimation}} in {{Videos}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
  year = {2015},
  pages = {1913--1921},
  address = {{Santiago, Chile}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfister et al_2015_Flowing ConvNets for Human Pose Estimation in Videos.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RCHSV7GM/Pfister_Flowing_ConvNets_for_ICCV_2015_paper.html}
}

@inproceedings{pfister_largescale_2013,
  title = {Large-Scale Learning of Sign Language by Watching {{TV}} (Using Co-Occurrences)},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
  year = {2013},
  month = sep,
  pages = {1--11},
  address = {{Bristol, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2013/Pfister et al/Pfister et al_2013_Large-scale learning of sign language by watching TV (using co-occurrences).pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pfister et al_2013_Large-scale learning of sign language by watching TV (using co-occurrences).pdf}
}

@inproceedings{phadtare_detecting_2012,
  title = {Detecting Hand-Palm Orientation and Hand Shapes for Sign Language Gesture Recognition Using {{3D}} Images},
  booktitle = {Western {{New York Image Processing Workshop}}},
  author = {Phadtare, L. K. and Kushalnagar, R. S. and Cahill, N. D.},
  year = {2012},
  month = nov,
  pages = {29--32},
  doi = {10.1109/WNYIPW.2012.6466652},
  abstract = {Automatic gesture recognition, specifically for the purpose of understanding sign language, can be an important aid in communicating with the deaf and hard-of-hearing. Recognition of sign languages requires understanding of various linguistic components such as palm orientation, hand shape, hand location and facial expression. We propose a method and system to estimate the palm orientation and the hand shape of a signer. Our system uses Microsoft Kinect to capture color and the depth images of a signer. It analyzes the depth data corresponding to the hand point region and fits plane to this data and defines the normal to this plane as the orientation of the palm. Then it uses 3-D shape context to determine the hand shape by comparing it to example shapes in the database. Palm orientation of the hand was found to be correct in varying poses. The shape context method for hand shape classification was found to identify 20 test hand shapes correctly and 10 shapes were matched to other but very similar shapes.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Phadtare et al_2012_Detecting hand-palm orientation and hand shapes for sign language gesture.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5TBKKAQS/6466652.html}
}

@article{pham_found_2018,
  title = {Found in {{Translation}}: {{Learning Robust Joint Representations}} by {{Cyclic Translations Between Modalities}}},
  shorttitle = {Found in {{Translation}}},
  author = {Pham, Hai and Liang, Paul Pu and Manzini, Thomas and Morency, Louis-Philippe and Poczos, Barnabas},
  year = {2018},
  month = dec,
  abstract = {Multimodal sentiment analysis is a core research area that studies speaker sentiment expressed from the language, visual, and acoustic modalities. The central challenge in multimodal learning involves inferring joint representations that can process and relate information from these modalities. However, existing work learns joint representations by requiring all modalities as input and as a result, the learned representations may be sensitive to noisy or missing modalities at test time. With the recent success of sequence to sequence (Seq2Seq) models in machine translation, there is an opportunity to explore new ways of learning joint representations that may not require all input modalities at test time. In this paper, we propose a method to learn robust joint representations by translating between modalities. Our method is based on the key insight that translation from a source to a target modality provides a method of learning joint representations using only the source modality as input. We augment modality translations with a cycle consistency loss to ensure that our joint representations retain maximal information from all modalities. Once our translation model is trained with paired multimodal data, we only need data from the source modality at test time for final sentiment prediction. This ensures that our model remains robust from perturbations or missing information in the other modalities. We train our model with a coupled translation-prediction objective and it achieves new state-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI, ICT-MMMO, and YouTube. Additional experiments show that our model learns increasingly discriminative joint representations with more input modalities while maintaining robustness to missing or perturbed modalities.},
  archivePrefix = {arXiv},
  eprint = {1812.07809},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pham et al_2018_Found in Translation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6UMBGM7G/1812.html},
  journal = {arXiv:1812.07809 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{pham_handobject_2018,
  title = {Hand-{{Object Contact Force Estimation}} from {{Markerless Visual Tracking}}},
  author = {Pham, T. and Kyriazis, N. and Argyros, A. A. and Kheddar, A.},
  year = {2018},
  month = dec,
  volume = {40},
  pages = {2883--2896},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2017.2759736},
  abstract = {We consider the problem of estimating realistic contact forces during manipulation, backed with ground-truth measurements, using vision alone. Interaction forces are usually measured by mounting force transducers onto the manipulated objects or the hands. Those are costly, cumbersome, and alter the objects' physical properties and their perception by the human sense of touch. Our work establishes that interaction forces can be estimated in a cost-effective, reliable, non-intrusive way using vision. This is a complex and challenging problem. Indeed, in multi-contact, a given motion can generally be caused by an infinity of possible force distributions. To alleviate the limitations of traditional models based on inverse optimization, we collect and release the first large-scale dataset on manipulation kinodynamics as 3.2 hours of synchronized force and motion measurements under 193 object-grasp configurations. We learn a mapping between high-level kinematic features based on the equations of motion and the underlying manipulation forces using recurrent neural networks (RNN). The RNN predictions are consistently refined using physics-based optimization through second-order cone programming (SOCP). We show that our method can successfully capture interaction forces compatible with both the observations and the way humans intuitively manipulate objects, using a single RGB-D camera.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pham et al_2018_Hand-Object Contact Force Estimation from Markerless Visual Tracking.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KL8CZ8AK/8085141.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {12}
}

@article{pham_seq2seq2sentiment_2018,
  title = {{{Seq2Seq2Sentiment}}: {{Multimodal Sequence}} to {{Sequence Models}} for {{Sentiment Analysis}}},
  shorttitle = {{{Seq2Seq2Sentiment}}},
  author = {Pham, Hai and Manzini, Thomas and Liang, Paul Pu and Poczos, Barnabas},
  year = {2018},
  month = jul,
  abstract = {Multimodal machine learning is a core research area spanning the language, visual and acoustic modalities. The central challenge in multimodal learning involves learning representations that can process and relate information from multiple modalities. In this paper, we propose two methods for unsupervised learning of joint multimodal representations using sequence to sequence (Seq2Seq) methods: a \textbackslash textit\{Seq2Seq Modality Translation Model\} and a \textbackslash textit\{Hierarchical Seq2Seq Modality Translation Model\}. We also explore multiple different variations on the multimodal inputs and outputs of these seq2seq models. Our experiments on multimodal sentiment analysis using the CMU-MOSI dataset indicate that our methods learn informative multimodal representations that outperform the baselines and achieve improved performance on multimodal sentiment analysis, specifically in the Bimodal case where our model is able to improve F1 Score by twelve points. We also discuss future directions for multimodal Seq2Seq methods.},
  archivePrefix = {arXiv},
  eprint = {1807.03915},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pham et al_2018_Seq2Seq2Sentiment.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z2FTA8QA/1807.html},
  journal = {arXiv:1807.03915 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{piater_video_2010,
  title = {Video Analysis for Continuous Sign Language Recognition},
  booktitle = {Proceedings of 4th {{Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}: {{Corpora}} and {{Sign Language Technologies}}},
  author = {Piater, Justus and Hoyoux, Thomas and Du, Wei},
  year = {2010},
  pages = {22--23},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Piater et al_2010_Video analysis for continuous sign language recognition.pdf}
}

@inproceedings{pigou_gesture_2017,
  title = {Gesture and {{Sign Language Recognition With Temporal Residual Networks}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Pigou, Lionel and Van Herreweghe, Mieke and Dambre, Joni},
  year = {2017},
  month = oct,
  pages = {3086--3093},
  address = {{Venice, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pigou et al_2017_Gesture and Sign Language Recognition With Temporal Residual Networks.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=10;signer=53;sl=FlemishSL;capturedBy=vision,isolated;vocab=10;signer=78;sl=NGT;capturedBy=vision,isolated;vocab=100;signer=53;sl=FlemishSL;capturedBy=vision,isolated;vocab=100;signer=78;sl=NGT;capturedBy=vision,isolated;vocab=20;signer=53;sl=FlemishSL;capturedBy=vision,isolated;vocab=20;signer=78;sl=NGT;capturedBy=vision,isolated;vocab=50;signer=53;sl=FlemishSL;capturedBy=vision,isolated;vocab=50;signer=78;sl=NGT;capturedBy=vision,modal=fullframe,signer=53,signer=78,sl=FlemishSL,sl=NGT,slreco,slreco;modal=fullframe,vocab<100,vocab<50,vocab<500,vocab=10,vocab=100,vocab=20,vocab=50}
}

@phdthesis{pigou_gesture_2018,
  title = {Gesture and {{Sign Language Recognition}} with {{Deep Learning}}},
  author = {Pigou, Lionel},
  year = {2018},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pigou_2018_Gesture and Sign Language Recognition with Deep Learning.pdf},
  school = {Ghent University},
  type = {{{PhD Thesis}}}
}

@inproceedings{pigou_sign_2014,
  title = {Sign {{Language Recognition Using Convolutional Neural Networks}}},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Pigou, Lionel and Dieleman, Sander and Kindermans, Pieter-Jan and Schrauwen, Benjamin},
  editor = {Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten},
  year = {2014},
  month = sep,
  volume = {I},
  pages = {572--578},
  publisher = {{Springer International Publishing}},
  address = {{Zurich, Switzerland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pigou et al_2014_Sign Language Recognition Using Convolutional Neural Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/A2G3UMX5/5796137.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FZ4W78PD/5796137.html},
  isbn = {978-3-319-16178-5}
}

@inproceedings{pigou_sign_2016,
  title = {Sign Classification in Sign Language {{Corpora}} with Deep Neural Networks},
  booktitle = {International {{Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}), {{Workshop}}, {{Proceedings}}},
  author = {Pigou, Lionel and Van Herreweghe, Mieke and Dambre, Joni},
  year = {2016},
  pages = {175--178},
  abstract = {Automatic and unconstrained sign language recognition (SLR) in image sequences remains a challenging problem. The variety of signers, backgrounds, sign executions and signer positions makes the development of SLR systems very challenging. Current methods try to alleviate this complexity by extracting engineered features to detect hand shapes, hand trajectories and facial expressions as an intermediate step for SLR. Our goal is to approach SLR based on feature learning rather than feature engineering. We tackle SLR using the recent advances in the domain of deep learning with deep neural networks. The problem is approached by classifying isolated signs from the Corpus VGT (Flemish Sign Language Corpus) and the Corpus NGT (Dutch Sign Language Corpus). Furthermore, we investigate cross-domain feature learning to boost the performance to cope with the fewer Corpus VGT annotations.},
  copyright = {I have transferred the copyright for this publication to the publisher},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pigou et al_2016_Sign classification in sign language Corpora with deep neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XRUDNHMR/7223485.html},
  isbn = {978-2-9517408-9-1},
  keywords = {capturedBy=vision,isolated,isolated;vocab=100;signer=53;sl=FlemishSL;capturedBy=vision,isolated;vocab=100;signer=78;sl=NGT;capturedBy=vision,modal=fullframe,signer=53,signer=78,sl=FlemishSL,sl=NGT,slreco,slreco;modal=fullframe,vocab<500,vocab=100},
  language = {eng}
}

@article{pigou_temporal_2018,
  title = {Beyond {{Temporal Pooling}}: {{Recurrence}} and {{Temporal Convolutions}} for {{Gesture Recognition}} in {{Video}}},
  shorttitle = {Beyond {{Temporal Pooling}}},
  author = {Pigou, Lionel and van den Oord, A{\"a}ron and Dieleman, Sander and Van Herreweghe, Mieke and Dambre, Joni},
  year = {2018},
  volume = {126},
  pages = {430--439},
  abstract = {Recent studies have demonstrated the power of recurrent neural networks for machine translation, image captioning and speech recognition. For the task of capturing temporal structure in video, however, there still remain numerous open research questions. Current research suggests using a simple temporal feature pooling strategy to take into account the temporal aspect of video. We demonstrate that this method is not sufficient for gesture recognition, where temporal information is more discriminative compared to general video classification tasks. We explore deep architectures for gesture recognition in video and propose a new end-to-end trainable neural network architecture incorporating temporal convolutions and bidirectional recurrence. Our main contributions are twofold; first, we show that recurrence is crucial for this task; second, we show that adding temporal convolutions leads to significant improvements. We evaluate the different approaches on the Montalbano gesture recognition dataset, where we achieve state-of-the-art results.},
  archivePrefix = {arXiv},
  eprint = {1506.01911},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pigou et al_2018_Beyond Temporal Pooling.pdf},
  journal = {International Journal of Computer Vision (IJCV)},
  keywords = {1dcnn=1,cnn=1,Deep neural networks},
  number = {2-4}
}

@inproceedings{pinheiro_imagelevel_2015,
  title = {From {{Image}}-Level to {{Pixel}}-Level {{Labeling}} with {{Convolutional Networks}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pinheiro, Pedro O. and Collobert, Ronan},
  year = {2015},
  pages = {1713--1721}
}

@incollection{pinter_investigating_2018,
  title = {Investigating {{Word Segmentation Techniques}} for {{German Using Finite}}-{{State Transducers}}},
  booktitle = {Speech and {{Computer}}},
  author = {Pint{\'e}r, G. and Schielke, M. and Petrick, R.},
  editor = {Karpov, A. and Jokisch, O. and Potapova, R.},
  year = {2018},
  month = aug,
  edition = {SPECOM 2018},
  volume = {11096},
  publisher = {{Springer, Cham}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pintér et al_2018_Investigating Word Segmentation Techniques for German Using Finite-State.pdf},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{piquard-kipffer_qualitative_2015,
  title = {Qualitative Investigation of the Display of Speech Recognition Results for Communication with Deaf People},
  booktitle = {Workshop on {{Speech}} and {{Language Processing}} for {{Assistive Technologies}}},
  author = {{Piquard-Kipffer}, Agn{\`e}s and Mella, Odile and Miranda, J{\'e}r{\'e}my and Jouvet, Denis and Orosanu, Luiza},
  year = {2015},
  pages = {7},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Piquard-Kipffer et al_2015_Qualitative investigation of the display of speech recognition results for.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5WSCRVXJ/hal-01183349.html}
}

@inproceedings{pishchulin_fast_2011,
  title = {The {{Fast}} and the {{Flexible}}: {{Extended Pseudo Two}}-{{Dimensional Warping}} for {{Face Recognition}}},
  booktitle = {Proc. {{Iberian Conf}}. on {{Pattern Recognition}} and {{Image Analysis}} ({{IbPRIA}})},
  author = {Pishchulin, Leonid and Gass, Tobias and Dreuw, Philippe and Ney, Hermann},
  year = {2011},
  month = jun,
  address = {{Gran Canaria, Spain}}
}

@inproceedings{pitsikalis_advances_2011,
  title = {Advances in Phonetics-Based Sub-Unit Modeling for Transcription Alignment and Sign Language Recognition},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Pitsikalis, V. and Theodorakis, S. and Vogler, C. and Maragos, P.},
  year = {2011},
  month = jun,
  pages = {1--6},
  doi = {10.1109/CVPRW.2011.5981681},
  abstract = {We explore novel directions for incorporating phonetic transcriptions into sub-unit based statistical models for sign language recognition. First, we employ a new symbolic processing approach for converting sign language annotations, based on HamNoSys symbols, into structured sequences of labels according to the Posture-Detention-Transition-Steady Shift phonetic model. Next, we exploit these labels, and their correspondence with visual features to construct phonetics-based statistical sub-unit models. We also align these sequences, via the statistical sub-unit construction and decoding, to the visual data to extract time boundary information that they would lack otherwise. The resulting phonetic sub-units offer new perspectives for sign language analysis, phonetic modeling, and automatic recognition. We evaluate this approach via sign language recognition experiments on an extended Lemmas Corpus of Greek Sign Language, which results not only in improved performance compared to pure data-driven approaches, but also in meaningful phonetic sub-unit models that can be further exploited in interdisciplinary sign language analysis.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pitsikalis et al_2011_Advances in phonetics-based sub-unit modeling for transcription alignment and2.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=961;signer=1;sl=GSL;capturedBy=vision,modal=location,modal=movement,signer=1,sl=GSL,slreco,slreco;modal=movement;modal=location,vocab<1000,vocab=961}
}

@inproceedings{pitsikalis_datadriven_2010,
  title = {Data-{{Driven Sub}}-{{Units}} and {{Modeling Structure}} for {{Continuous Sign Language Recognition}} with {{Multiple Cues}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Pitsikalis, V. and Theodorakis, S. and Maragos, P.},
  year = {2010},
  month = may,
  pages = {196--203},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pitsikalis et al_2010_Data-Driven Sub-Units and Modeling Structure for Continuous Sign Language.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=50;signer=1;sl=ASL;capturedBy=vision,modal=location,modal=movement,signer=1,sl=ASL,slreco,slreco;modal=location;modal=movement,vocab<100,vocab=50}
}

@article{pitsikalis_multimodal_2015,
  title = {Multimodal {{Gesture Recognition}} via {{Multiple Hypotheses Rescoring}}},
  author = {Pitsikalis, Vassilis and Katsamanis, Athanasios and Theodorakis, Stavros and Maragos, Petros},
  year = {2015},
  volume = {16},
  pages = {255--284},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pitsikalis et al_2015_Multimodal Gesture Recognition via Multiple Hypotheses Rescoring.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K895ANBU/pitsikalis15a.html},
  journal = {Journal of Machine Learning Research}
}

@phdthesis{pitz_investigations_2005,
  title = {Investigations on {{Linear Transformations}} for {{Speaker Adaptation}} and {{Normalization}}},
  author = {Pitz, Michael},
  year = {2005},
  month = mar,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pitz_2005_Investigations on Linear Transformations for Speaker Adaptation and.pdf},
  school = {RWTH Aachen University}
}

@phdthesis{plahl_neural_2014,
  title = {Neural {{Network}} Based {{Feature Extraction}} for {{Speech}} and {{Image Recognition}}},
  author = {Plahl, Christian},
  year = {2014},
  month = jan,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Plahl_2014_Neural Network based Feature Extraction for Speech and Image Recognition2.pdf},
  school = {RWTH Aachen University}
}

@article{poizner_perception_1981,
  title = {Perception of {{American}} Sign Language in Dynamic Point-Light Displays.},
  author = {Poizner, Howard and Bellugi, Ursula and {Lutes-Driscoll}, Venita},
  year = {1981},
  volume = {7},
  pages = {430},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Poizner et al_1981_Perception of American sign language in dynamic point-light displays.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VUAQ9VMM/1982-01533-001.html},
  journal = {Journal of experimental psychology: Human perception and performance},
  number = {2}
}

@article{pols_realtime_1971,
  title = {Real-{{Time Recognition}} of {{Spoken Words}}},
  author = {Pols, L. C. W.},
  year = {1971},
  month = sep,
  volume = {C-20},
  pages = {972--978},
  issn = {0018-9340},
  doi = {10.1109/T-C.1971.223391},
  abstract = {First a survey is given of a number of published vowel and word recognition systems. Then a new real-time word recognition system is described that uses only a small computer (8K memory) and a few analog peripherals. The essentials of the procedure are as follows. During the pronunciation of a word, a spectral analysis is carried out by a bank of 17 1/3-octave bandpass filters. The outputs of the filters are logarithmically amplified and the maximal amplitude of the envelope is determined and sampled every 15 ms. In this way a word is characterized by a sequence of sample points in a 17-dimensional space. Then a principal components analysis is performed, reducing the original 17 dimensions of the space to 3. After a linear time normalization, the 3-dimensional trace of the spoken word is compared with 20 reference traces, representing the 20 possible utterances (the digits, plus 10 computer commands). The machine responds by naming the best fitting trace. With the 20 speakers of the design set, the machine is correct 98.8 percent of the time.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pols_1971_Real-Time Recognition of Spoken Words.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DCFBRVLV/1671984.html},
  journal = {IEEE Transactions on Computers},
  number = {9}
}

@inproceedings{popovic_statistical_2006,
  title = {Statistical Machine Translation of German Compound Words},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Advances}} in {{Natural Language Processing}}},
  author = {Popovi{\'c}, Maja and Stein, Daniel and Ney, Hermann},
  year = {2006},
  month = aug,
  pages = {616--624},
  publisher = {{Springer-Verlag}},
  address = {{Turku, Finland}},
  doi = {10.1007/11816508_61},
  abstract = {German compound words pose special problems to statistical machine translation systems: the occurence of each of the components in the training data is not sufficient for successful translation. Even if the compound itself has been seen during training, the system may not be capable of translating it properly into two or more words. If German is the target language, the system might generate only separated components or may not be capable of choosing the correct compound. In this work, we investigate and compare different strategies for the treatment of German compound words in statistical machine translation systems. For translation from German, we compare linguistic-based and corpus-based compound splitting. For translation into German, we investigate splitting and rejoining German compounds, as well as joining English potential components. Additionaly, we investigate word alignments enhanced with knowledge about the splitting points of German compounds. The translation quality is consistently improved by all methods for both translation directions.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Popović et al_2006_Statistical machine translation of german compound words.pdf},
  isbn = {978-3-540-37334-6},
  series = {{{FinTAL}}'06}
}

@phdthesis{popovic09:phd,
  title = {Machine {{Translation}}: {{Statistical Approach}} with {{Additional Linguistic Knowledge}}},
  author = {Popovi{\'c}, Maja},
  year = {2009},
  month = apr,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Popović_2009_Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@article{poppe_visionbased_2007,
  title = {Vision-Based Human Motion Analysis: {{An}} Overview},
  shorttitle = {Vision-Based Human Motion Analysis},
  author = {Poppe, R.},
  year = {2007},
  volume = {108},
  pages = {4--18},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Poppe_2007_Vision-based human motion analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/236TQTE6/S1077314206002293.html},
  journal = {Computer Vision and Image Understanding (CVIU)},
  number = {1-2}
}

@article{poria_multimodal_2018,
  title = {Multimodal {{Sentiment Analysis}}: {{Addressing Key Issues}} and {{Setting Up}} the {{Baselines}}},
  shorttitle = {Multimodal {{Sentiment Analysis}}},
  author = {Poria, S. and Majumder, N. and Hazarika, D. and Cambria, E. and Gelbukh, A. and Hussain, A. and Cambria, E.},
  year = {2018},
  month = nov,
  volume = {33},
  pages = {17--25},
  issn = {1541-1672},
  doi = {10.1109/MIS.2018.2882362},
  abstract = {We compile baselines, along with dataset split, for multimodal sentiment analysis. In this paper, we explore three different deep-learning-based architectures for multimodal sentiment classification, each improving upon the previous. Further, we evaluate these architectures with multiple datasets with fixed train/test partition. We also discuss some major issues, frequently ignored in multimodal sentiment analysis research, e.g., the role of speaker-exclusive models, the importance of different modalities, and generalizability. This framework illustrates the different facets of analysis to be considered while performing multimodal sentiment analysis and, hence, serves as a new benchmark for future research in this emerging field.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Poria et al_2018_Multimodal Sentiment Analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/H7NKJWGQ/8636432.html},
  journal = {IEEE Intelligent Systems},
  number = {6}
}

@article{porta_rulebased_2014,
  title = {A Rule-Based Translation from Written {{Spanish}} to {{Spanish Sign Language}} Glosses},
  author = {Porta, Jordi and {L{\'o}pez-Colino}, Fernando and Tejedor, Javier and Col{\'a}s, Jos{\'e}},
  year = {2014},
  month = may,
  volume = {28},
  pages = {788--811},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2013.10.003},
  abstract = {One of the aims of Assistive Technologies is to help people with disabilities to communicate with others and to provide means of access to information. As an aid to Deaf people, we present in this work a production-quality rule-based machine system for translating from Spanish to Spanish Sign Language (LSE) glosses, which is a necessary precursor to building a full machine translation system that eventually produces animation output. The system implements a transfer-based architecture from the syntactic functions of dependency analyses. A sketch of LSE is also presented. Several topics regarding translation to sign languages are addressed: the lexical gap, the bootstrapping of a bilingual lexicon, the generation of word order for topic-oriented languages, and the treatment of classifier predicates and classifier names. The system has been evaluated with an open-domain testbed, reporting a 0.30 BLEU (BiLingual Evaluation Understudy) and 42\% TER (Translation Error Rate). These results show consistent improvements over a statistical machine translation baseline, and some improvements over the same system preserving the word order in the source sentence. Finally, the linguistic analysis of errors has identified some differences due to a certain degree of structural variation in LSE.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Porta et al_2014_A rule-based translation from written Spanish to Spanish Sign Language glosses.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TBK4GET7/S0885230813000867.html},
  journal = {Computer Speech and Language},
  number = {3}
}

@article{potamianos_audiovisual_2004,
  title = {Audio-Visual Automatic Speech Recognition: {{An}} Overview},
  shorttitle = {Audio-Visual Automatic Speech Recognition},
  author = {Potamianos, Gerasimos and Neti, Chalapathy and Luettin, Juergen and Matthews, Iain},
  year = {2004},
  volume = {22},
  pages = {23},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Potamianos et al_2004_Audio-visual automatic speech recognition.pdf},
  journal = {Issues in Visual and Audio-Visual Speech Processing}
}

@inproceedings{potamianos_image_1998,
  title = {An Image Transform Approach for {{HMM}} Based Automatic Lipreading},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Potamianos, Gerasimos and Graf, Hans Peter and Cosatto, Eric},
  year = {1998},
  pages = {173--177},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Potamianos et al_1998_An image transform approach for HMM based automatic lipreading.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/844PSBIU/cookiedetectresponse.html}
}

@article{potamianos_recent_2003,
  title = {Recent Advances in the Automatic Recognition of Audiovisual Speech},
  author = {Potamianos, G. and Neti, C. and Gravier, G. and Garg, A. and Senior, A.W.},
  year = {2003},
  month = sep,
  volume = {91},
  pages = {1306--1326},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2003.817150},
  abstract = {Visual speech information from the speaker's mouth region has been successfully shown to improve noise robustness of automatic speech recognizers, thus promising to extend their usability in the human computer interface. In this paper, we review the main components of audiovisual automatic speech recognition (ASR) and present novel contributions in two main areas: first, the visual front-end design, based on a cascade of linear image transforms of an appropriate video region of interest, and subsequently, audiovisual speech integration. On the latter topic, we discuss new work on feature and decision fusion combination, the modeling of audiovisual speech asynchrony, and incorporating modality reliability estimates to the bimodal recognition process. We also briefly touch upon the issue of audiovisual adaptation. We apply our algorithms to three multisubject bimodal databases, ranging from small- to large-vocabulary recognition tasks, recorded in both visually controlled and challenging environments. Our experiments demonstrate that the visual modality improves ASR over all conditions and data considered, though less so for visually challenging environments and large vocabulary tasks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Potamianos et al_2003_Recent advances in the automatic recognition of audiovisual speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KKZMIQ6K/abs_all.html},
  journal = {Proceedings of the IEEE},
  number = {9}
}

@inproceedings{potamias_nearest_2008,
  title = {Nearest Neighbor Search Methods for Handshape Recognition},
  booktitle = {Proc {{Int}}. {{Conf}}. on {{PErvasive Technologies Related}} to {{Assistive Environments}}},
  author = {Potamias, Michalis and Athitsos, Vassilis},
  year = {2008},
  pages = {30:1--30:8},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1389586.1389622},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Potamias_Athitsos_2008_Nearest neighbor search methods for handshape recognition.pdf},
  isbn = {978-1-60558-067-8},
  keywords = {read 2011-week49},
  series = {{{PETRA}} '08}
}

@inproceedings{povey_purely_2016,
  title = {Purely {{Sequence}}-{{Trained Neural Networks}} for {{ASR Based}} on {{Lattice}}-{{Free MMI}}},
  booktitle = {Interspeech},
  author = {Povey, Daniel and Peddinti, Vijayaditya and Galvez, Daniel and Ghahremani, Pegah and Manohar, Vimal and Na, Xingyu and Wang, Yiming and Khudanpur, Sanjeev},
  year = {2016},
  month = sep,
  pages = {2751--2755},
  address = {{San Francisco, California, USA}},
  doi = {10.21437/Interspeech.2016-595},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Povey et al_2016_Purely sequence-trained neural networks for ASR based on lattice-free MMI.pdf}
}

@article{povey_subspace_2011,
  title = {The Subspace {{Gaussian}} Mixture Model\textemdash{{A}} Structured Model for Speech Recognition},
  author = {Povey, Daniel and Burget, Luk{\'a}{\v s} and Agarwal, Mohit and Akyazi, Pinar and Kai, Feng and Ghoshal, Arnab and Glembek, Ond{\v r}ej and Goel, Nagendra and Karafi{\'a}t, Martin and Rastrow, Ariya and Rose, Richard C. and Schwarz, Petr and Thomas, Samuel},
  year = {2011},
  month = apr,
  volume = {25},
  pages = {404--439},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2010.06.003},
  abstract = {We describe a new approach to speech recognition, in which all Hidden Markov Model (HMM) states share the same Gaussian Mixture Model (GMM) structure with the same number of Gaussians in each state. The model is defined by vectors associated with each state with a dimension of, say, 50, together with a global mapping from this vector space to the space of parameters of the GMM. This model appears to give better results than a conventional model, and the extra structure offers many new opportunities for modeling innovations while maintaining compatibility with most standard techniques.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Povey et al_2011_The subspace Gaussian mixture model—A structured model for speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P3QDKLT4/S088523081000063X.html},
  journal = {Computer Speech and Language},
  number = {2},
  series = {Language and Speech Issues in the Engineering of Companionable Dialogue Systems}
}

@article{prabhavalkar_minimum_2017,
  title = {Minimum {{Word Error Rate Training}} for {{Attention}}-Based {{Sequence}}-to-{{Sequence Models}}},
  author = {Prabhavalkar, Rohit and Sainath, Tara N. and Wu, Yonghui and Nguyen, Patrick and Chen, Zhifeng and Chiu, Chung-Cheng and Kannan, Anjuli},
  year = {2017},
  month = dec,
  abstract = {Sequence-to-sequence models, such as attention-based models in automatic speech recognition (ASR), are typically trained to optimize the cross-entropy criterion which corresponds to improving the loglikelihood of the data. However, system performance is usually measured in terms of word error rate (WER), not log-likelihood. Traditional ASR systems benefit from discriminative sequence training which optimizes criteria such as the state-level minimum Bayes risk (sMBR) which are more closely related to WER.},
  archivePrefix = {arXiv},
  eprint = {1712.01818},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Prabhavalkar et al_2017_Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models.pdf},
  journal = {arXiv:1712.01818 [cs, eess, stat]},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@inproceedings{premaratne_australian_2013,
  title = {Australian {{Sign Language Recognition Using Moment Invariants}}},
  booktitle = {Intelligent {{Computing Theories}} and {{Technology}}},
  author = {Premaratne, Prashan and Yang, Shuai and Zou, ZhengMao and Vial, Peter},
  editor = {Huang, De-Shuang and Jo, Kang-Hyun and Zhou, Yong-Quan and Han, Kyungsook},
  year = {2013},
  pages = {509--514},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {Human Computer Interaction is geared towards seamless human machine integration without the need for LCDs, Keyboards or Gloves. Systems have already been developed to react to limited hand gestures especially in gaming and in consumer electronics control. Yet, it is a monumental task in bridging the well-developed sign languages in different parts of the world with a machine to interpret the meaning. One reason is the sheer extent of the vocabulary used in sign language and the sequence of gestures needed to communicate different words and phrases. Auslan the Australian Sign Language is comprised of numbers, finger spelling for words used in common practice and a medical dictionary. There are 7415 words listed in Auslan website. This research article tries to implement recognition of numerals using a computer using the static hand gesture recognition system developed for consumer electronics control at the University of Wollongong in Australia. The experimental results indicate that the numbers, zero to nine can be accurately recognized with occasional errors in few gestures. The system can be further enhanced to include larger numerals using a dynamic gesture recognition system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Premaratne et al_2013_Australian Sign Language Recognition Using Moment Invariants.pdf},
  isbn = {978-3-642-39482-9},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{prillwitz_hamnosys_1989,
  title = {{{HamNoSys}}. {{Version}} 2.0},
  author = {Prillwitz, S. and Leven, R. and Zienert, H. and Zienert, R. and Hanke, T.},
  year = {1989},
  publisher = {{Signum}},
  address = {{Hamburg}},
  series = {Int'l {{Studies}} on {{Sign Language}} and {{Communication}} of the {{Deaf}}}
}

@inproceedings{pu_dilated_2018,
  title = {Dilated {{Convolutional Network}} with {{Iterative Optimization}} for {{Continuous Sign Language Recognition}}.},
  booktitle = {{{IJCAI}}},
  author = {Pu, Junfu and Zhou, Wengang and Li, Houqiang},
  year = {2018},
  month = jul,
  pages = {885--891},
  address = {{Stockholm, Sweden}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pu et al_2018_Dilated Convolutional Network with Iterative Optimization for Continuous Sign.pdf},
  keywords = {augment=?,authorgroup=USTC,capturedBy=vision,cnnresiduals=1,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,dataSet=Phoenix14,loss=CE,loss=CTC,modal=fullframe,nn=3dcnn,nn=dilatedcnn,nnarch=3d-resnet,nnarch=3dresnet,nntrain=3dnn+dilatedcnn,paperinfo,paperinfo;authorgroup=USTC;shorttitle=Dilated Convolutions,phoenix,re-align=1,shorttitle=Dilated Convolutions,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;dataSet=CSL;nn=3dcnn;nn=dilatedcnn;nnarch=3d-resnet;cnnresiduals=1;re-align=1;loss=CTC;loss=CE;nntrain=3dnn+dilatedcnn;augment=?,slreco;modal=fullframe;dataSet=Phoenix14;wer=38.0/37.3;nn=3dcnn;nn=dilatedcnn;nnarch=3dresnet;cnnresiduals=1;re-align=1;loss=CTC;loss=CE;nntrain=3dnn+dilatedcnn;augment=?,vocab<500,vocab=1080,vocab=178,vocab>=1000,wer=38.0/37.3}
}

@inproceedings{pu_iterative_2019,
  title = {Iterative {{Alignment Network}} for {{Continuous Sign Language Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pu, Junfu and Zhou, Wengang and Li, Houqiang},
  year = {2019},
  month = jun,
  pages = {10},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pu et al_2019_Iterative Alignment Network for Continuous Sign Language Recognition.pdf},
  keywords = {augment=?,authorgroup=USTC,capturedBy=vision,cnnresiduals=1,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CLS,dataSet=CSL,dataSet=Phoenix14,loss=CE,loss=CTC,modal=fullframe,nn=3dcnn,nn=attention,nn=blstm,nn=lstm,nnarch=3d-resnet,nntrain=3dcnn+blstm,paperinfo,paperinfo;authorgroup=USTC;shorttitle=Iterative Alignment Network,re-align=1,shorttitle=Iterative Alignment Network,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;dataSet=CLS;nn=3dcnn;nn=blstm;nn=attention;nn=lstm;nnarch=3d-resnet;cnnresiduals=1;re-align=1;nntrain=3dcnn+blstm;augment=?;loss=CTC;loss=CE,slreco;modal=fullframe;dataSet=Phoenix14;wer=37.1/36.7;nn=3dcnn;nn=blstm;nn=attention;nn=lstm;nnarch=3d-resnet;cnnresiduals=1;re-align=1;nntrain=3dcnn+blstm;augment=?;loss=CTC;loss=CE,vocab<500,vocab=1080,vocab=178,vocab>=1000,wer=37.1/36.7}
}

@inproceedings{pu_sign_2016,
  title = {Sign {{Language Recognition Based}} on {{Trajectory Modeling}} with {{HMMs}}},
  booktitle = {Int. {{Conf}}. on {{Multimedia Modeling}}},
  author = {Pu, Junfu and Zhou, Wengang and Zhang, Jihai and Li, Houqiang},
  year = {2016},
  month = jan,
  pages = {686--697},
  doi = {10.1007/978-3-319-27671-7_58},
  abstract = {Sign language recognition targets on interpreting and understanding the sign language for convenience of communication between the deaf and the normal people, which has broad social impact. The problem is challenging due to the large variations for different signers and the subtle difference between sign words. In this paper, we propose a new method for isolated sign language recognition based on trajectory modeling with hidden Markov models (HMMs). In our approach, we first normalize and re-sample the raw trajectory data and partition the trajectory into multiple segments. To represent each trajectory segment, we proposed a new curve feature descriptor based on shape context. After that, hidden Markov model is used to model each isolated sign word for recognition. To evaluate the performance of our proposed algorithm, we have built a large isolated Chinese sign language vocabulary with Kinect 2.0. The dataset contains 100 unique isolated sign words, each of which is performed by 50 signers for 5 times. Experimental results demonstrate that the proposed method achieves a better performance compared with normal coordinate feature with HMM.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pu et al_2016_Sign Language Recognition Based on Trajectory Modeling with HMMs.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XJR2S2PM/978-3-319-27671-7_58.html},
  isbn = {978-3-319-27670-0 978-3-319-27671-7},
  keywords = {3dcnn=0,capturedBy=depth,capturedBy=vision,hmm=1,isolated,isolated;vocab=100;signer=14;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,nn=0,nonintrusive=1,rnn=0,signer=14,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement,svm=0,vocab<500,vocab=100},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{pu_sign_2016a,
  title = {Sign {{Language Recognition}} with {{Multi}}-Modal {{Features}}},
  booktitle = {Proc. {{Advances}} in {{Multimedia Information Processing}}},
  author = {Pu, Junfu and Zhou, Wengang and Li, Houqiang},
  year = {2016},
  month = sep,
  pages = {252--261},
  doi = {10.1007/978-3-319-48896-7_25},
  abstract = {We study the problem of recognizing sign language automatically using the RGB videos and skeleton coordinates captured by Kinect, which is of great significance in communication between the deaf and the hearing societies. In this paper, we propose a sign language recognition (SLR) system with data of two channels, including the gesture videos of the sign words and joint trajectories. In our framework, we extract two modals of features to represent the hand shape videos and hand trajectories for recognition. The variation of gesture is obtained by 3D CNN and the activations of fully connected layers are used as the representations of these sign videos. For trajectories, we use the shape context to describe each joint, and combine them all within a feature matrix. After that, a convolutional neural network is applied to generate a robust representation of these trajectories. Furthermore, we fuse these features and train a SVM classifier for recognition. We conduct some experiments on large vocabulary sign language dataset with up to 500 words and the results demonstrate the effectiveness of our proposed method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pu et al_2016_Sign Language Recognition with Multi-modal Features.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V5DRPCEK/978-3-319-48896-7_25.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=500;signer=50;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=50,sl=CSL,slreco,slreco;modal=2hshape;modal=2hmovement;modal=2hlocation;modal=bodyjoints,vocab<1000,vocab=500},
  language = {en}
}

@inproceedings{pugeault_spelling_2011,
  title = {Spelling {{It Out}}: {{Real}}\textendash{{Time ASL Fingerspelling Recognition}}},
  shorttitle = {Spelling {{It Out}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Pugeault, N. and Bowden, R.},
  year = {2011},
  month = nov,
  pages = {1114--1119},
  address = {{Barcelona, Spain}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pugeault_Bowden_2011_Spelling It Out.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NW2K7WM2/abs_all.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=24;signer=4;sl=ASL;capturedBy=vision;capturedBy=depth,modal=shape,read 2012-week01,signer=4,sl=ASL,slreco,slreco;modal=shape,vocab<50,vocab=24}
}

@article{pundak_deep_2018,
  title = {Deep Context: End-to-End Contextual Speech Recognition},
  shorttitle = {Deep Context},
  author = {Pundak, Golan and Sainath, Tara N. and Prabhavalkar, Rohit and Kannan, Anjuli and Zhao, Ding},
  year = {2018},
  month = aug,
  abstract = {In automatic speech recognition (ASR) what a user says depends on the particular context she is in. Typically, this context is represented as a set of word n-grams. In this work, we present a novel, all-neural, end-to-end (E2E) ASR system that utilizes such context. Our approach, which we refer to as Contextual Listen, Attend and Spell (CLAS) jointlyoptimizes the ASR components along with embeddings of the context n-grams. During inference, the CLAS system can be presented with context phrases which might contain out-ofvocabulary (OOV) terms not seen during training. We compare our proposed system to a more traditional contextualization approach, which performs shallow-fusion between independently trained LAS and contextual n-gram models during beam search. Across a number of tasks, we find that the proposed CLAS system outperforms the baseline method by as much as 68\% relative WER, indicating the advantage of joint optimization over individually trained components.},
  archivePrefix = {arXiv},
  eprint = {1808.02480},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Pundak et al_2018_Deep context.pdf},
  journal = {arXiv:1808.02480 [cs, eess, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{puviarasan_lip_2011,
  title = {Lip Reading of Hearing Impaired Persons Using {{HMM}}},
  author = {Puviarasan, N. and Palanivel, S.},
  year = {2011},
  month = apr,
  volume = {38},
  pages = {4477--4481},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2010.09.119},
  abstract = {This paper describes a method for lip reading of hearing impaired persons. The term lip reading refers to recognizing the spoken words using visual speech information such as lip movements. The visual speech video of the hearing impaired person is given as input to the face detection module for detecting the face region. The region of the mouth is determined relative to the face region. The mouth images are used for feature extraction. The features are extracted using discrete cosine transform (DCT) and discrete wavelet transform (DWT). Then, these features are applied separately as inputs to the hidden markov model (HMM) for recognizing the visual speech. To understand the visual speech of hearing impaired person in cash collection counters, 33 words are chosen. For each word, 20 samples are collected for training the HMM model and another five samples are used for testing the model. The experimental results show that the method gives the performance of 91.0\% for the DCT based lip features and 97.0\% for DWT based lip features.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Puviarasan_Palanivel_2011_Lip reading of hearing impaired persons using HMM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2AIT2N7E/S0957417410010766.html},
  journal = {Expert Systems with Applications},
  number = {4}
}

@inproceedings{quack_efficient_2007,
  title = {Efficient Mining of Frequent and Distinctive Feature Configurations},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Quack, Till and Ferrari, Vittorio and Leibe, Bastian and Gool, Luc Van},
  year = {2007},
  month = oct,
  pages = {1--8},
  address = {{Rio de Janeiro, Brazil}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Quack et al_2007_Efficient mining of frequent and distinctive feature configurations.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BATM6QTE/abs_all.html}
}

@incollection{quadros_methods_2015,
  title = {Methods in {{Bimodal Bilingualism Research}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {de Quadros, Ronice M{\"u}ller and Pichler, Deborah Chen and Lillo-Martin, Diane and Cruz, Carina Rebello and Kozak, L. Viola and Palmer, Jeffrey Levi and Pizzio, Aline Lemos and Reynolds, Wanette},
  year = {2015},
  pages = {250--280},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch14},
  abstract = {This chapter presents an overview of methods in bilingualism and bimodal bilingualism research. It begins with an introduction to a research project for which these methods were developed. Next, the chapter describes the process of selection, adaptation, or development of parallel test batteries for target languages. These test target children between the ages of 4 to 7 years in the areas of general language, vocabulary, morphosyntax, phonology, and non-verbal intelligence. The chapter also describes each test, including information on how the test was developed and how it is administered, as well as suggestions for the set-up of camera(s) during filming. It ends with a discussion of the advantages of the data collection fair concept (an innovative technique for child data collection designed to improve on traditional piecemeal collection) and with a list of best practices for optimizing test effectiveness.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Quadros et al_2015_Methods in Bimodal Bilingualism Research.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VQQTFI9H/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@inproceedings{quiroga_study_2017,
  title = {A {{Study}} of {{Convolutional Architectures}} for {{Handshape Recognition}} Applied to {{Sign Language}}},
  booktitle = {{{XXIII Congreso Argentino}} de {{Ciencias}} de La {{Computaci\'on}}},
  author = {Quiroga, Facundo and Antonio, Ramiro and Ronchetti, Franco and Lanzarini, Laura Cristina and Rosete, Alejandro},
  year = {2017},
  month = oct,
  pages = {13--22},
  address = {{La Plata, Argentinia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Quiroga et al_2017_A Study of Convolutional Architectures for Handshape Recognition applied to.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z5K9S8GS/63481.html},
  keywords = {capturedBy=coloredGlove,dataSet=LSA16,isolated,isolated;vocab=16;sl=ArgentinianSL;capturedBy=coloredGlove;dataSet=LSA16,LSA16,modal=shape,sl=ArgentinianSL,slreco,slreco;modal=shape,vocab<50,vocab=16}
}

@article{r._enhanced_,
  title = {Enhanced Dynamic Programming Approach for Subunit Modelling to Handle Segmentation and Recognition Ambiguities in Sign Language},
  author = {R., Elakkiya and K., Selvamani},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2017.07.001},
  abstract = {Sign language serves as a primary means of communication among the deaf impaired community. The major challenges faced by the Sign Language Recognition (SLR) system are recognizing signs from large vocabularies in continuous video sequences. In this research paper, a novel subunit sign modelling framework is proposed for vision-based SLR which aims in solving the major issues in SLR systems. The problem of hand segmentation ambiguities and segregating epentheses movements between two adjacent signs in continuous video sequences are addressed. A novel subunit sign modelling framework is presented and illustrated to embark upon these problems while considering large-vocabularies. This framework is developed using a novel methodology of Enhanced Dynamic Programming (EDP) approach in subunit sign modelling. This EDP framework works with a combination of dynamic time warping and spatiotemporal clustering techniques. Since, sign language consists of both spatial and temporal feature vectors, dynamic time warping is used as a distance measure to compute the distance between two adjacent signs in sign trajectories. This distance is used as a temporal feature vector during the clustering of spatial feature vectors using Minimum Entropy Clustering (MEC). This process is done recursively to cluster all the epentheses movements dynamically without using any explicit or implicit modelling. Experimental results have confirmed that the computation cost of the proposed system is less because the epenthesis movements are eliminated before classification and the gesture base space utilized by the sign gestures is very low because the proposed system does not require any modelling to handle epenthesis movements. The results obtained from the proposed subunit sign modelling framework is compared with other existing models in order to prove that the proposed system is best among the existing systems.},
  journal = {Journal of Parallel and Distributed Computing}
}

@article{r._subunit_2019,
  title = {Subunit Sign Modeling Framework for Continuous Sign Language Recognition},
  author = {R., Elakkiya and K., Selvamani},
  year = {2019},
  month = mar,
  volume = {74},
  pages = {379--390},
  issn = {0045-7906},
  doi = {10.1016/j.compeleceng.2019.02.012},
  abstract = {A new framework named three subunit sign modeling is introduced for automatic sign language recognition. This works on continuous video sequences consisting of isolated words, signed sentences under different signer variations and illuminations. Three major issues of automatic sign language recognition is addressed namely: (i) importance of discriminative feature extraction and selection (ii) handling epenthesis movements and segmentation ambiguities (iii) automatic recognition of large vocabulary sign sentences and signer adaptation in a single subunit sign modeling framework. The proposed work has been evaluated and experimented subjectively and quantitatively with real-time signing videos gathered from different corpora and different sign languages. The results of the experiments have proven that the proposed subunit sign modeling framework remains scalable while increasing the sign vocabulary. Further, the approach is more reliable and efficient enough to adapt to real-time constraints and signer independence.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7X8KME6R/S0045790618318755.html},
  journal = {Computers \& Electrical Engineering}
}

@article{rabiner_introduction_1986,
  title = {An Introduction to Hidden {{Markov}} Models},
  author = {Rabiner, Lawrence R. and Juang, Biing-Hwang},
  year = {1986},
  month = jan,
  volume = {3},
  pages = {4--16},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rabiner_Juang_1986_An introduction to hidden Markov models.pdf},
  journal = {IEEE ASSP Magazine},
  number = {1}
}

@article{rabiner_segmental_1986,
  title = {A {{Segmental}} K-{{Means Training Procedure}} for {{Connected Word Recognition}}},
  author = {Rabiner, Lawrence R. and Wilpon, Jay G. and Juang, Bling-Hwang},
  year = {1986},
  volume = {65},
  pages = {21--31},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1986.tb00368.x},
  abstract = {Algorithms for recognizing strings of connected words from whole-word patterns have become highly efficient and accurate, although computation rates remain high. Even the most ambitious connected-word recognition task is practical with today's integrated circuit technology, but extracting reliable, robust whole-word reference patterns still is difficult. In the past, connected-word recognizers relied on isolated-word reference patterns or patterns derived from a limited context (e.g., the middle digit from strings of three digits). These whole-word patterns were adequate for slow rates of articulated speech, but not for strings of words spoken at high rates (e.g., about 200 to 300 words per minute). To alleviate this difficulty, a segmental k-means training procedure was used to extract whole-word patterns from naturally spoken word strings. The segmented words are then used to create a set of word reference patterns for recognition. Recognition string accuracies were 98 to 99 percent for digits in variable length strings and 90 to 98 percent for sentences from an airline reservation task. These performance scores represent significant improvements over previous connected-word recognizers.},
  copyright = {\textcopyright{} 1986 AT\&T Technical Journal},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rabiner et al_1986_A segmental k-means training procedure for connected word recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XN9Y8P8A/j.1538-7305.1986.tb00368.html},
  journal = {AT\&T Technical Journal},
  language = {en},
  number = {3}
}

@article{rabiner_tutorial_1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, Lawrence R.},
  year = {1989},
  volume = {77},
  pages = {257--286},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BQWQVEBM/18626.html},
  journal = {Proceedings of the IEEE},
  number = {2}
}

@article{radford_language_2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Radford et al_2019_Language models are unsupervised multitask learners.pdf},
  journal = {URL https://openai. com/blog/better-language-models}
}

@article{raj_reconstruction_2004,
  title = {Reconstruction of Missing Features for Robust Speech Recognition},
  author = {Raj, Bhiksha and Seltzer, Michael L. and Stern, Richard M.},
  year = {2004},
  volume = {43},
  pages = {275--296},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Raj et al_2004_Reconstruction of missing features for robust speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SIQ43AVX/S0167639304000627.html},
  journal = {Speech communication},
  number = {4}
}

@inproceedings{rakowski_hand_2018,
  title = {Hand {{Shape Recognition Using Very Deep Convolutional Neural Networks}}},
  booktitle = {Proceedings of the 2018 {{International Conference}} on {{Control}} and {{Computer Vision}}},
  author = {Rakowski, Alexander and Wandzik, Lukasz},
  year = {2018},
  month = jun,
  pages = {8--12},
  publisher = {{ACM}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3232651.3232657},
  abstract = {This work examines the application of modern deep convolutional neural network architectures for classification tasks in the sign language domain. Transfer learning is performed by pre-training the models on the ImageNet dataset. After fine-tuning on the ASL fingerspelling and the 1 Million Hands datasets the models outperform state-of-the-art approaches on both hand shape classification tasks. Introspection of the trained models using Saliency Maps is also performed to analyze how the networks make their decisions. Finally, their robustness is investigated by occluding selected image regions.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rakowski_Wandzik_2018_Hand Shape Recognition Using Very Deep Convolutional Neural Networks.pdf},
  isbn = {978-1-4503-6470-6},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=24;signer=5;sl=ASL;capturedBy=vision;capturedBy=depth,modal=shape,signer=5,sl=ASL,slreco,slreco;modal=shape,vocab<50,vocab=24},
  series = {{{ICCCV}} '18}
}

@inproceedings{rakthanmanon_time_2011,
  title = {Time Series Epenthesis: {{Clustering}} Time Series Streams Requires Ignoring Some Data},
  shorttitle = {Time Series Epenthesis},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Data Mining}} ({{ICDM}})},
  author = {Rakthanmanon, Thanawin and Keogh, Eamonn J. and Lonardi, Stefano and Evans, Scott},
  year = {2011},
  pages = {547--556},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rakthanmanon et al_2011_Time series epenthesis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5FTWHDFB/login.html}
}

@inproceedings{ramakrishna_reconstructing_2012,
  title = {Reconstructing {{3D}} Human Pose from {{2D}} Image Landmarks},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
  year = {2012},
  pages = {573--586},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ramakrishna et al_2012_Reconstructing 3D human pose from 2D image landmarks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/G8EVBN4N/978-3-642-33765-9_41.html}
}

@inproceedings{ranzato_deep_2011,
  title = {On Deep Generative Models with Applications to Recognition},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ranzato, M. and Susskind, Joshua and Mnih, Volodymyr and Hinton, Geoffrey},
  year = {2011},
  pages = {2857--2864},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ranzato et al_2011_On deep generative models with applications to recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MMHZ662F/login.html}
}

@inproceedings{rao_attentionaware_2017,
  title = {Attention-{{Aware Deep Reinforcement Learning}} for {{Video Face Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  year = {2017},
  month = jul,
  pages = {3931--3940},
  address = {{Honolulu, HI, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rao et al_2017_Attention-Aware Deep Reinforcement Learning for Video Face Recognition.pdf}
}

@inproceedings{rao_deep_2018,
  title = {Deep Convolutional Neural Networks for Sign Language Recognition},
  booktitle = {Proc. {{Conf}}. on {{Signal Processing And Communication Engineering Systems}} ({{SPACES}})},
  author = {Rao, G. A. and Syamala, K. and Kishore, P. V. V. and Sastry, A. S. C. S.},
  year = {2018},
  month = jan,
  pages = {194--197},
  doi = {10.1109/SPACES.2018.8316344},
  abstract = {Extraction of complex head and hand movements along with their constantly changing shapes for recognition of sign language is considered a difficult problem in computer vision. This paper proposes the recognition of Indian sign language gestures using a powerful artificial intelligence tool, convolutional neural networks (CNN). Selfie mode continuous sign language video is the capture method used in this work, where a hearing-impaired person can operate the SLR mobile application independently. Due to non-availability of datasets on mobile selfie sign language, we initiated to create the dataset with five different subjects performing 200 signs in 5 different viewing angles under various background environments. Each sign occupied for 60 frames or images in a video. CNN training is performed with 3 different sample sizes, each consisting of multiple sets of subjects and viewing angles. The remaining 2 samples are used for testing the trained CNN. Different CNN architectures were designed and tested with our selfie sign language data to obtain better accuracy in recognition. We achieved 92.88\% recognition rate compared to other classifier models reported on the same dataset.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rao et al_2018_Deep convolutional neural networks for sign language recognition.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=200;signer=5;sl=IndianSL;capturedBy=vision,modal=2hlocation,modal=2hshape,signer=5,sl=IndianSL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<500,vocab=200}
}

@incollection{rao_rulebased_1996,
  title = {Is {{Rule}}-{{Based Acoustic}}-{{Phonetic Speech Recognition}} a {{Dead End}}?},
  booktitle = {Recent {{Research Towards Advanced Man}}-{{Machine Interface Through Spoken Language}}},
  author = {Rao, P. V. S.},
  editor = {Fujisaki, H.},
  year = {1996},
  month = jan,
  pages = {160--164},
  publisher = {{Elsevier Science B.V.}},
  address = {{Amsterdam}},
  doi = {10.1016/B978-044481607-8/50060-8},
  abstract = {Speech recognition consists in assigning to the tune domain acoustic signal, a sequence of labels taken from a label bank or vocabulary consisting of a finite number of distinct labels. The labels could be sentences, words, phonemes and any type of linguistically significant units. Recognition is feasible if there is some degree of consistency in the correspondence between the labels and their acoustic manifestations. The chapter discusses that speech recognition using sub-word units and Hidden Markov models has become very popular in the recent past in view of their versatility and good performance. Acoustic-phonetic rule based recognition, which has been the main stay for many years, appears no longer to be so attractive. The chapter studies all the three approaches and their implications to determine whether the acoustic-phonetic approach has a future at all. The chapter concludes that acoustic-phonetic approach holds a potential for the future based on the analysis. The chapter also visualizes the prospect of an integrated approach which combines the strategies of all three approaches.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rao_1996_Is Rule-Based Acoustic-Phonetic Speech Recognition a Dead End.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZU8VCIF7/B9780444816078500608.html},
  isbn = {978-0-444-81607-8}
}

@article{rao_selfie_2018,
  title = {Selfie Video Based Continuous {{Indian}} Sign Language Recognition System},
  author = {Rao, G. Ananth and Kishore, P. V. V.},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {1929--1939},
  issn = {2090-4479},
  doi = {10.1016/j.asej.2016.10.013},
  abstract = {This paper introduces a novel method to bring sign language closer to real time application on mobile platforms. Selfie captured sign language video is processed by constraining its computing power to that of a smart phone. Pre-filtering, segmentation and feature extraction on video frames creates a sign language feature space. Minimum Distance and Artificial Neural Network classifiers on the sign feature space is trained and tested iteratively. Sobel edge operator's power is enhanced with morphology and adaptive thresholding giving a near perfect segmentation of hand and head portions compensating for the small vibrations of the selfie stick. Word matching score (WMS) gives the performance of the proposed method with an average WMS of around 85.58\% for MDC and 90\% for ANN with a small variation of 0.3s in classification times. Neural network classifiers with fast training algorithms will certainly make this novel selfie sign language recognizer application into app stores.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rao_Kishore_2018_Selfie video based continuous Indian sign language recognition system.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V6GRU936/S2090447917300217.html},
  journal = {Ain Shams Engineering Journal},
  keywords = {capturedBy=vision,continuous,continuous;vocab=18;signer=10;sl=IndianSL;capturedBy=vision,modal=fullframe,signer=10,sl=IndianSL,slreco,slreco;modal=fullframe,vocab<50,vocab=18},
  number = {4}
}

@article{rao_unsupervised_2008,
  title = {Unsupervised {{Segmentation With Dynamical Units}}},
  author = {Rao, A.R. and Cecchi, G.A. and Peck, C.C. and Kozloski, J.R.},
  year = {2008},
  month = jan,
  volume = {19},
  pages = {168--182},
  issn = {1045-9227},
  doi = {10.1109/TNN.2007.905852},
  abstract = {In this paper, we present a novel network to separate mixtures of inputs that have been previously learned. A significant capability of the network is that it segments the components of each input object that most contribute to its classification. The network consists of amplitude-phase units that can synchronize their dynamics, so that separation is determined by the amplitude of units in an output layer, and segmentation by phase similarity between input and output layer units. Learning is unsupervised and based on a Hebbian update, and the architecture is very simple. Moreover, efficient segmentation can be achieved even when there is considerable superposition of the inputs. The network dynamics are derived from an objective function that rewards sparse coding in the generalized amplitude-phase variables. We argue that this objective function can provide a possible formal interpretation of the binding problem and that the implementation of the network architecture and dynamics is biologically plausible.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rao et al_2008_Unsupervised Segmentation With Dynamical Units.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NKB3ZNVM/abs_all.html},
  journal = {IEEE Transactions on Neural Networks},
  number = {1}
}

@inproceedings{raptis_realtime_2011,
  title = {Real-Time Classification of Dance Gestures from Skeleton Animation},
  booktitle = {Proc. {{ACM SIGGRAPH}}/{{Eurographics Symposium}} on {{Computer Animation}}},
  author = {Raptis, Michalis and Kirovski, Darko and Hoppe, Hugues},
  year = {2011},
  pages = {147--156},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2019406.2019426},
  abstract = {We present a real-time gesture classification system for skeletal wireframe motion. Its key components include an angular representation of the skeleton designed for recognition robustness under noisy input, a cascaded correlation-based classifier for multivariate time-series data, and a distance metric based on dynamic time-warping to evaluate the difference in motion between an acquired gesture and an oracle for the matching gesture. While the first and last tools are generic in nature and could be applied to any gesture-matching scenario, the classifier is conceived based on the assumption that the input motion adheres to a known, canonical time-base: a musical beat. On a benchmark comprising 28 gesture classes, hundreds of gesture instances recorded using the XBOX Kinect platform and performed by dozens of subjects for each gesture class, our classifier has an average accuracy of 96:9\%, for approximately 4-second skeletal motion recordings. This accuracy is remarkable given the input noise from the real-time depth sensor.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Raptis et al_2011_Real-time classification of dance gestures from skeleton animation.pdf},
  isbn = {978-1-4503-0923-3},
  series = {{{SCA}} '11}
}

@article{rastgoo_multimodal_2018,
  title = {Multi-{{Modal Deep Hand Sign Language Recognition}} in {{Still Images Using Restricted Boltzmann Machine}}},
  author = {Rastgoo, Razieh and Kiani, Kourosh and Escalera, Sergio},
  year = {2018},
  month = nov,
  volume = {20},
  pages = {809},
  doi = {10.3390/e20110809},
  abstract = {In this paper, a deep learning approach, Restricted Boltzmann Machine (RBM), is used to perform automatic hand sign language recognition from visual data. We evaluate how RBM, as a deep generative model, is capable of generating the distribution of the input data for an enhanced recognition of unseen data. Two modalities, RGB and Depth, are considered in the model input in three forms: original image, cropped image, and noisy cropped image. Five crops of the input image are used and the hand of these cropped images are detected using Convolutional Neural Network (CNN). After that, three types of the detected hand images are generated for each modality and input to RBMs. The outputs of the RBMs for two modalities are fused in another RBM in order to recognize the output sign label of the input image. The proposed multi-modal model is trained on all and part of the American alphabet and digits of four publicly available datasets. We also evaluate the robustness of the proposal against noise. Experimental results show that the proposed multi-modal model, using crops and the RBM fusing methodology, achieves state-of-the-art results on Massey University Gesture Dataset 2012, American Sign Language (ASL). and Fingerspelling Dataset from the University of Surrey\&rsquo;s Center for Vision, Speech and Signal Processing, NYU, and ASL Fingerspelling A datasets.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rastgoo et al_2018_Multi-Modal Deep Hand Sign Language Recognition in Still Images Using.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PWE9RUIA/809.html},
  journal = {Entropy},
  language = {en},
  number = {11}
}

@phdthesis{ravanelli_deep_2017a,
  title = {Deep {{Learning}} for {{Distant Speech Recognition}}},
  author = {Ravanelli, Mirco},
  year = {2017},
  month = dec,
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ravanelli_2017_Deep learning for distant speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GIGDDVZJ/Ravanelli - Deep Learning for Distant Speech Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/THXKBD7P/1712.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YKW3IW2J/2693.html},
  school = {University of Trento}
}

@inproceedings{ravyse_eye_2000,
  title = {Eye Gesture Estimation},
  booktitle = {21st {{Symposium}} on {{Information Theory}} in the {{Benelux}}, {{Wassenaar}}, {{The Netherlands}}},
  author = {Ravyse, I. and Reinders, M. J. T. and Cornelis, J. and Sahli, H.},
  year = {2000},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ravyse et al_2000_Eye gesture estimation.pdf}
}

@article{reddy_hearsayi_1976,
  title = {The {{Hearsay}}-{{I}} Speech Understanding System: An Example of the Recognition Process},
  shorttitle = {The {{Hearsay}}-{{I}} Speech Understanding System},
  author = {Reddy, Dabbala Rajagopal and Erman, Lee D. and Fennell, Richard D. and Neely, Richard B.},
  year = {1976},
  pages = {422--431},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Reddy et al_1976_The Hearsay-I speech understanding system.pdf},
  journal = {IEEE Transactions on Computers},
  number = {4}
}

@article{reddy_speech_1976,
  title = {Speech Recognition by Machine: {{A}} Review},
  shorttitle = {Speech Recognition by Machine},
  author = {Reddy, D. R.},
  year = {1976},
  month = apr,
  volume = {64},
  pages = {501--531},
  issn = {0018-9219},
  doi = {10.1109/PROC.1976.10158},
  abstract = {This paper provides a review of recent developments in speech recognition research. The concept of sources of knowledge is introduced and the use of knowledge to generate and verify hypotheses is discussed. The difficulties that arise in the construction of different types of speech recognition systems are discussed and the structure and performance of several such systems is presented. Aspects of component subsystems at the acoustic, phonetic, syntactic, and semantic levels are presented. System organizations that are required for effective interaction and use of various component subsystems in the presence of error and ambiguity are discussed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Reddy_1976_Speech recognition by machine.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LTHZ3MNX/1454427.html},
  journal = {Proceedings of the IEEE},
  number = {4}
}

@inproceedings{Redmon_2016_CVPR,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = jun
}

@inproceedings{rekha_hand_2011,
  title = {Hand {{Gesture Recognition}} for {{Sign Language}}: {{A New Hybrid Approach}}},
  shorttitle = {Hand {{Gesture Recognition}} for {{Sign Language}}},
  booktitle = {Int. {{Conf}}.  {{Image Processing}}, {{Computer Vision}}, and {{Pattern Recognition}}},
  author = {Rekha, J. and Bhattacharya, J. and Majumder, S.},
  year = {2011},
  month = jul,
  pages = {80--86},
  address = {{Las Vegas, NV, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rekha et al_2011_Hand Gesture Recognition for Sign Language.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=10;sl=ASL;capturedBy=vision,modal=shape,sl=ASL,slreco,slreco;modal=shape,vocab<50,vocab=10}
}

@inproceedings{ren_human_2002,
  title = {Human Action Recognition with Primitive-Based Coupled-{{HMM}}},
  booktitle = {Object Recognition Supported by User Interaction for Service Robots},
  author = {Ren, Haibing and Xu, Guangyou},
  year = {2002},
  volume = {2},
  pages = {494-498 vol.2},
  doi = {10.1109/ICPR.2002.1048346},
  abstract = {This paper presents a new approach named primitive-based coupled-HMM for human natural complex action recognition. First, the system proposes a hybrid human model and employs 2-order B-spline function to detect the two shoulder joints in the silhouette image to obtain the basic motion features including the elbow angles, motion parameters of the face and two hands. Then, primitive-based coupled hidden Markov model (PCHMM) is presented for natural context-dependent action recognition. Lastly, comparison experiments show that PCHMM is better than the conventional HMM and coupled HMM.}
}

@article{ren_object_2015,
  title = {Object Detection Networks on Convolutional Feature Maps},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Zhang, Xiangyu and Sun, Jian},
  year = {2015},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ren et al_2015_Object detection networks on convolutional feature maps.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/H6KW9FGE/1504.html},
  journal = {arXiv preprint arXiv:1504.06066}
}

@article{richard_neural_1991,
  title = {Neural {{Network Classifiers Estimate Bayesian}} a Posteriori {{Probabilities}}},
  author = {Richard, Michael D. and Lippmann, Richard P.},
  year = {1991},
  volume = {3},
  pages = {461--483},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Richard_Lippmann_1991_Neural Network Classifiers Estimate Bayesian a posteriori Probabilities.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6VR4HTXK/freeabs_all.html},
  journal = {Neural Computation},
  number = {4}
}

@inproceedings{richard_neuralnetworkviterbi_2018,
  title = {{{NeuralNetwork}}-{{Viterbi}}: {{A Framework}} for {{Weakly Supervised Video Learning}}},
  shorttitle = {{{NeuralNetwork}}-{{Viterbi}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Richard, Alexander and Kuehne, Hilde and Iqbal, Ahsan and Gall, Juergen},
  year = {2018},
  pages = {7386--7395},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Richard et al_2018_NeuralNetwork-Viterbi.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/B2VIWLB9/Richard_NeuralNetwork-Viterbi_A_Framework_CVPR_2018_paper.html}
}

@inproceedings{richard_weakly_2017,
  title = {Weakly {{Supervised Action Learning}} with {{RNN}} Based {{Fine}}-to-Coarse {{Modeling}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Richard, Alexander and Kuehne, Hilde and Gall, Juergen},
  year = {2017},
  month = mar,
  address = {{Hawaii, USA}},
  abstract = {We present an approach for weakly supervised learning of human actions. Given a set of videos and an ordered list of the occurring actions, the goal is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. To address this task, we propose a combination of a discriminative representation of subactions, modeled by a recurrent neural network, and a coarse probabilistic model to allow for a temporal alignment and inference over long sequences. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes. To this end, we adapt the number of subaction classes by iterating realignment and reestimation during training. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.},
  archivePrefix = {arXiv},
  eprint = {1703.08132},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Richard et al_2017_Weakly Supervised Action Learning with RNN based Fine-to-coarse Modeling.pdf}
}

@article{richarz_visual_2011,
  title = {Visual Recognition of {{3D}} Emblematic Gestures in an {{HMM}} Framework},
  author = {Richarz, J. and Fink, G. A},
  year = {2011},
  volume = {3},
  pages = {193--211},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Richarz_Fink_2011_Visual recognition of 3D emblematic gestures in an HMM framework.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4S2GSDIM/Richarz und Fink - 2011 - Visual recognition of 3D emblematic gestures in an.html},
  journal = {Journal of Ambient Intelligence and Smart Environments},
  number = {3}
}

@inproceedings{riedl_unsupervised_2016,
  title = {Unsupervised {{Compound Splitting With Distributional Semantics Rivals Supervised Methods}}},
  booktitle = {Proc. {{Conf}}. of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Riedl, Martin and Biemann, Chris},
  year = {2016},
  month = jun,
  pages = {617--622},
  publisher = {{Association for Computational Linguistics}},
  address = {{San Diego, California}},
  doi = {10.18653/v1/N16-1075},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Riedl_Biemann_2016_Unsupervised Compound Splitting With Distributional Semantics Rivals Supervised.pdf}
}

@inproceedings{rodriguez-fuentes_multisite_2011,
  title = {Multi-Site Heterogeneous System Fusions for the {{Albayzin}} 2010 {{Language Recognition Evaluation}}},
  booktitle = {Proc. {{ASRU}} 2011 - {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}}},
  author = {{Rodriguez-Fuentes}, Luis Javier and Penagarikano, Mikel and Varona, Amparo and Diez, Mireia and Bordel, German and Martinez, David and Villalba, Jesus and Miguel, Antonio and Ortega, Alfonso and Lleida, Eduardo and Abad, Alberto and Koller, Oscar and Trancoso, Isabel and {Lopez-Otero}, Paula and {Docio-Fernandez}, Laura and {Garcia-Mateo}, Carmen and Saeidi, Rahim and Soufifar, Mehdi and Kinnunen, Tomi and Svendsen, Torbjorn and Franti, Pasi},
  year = {2011},
  month = dec,
  pages = {377--382},
  publisher = {{IEEE}},
  address = {{Hawaii, USA}},
  doi = {10.1109/ASRU.2011.6163961},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rodriguez-Fuentes et al_2011_Multi-site heterogeneous system fusions for the Albayzin 2010 Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CHBBU55G/downloadCitations.html},
  isbn = {978-1-4673-0367-5 978-1-4673-0365-1 978-1-4673-0366-8}
}

@inproceedings{rogez_3d_2014,
  title = {{{3D Hand Pose Detection}} in {{Egocentric RGB}}-{{D Images}}},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Rogez, Gr{\'e}gory and Khademi, Maryam and Supan{\v c}i{\v c} III, J. S. and Montiel, Jose Maria Martinez and Ramanan, Deva},
  year = {2014},
  month = sep,
  pages = {356--371},
  address = {{Zurich, Switzerland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rogez et al_2014_3D Hand Pose Detection in Egocentric RGB-D Images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5TN82SM6/978-3-319-16178-5_25.html}
}

@inproceedings{rogez_understanding_2015,
  title = {Understanding {{Everyday Hands}} in {{Action}} from {{RGB}}-{{D Images}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Rogez, G. and Supancic, J. S. and Ramanan, D.},
  year = {2015},
  month = dec,
  pages = {3889--3897},
  address = {{Santiago, Chile}},
  doi = {10.1109/ICCV.2015.443},
  abstract = {We analyze functional manipulations of handheld objects, formalizing the problem as one of fine-grained grasp classification. To do so, we make use of a recently developed fine-grained taxonomy of human-object grasps. We introduce a large dataset of 12000 RGB-D images covering 71 everyday grasps in natural interactions. Our dataset is different from past work (typically addressed from a robotics perspective) in terms of its scale, diversity, and combination of RGB and depth data. From a computer-vision perspective, our dataset allows for exploration of contact and force prediction (crucial concepts in functional grasp analysis) from perceptual cues. We present extensive experimental results with state-of-the-art baselines, illustrating the role of segmentation, object context, and 3D-understanding in functional grasp analysis. We demonstrate a near 2X improvement over prior work and a naive deep baseline, while pointing out important directions for improvement.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UMZ9QG9H/freeabs_all.html}
}

@inproceedings{romero_hands_2010,
  title = {Hands in Action: Real-Time {{3D}} Reconstruction of Hands in Interaction with Objects},
  shorttitle = {Hands in Action},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Romero, J. and Kjellstr{\"o}m, H. and Kragic, D.},
  year = {2010},
  month = may,
  pages = {458--463},
  doi = {10.1109/ROBOT.2010.5509753},
  abstract = {This paper presents a method for vision based estimation of the pose of human hands in interaction with objects. Despite the fact that most robotics applications of human hand tracking involve grasping and manipulation of objects, the majority of methods in the literature assume a free hand, isolated from the surrounding environment. Our hand tracking method is non-parametric, performing a nearest neighbor search in a large database (100000 entries) of hand poses with and without grasped objects. The system operates in real time, it is robust to self occlusions, object occlusions and segmentation errors, and provides full hand pose reconstruction from markerless video. Temporal consistency in hand pose is taken into account, without explicitly tracking the hand in the high dimensional pose space.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Romero et al_2010_Hands in action.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2MR73CIE/abs_all.html}
}

@article{ronchetti_handshape_2016,
  title = {Handshape {{Recognition}} for {{Argentinian Sign Language}} Using {{ProbSom}}},
  author = {Ronchetti, Franco and Quiroga, Facundo and Lanzarini, Laura and Estrebou, Cesar},
  year = {2016},
  volume = {16},
  pages = {1--5},
  issn = {1666-6038},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ronchetti et al_2016_Handshape Recognition for Argentinian Sign Language using ProbSom.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X44MZETT/52376.html},
  journal = {Journal of Computer Science and Technology},
  keywords = {capturedBy=coloredGlove,dataSet=LSA16,isolated,isolated;vocab=16;sl=ArgentinianSL;capturedBy=coloredGlove;dataSet=LSA16,LSA16,modal=shape,sl=ArgentinianSL,slreco,slreco;modal=shape,vocab<50,vocab=16},
  number = {1}
}

@article{rorissa_exploring_2008,
  title = {Exploring the Relationship between Feature and Perceptual Visual Spaces},
  author = {Rorissa, Abebe and Clough, Paul D and Deselaers, Thomas},
  year = {2008},
  month = mar,
  volume = {59},
  pages = {770--784},
  journal = {Journal of the American Society for Information Science and Technology},
  number = {5}
}

@inproceedings{rosenblum_human_1994,
  title = {Human Emotion Recognition from Motion Using a Radial Basis Function Network Architecture},
  booktitle = {Proc. {{IEEE Workshop}} on {{Motion}} of {{Non}}-{{Rigid}} and {{Articulated Objects}}},
  author = {Rosenblum, Mark and Yacoob, Yaser and Davis, Larry},
  year = {1994},
  pages = {43--49},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rosenblum et al_1994_Human emotion recognition from motion using a radial basis function network.pdf}
}

@inproceedings{rosenfeld_optimizing_1995,
  title = {Optimizing {{Lexical}} and \{\vphantom\}{{N}}-Gram\vphantom\{\} {{Coverage Via Judicious Use}} of {{Linguistic Data}}},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Rosenfeld, R.},
  year = {1995},
  pages = {1763--1766},
  address = {{Madrid, Spain}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rosenfeld_1995_Optimizing Lexical and N-gram Coverage Via Judicious Use of Linguistic Data.pdf}
}

@article{rouas_language_2008,
  title = {Language and {{Variety Verification}} on {{Broadcast News}} for \{\vphantom\}{{Portuguese}}\vphantom\{\}},
  author = {Rouas, J. and Trancoso, I. and Viana, C. and Abreu, M.},
  year = {2008},
  volume = {50},
  pages = {965--979},
  journal = {Speech Communication},
  number = {11-12}
}

@article{roussos_dynamic_2013,
  title = {Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos},
  author = {Roussos, Anastasios and Theodorakis, Stavros and Pitsikalis, Vassilis and Maragos, Petros},
  year = {2013},
  volume = {14},
  pages = {1627--1663},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Roussos et al_2013_Dynamic affine-invariant shape-appearance handshape features and classification.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7JNE3775/citation.html},
  journal = {The Journal of Machine Learning Research},
  keywords = {capturedBy=vision,isolated,isolated;vocab=100;signer=2;sl=GSL;capturedBy=vision,modal=location,modal=movement,modal=shape,signer=2,sl=GSL,slreco,slreco;modal=shape;modal=movement;modal=location,vocab<500,vocab=100},
  number = {1}
}

@article{rudzicz_torgo_2012,
  title = {The {{TORGO}} Database of Acoustic and Articulatory Speech from Speakers with Dysarthria},
  author = {Rudzicz, Frank and Namasivayam, Aravind Kumar and Wolff, Talya},
  year = {2012},
  month = dec,
  volume = {46},
  pages = {523--541},
  issn = {1574-0218},
  doi = {10.1007/s10579-011-9145-0},
  abstract = {This paper describes the acquisition of a new database of dysarthric speech in terms of aligned acoustics and articulatory data. This database currently includes data from seven individuals with speech impediments caused by cerebral palsy or amyotrophic lateral sclerosis and age- and gender-matched control subjects. Each of the individuals with speech impediments are given standardized assessments of speech-motor function by a speech-language pathologist. Acoustic data is obtained by one head-mounted and one directional microphone. Articulatory data is obtained by electromagnetic articulography, which allows the measurement of the tongue and other articulators during speech, and by 3D reconstruction from binocular video sequences. The stimuli are obtained from a variety of sources including the TIMIT database, lists of identified phonetic contrasts, and assessments of speech intelligibility. This paper also includes some analysis as to how dysarthric speech differs from non-dysarthric speech according to features such as length of phonemes, and pronunciation errors.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rudzicz et al_2012_The TORGO database of acoustic and articulatory speech from speakers with.pdf},
  journal = {Language Resources and Evaluation},
  language = {en},
  number = {4}
}

@article{rumelhart_learning_1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  volume = {323},
  pages = {533},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Nature Publishing Group},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rumelhart et al_1986_Learning representations by back-propagating errors.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VWJZC996/323533a0.html},
  journal = {Nature},
  language = {En},
  number = {6088}
}

@article{russakovsky_imagenet_2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  year = {2015},
  month = dec,
  volume = {115},
  pages = {211--252},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Russakovsky et al_2015_ImageNet Large Scale Visual Recognition Challenge.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JF57PZNS/s11263-015-0816-y.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XXDD5USD/1409.html},
  journal = {International Journal of Computer Vision (IJCV)},
  language = {en},
  number = {3}
}

@inproceedings{russell_energy_2011,
  title = {Energy Based Multiple Model Fitting for Non-Rigid Structure from Motion},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Russell, Chris and Fayad, Joao and Agapito, Lourdes},
  year = {2011},
  pages = {3009--3016},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Russell et al_2011_Energy based multiple model fitting for non-rigid structure from motion.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/A9UQ9RJT/cookiedetectresponse.html}
}

@inproceedings{russell_techniques_1983,
  title = {Some Techniques for Incorporating Local Timescale Variability Information into a Dynamic Time-Warping Algorithm for Automatic Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Russell, M. and Moore, R. and Tomlinson, M.},
  year = {1983},
  month = apr,
  volume = {8},
  pages = {1037--1040},
  doi = {10.1109/ICASSP.1983.1171934},
  abstract = {Dynamic Time-Warping is one the most important tools available for overcoming timescale variability problems in Automatic Speech Recognition. One of the main problems associated with the technique is to constrain the behaviour of the algorithm in order to avoid unlikely timescale distortion. This paper describes techniques for incorporating information about timescale variability directly into the Dynamic Time-Warping process. Results are presented which show that these techniques can lead to considerable improvements in recognition accuracy, especially if the differences between word classes are mainly due to temporal structure.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Russell et al_1983_Some techniques for incorporating local timescale variability information into.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NZZ88UTQ/1171934.html}
}

@incollection{rutkowski_polish_2013,
  title = {Polish {{Sign Language}} ({{PJM}}) Corpus: Assumptions - Procedures - Methodology},
  shorttitle = {The Corpus of {{Polish Sign Language}} ({{PJM}})},
  booktitle = {Deafness Studies Condition in {{Poland}}},
  author = {Rutkowski, Pawe{\l} and {\L}ozi{\'n}ska, Sylwia and Filipczak, Joanna and {\L}acheta, Joanna and Mostowski, Piotr},
  year = {2013},
  address = {{Poland}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BAVSSNK8/9841.html}
}

@article{ruzsics_multilevel_2019,
  title = {Multilevel {{Text Normalization}} with {{Sequence}}-to-{{Sequence Networks}} and {{Multisource Learning}}},
  author = {Ruzsics, Tatyana and Samard{\v z}i{\'c}, Tanja},
  year = {2019},
  month = mar,
  abstract = {We define multilevel text normalization as sequence-to-sequence processing that transforms naturally noisy text into a sequence of normalized units of meaning (morphemes) in three steps: 1) writing normalization, 2) lemmatization, 3) canonical segmentation. These steps are traditionally considered separate NLP tasks, with diverse solutions, evaluation schemes and data sources. We exploit the fact that all these tasks involve sub-word sequence-to-sequence transformation to propose a systematic solution for all of them using neural encoder-decoder technology. The specific challenge that we tackle in this paper is integrating the traditional know-how on separate tasks into the neural sequence-to-sequence framework to improve the state of the art. We address this challenge by enriching the general framework with mechanisms that allow processing the information on multiple levels of text organization (characters, morphemes, words, sentences) in combination with structural information (multilevel language model, part-of-speech) and heterogeneous sources (text, dictionaries). We show that our solution consistently improves on the current methods in all three steps. In addition, we analyze the performance of our system to show the specific contribution of the integrating components to the overall improvement.},
  archivePrefix = {arXiv},
  eprint = {1903.11340},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ruzsics_Samardžić_2019_Multilevel Text Normalization with Sequence-to-Sequence Networks and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HYIHRNVM/1903.html},
  journal = {arXiv:1903.11340 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{ruzsics_neural_2019,
  title = {Neural Text Normalization with Adapted Decoding and {{POS}} Features},
  author = {Ruzsics, T. and Lusetti, M. and G{\"o}hring, A. and Samard{\v z}i{\'c}, T. and Stark, E.},
  year = {2019},
  month = sep,
  volume = {25},
  pages = {585--605},
  publisher = {{Cambridge University Press}},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324919000391},
  abstract = {Text normalization is the task of mapping noncanonical language, typical of speech transcription and computer-mediated communication, to a standardized writing. This task is especially important for languages such as Swiss German, with strong regional variation and no written standard. In this paper, we propose a novel solution for normalizing Swiss German WhatsApp messages using the encoder\textendash decoder neural machine translation (NMT) framework. We enhance the performance of a plain character-level NMT model with the integration of a word-level language model and linguistic features in the form of part-of-speech (POS) tags. The two components are intended to improve the performance by addressing two specific issues: the former is intended to improve the fluency of the predicted sequences, whereas the latter aims at resolving cases of word-level ambiguity. Our systematic comparison shows that our proposed solution results in an improvement over a plain NMT system and also over a comparable character-level statistical machine translation system, considered the state of the art in this task till recently. We perform a thorough analysis of the compared systems' output, showing that our two components produce indeed the intended, complementary improvements.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ruzsics et al_2019_Neural text normalization with adapted decoding and POS features.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9Q3FR2XW/474B380A32EF96CCED1708229848F3FB.html},
  journal = {Natural Language Engineering},
  keywords = {Neural machine translation,Swiss German,Text normalization},
  language = {en},
  number = {5}
}

@phdthesis{rybach_appearancebased_2006,
  title = {Appearance-{{Based Features}} for {{Automatic Continuous Sign Language Recognition}}},
  author = {Rybach, David},
  year = {2006},
  month = jun,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rybach_2006_Appearance-Based Features for Automatic Continuous Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision;dataSet=BU-104,dataSet=BU-104,modal=head,modal=movement,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=shape;modal=movement;modal=head,vocab<500,vocab=103},
  school = {Human Language Technology and Pattern Recognition Group, RWTH Aachen University}
}

@phdthesis{rybach_investigations_2014,
  title = {Investigations on {{Search Methods}} for {{Speech Recognition}} Using {{Weighted Finite}}-{{State Transducers}}},
  author = {Rybach, David},
  year = {2014},
  month = apr,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rybach_2014_Investigations on Search Methods for Speech Recognition using Weighted.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{rybach_rwth_2009,
  title = {The {{RWTH Aachen University}} Open Source Speech Recognition System},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Rybach, D. and Gollan, C. and Heigold, G. and Hoffmeister, B. and L{\"o}{\"o}f, J. and Schl{\"u}ter, R. and Ney, H.},
  year = {2009},
  pages = {2111--2114},
  address = {{Brighton, UK}}
}

@inproceedings{rybach2011:rasr,
  title = {{{RASR}} - {{The RWTH Aachen University Open Source Speech Recognition Toolkit}}},
  booktitle = {{{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Rybach, David and Hahn, Stefan and Lehnen, Patrick and Nolden, David and Sundermeyer, Martin and T{\"u}ske, Zolt{\'a}n and Wiesler, Simon and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2011},
  month = dec,
  address = {{Waikoloa, HI, USA}},
  booktitlelink = {http://www.asru2011.org},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Rybach et al_2011_RASR - The RWTH Aachen University Open Source Speech Recognition Toolkit.pdf}
}

@inproceedings{sabyrov_realtime_2019,
  title = {Towards {{Real}}-Time {{Sign Language Interpreting Robot}}: {{Evaluation}} of {{Non}}-Manual {{Components}} on {{Recognition Accuracy}}},
  shorttitle = {Towards {{Real}}-Time {{Sign Language Interpreting Robot}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Sabyrov, Arman and Mukushev, Medet and Kimmelman, Vadim},
  year = {2019},
  pages = {75--82},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sabyrov et al_2019_Towards Real-time Sign Language Interpreting Robot.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QR6W5XGP/Sabyrov_Towards_Real-time_Sign_Language_Interpreting_Robot_Evaluation_of_Non-manual_Components_.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=20;signer=3;sl=krsl;capturedBy=vision,modal=bodyjoints,modal=eyebrows,modal=eyes,modal=head,modal=mouth,signer=3,sl=krsl,slreco,slreco;modal=bodyjoints;modal=head;modal=eyebrows;modal=eyes;modal=mouth,vocab<50,vocab=20}
}

@article{saenko_multistream_2009,
  title = {Multistream Articulatory Feature-Based Models for Visual Speech Recognition},
  author = {Saenko, Kate and Livescu, Karen and Glass, James and Darrell, Trevor},
  year = {2009},
  volume = {31},
  pages = {1700--1707},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Saenko et al_2009_Multistream articulatory feature-based models for visual speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7BD4KE9G/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {9}
}

@inproceedings{sagawa_method_2000,
  title = {A Method for Recognizing a Sequence of Sign Language Words Represented in a {{Japanese}} Sign Language Sentence},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Sagawa, H. and Takeuchi, M.},
  year = {2000},
  month = mar,
  pages = {434--439},
  address = {{Grenoble, France}},
  doi = {10.1109/AFGR.2000.840671},
  abstract = {To automatically interpret Japanese sign language (JSL), the recognition of signed words must be more accurate and the effects of extraneous gestures removed. We describe the parameters and the algorithms used to accomplish this. We experimented with 200 JSL sentences and demonstrated that recognition performance could be considerably improved},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sagawa_Takeuchi_2000_A method for recognizing a sequence of sign language words represented in a.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/675G6MGB/840671.html},
  keywords = {capturedBy=electronicGlove,continuous,continuous;vocab=17;sl=JSL;capturedBy=electronicGlove,modal=2hmovement,modal=2horientation,modal=2hshape,sl=JSL,slreco,slreco;modal=2hshape;modal=2hmovement;modal=2horientation,vocab<50,vocab=17}
}

@article{sagawa_pattern_1996,
  title = {Pattern {{Recognition}} and {{Synthesis}} for a {{Sign Language Translation System}}},
  author = {Sagawa, Hirohiko and Ohki, Masaru and Sakiyama, Tomoko and Oohira, Eiji and Ikeda, Hisashi and Fujisawa, Hiromichi},
  year = {1996},
  month = mar,
  volume = {7},
  pages = {109--127},
  issn = {1045-926X},
  doi = {10.1006/jvlc.1996.0007},
  abstract = {Sign language is one means of communication for hearing-impaired people. Words and sentences in sign language are mainly represented by hand gestures. In this report, we describe a sign language translation system which we are developing. The system translates Japanese Sign Language into Japanese text andvice versa. In this system, hand shapes and positions are provided by hand-based input in the form of time series. Then, hand gestures are recognized and translated into Japanese sentences using continuous DP (dynamic programming) matching. Japanese text is translated into sign language by generating three-dimensional computer-graphic animation of sign language gestures.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sagawa et al_1996_Pattern Recognition and Synthesis for a Sign Language Translation System.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UTXKTHKB/S1045926X96900075.html},
  journal = {Journal of Visual Languages \& Computing},
  number = {1}
}

@inproceedings{sak_long_2014,
  title = {Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling.},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Sak, Hasim and Senior, Andrew W. and Beaufays, Fran{\c c}oise},
  year = {2014},
  pages = {338--342},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014/Sak et al/Sak et al_2014_Long short-term memory recurrent neural network architectures for large scale.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sak et al_2014_Long short-term memory recurrent neural network architectures for large scale.pdf}
}

@inproceedings{sak_recurrent_2017,
  title = {Recurrent {{Neural Aligner}}: {{An Encoder}}-{{Decoder Neural Network Model}} for {{Sequence}} to {{Sequence Mapping}}},
  shorttitle = {Recurrent {{Neural Aligner}}},
  booktitle = {Interspeech 2017},
  author = {Sak, Ha{\c s}im and Shannon, Matt and Rao, Kanishka and Beaufays, Fran{\c c}oise},
  year = {2017},
  month = aug,
  pages = {1298--1302},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-1705},
  abstract = {We introduce an encoder-decoder recurrent neural network model called Recurrent Neural Aligner (RNA) that can be used for sequence to sequence mapping tasks. Like connectionist temporal classification (CTC) models, RNA defines a probability distribution over target label sequences including blank labels corresponding to each time step in input. The probability of a label sequence is calculated by marginalizing over all possible blank label positions. Unlike CTC, RNA does not make a conditional independence assumption for label predictions; it uses the predicted label at time t-1 as an additional input to the recurrent model when predicting the label at time t. We apply this model to end-to-end speech recognition. RNA is capable of streaming recognition since the decoder does not employ attention mechanism. The model is trained on transcribed acoustic data to predict graphemes and no external language and pronunciation models are used for decoding. We employ an approximate dynamic programming method to optimize negative log likelihood, and a sampling-based sequence discriminative training technique to fine-tune the model to minimize expected word error rate. We show that the model achieves competitive accuracy without using an external language model nor doing beam search decoding.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sak et al_2017_Recurrent Neural Aligner.pdf},
  language = {en}
}

@inproceedings{sako_subunit_2013,
  title = {Subunit {{Modeling}} for {{Japanese Sign Language Recognition Based}} on {{Phonetically Depend Multi}}-Stream {{Hidden Markov Models}}},
  booktitle = {Universal {{Access}} in {{Human}}-{{Computer Interaction}}. {{Design Methods}}, {{Tools}}, and {{Interaction Techniques}} for {{eInclusion}}},
  author = {Sako, Shinji and Kitamura, Tadashi},
  editor = {Stephanidis, Constantine and Antona, Margherita},
  year = {2013},
  month = jul,
  pages = {548--555},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Las Vegas, NV, USA}},
  abstract = {We work on automatic Japanese sign Language (JSL) recognition using Hidden Markov Model (HMM). An important issue for modeling sign is that how to determine the constituent element of sign (i.e., subunit) like ``phoneme'' in spoken language. We focused on special feature of sign language that JSL is composed of three types of phonological elements which is hand local information, position, and movement. In this paper, we propose an efficiently method of generating subunit using multi-stream HMM which is correspond to phonological elements. An isolated word recognition experiment has confirmed the effectiveness of our proposed method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sako_Kitamura_2013_Subunit Modeling for Japanese Sign Language Recognition Based on Phonetically2.pdf},
  isbn = {978-3-642-39188-0},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{sakoe_twolevel_1979,
  title = {Two-Level {{DP}}-Matching\textendash{{A}} Dynamic Programming-Based Pattern Matching Algorithm for Connected Word Recognition},
  author = {Sakoe, H.},
  year = {1979},
  month = dec,
  volume = {27},
  pages = {588--595},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1979.1163310},
  abstract = {This paper reports a pattern matching approach to connected word recognition. First, a general principle of connected word recognition is given based on pattern matching between unknown continuous speech and artificially synthesized connected reference patterns. Time-normalization capability is allowed by use of dynamic programming-based time-warping technique (DP-matching). Then, it is shown that the matching process is efficiently carried out by breaking it down into two steps. The derived algorithm is extensively subjected to recognition experiments. It is shown in a talker-adapted recognition experiment that digit data (one to four digits) connectedly spoken by five persons are recognized with as high as 99.6 percent accuracy. Computation time and memory requirement are both proved to be within reasonable limits.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sakoe_1979_Two-level DP-matching–A dynamic programming-based pattern matching algorithm.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GLQ6Q8GK/1163310.html},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  number = {6}
}

@article{salloum_crowdsourced_2017,
  title = {Crowdsourced {{Continuous Improvement}} of {{Medical Speech Recognition}}},
  author = {Salloum, W. and Edwards, E. and Ghaffarzadegan, S. and {Suendermann-Oeft}, D. and Miller, M.},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2017/Salloum et al/Salloum et al_2017_Crowdsourced Continuous Improvement of Medical Speech Recognition.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Salloum et al_2017_Crowdsourced Continuous Improvement of Medical Speech Recognition.pdf}
}

@article{samardzic_archimoba_2016,
  title = {Archimob-a Corpus of Spoken {{Swiss German}}},
  author = {Samardzic, Tanja and Scherrer, Yves and Glaser, Elvira},
  year = {2016},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NU5R8ANN/Samardzic et al. - 2016 - Archimob-a corpus of spoken Swiss German.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DN984MCR/unige91722.html}
}

@article{san-segundo_speech_2008,
  title = {Speech to Sign Language Translation System for {{Spanish}}},
  author = {{San-Segundo}, R. and Barra, R. and C{\'o}rdoba, R. and D'Haro, L. F. and Fern{\'a}ndez, F. and Ferreiros, J. and Lucas, J. M. and {Mac{\'i}as-Guarasa}, J. and Montero, J. M. and Pardo, J. M.},
  year = {2008},
  month = nov,
  volume = {50},
  pages = {1009--1020},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2008.02.001},
  abstract = {This paper describes the development of and the first experiments in a Spanish to sign language translation system in a real domain. The developed system focuses on the sentences spoken by an official when assisting people applying for, or renewing their Identity Card. The system translates official explanations into Spanish Sign Language (LSE: Lengua de Signos Espa\~nola) for Deaf people. The translation system is made up of a speech recognizer (for decoding the spoken utterance into a word sequence), a natural language translator (for converting a word sequence into a sequence of signs belonging to the sign language), and a 3D avatar animation module (for playing back the hand movements). Two proposals for natural language translation have been evaluated: a rule-based translation module (that computes sign confidence measures from the word confidence measures obtained in the speech recognition module) and a statistical translation module (in this case, parallel corpora were used for training the statistical model). The best configuration reported 31.6\% SER (Sign Error Rate) and 0.5780 BLEU (BiLingual Evaluation Understudy). The paper also describes the eSIGN 3D avatar animation module (considering the sign confidence), and the limitations found when implementing a strategy for reducing the delay between the spoken utterance and the sign sequence animation.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/San-Segundo et al_2008_Speech to sign language translation system for Spanish.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RCM6SPR8/S0167639308000149.html},
  journal = {Speech Communication},
  number = {11},
  series = {Iberian {{Languages}}}
}

@article{san-segundo_spoken_2010,
  title = {Spoken {{Spanish}} Generation from Sign Language},
  author = {{San-Segundo}, Rub{\'e}n and Pardo, Jos{\'e} Manuel and Ferreiros, Javier and Sama, Valent{\'i}n and {Barra-Chicote}, Roberto and Lucas, Juan Manuel and S{\'a}nchez, D. and Garc{\'i}a, A.},
  year = {2010},
  volume = {22},
  pages = {123--139},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/San-Segundo et al_2010_Spoken Spanish generation from sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/D2DFNUKP/S095354380900099X.html},
  journal = {Interacting with Computers},
  number = {2}
}

@article{sandler_challenge_2017,
  title = {The {{Challenge}} of {{Sign Language Phonology}}},
  author = {Sandler, Wendy},
  year = {2017},
  volume = {3},
  pages = {43--63},
  doi = {10.1146/annurev-linguistics-011516-034122},
  abstract = {Comparing phonology in spoken language and sign language reveals that core properties, such as features, feature categories, the syllable, and constraints on form, exist in both naturally occurring language modalities. But apparent ubiquity can be deceptive. The features themselves are quintessentially different, and key properties, such as linearity and arbitrariness, although universal, occur in inverse proportions to their counterparts, simultaneity and iconicity, in the two modalities. Phonology does not appear full blown in a new sign language, but it does gradually emerge, accruing linguistic structure over time. Sign languages suggest that the phonological component of the language faculty is a product of the ways in which the physical system, cognitive structure, and language use among people interact over time.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sandler_2017_The Challenge of Sign Language Phonology.pdf},
  journal = {Annual Review of Linguistics},
  number = {1}
}

@article{sandler_emergence_2011,
  title = {The Emergence of Complexity in Prosody and Syntax},
  author = {Sandler, Wendy and Meir, Irit and Dachkovsky, Svetlana and Padden, Carol and Aronoff, Mark},
  year = {2011},
  month = oct,
  volume = {121},
  pages = {2014--2033},
  issn = {0024-3841},
  doi = {10.1016/j.lingua.2011.05.007},
  abstract = {The relation between prosody and syntax is investigated here by tracing the emergence of each in a new language, Al-Sayyid Bedouin Sign Language. We analyze the structure of narratives of four signers of this language: two older second generation signers, and two about 15 years younger. We find that younger signers produce prosodic cues to dependency between semantically related constituents, e.g., the two clauses of conditionals, revealing a type and degree of complexity in their language that is not frequent in that of the older pair. In these younger signers, several rhythmic and (facial) intonational cues are aligned at constituent boundaries, indicating the emergence of a grammatical system. There are no overt syntactic markers (such as complementizers) to relate clauses; prosody is the only clue. But this prosodic complexity is matched by syntactic complexity inside propositions in the younger signers, who are more likely to use pronouns as abstract grammatical markers of arguments, and to combine predicates with their arguments within in a constituent. As the prosodic means emerge for identifying constituent types and signaling dependency relations between them, the constituents themselves become increasingly complex. Finally, our study shows that the emergence of grammatical complexity is gradual.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sandler et al_2011_The emergence of complexity in prosody and syntax.pdf},
  journal = {Lingua. International review of general linguistics. Revue internationale de linguistique generale},
  number = {13},
  pmcid = {PMC3476057},
  pmid = {23087486}
}

@article{sandler_phonological_1996,
  title = {Phonological Features and Feature Classes: {{The}} Case of Movements in Sign Language},
  shorttitle = {Phonological Features and Feature Classes},
  author = {Sandler, Wendy},
  year = {1996},
  month = mar,
  volume = {98},
  pages = {197--220},
  issn = {0024-3841},
  doi = {10.1016/0024-3841(95)00038-0},
  abstract = {The signs of sign language consist phonetically of hand configurations, locations on the body or in space, and movements. Some models claim that dynamic movements and static locations are the sequential segments of sign language, and even that movements are analogous to vowels. Others claim that movements are redundant, or in any case should not be represented as fully-fledged sequential segments. The present study measures movements against stringent phonological and morphological criteria for featurehood and classhood, in light of the current controversy over their status. Data from American Sign Language and from Israeli Sign Language support the claims made here, among them, that there is a set of phonologically contrastive features of movement which is phonetically coherent, and that these features constitute a class that is referred to in a blocking constraint on Multiple inflection and other processes. It is shown that the distinction between sequences of dynamic movements and static elements in signs is exploited in templatic morphology in both sign languages. While this analysis supports the claim that movements are phonologically significant at the underlying level, it suggests that their linear position need not be lexically specified.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sandler_1996_Phonological features and feature classes.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/79PVDSMG/0024384195000380.html},
  journal = {Lingua},
  number = {1},
  series = {Sign {{Linguistics Phonetics}}, {{Phonology}} and {{Morpho}}-Syntax}
}

@article{sandler_phonological_2012,
  title = {The {{Phonological Organization}} of {{Sign Languages}}},
  author = {Sandler, Wendy},
  year = {2012},
  month = mar,
  volume = {6},
  pages = {162--182},
  issn = {1749-818X},
  doi = {10.1002/lnc3.326},
  abstract = {Visually perceivable and movable parts of the body \textendash{} the hands, facial features, head, and upper body \textendash{} are the articulators of sign language. It is through these articulators that that words are formed, constrained, and contrasted with one another, and that prosody is conveyed. This article provides an overview of the way in which phonology is organized in the alternative modality of sign language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/SANDLER_2012_THE PHONOLOGICAL ORGANIZATION OF SIGN LANGUAGES.pdf},
  journal = {Language and linguistics compass},
  number = {3},
  pmcid = {PMC3608481},
  pmid = {23539295}
}

@incollection{sandler_phonology_2011,
  title = {The {{Phonology}} of {{Movement}} in {{Sign Language}}},
  booktitle = {The {{Blackwell Companion}} to {{Phonology}}},
  author = {Sandler, Wendy},
  year = {2011},
  pages = {1--27},
  publisher = {{American Cancer Society}},
  doi = {10.1002/9781444335262.wbctp0024},
  abstract = {To watch a person communicating in a sign language is to observe a well-coordinated, multi-channel display of bodily motion. Most salient in this display is movement of the hands, which transmit lexical, morphological, and timing information. In coordination with the hands, motion of the mouth and lower face performs phonological, morphological, and gestural functions. Simultaneously, movement of the head and body provides a kind of prosodic shell to house the signing hands. Prominently embedded in this outer shell are the brows and eyes, whose movements provide intonation, the visual ``tunes'' of the message. The same physical articulators are all exploited by speakers as well, to augment the linguistically organized vocal\textendash auditory signal.1 But in sign language, it is these visually perceived actions that convey the linguistic signal itself, in a synchronized panoply of motion.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HZY6ZDJG/9781444335262.html},
  isbn = {978-1-4443-3526-2},
  language = {en}
}

@article{sandler_sign_1993,
  title = {Sign Language and Modularity},
  author = {Sandler, Wendy},
  year = {1993},
  month = apr,
  volume = {89},
  pages = {315--351},
  issn = {0024-3841},
  doi = {10.1016/0024-3841(93)90019-S},
  abstract = {There are two known language modalities used by humans: the oral-aural modality of spoken languages, and the manual-visual modality of signed languages found in the deaf communities of the world. The coexistence of these two systems raises fundamental questions about language and cognition. Sign language is considered here in the context of a particular theory of cognitive organization, the modularity thesis of Fodor (1983). It is argued that that modularity theory, which is based on biological as well as representational/computational considerations, does not stand up in the face of the existence of sign language.

To demonstrate that sign language must be considered to be part of the same cognitive system as spoken language, a formal comparison is drawn at the level of phonology \textemdash{} perhaps the least expected level of linguistic organization because it is the most physiologically bound. It is then argued that despite this similarity, a language module that is stimulus-domain specific and informationally encapsulated could not include sign language.

Sign language is shown to be a challenge for the development of a comprehensive theory of language, which, it is suggested, should aim to predict both similarities and differences between natural languages in the two modalities.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sandler_1993_Sign language and modularity.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DFR6KX5H/002438419390019S.html},
  journal = {Lingua},
  number = {4}
}

@book{sandler_sign_2006,
  title = {Sign {{Language}} and {{Linguistic Universals}}},
  author = {Sandler, Wendy},
  year = {2006},
  month = feb,
  publisher = {{Cambridge University Press}},
  abstract = {Sign Languages are of great interest to linguists, because while they are produced by the same brain, their physical transmission differs greatly from that of spoken languages. In this pioneering study, Wendy Sandler and Diane Lillo-Martin compare spoken languages with those that are signed, in order to seek universal properties of human languages. No prior background in sign language linguistics is assumed, and numerous pictures are provided to make descriptions accessible to readers. This book will be invaluable to all those interested in linguistics and its application to sign languages.},
  isbn = {978-0-521-48248-6},
  language = {en}
}

@article{sandler_spreading_1986,
  title = {The {{Spreading Hand Autosegment}} of {{American Sign Language}}},
  author = {Sandler, Wendy},
  year = {1986},
  volume = {50},
  pages = {1--28},
  issn = {0302-1475},
  abstract = {[The Hand Tier (HT) model presented here can account for sign language sign structure by showing hand configuration associating with location and movement on the segmental tier. This model preserves the structural and functional importance of location, movement, and hand configuration, and reveals special properties of the hand tier, which spreads across signs according to its own timetable. Structural relationships among signs in the lexicon, as well as productive phonological and morphological rules, provide evidence that this model can account for surface data in a way that reveals key information about the core of ASL structure.]},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sandler_1986_The Spreading Hand Autosegment of American Sign Language.pdf},
  journal = {Sign Language Studies}
}

@inproceedings{sanin_spatiotemporal_2013,
  title = {Spatio-Temporal Covariance Descriptors for Action and Gesture Recognition},
  booktitle = {{{IEEE Workshop}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Sanin, Andres and Sanderson, Conrad and Harandi, Mehrtash T. and Lovell, Brian C.},
  year = {2013},
  month = jan,
  pages = {103--110},
  address = {{Tampa, FL, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sanin et al_2013_Spatio-temporal covariance descriptors for action and gesture recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3VFNR6W5/abs_all.html}
}

@inproceedings{santemiz_automatic_2009,
  title = {Automatic Sign Segmentation from Continuous Signing via Multiple Sequence Alignment},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Santemiz, Pinar and Aran, Oya and Saraclar, Murat and Akarun, Lale},
  year = {2009},
  month = sep,
  pages = {2001--2008},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Santemiz et al_2009_Automatic sign segmentation from continuous signing via multiple sequence.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=40;signer=1;sl=TSL;capturedBy=vision,modal=2hlocation,modal=2hshape,signer=1,sl=TSL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<50,vocab=40}
}

@inproceedings{sarkar_segmentationrobust_2011,
  title = {Segmentation-Robust Representations, Matching, and Modeling for Sign Language},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Sarkar, Sudeep and Loeding, Barbara and Yang, Ruiduo and Nayak, Sunita and Parashar, Ayush},
  year = {2011},
  month = jun,
  pages = {13--19},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sarkar et al_2011_Segmentation-robust representations, matching, and modeling for sign language.PDF;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EKRXBWCV/login.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=65;sl=ASL;capturedBy=vision,isolated,isolated;vocab=147;signer=10;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=head,signer=10,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=head,vocab<100,vocab<500,vocab=147,vocab=65}
}

@inproceedings{saul_boltzmann_1995,
  title = {Boltzmann Chains and Hidden {{Markov}} Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Saul, Lawrence K. and Jordan, Michael I.},
  year = {1995},
  pages = {435--442},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Saul_Jordan_1995_Boltzmann chains and hidden Markov models.pdf}
}

@article{saunders_progressive_2020,
  title = {Progressive {{Transformers}} for {{End}}-to-{{End Sign Language Production}}},
  author = {Saunders, Ben and Camgoz, Necati Cihan and Bowden, Richard},
  year = {2020},
  month = apr,
  abstract = {The goal of automatic Sign Language Production (SLP) is to translate spoken language to a continuous stream of sign language video at a level comparable to a human translator. If this was achievable, then it would revolutionise Deaf hearing communications. Previous work on predominantly isolated SLP has shown the need for architectures that are better suited to the continuous domain of full sign sequences. In this paper, we propose Progressive Transformers, a novel architecture that can translate from discrete spoken language sentences to continuous 3D skeleton pose outputs representing sign language. We present two model configurations, an end-to-end network that produces sign direct from text and a stacked network that utilises a gloss intermediary. Our transformer network architecture introduces a counter that enables continuous sequence generation at training and inference. We also provide several data augmentation processes to overcome the problem of drift and improve the performance of SLP models. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future research.},
  archivePrefix = {arXiv},
  eprint = {2004.14874},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Saunders et al_2020_Progressive Transformers for End-to-End Sign Language Production.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YAPFI9UE/2004.html},
  journal = {arXiv:2004.14874 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{savran_automatic_2009,
  title = {Automatic Detection of Facial Actions from {{3D}} Data},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Savran, Arman and Sankur, B{\"u}lent},
  year = {2009},
  pages = {1993--2000},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Savran_Sankur_2009_Automatic detection of facial actions from 3D data.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JQXWSVSG/cookiedetectresponse.html}
}

@article{savran_regressionbased_2012,
  title = {Regression-Based Intensity Estimation of Facial Action Units},
  author = {Savran, Arman and Sankur, Bulent and Taha Bilge, M.},
  year = {2012},
  volume = {30},
  pages = {774--784},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Savran et al_2012_Regression-based intensity estimation of facial action units.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GDBASPUX/S0262885611001326.html},
  journal = {Image and Vision Computing},
  number = {10}
}

@inproceedings{scanzio_use_2008,
  title = {On the {{Use}} of a {{Multilingual Neural Network Front}}-{{End}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Scanzio, Stefano and Laface, Pietro and Fissore, Luciano and Gemello, Roberto and Mana, Franco},
  year = {2008},
  month = sep,
  pages = {2711--2714},
  publisher = {{ISCA}},
  address = {{Brisbane, Australia}},
  abstract = {This paper presents a front-end consisting of an Artificial Neural Network (ANN) architecture trained with multilingual corpora. The idea is to train an ANN front-end able to integrate the acoustic variations included in databases collected for different languages, through different channels, or even for specific tasks. This ANN front-end produces discriminant features that can be used as observation vectors for language or task dependent recognizers.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scanzio et al_On the Use of a Multilingual Neural Network Front-End.pdf},
  language = {en}
}

@inproceedings{schembri_british_2008,
  title = {British Sign Language Corpus Project: {{Open}} Access Archives and the Observer's Paradox},
  shorttitle = {British Sign Language Corpus Project},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Schembri, Adam},
  year = {2008},
  pages = {165--169},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schembri_2008_British sign language corpus project.pdf}
}

@misc{schembri_british_2011,
  title = {British {{Sign Language Corpus Project}}: {{A}} Corpus of Digital Video Data of {{British Sign Language}} 2008-2011 ({{First Edition}})},
  author = {Schembri, Adam and Fenlon, Jordan and Rentelis, Ramas and Cormier, Kearsy},
  year = {2011},
  howpublished = {http://www.bslcorpusproject.org},
  journal = {London: University College London}
}

@misc{schembri_british_2014,
  title = {British {{Sign Language Corpus Project}}: {{A}} Corpus of Digital Video Data and Annotations of {{British Sign Language}} 2008-2014 ({{Second Edition}})},
  author = {Schembri, Adam and Fenlon, Jordan and Rentelis, Ramas and Cormier, Kearsy},
  year = {2014},
  howpublished = {http://www.bslcorpusproject.org},
  journal = {London: University College London}
}

@misc{schembri_mouth_2011,
  title = {Mouth Gestures in {{British Sign Language}} ({{BSL}}): {{A}} Case Study of Tongue Protrusion in {{BSL}} Narratives},
  shorttitle = {Mouth Gestures in {{British Sign Language}} ({{BSL}})},
  author = {Schembri, Adam},
  year = {2011},
  abstract = {Mouth gestures in British Sign Language (BSL): A case study of tongue protrusion in BSL narratives},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MUEXA8F2/Mouth_gestures_in_British_Sign_Language_BSL_A_case_study_of_tongue_protrusion_in_BSL_narratives.html},
  howpublished = {https://www.academia.edu/2937197/Mouth\_gestures\_in\_British\_Sign\_Language\_BSL\_A\_case\_study\_of\_tongue\_protrusion\_in\_BSL\_narratives}
}

@inproceedings{scherrer_adaptive_2007,
  title = {Adaptive String Distance Measures for Bilingual Dialect Lexicon Induction},
  booktitle = {Proceedings of the 45th {{Annual Meeting}} of the {{ACL}}: {{Student Research Workshop}}},
  author = {Scherrer, Yves},
  year = {2007},
  pages = {55--60},
  publisher = {{Association for Computational Linguistics}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer_2007_Adaptive string distance measures for bilingual dialect lexicon induction.pdf}
}

@article{scherrer_automatic_2016,
  title = {Automatic Normalisation of the {{Swiss German ArchiMob}} Corpus Using Character-Level Machine Translation},
  author = {Scherrer, Yves and Ljube{\v s}i{\'c}, Nikola},
  year = {2016},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer_Ljubešić_2016_Automatic normalisation of the Swiss German ArchiMob corpus using.pdf}
}

@article{scherrer_digitising_2019,
  title = {Digitising {{Swiss German}}: How to Process and Study a Polycentric Spoken Language},
  shorttitle = {Digitising {{Swiss German}}},
  author = {Scherrer, Yves and Samard{\v z}i{\'c}, Tanja and Glaser, Elvira},
  year = {2019},
  month = dec,
  volume = {53},
  pages = {735--769},
  issn = {1574-0218},
  doi = {10.1007/s10579-019-09457-5},
  abstract = {Swiss dialects of German are, unlike many dialects of other standardised languages, widely used in everyday communication. Despite this fact, automatic processing of Swiss German is still a considerable challenge due to the fact that it is mostly a spoken variety and that it is subject to considerable regional variation. This paper presents the ArchiMob corpus, a freely available general-purpose corpus of spoken Swiss German based on oral history interviews. The corpus is a result of a long design process, intensive manual work and specially adapted computational processing. We first present the modalities of access of the corpus for linguistic, historic and computational research. We then describe how the documents were transcribed, segmented and aligned with the sound source. This work involved a series of experiments that have led to automatically annotated normalisation and part-of-speech tagging layers. Finally, we present several case studies to motivate the use of the corpus for digital humanities in general and for dialectology in particular.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer et al_2019_Digitising Swiss German.pdf},
  journal = {Language Resources and Evaluation},
  language = {en},
  number = {4}
}

@phdthesis{scherrer_generating_2012,
  title = {Generating {{Swiss German}} Sentences from {{Standard German}}: A Multi-Dialectal Approach},
  shorttitle = {Generating {{Swiss German}} Sentences from {{Standard German}}},
  author = {Scherrer, Yves},
  year = {2012},
  doi = {10.13097/archive-ouverte/unige:26361},
  abstract = {This thesis proposes to combine methods and data from two rather distant fields of language science \textendash{} dialectology and human language technology \textendash{} into a system that automatically transforms Standard German words and sentences into multiple Swiss German dialects. Our work is inspired by previous research in generative dialectology and computational linguistics, which attempts to derive multiple dialect systems from a single reference system with the help of hand-written transformation rules. We propose to call such rules 'georeferenced', in the sense that they are linked to probability maps that specify their area of validity. These probability maps are extracted by interpolation from existing dialectological atlases. Finally, as a consequence of our map digitization efforts, we are able to present original dialectometrical results for the Swiss German dialect landscape.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer_2012_Generating Swiss German sentences from Standard German.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7QLBY98M/unige26361.html},
  language = {eng},
  school = {University of Geneva}
}

@inproceedings{scherrer_machine_2012,
  title = {Machine Translation into Multiple Dialects: {{The}} Example of {{Swiss German}}},
  shorttitle = {Machine Translation into Multiple Dialects},
  booktitle = {7th {{SIDG Congress}} - {{Dialect}} 2.0},
  author = {Scherrer, Yves},
  year = {2012},
  abstract = {In this paper, we propose to approach dialects and dialectology from a Natural Language Processing (NLP) point of view. NLP covers a series of computational applications that analyze and/or transform linguistic data, such as machine translation, parsing, or text summarization. For practical reasons, most NLP applications focus on standardized, written language varieties. We argue that non-standard varieties, often also characterized by internal variation, can result in interesting methodological insights in NLP. Our work focuses on Swiss German dialects. Today, dialect represents the default variety of oral communication in the German-speaking part of Switzerland (Standard German is almost exclusively used for writing). Recently, dialect writing has also become popular in electronic media (Siebenhaar 2003). This evolution justifies the development of dialect NLP tools, and at the same time provides us with data to validate them. We present a system that automatically translates (written) Standard German sentences into (written) sentences of any Swiss German dialect. It is based on hand-built transfer rules operating on several linguistic levels such as phonology, morphology and syntax (Scherrer 2011a, 2011b). The target variety is not homogeneous, but a continuum of dialects. This multi-dialectal approach requires the rules to be linked to distributional information. We extracted these data from existing Swiss German dialect atlases (Hotzenk\"ocherle et al. 1962-1997; Bucheli \& Glaser 2002). This paper focuses on two crucial aspects of our work. First, we discuss methodological choices and issues involved in processing the dialectological data. Indeed, a large part of the maps had to be digitized and interpolated to fit our needs for probabilistic interpretation. For this task, we rely on methods recently proposed in dialectometry (Rumpf et al. 2009). Second, we present different datasets and different methodologies that have been used to evaluate the proposed system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer_2012_Machine translation into multiple dialects.pdf},
  language = {eng}
}

@article{scherrer_natural_2010,
  title = {Natural Language Processing for the {{Swiss German}} Dialect Area},
  author = {Scherrer, Yves and Rambow, Owen},
  year = {2010},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer_Rambow_2010_Natural language processing for the Swiss German dialect area.pdf}
}

@inproceedings{scherrer_quantitative_2017,
  title = {A {{Quantitative Approach}} to {{Swiss German Dialect Syntax}}},
  booktitle = {Int. {{Conf}}. on {{Language Variation}} in {{Europe}} ({{ICLAVE}} 9)},
  author = {Scherrer, Yves and Stoeckle, Philipp},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer_Stoeckle_2017_A Quantitative Approach to Swiss German Dialect Syntax.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SWLUF5RD/unige95472.html}
}

@inproceedings{scherrer_syntactic_2011,
  title = {Syntactic Transformations for {{Swiss German}} Dialects},
  booktitle = {Proceedings of the {{First Workshop}} on {{Algorithms}} and {{Resources}} for {{Modelling}} of {{Dialects}} and {{Language Varieties}}},
  author = {Scherrer, Yves},
  year = {2011},
  pages = {30--38},
  publisher = {{Association for Computational Linguistics}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Scherrer_2011_Syntactic transformations for Swiss German dialects.pdf}
}

@article{schlenker_sign_2017,
  title = {Sign {{Language}} and the {{Foundations}} of {{Anaphora}}},
  author = {Schlenker, Philippe},
  year = {2017},
  volume = {3},
  pages = {149--177},
  doi = {10.1146/annurev-linguistics-011415-040715},
  abstract = {Sign language anaphora is often realized very differently from its spoken language counterpart. In simple cases, an antecedent is associated with a position or ``locus'' in signing space, and an anaphoric link is obtained by pointing toward that locus to recover its semantic value. This mechanism may sometimes be an overt realization of coindexation in formal syntax and semantics. I discuss two kinds of insights that sign language research can bring to the foundations of anaphora. First, in some cases the overt nature of indices in sign language allows one to bring overt evidence to bear on classic debates in semantics. I consider two: the availability of situation-denoting variables in natural language and the availability of binding without c-command. Second, in some cases sign language pronouns raise new challenges for formal semantics. Loci may function simultaneously as formal variables and as simplified depictions of what they denote, requiring the construction of a formal semantics with iconicity to analyze their properties.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schlenker_2017_Sign Language and the Foundations of Anaphora.pdf},
  journal = {Annual Review of Linguistics},
  number = {1}
}

@article{schluter_does_2012,
  title = {Does the {{Cost Function Matter}} in {{Bayes Decision Rule}}?},
  author = {Schl{\"u}ter, R. and {Nussbaum-Thom}, M. and Ney, H.},
  year = {2012},
  month = feb,
  volume = {34},
  pages = {292--301},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2011.163},
  abstract = {In many tasks in pattern recognition, such as automatic speech recognition (ASR), optical character recognition (OCR), part-of-speech (POS) tagging, and other string recognition tasks, we are faced with a well-known inconsistency: The Bayes decision rule is usually used to minimize string (symbol sequence) error, whereas, in practice, we want to minimize symbol (word, character, tag, etc.) error. When comparing different recognition systems, we do indeed use symbol error rate as an evaluation measure. The topic of this work is to analyze the relation between string (i.e., 0-1) and symbol error (i.e., metric, integer valued) cost functions in the Bayes decision rule, for which fundamental analytic results are derived. Simple conditions are derived for which the Bayes decision rule with integer-valued metric cost function and with 0-1 cost gives the same decisions or leads to classes with limited cost. The corresponding conditions can be tested with complexity linear in the number of classes. The results obtained do not make any assumption w.r.t. the structure of the underlying distributions or the classification problem. Nevertheless, the general analytic results are analyzed via simulations of string recognition problems with Levenshtein (edit) distance cost function. The results support earlier findings that considerable improvements are to be expected when initial error rates are high.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schlüter et al_2012_Does the Cost Function Matter in Bayes Decision Rule.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MTI3S33I/5989822.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {2}
}

@phdthesis{schluter00:phd,
  title = {Investigations on {{Discriminative Training Criteria}}},
  author = {Schl{\"u}ter, Ralf},
  year = {2000},
  month = sep,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schlüter_2000_Investigations on Discriminative Training Criteria.pdf},
  school = {RWTH Aachen University}
}

@article{schmidhuber_deep_2015,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  volume = {61},
  pages = {85--117},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schmidhuber_2015_Deep learning in neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/I8NUGP73/S0893608014002135.html},
  journal = {Neural Networks}
}

@inproceedings{schmidt_enhancing_2013,
  title = {Enhancing {{Gloss}}-{{Based Corpora}} with {{Facial Features Using Active Appearance Models}}},
  booktitle = {International {{Symposium}} on {{Sign Language Translation}} and {{Avatar Technology}}},
  author = {Schmidt, Christoph and Koller, Oscar and Ney, Hermann and Hoyoux, Thomas and Piater, Justus},
  year = {2013},
  month = oct,
  volume = {2},
  address = {{Chicago, IL, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schmidt et al_2013_Enhancing Gloss-Based Corpora with Facial Features Using Active Appearance.pdf}
}

@article{schmidt_swiss_2020,
  title = {A {{Swiss German Dictionary}}: {{Variation}} in {{Speech}} and {{Writing}}},
  shorttitle = {A {{Swiss German Dictionary}}},
  author = {Schmidt, Larissa and Linder, Lucy and Djambazovska, Sandra and Lazaridis, Alexandros and Samard{\v z}i{\'c}, Tanja and Musat, Claudiu},
  year = {2020},
  month = mar,
  abstract = {We introduce a dictionary containing forms of common words in various Swiss German dialects normalized into High German. As Swiss German is, for now, a predominantly spoken language, there is a significant variation in the written forms, even between speakers of the same dialect. To alleviate the uncertainty associated with this diversity, we complement the pairs of Swiss German - High German words with the Swiss German phonetic transcriptions (SAMPA). This dictionary becomes thus the first resource to combine large-scale spontaneous translation with phonetic transcriptions. Moreover, we control for the regional distribution and insure the equal representation of the major Swiss dialects. The coupling of the phonetic and written Swiss German forms is powerful. We show that they are sufficient to train a Transformer-based phoneme to grapheme model that generates credible novel Swiss German writings. In addition, we show that the inverse mapping - from graphemes to phonemes - can be modeled with a transformer trained with the novel dictionary. This generation of pronunciations for previously unknown words is key in training extensible automated speech recognition (ASR) systems, which are key beneficiaries of this dictionary.},
  archivePrefix = {arXiv},
  eprint = {2004.00139},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schmidt et al_2020_A Swiss German Dictionary.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8FTTQ6DZ/2004.html},
  journal = {arXiv:2004.00139 [cs]},
  keywords = {68T50; 68T10,A.2,Computer Science - Computation and Language,I.2.7,J.5},
  primaryClass = {cs}
}

@inproceedings{schmidt_using_2013,
  title = {Using {{Viseme Recognition}} to {{Improve}} a {{Sign Language Translation System}}},
  booktitle = {International {{Workshop}} on {{Spoken Language Translation}}},
  author = {Schmidt, Christoph and Koller, Oscar and Ney, Hermann and Hoyoux, Thomas and Piater, Justus},
  year = {2013},
  month = dec,
  pages = {197--203},
  address = {{Heidelberg, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schmidt et al_2013_Using Viseme Recognition to Improve a Sign Language Translation System.pdf}
}

@phdthesis{schmidt2016:phd,
  title = {Handling {{Multimodality}} and {{Scarce Resources}} in {{Sign Language Machine Translation}}},
  author = {Schmidt, Christoph Andreas},
  year = {2016},
  month = aug,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schmidt_2016_Handling Multimodality and Scarce Resources in Sign Language Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{schneider_wav2vec_2019,
  title = {Wav2vec: {{Unsupervised Pre}}-{{Training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec},
  booktitle = {Interspeech 2019},
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  year = {2019},
  month = sep,
  pages = {3465--3469},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1873},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schneider et al_2019_wav2vec.pdf},
  language = {en}
}

@inproceedings{Schober_2017_CVPR,
  title = {Dynamic {{Time}}-{{Of}}-{{Flight}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schober, Michael and Adam, Amit and Yair, Omer and Mazor, Shai and Nowozin, Sebastian},
  year = {2017},
  month = jul
}

@phdthesis{schramm:phdthesis2006,
  title = {Modeling {{Spontaneous Speech Variability}} for {{Large Vocabulary Continuous Speech Recognition}}},
  author = {Schramm, H.},
  year = {2006},
  month = mar,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schramm_2006_Modeling Spontaneous Speech Variability for Large Vocabulary Continuous Speech.pdf},
  school = {RWTH Aachen University}
}

@incollection{schroettle_gewalt_2014,
  title = {{Gewalt gegen M\"adchen und Frauen im Kontext von Behinderung, Migration und Geschlecht}},
  booktitle = {{Behinderung und Migration}},
  author = {Schr{\"o}ttle, Dr Monika and Glammeier, Dr Sandra},
  editor = {Wansing, Gudrun and Westphal, Manuela},
  year = {2014},
  pages = {285--308},
  publisher = {{Springer Fachmedien Wiesbaden}},
  copyright = {\textcopyright 2014 Springer Fachmedien Wiesbaden},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schröttle_Glammeier_2014_Gewalt gegen Mädchen und Frauen im Kontext von Behinderung, Migration und.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X7CZR3HG/978-3-531-19401-1_15.html},
  isbn = {978-3-531-19400-4 978-3-531-19401-1},
  language = {de}
}

@article{schuller_recognising_2011,
  title = {Recognising Realistic Emotions and Affect in Speech: {{State}} of the Art and Lessons Learnt from the First Challenge},
  shorttitle = {Recognising Realistic Emotions and Affect in Speech},
  author = {Schuller, Bj{\"o}rn and Batliner, Anton and Steidl, Stefan and Seppi, Dino},
  year = {2011},
  volume = {53},
  pages = {1062--1087},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Schuller et al_2011_Recognising realistic emotions and affect in speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VUGXB7WN/S0167639311000185.html},
  journal = {Speech Communication},
  number = {9-10}
}

@inproceedings{seltzer_investigation_2013,
  title = {An Investigation of Deep Neural Networks for Noise Robust Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Seltzer, Michael L. and Yu, Dong and Wang, Yongqiang},
  year = {2013},
  pages = {7398--7402},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Seltzer et al_2013_An investigation of deep neural networks for noise robust speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3F5WTQ23/6639100.html}
}

@article{seltzer_likelihoodmaximizing_2004,
  title = {Likelihood-Maximizing Beamforming for Robust Hands-Free Speech Recognition},
  author = {Seltzer, Michael L. and Raj, Bhiksha and Stern, Richard M.},
  year = {2004},
  volume = {12},
  pages = {489--498},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Seltzer et al_2004_Likelihood-maximizing beamforming for robust hands-free speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3Z5UKU7E/1323085.html},
  journal = {IEEE Transactions on Speech and Audio Processing},
  number = {5}
}

@phdthesis{seltzer_microphone_2003,
  title = {Microphone Array Processing for Robust Speech Recognition},
  author = {Seltzer, Michael L.},
  year = {2003},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Seltzer_2003_Microphone array processing for robust speech recognition.pdf},
  school = {Carnegie Mellon University Pittsburgh, PA}
}

@inproceedings{seltzer_multitask_2013,
  title = {Multi-Task Learning in Deep Neural Networks for Improved Phoneme Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Seltzer, M. L. and Droppo, J.},
  year = {2013},
  month = may,
  pages = {6965--6969},
  doi = {10.1109/ICASSP.2013.6639012},
  abstract = {In this paper we demonstrate how to improve the performance of deep neural network (DNN) acoustic models using multi-task learning. In multi-task learning, the network is trained to perform both the primary classification task and one or more secondary tasks using a shared representation. The additional model parameters associated with the secondary tasks represent a very small increase in the number of trained parameters, and can be discarded at runtime. In this paper, we explore three natural choices for the secondary task: the phone label, the phone context, and the state context. We demonstrate that, even on a strong baseline, multi-task learning can provide a significant decrease in error rate. Using phone context, the phonetic error rate (PER) on TIMIT is reduced from 21.63\% to 20.25\% on the core test set, and surpassing the best performance in the literature for a DNN that uses a standard feed-forward network architecture.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Seltzer_Droppo_2013_Multi-task learning in deep neural networks for improved phoneme recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/54N7EI8P/6639012.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GM4B9CA8/6639012.html}
}

@inproceedings{selvaraju_gradcam_2017,
  title = {Grad-{{CAM}}: {{Visual Explanations From Deep Networks}} via {{Gradient}}-{{Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2017},
  month = oct,
  pages = {618--626},
  address = {{Venice, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Selvaraju et al_2017_Grad-CAM.pdf}
}

@inproceedings{senior_context_2015,
  title = {Context Dependent Phone Models for {{LSTM RNN}} Acoustic Modelling},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Senior, A. and Sak, H. and Shafran, I.},
  year = {2015},
  month = apr,
  pages = {4585--4589},
  doi = {10.1109/ICASSP.2015.7178839},
  abstract = {Long Short Term Memory Recurrent Neural Networks (LSTM RNNs), combined with hidden Markov models (HMMs), have recently been show to outperform other acoustic models such as Gaussian mixture models (GMMs) and deep neural networks (DNNs) for large scale speech recognition. We argue that using multi-state HMMs with LSTM RNN acoustic models is an unnecessary vestige of GMM-HMM and DNN-HMM modelling since LSTM RNNs are able to predict output distributions through continuous, instead of piece-wise stationary, modelling of the acoustic trajectory. We demonstrate equivalent results for context independent whole-phone or 3-state models and show that minimum-duration modelling can lead to improved results. We go on to show that context dependent whole-phone models can perform as well as context dependent states, given a minimum duration model.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZIPP3QX3/7178839.html}
}

@inproceedings{senior_gmmfree_2014,
  title = {{{GMM}}-Free {{DNN}} Training},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Senior, Andrew and Heigold, Georg and Bacchiani, Michiel and Liao, Hank},
  year = {2014},
  month = may,
  pages = {5639--5643},
  address = {{Florence, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Senior et al_2014_GMM-free DNN training.pdf}
}

@inproceedings{sennrich_neural_2016,
  title = {Neural {{Machine Translation}} of {{Rare Words}} with {{Subword Units}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = aug,
  pages = {1715--1725},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-1162},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sennrich et al_2016_Neural Machine Translation of Rare Words with Subword Units.pdf}
}

@phdthesis{sev_finitestate_2015,
  title = {Finite-{{State Sign Language Morphophonology}}},
  author = {Sev, Ay{\c c}a M{\"u}ge},
  year = {2015},
  month = sep,
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4GWAGPNS/Sev - MIDDLE EAST TECHNICAL UNIVERSITY.pdf},
  language = {en},
  school = {Middle East Technical University, Ankara, Turkey}
}

@phdthesis{shaik2016:phd,
  title = {Investigation on {{Language Modelling Approaches}} for {{Open Vocabulary Speech Recognition}}},
  author = {Shaik, Mahaboob Ali Basha},
  year = {2016},
  month = dec,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shaik_2016_Investigation on Language Modelling Approaches for Open Vocabulary Speech.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{shamraev_weighted_2015,
  title = {Weighted Finite-State Transducer Approach to {{German}} Compound Words Reconstruction for {{Speech Recognition}}},
  booktitle = {Artificial {{Intelligence}} and {{Natural Language}} and {{Information Extraction}}, {{Social Media}} and {{Web Search FRUCT Conference}} ({{AINL}}-{{ISMW FRUCT}})},
  author = {Shamraev, Nickolay and Batalshchikov, Alexander and Zulkarneev, Mikhail and Repalov, Sergey and Shirokova, Anna},
  year = {2015},
  month = nov,
  pages = {96--101},
  publisher = {{IEEE}},
  address = {{St. Petersburg, Russia}},
  doi = {10.1109/AINL-ISMW-FRUCT.2015.7382976},
  abstract = {An approach is proposed for German Large Vocabulary Speech Recognition, dealing with the problem of compound words, based on unsupervised word decomposition for German words and a probabilistic method for combining the words using finite state transducers. The basic idea of the method is to train n-gram language model on the texts where compound words are substituted by their parts plus concatenation symbol. Thus, the context information is taken into account for the compound words and is used in the process of recombination to find most probable variant for recognition result. The advantage of this approach is the improvement of the word recognition accuracy and a more precise recombination of compound words.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shamraev et al_2015_Weighted finite-state transducer approach to German compound words.pdf},
  isbn = {978-952-68397-0-7},
  language = {en}
}

@article{shan_facial_2009,
  title = {Facial Expression Recognition Based on {{Local Binary Patterns}}: {{A}} Comprehensive Study},
  shorttitle = {Facial Expression Recognition Based on {{Local Binary Patterns}}},
  author = {Shan, Caifeng and Gong, Shaogang and McOwan, Peter W.},
  year = {2009},
  month = may,
  volume = {27},
  pages = {803--816},
  issn = {0262-8856},
  doi = {10.1016/j.imavis.2008.08.005},
  abstract = {Automatic facial expression analysis is an interesting and challenging problem, and impacts important applications in many areas such as human\textendash computer interaction and data-driven animation. Deriving an effective facial representation from original face images is a vital step for successful facial expression recognition. In this paper, we empirically evaluate facial representation based on statistical local features, Local Binary Patterns, for person-independent facial expression recognition. Different machine learning methods are systematically examined on several databases. Extensive experiments illustrate that LBP features are effective and efficient for facial expression recognition. We further formulate Boosted-LBP to extract the most discriminant LBP features, and the best recognition performance is obtained by using Support Vector Machine classifiers with Boosted-LBP features. Moreover, we investigate LBP features for low-resolution facial expression recognition, which is a critical problem but seldom addressed in the existing work. We observe in our experiments that LBP features perform stably and robustly over a useful range of low resolutions of face images, and yield promising performance in compressed low-resolution video sequences captured in real-world environments.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shan et al_2009_Facial expression recognition based on Local Binary Patterns.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KWB36FQW/S0262885608001844.html},
  journal = {Image and Vision Computing},
  number = {6}
}

@article{shanableh_spatiotemporal_2007,
  title = {Spatio-{{Temporal Feature}}-{{Extraction Techniques}} for {{Isolated Gesture Recognition}} in {{Arabic Sign Language}}},
  author = {Shanableh, T. and Assaleh, K. and {Al-Rousan}, M.},
  year = {2007},
  month = jun,
  volume = {37},
  pages = {641--650},
  issn = {1083-4419},
  doi = {10.1109/TSMCB.2006.889630},
  abstract = {This paper presents various spatio-temporal feature-extraction techniques with applications to online and offline recognitions of isolated Arabic Sign Language gestures. The temporal features of a video-based gesture are extracted through forward, backward, and bidirectional predictions. The prediction errors are thresholded and accumulated into one image that represents the motion of the sequence. The motion representation is then followed by spatial-domain feature extractions. As such, the temporal dependencies are eliminated and the whole video sequence is represented by a few coefficients. The linear separability of the extracted features is assessed, and its suitability for both parametric and nonparametric classification techniques is elaborated upon. The proposed feature-extraction scheme was complemented by simple classification techniques, namely, K nearest neighbor (KNN) and Bayesian, i.e., likelihood ratio, classifiers. Experimental results showed classification performance ranging from 97\% to 100\% recognition rates. To validate our proposed technique, we have conducted a series of experiments using the classical way of classifying data with temporal dependencies, namely, hidden Markov models (HMMs). Experimental results revealed that the proposed feature-extraction scheme combined with simple KNN or Bayesian classification yields comparable results to the classical HMM-based scheme. Moreover, since the proposed scheme compresses the motion information of an image sequence into a single image, it allows for using simple classification techniques where the temporal dimension is eliminated. This is actually advantageous for both computational and storage requirements of the classifier},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shanableh et al_2007_Spatio-Temporal Feature-Extraction Techniques for Isolated Gesture Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/R9XSMVX9/abs_all.html},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  keywords = {capturedBy=vision,isolated,isolated;vocab=23;signer=3;sl=ArSL;capturedBy=vision,modal=2hlocation,signer=3,sl=ArSL,slreco,slreco;modal=2hlocation,vocab<50,vocab=23},
  number = {3}
}

@article{shanableh_userindependent_2011,
  title = {User-Independent Recognition of {{Arabic}} Sign Language for Facilitating Communication with the Deaf Community},
  author = {Shanableh, T. and Assaleh, K.},
  year = {2011},
  month = jul,
  volume = {21},
  pages = {535--542},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2011.01.015},
  abstract = {This paper presents a solution for user-independent recognition of isolated Arabic sign language gestures. The video-based gestures are preprocessed to segment out the hands of the signer based on color segmentation of the colored gloves. The prediction errors of consecutive segmented images are then accumulated into two images according to the directionality of the motion. Different accumulation weights are employed to further help preserve the directionality of the projected motion. Normally, a gesture is represented by hand movements; however, additional user-dependent head and body movements might be present. In the user-independent mode we seek to filter out such user-dependent information. This is realized by encapsulating the movements of the segmented hands in a bounding box. The encapsulated images of the projected motion are then transformed into the frequency domain using Discrete Cosine Transformation (DCT). Feature vectors are formed by applying Zonal coding to the DCT coefficients with varying cutoff values. Classification techniques such as KNN and polynomial classifiers are used to assess the validity of the proposed user-independent feature extraction schemes. An average classification rate of 87\% is reported.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shanableh_Assaleh_2011_User-independent recognition of Arabic sign language for facilitating.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/T6I88SCT/S1051200411000182.html},
  journal = {Digital Signal Processing},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=23;signer=3;sl=ArSL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,signer=3,sl=ArSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=23},
  number = {4}
}

@article{shankar_generalizing_2018,
  title = {Generalizing {{Across Domains}} via {{Cross}}-{{Gradient Training}}},
  author = {Shankar, Shiv and Piratla, Vihari and Chakrabarti, Soumen and Chaudhuri, Siddhartha and Jyothi, Preethi and Sarawagi, Sunita},
  year = {2018},
  month = may,
  abstract = {We present CROSSGRAD, a method to use multi-domain training data to learn a classifier that generalizes to new domains. CROSSGRAD does not need an adaptation phase via labeled or unlabeled data, or domain features in the new domain. Most existing domain adaptation methods attempt to erase domain signals using techniques like domain adversarial training. In contrast, CROSSGRAD is free to use domain signals for predicting labels, if it can prevent overfitting on training domains. We conceptualize the task in a Bayesian setting, in which a sampling step is implemented as data augmentation, based on domain-guided perturbations of input instances. CROSSGRAD parallelly trains a label and a domain classifier on examples perturbed by loss gradients of each other's objectives. This enables us to directly perturb inputs, without separating and re-mixing domain signals while making various distributional assumptions. Empirical evaluation on three different applications where this setting is natural establishes that (1) domain-guided perturbation provides consistently better generalization to unseen domains, compared to generic instance perturbation methods, and that (2) data augmentation is a more stable and accurate method than domain adversarial training.},
  archivePrefix = {arXiv},
  eprint = {1804.10745},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shankar et al_2018_Generalizing Across Domains via Cross-Gradient Training.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7MUQXW7F/1804.html},
  journal = {arXiv:1804.10745 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{shannon_optimizing_2017,
  title = {Optimizing Expected Word Error Rate via Sampling for Speech Recognition},
  author = {Shannon, Matt},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shannon_2017_Optimizing expected word error rate via sampling for speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9YSQE8F9/1706.html},
  journal = {arXiv preprint arXiv:1706.02776}
}

@article{sheerman-chase_cultural_,
  title = {Cultural {{Factors}} in the {{Regression}} of {{Non}}-Verbal {{Communication Perception}}},
  author = {{Sheerman-Chase}, T. and Ong, E. J and Bowden, R.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sheerman-Chase et al_Cultural Factors in the Regression of Non-verbal Communication Perception.pdf}
}

@inproceedings{sheet_deep_2015,
  title = {Deep Learning of Tissue Specific Speckle Representations in Optical Coherence Tomography and Deeper Exploration for in Situ Histology},
  booktitle = {2015 {{IEEE}} 12th {{International Symposium}} on {{Biomedical Imaging}} ({{ISBI}})},
  author = {Sheet, D. and Karri, S.P.K. and Katouzian, A. and Navab, N. and Ray, A.K. and Chatterjee, J.},
  year = {2015},
  month = apr,
  pages = {777--780},
  doi = {10.1109/ISBI.2015.7163987},
  abstract = {Optical coherence tomography (OCT) relies on speckle image formation by coherent sensing of photons diffracted from a broadband laser source incident on tissues. Its non-ionizing nature and tissue specific speckle appearance has leveraged rapid clinical translation for non-invasive high-resolution in situ imaging of critical organs and tissue viz. coronary vessels, healing wounds, retina and choroid. However the stochastic nature of speckles introduces inter- and intra-observer reporting variability challenges. This paper proposes a deep neural network (DNN) based architecture for unsupervised learning of speckle representations in swept-source OCT using denoising auto-encoders (DAE) and supervised learning of tissue specifics using stacked DAEs for histologically characterizing healthy skin and healing wounds with the aim of reducing clinical reporting variability. Performance of our deep learning based tissue characterization method in comparison with conventional histology of healthy and wounded mice skin strongly advocates its use for in situ histology of live tissues.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sheet et al_2015_Deep learning of tissue specific speckle representations in optical coherence.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DN3Q36GU/login.html}
}

@inproceedings{sheet_transfer_2014,
  title = {Transfer Learning of Tissue Photon Interaction in Optical Coherence Tomography Towardsin Vivo Histology of the Oral Mucosa},
  booktitle = {Biomedical {{Imaging}} ({{ISBI}}), 2014 {{IEEE}} 11th {{International Symposium}} On},
  author = {Sheet, Debdoot and Banerjee, Satarupa and Karri, Sri Phani Krishna and Bag, Swarnendu and Anura, Anji and Giri, Amita and Paul, Ranjan Rashmi and Pal, Mousumi and Sarkar, Badal C. and Ghosh, Ranjan and others},
  year = {2014},
  pages = {1389--1392},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sheet et al_2014_Transfer learning of tissue photon interaction in optical coherence tomography.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/A4FC43AH/login.html}
}

@inproceedings{sheikh_modeseeking_2007,
  title = {Mode-Seeking by Medoidshifts},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Sheikh, Yaser Ajmal and Khan, Erum A. and Kanade, Takeo},
  year = {2007},
  pages = {1--8},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sheikh et al_2007_Mode-seeking by medoidshifts.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6TRE538R/abs_all.html}
}

@inproceedings{shen_improving_2007,
  title = {Improving {{Phonotactic Language Recognition}} with {{Acoustic Adaptation}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Shen, W. and Reynolds, D.},
  year = {2007},
  address = {{Antwerp, Belgium}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shen_Reynolds_2007_Improving Phonotactic Language Recognition with Acoustic Adaptation.pdf}
}

@inproceedings{shenoy_realtime_2018,
  title = {Real-Time {{Indian Sign Language}} ({{ISL}}) {{Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Computing}}, {{Communication}} and {{Networking Technologies}} ({{ICCCNT}})},
  author = {Shenoy, K. and Dastane, T. and Rao, V. and Vyavaharkar, D.},
  year = {2018},
  month = jul,
  pages = {1--9},
  doi = {10.1109/ICCCNT.2018.8493808},
  abstract = {This paper presents a system which can recognise hand poses \& gestures from the Indian Sign Language (ISL) in real-time using grid-based features. This system attempts to bridge the communication gap between the hearing and speech impaired and the rest of the society. The existing solutions either provide relatively low accuracy or do not work in real-time. This system provides good results on both the parameters. It can identify 33 hand poses and some gestures from the ISL. Sign Language is captured from a smartphone camera and its frames are transmitted to a remote server for processing. The use of any external hardware (such as gloves or the Microsoft Kinect sensor) is avoided, making it user-friendly. Techniques such as Face detection, Object stabilisation and Skin Colour Segmentation are used for hand detection and tracking. The image is further subjected to a Grid-based Feature Extraction technique which represents the hand's pose in the form of a Feature Vector. Hand poses are then classified using the k-Nearest Neighbours algorithm. On the other hand, for gesture classification, the motion and intermediate hand poses observation sequences are fed to Hidden Markov Model chains corresponding to the 12 pre-selected gestures defined in ISL. Using this methodology, the system is able to achieve an accuracy of 99.7\% for static hand poses, and an accuracy of 97.23\% for gesture recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shenoy et al_2018_Real-time Indian Sign Language (ISL) Recognition.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shenoy et al_2018_Real-time Indian Sign Language (ISL) Recognition2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5LRAPFQK/8493808.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X73ITDBM/8493808.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=12;sl=IndianSL;capturedBy=vision,modal=shape,sl=IndianSL,slreco,slreco;modal=shape,vocab<50,vocab=12}
}

@article{shi_american_2018,
  title = {American {{Sign Language}} Fingerspelling Recognition in the Wild},
  author = {Shi, Bowen and Del Rio, Aurora Martinez and Keane, Jonathan and Michaux, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2018},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shi et al_2018_American Sign Language fingerspelling recognition in the wild.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P86RMUZV/1810.html},
  journal = {arXiv preprint arXiv:1810.11438}
}

@inproceedings{shi_american_2018a,
  title = {American {{Sign Language Fingerspelling Recognition}} in the {{Wild}}},
  booktitle = {2018 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Shi, B. and Rio, A. M. Del and Keane, J. and Michaux, J. and Brentari, D. and Shakhnarovich, G. and Livescu, K.},
  year = {2018},
  month = dec,
  pages = {145--152},
  doi = {10.1109/SLT.2018.8639639},
  abstract = {We address the problem of American Sign Language fingerspelling recognition ``in the wild'', using videos collected from websites. We introduce the largest data set available so far for the problem of fingerspelling recognition, and the first using naturally occurring video data. Using this data set, we present the first attempt to recognize fingerspelling sequences in this challenging setting. Unlike prior work, our video data is extremely challenging due to low frame rates and visual variability. To tackle the visual challenges, we train a special-purpose signing hand detector using a small subset of our data. Given the hand detector output, a sequence model decodes the hypothesized fingerspelled letter sequence. For the sequence model, we explore attention-based recurrent encoder-decoders and CTC-based approaches. As the first attempt at fingerspelling recognition in the wild, this work is intended to serve as a baseline for future work on sign language recognition in realistic conditions. We find that, as expected, letter error rates are much higher than in previous work on more controlled data, and we analyze the sources of error and effects of model variants.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shi et al_2018_American Sign Language Fingerspelling Recognition in the Wild.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7ZE6P7AL/8639639.html}
}

@inproceedings{shi_fingerspelling_2019a,
  title = {Fingerspelling {{Recognition}} in the {{Wild With Iterative Visual Attention}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Shi, Bowen and Rio, Aurora Martinez Del and Keane, Jonathan and Brentari, Diane and Shakhnarovich, Greg and Livescu, Karen},
  year = {2019},
  pages = {5400--5409},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shi et al_2019_Fingerspelling Recognition in the Wild With Iterative Visual Attention.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3FSPEK6V/Shi et al. - 2019 - Fingerspelling Recognition in the Wild With Iterat.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4NRYEALH/Shi_Fingerspelling_Recognition_in_the_Wild_With_Iterative_Visual_Attention_ICCV_2019_paper.html}
}

@article{shi_multitask_2017,
  title = {Multitask Training with Unlabeled Data for End-to-End Sign Language Fingerspelling Recognition},
  author = {Shi, Bowen and Livescu, Karen},
  year = {2017},
  month = oct,
  abstract = {We address the problem of automatic American Sign Language fingerspelling recognition from video. Prior work has largely relied on frame-level labels, hand-crafted features, or other constraints, and has been hampered by the scarcity of data for this task. We introduce a model for fingerspelling recognition that addresses these issues. The model consists of an auto-encoder-based feature extractor and an attention-based neural encoder-decoder, which are trained jointly. The model receives a sequence of image frames and outputs the fingerspelled word, without relying on any frame-level training labels or hand-crafted features. In addition, the auto-encoder subcomponent makes it possible to leverage unlabeled data to improve the feature learning. The model achieves 11.6\% and 4.4\% absolute letter accuracy improvement respectively in signer-independent and signer- adapted fingerspelling recognition over previous approaches that required frame-level training labels.},
  archivePrefix = {arXiv},
  eprint = {1710.03255},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2017/Shi_Livescu/Shi_Livescu_2017_Multitask training with unlabeled data for end-to-end sign language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shi_Livescu_2017_Multitask training with unlabeled data for end-to-end sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/G4694CEM/1710.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RQY8SC9S/1710.html},
  journal = {arXiv:1710.03255 [cs]},
  primaryClass = {cs}
}

@article{shin_realtime_2011,
  title = {Real-Time Lip Reading System for Isolated {{Korean}} Word Recognition},
  author = {Shin, Jongju and Lee, Jin and Kim, Daijin},
  year = {2011},
  month = mar,
  volume = {44},
  pages = {559--571},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2010.09.011},
  abstract = {This paper proposes a real-time lip reading system (consisting of a lip detector, lip tracker, lip activation detector, and word classifier), which can recognize isolated Korean words. Lip detection is performed in several stages: face detection, eye detection, mouth detection, mouth end-point detection, and active appearance model (AAM) fitting. Lip tracking is then undertaken via a novel two-stage lip tracking method, where the model-based Lucas\textendash Kanade feature tracker is used to track the outer lip, and then a fast block matching algorithm is used to track the inner lip. Lip activation detection is undertaken through a neural network classifier, the input for which being a combination of the lip motion energy function and the first dominant shape feature. In the last step, input words are defined and recognized by three different classifiers: HMM, ANN, and K-NN. We combine the proposed lip reading system with an audio-only automatic speech recognition (ASR) system to improve the word recognition performance in the noisy environments. We then demonstrate the potential applicability of the combined system for use within hands free in-vehicle navigation devices. Results from experiments undertaken on 30 isolated Korean words using the K-NN classifier at a speed of 15 fps demonstrate that the proposed lip reading system achieves a 92.67\% word correct rate (WCR) for person-dependent tests, and a 46.50\% WCR for person-independent tests. Also, the combined audio-visual ASR system increases the WCR from 0\% to 60\% in a noisy environment.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shin et al_2011_Real-time lip reading system for isolated Korean word recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/38PI5HKV/S0031320310004516.html},
  journal = {Pattern Recognition},
  number = {3}
}

@inproceedings{shipman_crawling_2019,
  title = {Crawling and {{Classification Strategies}} for {{Generating}} a {{Multi}}-{{Language Corpus}} of {{Sign Language Video}}},
  booktitle = {2019 {{ACM}}/{{IEEE Joint Conference}} on {{Digital Libraries}} ({{JCDL}})},
  author = {Shipman, Frank M. and Monteiro, Caio D.D.},
  year = {2019},
  month = jun,
  pages = {97--106},
  doi = {10.1109/JCDL.2019.00023},
  abstract = {Although there is considerable sign language content available online, it can be hard to locate content in a specific sign language on a particular topic. The Sign Language Digital Library (SLaDL) aims to improve access through the generation of a multi-language corpus of sign language video. SLaDL uses a combination of crawling to collect potential sign language content and applying multimodal sign language detection and identification classifiers to winnow the collected videos to those believed to be in a particular sign language. Here we compare the quantity and variety of sign language videos located via breadth-first, depth-first, and focused crawling strategies. Then we examine the accuracy of different approaches to combining textual metadata and video features for the 3-way classification task of identifying videos in American Sign Language (ASL), British Sign Language (BSL), and without-sign language. Finally, due to the high computational cost of generating the video features used for classification, we explore the tradeoffs when using a cascading classifier and when generating features based on motion in sampled frames on classifier accuracy.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shipman_Monteiro_2019_Crawling and Classification Strategies for Generating a Multi-Language Corpus.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YAST5JQ5/8791221.html},
  keywords = {American sign language,Assistive technology,British sign language,classification strategies,collection generation,Computational efficiency,Crawlers,crawling strategies,crawling techniques,digital libraries,feature extraction,Feature extraction,Gesture recognition,identification classifiers,image classification,Internet,Libraries,meta data,metadata extraction,multilanguage corpus,multimodal classification,multimodal sign language detection,natural language processing,query processing,sign language,sign language content,sign language digital library,sign language recognition,sign language video,SLaDL,text analysis,textual metadata,video features,video sharing,video signal processing,without-sign language,YouTube}
}

@inproceedings{shipman_distributed_2015,
  title = {Towards a {{Distributed Digital Library}} for {{Sign Language Content}}},
  booktitle = {Proceedings of the 15th {{ACM}}/{{IEEE}}-{{CS Joint Conference}} on {{Digital Libraries}}},
  author = {Shipman, Frank and {Gutierrez-Osuna}, Ricardo and Shipman, Tamra and Monteiro, Caio and Karappa, Virendra},
  year = {2015},
  month = jun,
  pages = {187--190},
  publisher = {{Association for Computing Machinery}},
  address = {{Knoxville, Tennessee, USA}},
  doi = {10.1145/2756406.2756945},
  abstract = {The Internet provides access to content in almost all languages through a combination of crawling, indexing, and ranking capabilities. The ability to locate content on almost any topic has become expected for most users. But it is not the case for those whose primary language is a sign language. Members of this community communicate via the Internet, but they pass around links to videos via email and social media. In this paper, we describe the need for, the architecture of, and initial software components of a distributed digital library of sign language content, called SLaDL. Our initial efforts have been to develop a model of collection development that enables community involvement without assuming it. This goal necessitated the development of video processing techniques that automatically detect sign language content in video.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shipman et al_2015_Towards a Distributed Digital Library for Sign Language Content.pdf},
  isbn = {978-1-4503-3594-2},
  keywords = {digital library architecture,distributed digital library collection,sign language detection,sign language digital library},
  series = {{{JCDL}} '15}
}

@inproceedings{shipman_speedaccuracy_2017,
  title = {Speed-{{Accuracy Tradeoffs}} for {{Detecting Sign Language Content}} in {{Video Sharing Sites}}},
  booktitle = {Proc. {{Int}}. {{ACM SIGACCESS Conf}}. on {{Computers}} and {{Accessibility}} ({{ASSETS}})},
  author = {Shipman, Frank M. and Duggina, Satyakiran and Monteiro, Caio D.D. and {Gutierrez-Osuna}, Ricardo},
  year = {2017},
  pages = {185--189},
  address = {{New York, NY, USA}},
  doi = {10.1145/3132525.3132559},
  abstract = {Sign language is the primary medium of communication for many people who are deaf or hard of hearing. Members of this community access online sign language (SL) content posted on video sharing sites to stay informed. Unfortunately, locating SL videos can be difficult since the text-based search on video sharing sites is based on metadata rather than on the video content. Low cost or real-time video classification techniques would be invaluable for improving access to this content. Our prior work developed a technique to identify SL content based on video features alone but is computationally expensive. Here we describe and evaluate three optimization strategies that have the potential to reduce the computation time without overly impacting precision and recall. Two optimizations reduce the cost of face-detection, whereas the third focuses on analyzing shorter segments of the video. Our results identify a combination of these techniques that yields a 96\% reduction in computation time while losing only 1\% in F1 score. To further reduce computation, we additionally explore a keyframe-based approach that achieves comparable recall but lower precision than the above techniques, making it appropriate as an early filter in a staged classifier.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shipman et al_2017_Speed-Accuracy Tradeoffs for Detecting Sign Language Content in Video Sharing.pdf},
  isbn = {978-1-4503-4926-0},
  series = {{{ASSETS}} '17}
}

@article{shohieb_signsworld_,
  title = {{{SignsWorld Atlas}}; a Benchmark {{Arabic Sign Language}} Database},
  author = {Shohieb, Samaa M. and Elminir, Hamdy K. and Riad, A. M.},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2014.03.011},
  abstract = {Research has increased notably in vision-based automatic sign language recognition (ASLR). However, there has been little attention given to building a uniform platform for these purposes. Sign language (SL) includes not only static hand gestures, finger spelling, hand motions (which are called manual signs ``MS'') but also facial expressions, lip reading, and body language (which are called non-manual signs ``NMS''). Building up a database (DB) that includes both MS and NMS is the main first step for any SL recognition task. In addition to this, the Arabic Sign Language (ArSL) has no standard database. For this purpose, this paper presents a DB developed for the ArSL MS and NM signs which we call SignsWorld Atlas. The postures, gestures, and motions included in this DB are collected in lighting and background laboratory conditions. Individual facial expression recognition and static hand gestures recognition tasks were tested by the authors using the SignsWorld Atlas, achieving a recognition rate of 97\% and 95.28\%, respectively.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shohieb et al_SignsWorld Atlas\; a benchmark Arabic Sign Language database.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5DCP3MBJ/S1319157814000548.html},
  journal = {Journal of King Saud University - Computer and Information Sciences}
}

@inproceedings{shotton_realtime_2011,
  title = {Real-Time Human Pose Recognition in Parts from Single Depth Images},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Shotton, J. and Fitzgibbon, A. and Cook, M. and Sharp, T. and Finocchio, M. and Moore, R. and Kipman, A. and Blake, A.},
  year = {2011},
  month = jun,
  pages = {1297--1304},
  doi = {10.1109/CVPR.2011.5995316},
  abstract = {We propose a new method to quickly and accurately predict 3D positions of body joints from a single depth image, using no temporal information. We take an object recognition approach, designing an intermediate body parts representation that maps the difficult pose estimation problem into a simpler per-pixel classification problem. Our large and highly varied training dataset allows the classifier to estimate body parts invariant to pose, body shape, clothing, etc. Finally we generate confidence-scored 3D proposals of several body joints by reprojecting the classification result and finding local modes. The system runs at 200 frames per second on consumer hardware. Our evaluation shows high accuracy on both synthetic and real test sets, and investigates the effect of several training parameters. We achieve state of the art accuracy in our comparison with related work and demonstrate improved generalization over exact whole-skeleton nearest neighbor matching.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Shotton et al_2011_Real-time human pose recognition in parts from single depth images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7X2AR2M9/articleDetails.html}
}

@inproceedings{silva_silfa_2020,
  title = {{{SILFA}}: {{Sign Language Facial Action Database}} for the {{Development}} of {{Assistive Technologies}} for the {{Deaf}}},
  shorttitle = {{{SILFA}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Silva, E. and Costa, P. and Kumada, K. and De Martino, Jos{\'e} Mario},
  year = {2020},
  month = may,
  pages = {382--386},
  address = {{Buenos Aires, Argentina}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Silva et al_2020_SILFA.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NNGJSS8N/1kecIdR70Y0.html}
}

@inproceedings{simon_action_2010,
  title = {Action Unit Detection with Segment-Based {{SVMs}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Simon, Tomas and Nguyen, Minh Hoai and De La Torre, Fernando and Cohn, Jeffrey F.},
  year = {2010},
  pages = {2737--2744},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Simon et al_2010_Action unit detection with segment-based SVMs.pdf}
}

@inproceedings{simon_hand_2017,
  title = {Hand {{Keypoint Detection}} in {{Single Images Using Multiview Bootstrapping}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Simon, Tomas and Joo, Hanbyul and Matthews, Iain and Sheikh, Yaser},
  year = {2017},
  pages = {1145--1153},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Simon et al_2017_Hand Keypoint Detection in Single Images Using Multiview Bootstrapping.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2KIVXZH3/Simon_Hand_Keypoint_Detection_CVPR_2017_paper.html}
}

@inproceedings{simonyan_very_2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  booktitle = {Int. {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = may,
  address = {{San Diego, CA, USA}},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archivePrefix = {arXiv},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Simonyan_Zisserman/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BPN5U92D/1409.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CW8DCIV9/1409.html}
}

@article{singh_hand_2014,
  title = {Hand {{Tracking}} and {{Head Movement Detection}}: {{A State}}-of-{{Art Survey}}},
  shorttitle = {Hand {{Tracking}} and {{Head Movement Detection}}},
  author = {Singh, Pragati and Lodhi, Rashmi Singh},
  year = {2014},
  volume = {1},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Singh_Lodhi_2014_Hand Tracking and Head Movement Detection.pdf},
  journal = {Hand},
  number = {02}
}

@incollection{singleton_ethics_2015,
  title = {Ethics, {{Deaf}}-{{Friendly Research}}, and {{Good Practice When Studying Sign Languages}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Singleton, Jenny L. and Martin, Amber J. and Morgan, Gary},
  year = {2015},
  pages = {5--20},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch1},
  abstract = {This chapter addresses a range of issues that become important during sign language research, where hearing and Deaf researchers work together. The aim of the chapter is to highlight ethical and practical factors that sometimes can get sidelined during the research process but are crucial for its sustainability. The question of how Deaf people are involved in the research process is very important; some scholars argue that the authority for the construction of ``admissible evidence'' rests only with sign language community members themselves. The chapter focuses on four ethical considerations that are unique to research fieldwork carried out in developing countries: informed consent regarding the nature of the research; appropriate compensation; maintaining personal and professional relationships; and preparing the research team for fieldwork. It outlines the development of formalized agreements between researchers and schools where Deaf children are educated in the United Kingdom.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Singleton et al_2015_Ethics, Deaf-Friendly Research, and Good Practice When Studying Sign Languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U73L87ZP/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@article{siple_visual_1978,
  title = {Visual Constraints for Sign Language Communication},
  author = {Siple, Patricia},
  year = {1978},
  volume = {19},
  pages = {95--110},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Siple_1978_Visual constraints for sign language communication.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FJRST8J7/19.siple.html},
  journal = {Sign Language Studies},
  number = {1}
}

@article{sixtus_withinword_2002,
  title = {From Within-Word Model Search to across-Word Model Search in Large Vocabulary Continuous Speech Recognition},
  author = {Sixtus, Achim and Ney, Hermann},
  year = {2002},
  month = apr,
  volume = {16},
  pages = {245--271},
  issn = {0885-2308},
  doi = {10.1006/csla.2002.0192},
  abstract = {In this paper we report on the application of across-word context dependent acoustic phoneme models in a single-pass large vocabulary continuous speech recognizer.
Although across-word models are used by many groups today, only an outline of the recognizers is usually given in the publications. Implementation details are often missing.
We present both a formal derivation of across-word model search and a detailed description of our implementation. The across-word model system is compared with a conventional within-word model system regarding word error rate and computational effort. Compared to the baseline within-word system a straightforward implementation of across-word model search results in a substantial increase of the computational effort. Therefore, several optimization steps are studied that result in a more efficient organization of the search space and a more efficient pruning. The effects of these optimizations are analysed in a detailed profiling. In combination they accelerate the straightforward implementation of across-word model search by nearly a factor of three.
In addition we discuss the construction of word graphs during across-word model search. Starting from a word graph method based on within-word model search, we derive a formal specification of across-word word graphs. We show that the resulting word graphs are a good representation of the active search space.},
  journal = {Computer Speech and Language},
  number = {2}
}

@phdthesis{sixtus03:awpm,
  title = {Across-{{Word Phoneme Models}} for {{Large Vocabulary Continuous Speech Recognition}}},
  author = {Sixtus, Achim},
  year = {2003},
  month = jan,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sixtus_2003_Across-Word Phoneme Models for Large Vocabulary Continuous Speech Recognition.pdf},
  school = {RWTH Aachen University}
}

@misc{slevinski_modern_2012,
  title = {Modern {{SignWriting}}},
  author = {Slevinski, Stephen E},
  year = {2012},
  month = feb
}

@inproceedings{smit_improved_2017,
  title = {Improved {{Subword Modeling}} for {{WFST}}-{{Based Speech Recognition}}},
  booktitle = {Interspeech 2017},
  author = {Smit, Peter and Virpioja, Sami and Kurimo, Mikko},
  year = {2017},
  month = aug,
  pages = {2551--2555},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-103},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Smit et al_2017_Improved Subword Modeling for WFST-Based Speech Recognition.pdf},
  language = {en}
}

@inproceedings{smith_joint_2012,
  title = {Joint Face Alignment with Non-Parametric Shape Models},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Smith, Brandon M. and Zhang, Li},
  year = {2012},
  pages = {43--56},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M97MB55X/978-3-642-33712-3_4.html}
}

@article{smyth_clustering_1997,
  title = {Clustering Sequences with Hidden {{Markov}} Models},
  author = {Smyth, Padhraic and others},
  year = {1997},
  pages = {648--654},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Smyth_others_1997_Clustering sequences with hidden Markov models.pdf},
  journal = {Advances in Neural Information Processing Systems}
}

@inproceedings{song_parallel_2019,
  ids = {song\_parallel\_2019-1},
  title = {Parallel {{Temporal Encoder For Sign Language Translation}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Song, P. and Guo, D. and Xin, H. and Wang, M.},
  year = {2019},
  month = sep,
  pages = {1915--1919},
  doi = {10.1109/ICIP.2019.8803123},
  abstract = {This paper addresses the sign video interpretation which is a weakly supervised task. Each sign action in videos lacks exact boundaries or labels. We design a Parallel Temporal Encoder (PTEnc) to learn the temporal relation of a sign video from local and global sequential learning views in parallel. PTEnc utilizes the complementarity between the local and global temporal cues. Then, fused encoded feature sequence is fed into a Connectionist Temporal Classification (CTC) based sentence decoder. In addition, in order to enhance the temporal cues in each video, we introduce a reconstruction loss, which performs in an unsupervised way without additional labels. The CTC loss cooperates with the reconstruction loss in an end-to-end training manner. Experimental results on a benchmark dataset demonstrate the effectiveness of the proposed method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Song et al_2019_Parallel Temporal Encoder For Sign Language Translation.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Song et al_2019_Parallel Temporal Encoder For Sign Language Translation2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8SE73HK9/8803123.html},
  keywords = {augment=?,authorgroup=Hefei Tech,capturedBy=vision,cnnresiduals=1,connectionist temporal classification,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=38.1/38.3,dataSet=Phoenix14,loss=CTC,loss=MSE,modal=fullframe,nn=2dcnn,nn=3dcnn,nn=blstm,nnarch=c3d-resnet,paperinfo,paperinfo;authorgroup=Hefei Tech;shorttitle=Parallel Temp. Encoder,parallel temporal encoder,re-align=1,reconstruction loss,shorttitle=Parallel Temp. Encoder,Sign language translation,signer=9,sl=DGS,slreco,slreco;modal=fullframe;nn=3dcnn;nn=blstm;nn=2dcnn;nnarch=c3d-resnet;cnnresiduals=1;re-align=1;augment=?;loss=CTC;loss=MSE,vocab=1080,vocab>=1000,wer=38.1/38.3}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  volume = {15},
  pages = {1929--1958},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Srivastava et al_2014_Dropout.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YGJSQXF8/srivastava14a.html},
  journal = {Journal of Machine Learning Research}
}

@article{srivastava_highway_2015,
  title = {Highway {{Networks}}},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = may,
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  archivePrefix = {arXiv},
  eprint = {1505.00387},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Srivastava et al_2015_Highway Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MTMNQC8J/1505.html},
  journal = {arXiv:1505.00387 [cs]},
  primaryClass = {cs}
}

@phdthesis{stadermann_automatische_2006,
  title = {{Automatische Spracherkennung mit hybriden akustischen Modellen}},
  author = {Stadermann, J.},
  year = {2006},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stadermann_2006_Automatische Spracherkennung mit hybriden akustischen Modellen.pdf},
  language = {de},
  school = {Technische Universit\"at M\"unchen}
}

@inproceedings{starner_realtime_1995,
  title = {Real-Time American Sign Language Recognition from Video Using Hidden Markov Models},
  booktitle = {International {{Symposium}} on {{Computer Vision}}},
  author = {Starner, T. and Pentland, A.},
  year = {1995},
  month = nov,
  pages = {265--270},
  address = {{Coral Gables, Florida, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Starner_Pentland_1995_Real-time american sign language recognition from video using hidden markov.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/H4KHMAZ4/freeabs_all.html},
  isbn = {0-8186-7190-4},
  keywords = {capturedBy=vision,continuous,continuous;vocab=40;sl=ASL;capturedBy=vision,modal=2hshape,sl=ASL,slreco,slreco;modal=2hshape,vocab<50,vocab=40}
}

@article{starner_realtime_1998,
  title = {Real-Time American Sign Language Recognition Using Desk and Wearable Computer Based Video},
  author = {Starner, Thad and Weaver, Joshua and Pentland, Alex},
  year = {1998},
  month = dec,
  volume = {20},
  pages = {1371--1375},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/1998/Starner et al/Starner et al_1998_Real-time american sign language recognition using desk and wearable computer.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Starner et al_1998_Real-time american sign language recognition using desk and wearable computer.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FPW92P23/cookiedetectresponse.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FZGR44JE/summary.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {capturedBy=vision,continuous,continuous;vocab=40;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=40},
  number = {12}
}

@techreport{starner_visual_1995,
  title = {Visual {{Recognition}} of {{American Sign Language Using Hidden Markov Models}}.},
  author = {Starner, Thad E.},
  year = {1995},
  institution = {{Massachusetts Inst Of Tech Cambridge Dept Of Brain And Cognitive Sciences}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Starner_1995_Visual Recognition of American Sign Language Using Hidden Markov Models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/I5F74M33/ADA344219.html}
}

@inproceedings{stefan_translation_2008,
  title = {Translation and Scale-Invariant Gesture Recognition in Complex Scenes},
  booktitle = {Proc {{Int}}. {{Conf}}. on {{PErvasive Technologies Related}} to {{Assistive Environments}}},
  author = {Stefan, A. and Athitsos, V. and Alon, J. and Sclaroff, S.},
  year = {2008},
  pages = {7},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stefan et al_2008_Translation and scale-invariant gesture recognition in complex scenes.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XTI44H24/citation.html},
  keywords = {multiple candidate tracking,read 2011-week48,scale invariance,translation invariance}
}

@inproceedings{stein_analysis_2010,
  title = {Analysis of the {{German Sign Language Weather Forecast Corpus}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Stein, Daniel and Forster, Jens and Zelle, Uwe and Dreuw, Philippe and Ney, Hermann},
  year = {2010},
  month = may,
  pages = {225--230},
  address = {{Valletta, Malta}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stein et al_2010_Analysis of the German Sign Language Weather Forecast Corpus.pdf},
  keywords = {read 2011-week50}
}

@inproceedings{stein_hand_2007,
  title = {Hand in {{Hand}}: {{Automatic Sign Language}} to {{Speech Translation}}},
  booktitle = {Proc. {{Conf}}. on {{Theoretical}} and {{Methodological Issues}} in {{Machine Translation}}},
  author = {Stein, Daniel and Dreuw, Philippe and Ney, Hermann and Morrissey, Sara and Way, Andy},
  year = {2007},
  month = sep,
  pages = {214--220},
  address = {{Sk"ovde, Sweden}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stein et al_2007_Hand in Hand.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision;dataSet=BU-104,dataSet=BU-104,modal=fullframe,signer=3,sl=ASL,slreco,slreco;modal=fullframe,vocab<500,vocab=103}
}

@phdthesis{stein_morphosyntax_2005,
  title = {Morpho-{{Syntax Based Statistical Methods}} for {{Sign Language Translation}}},
  author = {Stein, Daniel},
  year = {2005},
  month = nov,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stein_2005_Morpho-Syntax Based Statistical Methods for Sign Language Translation.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{stein_morphosyntax_2006,
  title = {Morpho-{{Syntax Based Statistical Methods}} for {{Sign Language Translation}}},
  booktitle = {Conf. of the {{European Association}} for {{Machine Translation}}},
  author = {Stein, Daniel and Bungeroth, Jan and Ney, Hermann},
  year = {2006},
  month = jun,
  pages = {169--177},
  address = {{Oslo, Norway}}
}

@inproceedings{stein_sign_2010,
  title = {Sign {{Language Machine Translation Overkill}}},
  booktitle = {International {{Workshop}} on {{Spoken Language Translation}}},
  author = {Stein, Daniel and Schmidt, Christoph and Ney, Hermann},
  editor = {Federico, Marcello and Lane, Ian and Paul, Michael and Yvon, Fran{\c c}ois},
  year = {2010},
  month = dec,
  pages = {337--344},
  address = {{Paris, France}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stein et al_2010_Sign Language Machine Translation Overkill.pdf}
}

@phdthesis{stein_soft_2012,
  title = {Soft {{Features}} for {{Statistical Machine Translation}} of {{Spoken}} and {{Signed Languages}}},
  author = {Stein, Daniel},
  year = {2012},
  month = jan,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stein_2012_Soft Features for Statistical Machine Translation of Spoken and Signed Languages.pdf},
  school = {RWTH Aachen University}
}

@article{stein12:analysis,
  title = {Analysis, Preparation, and Optimization of Statistical Sign Language Machine Translation},
  author = {Stein, Daniel and Schmidt, Christoph and Ney, Hermann},
  year = {2012},
  month = dec,
  volume = {26},
  pages = {325--357},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stein et al_2012_Analysis, preparation, and optimization of statistical sign language machine.pdf},
  journal = {Machine Translation},
  journallink = {http://www.springer.com/computer/ai/journal/10590},
  number = {4}
}

@incollection{steinbach_gebaerdensprache_2007,
  title = {{Geb\"ardensprache}},
  booktitle = {{Schnittstellen der germanistischen Linguistik}},
  author = {Steinbach, Markus and Albert, Ruth and Girnth, Heiko and Hohenberger, Annette and {K{\"u}mmerling-Meibauer}, Bettina and Meibauer, J{\"o}rg and Rothweiler, Monika and {Schwarz-Friesel}, Monika},
  editor = {Steinbach, Markus and Albert, Ruth and Girnth, Heiko and Hohenberger, Annette and {K{\"u}mmerling-Meibauer}, Bettina and Meibauer, J{\"o}rg and Rothweiler, Monika and {Schwarz-Friesel}, Monika},
  year = {2007},
  pages = {137--185},
  publisher = {{J.B. Metzler}},
  address = {{Stuttgart}},
  doi = {10.1007/978-3-476-05042-7_5},
  abstract = {Eines der interessantesten Ergebnisse der modernen Linguistik ist, dass Menschen zwei grunds\"atzlich verschiedene Arten der Artikulation zur Verf\"ugung stehen, um miteinander zu kommunizieren. Lange Zeit gingen selbst Linguisten davon aus, dass menschliche Sprachen notwendigerweise an die Artikulation von Lauten gebunden sind. Erst Mitte des letzten Jahrhunderts haben erste linguistische Studien gezeigt, dass neben Lautsprachen noch eine zweite Gruppe nat\"urlicher Sprachen existiert, die Geb\"ardensprachen (vgl. Stokoe 1960; Klima/Bellugi 1979). Dem Menschen stehen demnach zwei Modalit\"aten f\"ur nat\"urliche Sprachen zur Verf\"ugung:\ding{110}die lautlich-auditive Modalit\"at der Lautsprachen\ding{110}die gestisch-visuelle Modalit\"at der Geb\"ardensprachen.Laute werden mithilfe der Stimmlippen, der Zunge und des Unterkiefers im Vokaltrakt gebildet und auditiv wahrgenommen. Geb\"arden werden im Gegensatz dazu mithilfe der H\"ande, Arme, des Oberk\"orpers und des Gesichts gebildet und visuell wahrgenommen. Entgegen landl\"aufiger Vorurteile verf\"ugen Geb\"ardensprachen \"uber dieselbe grammatische Komplexit\"at wie Lautsprachen und mit ihrer Hilfe lassen sich dieselben Sachverhalte in derselben Geschwindigkeit ausdr\"ucken wie mit Lautsprachen. Dies bedeutet, dass grundlegende grammatische Strukturen genauso wie kognitive Repr\"asentationen und Prozesse bei nat\"urlichen Sprachen weitgehend modalit\"atsneutral sind.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Steinbach et al_2007_Gebärdensprache.pdf},
  isbn = {978-3-476-05042-7},
  language = {de}
}

@phdthesis{stoiber_modeling_2010,
  title = {Modeling Emotionnal Facial Expressions and Their Dynamics for Realistic Interactive Facial Animation on Virtual Characters},
  author = {Stoiber, Nicolas},
  year = {2010},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stoiber_2010_Modeling emotionnal facial expressions and their dynamics for realistic.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2UR5W3MT/tel-00558851.html},
  school = {Universit\'e Rennes 1}
}

@book{stokoe_dictionary_1965,
  title = {A {{Dictionary}} of {{American Sign Language}} on {{Linguistic Principles}}},
  author = {Stokoe, W. C. and Casterline, D. and Croneberg, C.},
  year = {1965},
  publisher = {{Linstok Press}}
}

@book{stokoe_field_1979,
  title = {A {{Field Guide}} for {{Sign Language Research}}},
  author = {Stokoe, William C. and Kuschel, Rolf},
  year = {1979},
  publisher = {{Linstok Press}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stokoe_Kuschel_1979_A Field Guide for Sign Language Research.pdf},
  googlebooks = {5v9eQgAACAAJ},
  isbn = {978-0-932130-06-8},
  language = {en}
}

@article{stokoe_sign_1960,
  title = {Sign {{Language Structure}}: {{An Outline}} of the {{Visual Communication Systems}} of the {{American Deaf}}},
  shorttitle = {Sign {{Language Structure}}},
  author = {Stokoe, William C.},
  year = {1960},
  volume = {8},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stokoe_1960_Sign Language Structure.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VSN4CP7K/3.html},
  journal = {Studies in linguistics: Occasional papers},
  language = {en},
  number = {8},
  series = {Studies in Linguistics: {{Occasional}} Papers}
}

@article{stokoe_sign_1980,
  title = {Sign {{Language Structure}}},
  author = {Stokoe, William C.},
  year = {1980},
  volume = {9},
  pages = {365--390},
  issn = {0084-6570},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stokoe_1980_Sign Language Structure.pdf},
  journal = {Annual Review of Anthropology}
}

@article{stokoe_sign_2005,
  title = {Sign {{Language Structure}}: {{An Outline}} of the {{Visual Communication Systems}} of the {{American Deaf}}},
  shorttitle = {Sign {{Language Structure}}},
  author = {Stokoe, William C.},
  year = {2005},
  month = jan,
  volume = {10},
  pages = {3--37},
  issn = {1081-4159, 1465-7325},
  doi = {10.1093/deafed/eni001},
  abstract = {It is approaching a half century since Bill Stokoe published his revolutionary monograph, Sign Language Structure: An Outline of the Visual Communication Systems of the American Deaf. It is rare for a work of innovative scholarship to spark a social as well as an intellectual revolution, but that is just what Stokoe's 1960 paper did. And it is indicative both of Stokoe's genius and of his commitment that he did not simply publish his groundbreaking work and then sit back to watch the revolutions unfold. He actively promoted important changes in at least three areas of social and intellectual life. First, and perhaps most important, his work, that was ultimately generally accepted as showing the signing of deaf people to be linguistic, supported significant changes in the way deaf children are educated around the globe. Second, his work led to a general rethinking of what is fundamental about human language; and, third, it helped to reenergize the moribund field of language origin studies. This truly revolutionary paper has been reprinted at least twice, in revised and original versions, since its initial release in 1960, and now, five years after Bill's death, it is good to see it once again brought before the general public. \textendash{} David F. Armstrong, Gallaudet University},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Stokoe_2005_Sign Language Structure.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IMSUPB8N/3.html},
  journal = {Journal of Deaf Studies and Deaf Education},
  language = {en},
  number = {1},
  pmid = {15585746}
}

@book{Stokoe1960,
  title = {American {{Sign Language Structure}}},
  author = {Stokoe, William C.},
  year = {1960},
  publisher = {{Silver Spring, Md.: Linstok Press}}
}

@inproceedings{stolcke_srilm_2002,
  title = {{{SRILM}}- an {{Extensible Language Modeling Toolkit}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Stolcke, A.},
  year = {2002},
  month = sep,
  pages = {901--904},
  address = {{Denver, Colorado}}
}

@inproceedings{stoll_sign_2018,
  title = {Sign {{Language Production}} Using {{Neural Machine Translation}} and {{Generative Adversarial Networks}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Stoll, Stephanie and Camg{\"o}z, Necati Cihan and Hadfield, Simon and Bowden, Richard},
  year = {2018},
  month = sep,
  publisher = {{British Machine Vision Association}},
  address = {{Northumbria University, Newcastle Upon Tyne, UK}},
  abstract = {We present a novel approach to automatic Sign Language Production using stateof-
the-art Neural Machine Translation (NMT) and Image Generation techniques. Our
system is capable of producing sign videos from spoken language sentences. Contrary to
current approaches that are dependent on heavily annotated data, our approach requires
minimal gloss and skeletal level annotations for training. We achieve this by breaking
down the task into dedicated sub-processes. We first translate spoken language sentences
into sign gloss sequences using an encoder-decoder network. We then find a data driven
mapping between glosses and skeletal sequences. We use the resulting pose information
to condition a generative model that produces sign language video sequences. We
evaluate our approach on the recently released PHOENIX14T Sign Language Translation
dataset. We set a baseline for text-to-gloss translation, reporting a BLEU-4 score of
16.34/15.26 on dev/test sets. We further demonstrate the video generation capabilities
of our approach by sharing qualitative results of generated sign sequences given their
skeletal correspondence.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CYUTT7DT/848809.html},
  language = {en}
}

@article{stoll_text2sign_2020,
  title = {{{Text2Sign}}: {{Towards Sign Language Production Using Neural Machine Translation}} and {{Generative Adversarial Networks}}},
  shorttitle = {{{Text2Sign}}},
  author = {Stoll, Stephanie and Camgoz, Necati Cihan and Hadfield, Simon and Bowden, Richard},
  year = {2020},
  month = apr,
  volume = {128},
  pages = {891--908},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01281-2},
  abstract = {We present a novel approach to automatic Sign Language Production using recent developments in Neural Machine Translation (NMT), Generative Adversarial Networks, and motion generation. Our system is capable of producing sign videos from spoken language sentences. Contrary to current approaches that are dependent on heavily annotated data, our approach requires minimal gloss and skeletal level annotations for training. We achieve this by breaking down the task into dedicated sub-processes. We first translate spoken language sentences into sign pose sequences by combining an NMT network with a Motion Graph. The resulting pose information is then used to condition a generative model that produces photo realistic sign language video sequences. This is the first approach to continuous sign video generation that does not use a classical graphical avatar. We evaluate the translation abilities of our approach on the PHOENIX14T Sign Language Translation dataset. We set a baseline for text-to-gloss translation, reporting a BLEU-4 score of 16.34/15.26 on dev/test sets. We further demonstrate the video generation capabilities of our approach for both multi-signer and high-definition settings qualitatively and quantitatively using broadcast quality assessment metrics.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5DB9BZIR/Stoll et al. - 2020 - Text2Sign Towards Sign Language Production Using .pdf},
  journal = {International Journal of Computer Vision},
  keywords = {translation;vocab=2887;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T},
  language = {en},
  number = {4}
}

@article{sturman_survey_1994,
  title = {A Survey of Glove-Based Input},
  author = {Sturman, David J. and Zeltzer, David},
  year = {1994},
  volume = {14},
  pages = {30--39},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sturman_Zeltzer_1994_A survey of glove-based input.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8MDJM6J3/250916.html},
  journal = {IEEE Computer graphics and Applications},
  number = {1}
}

@article{su_fuzzy_2001,
  title = {A Fuzzy Rule-Based Approach to Recognizing 3-{{D}} Arm Movements},
  author = {Su, Mu-Chun and Zhao, Yu-Xiang and Chen, Hsuan-Fan},
  year = {2001},
  month = jun,
  volume = {9},
  pages = {191--201},
  issn = {1534-4320},
  doi = {10.1109/7333.928579},
  abstract = {One particular application of gesture-based systems is to implement a speaking aid for the deaf. For this to happen, it requires a module that can recognize three-dimensional (3-D) arm movements since arm movement is one of the four main attributes characterizing a sign word. In this paper the authors propose a fuzzy rule-based recognition method. The effectiveness of this system is evaluated by recognizing 3-D arm movements involved in Taiwanese Sign Language (TSL).},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Su et al_2001_A fuzzy rule-based approach to recognizing 3-D arm movements.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2YC3H7DC/928579.html},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=90;signer=2;sl=TwSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hmovement,signer=2,sl=TwSL,slreco,slreco;modal=2hmovement,vocab<100,vocab=90},
  number = {2}
}

@inproceedings{Sudowe16BMVC,
  title = {{{PatchIt}}: {{Self}}-{{Supervised Network Weight Initialization}} for {{Fine}}-Grained {{Recognition}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Sudowe, Patrick and Leibe, Bastian},
  year = {2016},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sudowe_Leibe_2016_PatchIt.pdf}
}

@inproceedings{Sugano_2014_CVPR,
  title = {Learning-by-{{Synthesis}} for {{Appearance}}-Based {{3D Gaze Estimation}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sugano, Yusuke and Matsushita, Yasuyuki and Sato, Yoichi},
  year = {2014},
  month = jun,
  pages = {1821--1828},
  address = {{Columbus, Ohio, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sugano et al_2014_Learning-by-Synthesis for Appearance-based 3D Gaze Estimation.pdf}
}

@inproceedings{sugisaki_german_2018,
  title = {German Compound Splitting Using the Compound Productivity of Morphemes},
  booktitle = {14th {{Conference}} on {{Natural Language Processing}} - {{KONVENS}} 2018},
  author = {Sugisaki, Kyoko and Tuggener, Don},
  year = {2018},
  pages = {141--147},
  publisher = {{Austrian Academy of Sciences Press}},
  doi = {10.21256/zhaw-4974},
  abstract = {In this work, we present a novel compound splitting method for German by capturing the compound productivity of morphemes. We use a giga web corpus to create a lexicon and decompose noun compounds by computing the probabilities of compound elements as bound and free morphemes. Furthermore, we provide a uniformed evaluation of several unsupervised approaches and morphological analysers for the task. Our method achieved a high F1 score of 0.92, which was a comparable result to state-of-the-art methods.},
  copyright = {Licence according to publishing contract},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sugisaki_Tuggener_2018_German compound splitting using the compound productivity of morphemes.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P5YDJ9LX/14372.html},
  language = {en}
}

@article{suharjito_sign_2017,
  title = {Sign {{Language Recognition Application Systems}} for {{Deaf}}-{{Mute People}}: {{A Review Based}} on {{Input}}-{{Process}}-{{Output}}},
  shorttitle = {Sign {{Language Recognition Application Systems}} for {{Deaf}}-{{Mute People}}},
  author = {{Suharjito} and Anderson, Ricky and Wiryana, Fanny and Ariesta, Meita Chandra and Kusuma, Gede Putra},
  year = {2017},
  month = jan,
  volume = {116},
  pages = {441--448},
  issn = {1877-0509},
  doi = {10.1016/j.procs.2017.10.028},
  abstract = {Sign Language Recognition is a breakthrough for helping deaf-mute people and has been researched for many years. Unfortunately, every research has its own limitations and are still unable to be used commercially. Some of the researches have known to be successful for recognizing sign language, but require an expensive cost to be commercialized. Nowadays, researchers have gotten more attention for developing Sign Language Recognition that can be used commercially. Researchers do their researches in various ways. It starts from the data acquisition methods. The data acquisition method varies because of the cost needed for a good device, but cheap method is needed for the Sign Language Recognition System to be commercialized. The methods used in developing Sign Language Recognition are also varied between researchers. Each method has its own strength compare to other methods and researchers are still using different methods in developing their own Sign Language Recognition. Each method also has its own limitations compared to other methods. The aim of this paper is to review the sign language recognition approaches and find the best method that has been used by researchers. Hence other researchers can get more information about the methods used and could develop better Sign Language Application Systems in the future.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Suharjito et al_2017_Sign Language Recognition Application Systems for Deaf-Mute People.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6I7T777C/S1877050917320720.html},
  journal = {Procedia Computer Science},
  series = {Discovery and Innovation of Computer Science Technology in Artificial Intelligence Era: {{The}} 2nd {{International Conference}} on {{Computer Science}} and {{Computational Intelligence}} ({{ICCSCI}} 2017)}
}

@inproceedings{sun_deep_2013,
  title = {Deep Convolutional Network Cascade for Facial Point Detection},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
  year = {2013},
  pages = {3476--3483},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sun et al_2013_Deep convolutional network cascade for facial point detection.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SPPPXD3H/login.html}
}

@article{sun_discriminative_2013,
  title = {Discriminative {{Exemplar Coding}} for {{Sign Language Recognition With Kinect}}},
  author = {Sun, C. and Zhang, T. and Bao, B. and Xu, C. and Mei, T.},
  year = {2013},
  month = oct,
  volume = {43},
  pages = {1418--1428},
  issn = {2168-2267},
  doi = {10.1109/TCYB.2013.2265337},
  abstract = {Sign language recognition is a growing research area in the field of computer vision. A challenge within it is to model various signs, varying with time resolution, visual manual appearance, and so on. In this paper, we propose a discriminative exemplar coding (DEC) approach, as well as utilizing Kinect sensor, to model various signs. The proposed DEC method can be summarized as three steps. First, a quantity of class-specific candidate exemplars are learned from sign language videos in each sign category by considering their discrimination. Then, every video of all signs is described as a set of similarities between frames within it and the candidate exemplars. Instead of simply using a heuristic distance measure, the similarities are decided by a set of exemplar-based classifiers through the multiple instance learning, in which a positive (or negative) video is treated as a positive (or negative) bag and those frames similar to the given exemplar in Euclidean space as instances. Finally, we formulate the selection of the most discriminative exemplars into a framework and simultaneously produce a sign video classifier to recognize sign. To evaluate our method, we collect an American sign language dataset, which includes approximately 2000 phrases, while each phrase is captured by Kinect sensor with color, depth, and skeleton information. Experimental results on our dataset demonstrate the feasibility and effectiveness of the proposed approach for sign language recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sun et al_2013_Discriminative Exemplar Coding for Sign Language Recognition With Kinect.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/L6CWBPIJ/6544211.html},
  journal = {IEEE Transactions on Cybernetics},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=73;signer=9;sl=ASL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=9,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=bodyjoints,vocab<100,vocab=73},
  number = {5}
}

@inproceedings{sun_latent_2013,
  title = {Latent Support Vector Machine for Sign Language Recognition with {{Kinect}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Sun, Chao and Zhang, Tianzhu and Bao, Bing-Kun and Xu, Changsheng},
  year = {2013},
  month = sep,
  pages = {4190--4194},
  doi = {10.1109/ICIP.2013.6738863},
  abstract = {In this paper, we propose a novel algorithm to model and recognize sign language with Kinect sensor. We assume that in a sign language video, some frames are expected to be both discriminative and representative. Under this assumption, each frame in training videos is assigned a binary latent variable indicating its discriminative capability. A Latent Support Vector Machine model is then developed to classify the signs, as well as localize the discriminative and representative frames in videos. In addition, we utilize the depth map together with color image captured by Kinect sensor to obtain more effective and accurate feature to enhance the recognition accuracy. To evaluate our approach, we collected an American Sign Language (ASL) dataset which included approximately 2000 phrases, while each phrase was captured by Kinect sensor and hence included color, depth and skeleton information. Experiments on our dataset demonstrate the effectiveness of the proposed method for sign language recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sun et al_2013_Latent support vector machine for sign language recognition with Kinect.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P7U2UQB4/abs_all.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=73;signer=9;sl=ASL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=9,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=bodyjoints,vocab<100,vocab=73}
}

@article{sundermeyer_feedforward_2015,
  title = {From {{Feedforward}} to {{Recurrent LSTM Neural Networks}} for {{Language Modeling}}},
  author = {Sundermeyer, M. and Ney, H. and Schluter, R.},
  year = {2015},
  month = mar,
  volume = {23},
  pages = {517--529},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2015.2400218},
  abstract = {Language models have traditionally been estimated based on relative frequencies, using count statistics that can be extracted from huge amounts of text data. More recently, it has been found that neural networks are particularly powerful at estimating probability distributions over word sequences, giving substantial improvements over state-of-the-art count models. However, the performance of neural network language models strongly depends on their architectural structure. This paper compares count models to feedforward, recurrent, and long short-term memory (LSTM) neural network variants on two large-vocabulary speech recognition tasks. We evaluate the models in terms of perplexity and word error rate, experimentally validating the strong correlation of the two quantities, which we find to hold regardless of the underlying type of the language model. Furthermore, neural networks incur an increased computational complexity compared to count models, and they differently model context dependences, often exceeding the number of words that are taken into account by count based approaches. These differences require efficient search methods for neural networks, and we analyze the potential improvements that can be obtained when applying advanced algorithms to the rescoring of word lattices on large-scale setups.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sundermeyer et al_2015_From Feedforward to Recurrent LSTM Neural Networks for Language Modeling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6BK4QXVP/abs_all.html},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  number = {3}
}

@inproceedings{sundermeyer_lstm_2012,
  title = {{{LSTM}} Neural Networks for Language Modeling},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2012},
  month = sep,
  pages = {194--197},
  address = {{Portland, OR, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sundermeyer et al_2012_LSTM neural networks for language modeling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IPYT5KDX/i12_0194.html}
}

@inproceedings{sundermeyer_rwth_2011,
  title = {The {{RWTH}} 2010 {{Quaero ASR}} Evaluation System for {{English}}, {{French}}, and {{German}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sundermeyer, M. and {Nu{\ss}baum-Thom}, M. and Wiesler, S. and Plahl, C. and Mousa, A. El-Desoky and Hahn, S. and Nolden, D. and Schl{\"u}ter, R. and Ney, H.},
  year = {2011},
  month = may,
  pages = {2212--2215},
  doi = {10.1109/ICASSP.2011.5946920},
  abstract = {Recognizing Broadcast Conversational (BC) speech data is a difficult task, which can be regarded as one of the major challenges beyond the recognition of Broadcast News (BN). This paper presents the automatic speech recognition systems developed by RWTH for the English, French, and German language which attained the best word error rates for English and German, and competitive results for the French task in the 2010 Quaero evaluation for BC and BN data. At the same time, the RWTH German system used the least amount of training data among all participants. Large reductions in word error rate were obtained by the incorporation of the new Bottleneck Multilayer Perception (MLP) features for all three languages. Additional improvements were obtained for the German system by applying a new language modeling technique, decomposing words into sublexical components.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RDWZWKPZ/5946920.html}
}

@phdthesis{sundermeyer2016:phd,
  title = {Improvements in {{Language}} and {{Translation Modeling}}},
  author = {Sundermeyer, Martin},
  year = {2016},
  month = jun,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sundermeyer_2016_Improvements in Language and Translation Modeling.pdf},
  school = {RWTH Aachen University}
}

@phdthesis{supalla_structure_1982,
  title = {Structure and Acquisition of Verbs of Motion and Location in {{American Sign Language}}.},
  author = {Supalla, Ted Roland},
  year = {1982},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Supalla_1982_Structure and acquisition of verbs of motion and location in American Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/S8FSD2DL/item.html},
  school = {University of California, San Diego}
}

@inproceedings{sutskever_sequence_2014,
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc},
  year = {2014},
  month = dec,
  pages = {3104--3112},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf}
}

@book{sutton_signwriting_2000,
  title = {{{SignWriting Basics Instruction Manual}}},
  author = {Sutton, Valery},
  year = {2000},
  publisher = {{The SignWriting Press}},
  address = {{La Jolla, CA, United States}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sutton_Writing_2000_Sign writing.pdf},
  isbn = {978-0-914336-49-5},
  language = {en}
}

@book{sutton-spence_linguistics_1999,
  title = {The Linguistics of {{British Sign Language}}: An Introduction},
  shorttitle = {The Linguistics of {{British Sign Language}}},
  author = {{Sutton-Spence}, Rachel and Woll, Bencie},
  year = {1999},
  publisher = {{Cambridge University Press}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sutton-Spence_Woll_1999_The linguistics of British Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V8TAWH43/books.html}
}

@incollection{sutton-spence_mouthings_2007,
  title = {Mouthings and {{Simultaneity}} in {{British Sign Language}}},
  booktitle = {Simultaneity in {{Signed Languages}}: {{Form}} and {{Function}}},
  author = {{Sutton-Spence}, Rachel},
  editor = {Vermeerbergen, Myriam and Leeson, Lorraine and Crasborn, Onno Alex},
  year = {2007},
  pages = {147},
  publisher = {{John Benjamins Publishing}},
  abstract = {Signed language users can draw on a range of articulators when expressing linguistic messages, including the hands, torso, eye gaze, and mouth. Sometimes these articulators work in tandem to produce one lexical item while in other instances they operate to convey different types of information simultaneously. Over the past fifteen years, there has been a growing interest in the issue of simultaneity in signed languages. However, this book is the first to offer a comprehensive treatment of this topic, presenting a collection of papers dealing with different aspects of simultaneity in a range of related and unrelated signed languages, in descriptive and cross-linguistic treatments which are set in different theoretical frameworks. This volume has relevance for those interested in sign linguistics, in teaching and learning signed languages, and is also highly recommended to anyone interested in the fundamental underpinnings of human language and the effects of signed versus spoken modality.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Sutton-Spence_2007_Mouthings and Simultaneity in British Sign Language.pdf},
  isbn = {90-272-4796-X},
  language = {en}
}

@article{suzgun_hospisign_,
  title = {{{HospiSign}}: {{An Interactive Sign Language Platform}} for {{Hearing Impaired}}},
  shorttitle = {{{HospiSign}}},
  author = {S{\"u}zg{\"u}n, Muhammed Mira{\c c} and Ozdemir, Hilal and Camg{\"o}z, Necati Cihan and K{\i}nd{\i}roglu, Ahmet Alp and Basaran, Doga{\c c} and Togay, Cengiz and Akarun, Lale},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Süzgün et al_HospiSign.pdf}
}

@inproceedings{szegedy_going_2014,
  title = {Going {{Deeper With Convolutions}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  address = {{Boston, Ma, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Szegedy et al_2015_Going Deeper With Convolutions.pdf}
}

@article{szegedy_rethinking_2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015},
  month = dec,
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  archivePrefix = {arXiv},
  eprint = {1512.00567},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Szegedy et al_2015_Rethinking the Inception Architecture for Computer Vision.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M2JHK8E7/1512.html},
  journal = {arXiv:1512.00567 [cs]},
  primaryClass = {cs}
}

@article{tachbelie_using_2014,
  title = {Using Morphemes in Language Modeling and Automatic Speech Recognition of {{Amharic}}},
  author = {Tachbelie, Martha Yifiru and Abate, Solomon Teferra and Menzel, Wolfgang},
  year = {2014},
  month = apr,
  volume = {20},
  pages = {235--259},
  issn = {1351-3249, 1469-8110},
  doi = {10.1017/S1351324912000356},
  abstract = {This paper presents morpheme-based language models developed for Amharic (a morphologically rich Semitic language) and their application to a speech recognition task. A substantial reduction in the out of vocabulary rate has been observed as a result of using subwords or morphemes. Thus a severe problem of morphologically rich languages has been addressed. Moreover, lower perplexity values have been obtained with morpheme-based language models than with word-based models. However, when comparing the quality based on the probability assigned to the test sets, word-based models seem to fare better. We have studied the utility of morpheme-based language models in speech recognition systems and found that the performance of a relatively small vocabulary (5k) speech recognition system improved significantly as a result of using morphemes as language modeling and dictionary units. However, as the size of the vocabulary increases (20k or more) the morpheme-based systems suffer from acoustic confusability and did not achieve a significant improvement over a word-based system with an equivalent vocabulary size even with the use of higher order (quadrogram) n-gram language models.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tachbelie et al_2014_Using morphemes in language modeling and automatic speech recognition of Amharic.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GDKPUPRT/019ADBC17DFA972042C472D44A2C5091.html},
  journal = {Natural Language Engineering},
  language = {en},
  number = {2}
}

@phdthesis{tahir2015:phd,
  title = {Discriminative {{Training}} of {{Linear Transformations}} and {{Mixture Density Splitting}} for {{Speech Recognition}}},
  author = {Tahir, Muhammad Ali},
  year = {2015},
  month = nov,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tahir_2015_Discriminative Training of Linear Transformations and Mixture Density Splitting.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{tamer_keyword_2020,
  ids = {tamer\_keyword\_2020a},
  title = {Keyword {{Search}} for {{Sign Language}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tamer, Nazif Can and Sara{\c c}lar, Murat},
  year = {2020},
  month = may,
  pages = {8184--8188},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054678},
  abstract = {Keyword search is the search for a written query in an archive, which is often assumed to be a collection from a spoken language. Yet, the main languages of the Deaf, i.e. sign languages, are mostly neglected in this definition due to being visual languages. In this paper, we propose a keyword search (KWS) system for a sign language. In this technique, we first extract body and hand joints from the frames of a sign language sentence and represent it with a unified spatio-temporal graph of skeleton joints. We then train our graph-convolutional-network encoder, query embedding, and selection mechanism together in a weakly supervised, end-to-end fashion. Experimental results are reported on RWTH-PHOENIX-Weather 2014T dataset.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tamer_Saraçlar_2020_Keyword Search for Sign Language.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tamer_Saraçlar_2020_Keyword Search for Sign Language2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/RQTSJJ8R/9054678.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V346UNBW/9054678.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=1066;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,dataSet=Phoenix14T,Keyword search,modal=bodyjoints,sign language,signer=9,sl=DGS,slreco,slreco;modal=bodyjoints,vocab=1066,vocab>=1000}
}

@article{tamura_recognition_1988,
  title = {Recognition of Sign Language Motion Images},
  author = {Tamura, Shinichi and Kawasaki, Shingo},
  year = {1988},
  volume = {21},
  pages = {343--353},
  issn = {0031-3203},
  doi = {10.1016/0031-3203(88)90048-9},
  abstract = {This paper describes a method of classifying single view deaf-and-mute sign language motion images. We suppose the sign language word is composed of a time sequence of units called cheremes. The chereme is described by handshape, movement, and location of the hand, which can be said to express the 3-D features of the sign language. First, a dictionary for recognizing the sign language is made based on the cheremes. Then, the macro 2-D features of the location of a hand and its movement are extracted from the red component of the input color image sequence. Further, the micro 2-D features of the shape of the hand are also extracted if necessary. The 3-D feature descriptions of the dictionary are converted into 2-D image features, and the input sign language image is classified according to the extracted features of the 2-D image.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tamura_Kawasaki_1988_Recognition of sign language motion images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4NIB7TCV/0031320388900489.html},
  journal = {Pattern Recognition},
  keywords = {capturedBy=vision,isolated,isolated;vocab=10;sl=JSL;capturedBy=vision,modal=location,modal=movement,modal=shape,sl=JSL,slreco,slreco;modal=shape;modal=movement;modal=location,vocab<50,vocab=10},
  number = {4}
}

@inproceedings{tan_philstm_2016,
  title = {Phi-{{LSTM}}: A Phrase-Based Hierarchical {{LSTM}} Model for Image Captioning},
  shorttitle = {Phi-{{LSTM}}},
  booktitle = {Asian {{Conf}}. on {{Computer Vision}} ({{ACCV}})},
  author = {Tan, Ying Hua and Chan, Chee Seng},
  year = {2016},
  month = nov,
  pages = {101--117},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tan_Chan_2016_phi-LSTM.pdf}
}

@inproceedings{tang_latent_2014,
  title = {Latent {{Regression Forest}}: {{Structured Estimation}} of {{3D Articulated Hand Posture}}},
  shorttitle = {Latent {{Regression Forest}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tang, Danhang and Chang, Hyung Jin and Tejani, A. and Kim, Tae-Kyun},
  year = {2014},
  month = jun,
  pages = {3786--3793},
  doi = {10.1109/CVPR.2014.490},
  abstract = {In this paper we present the Latent Regression Forest (LRF), a novel framework for real-time, 3D hand pose estimation from a single depth image. In contrast to prior forest-based methods, which take dense pixels as input, classify them independently and then estimate joint positions afterwards, our method can be considered as a structured coarse-to-fine search, starting from the centre of mass of a point cloud until locating all the skeletal joints. The searching process is guided by a learnt Latent Tree Model which reflects the hierarchical topology of the hand. Our main contributions can be summarised as follows: (i) Learning the topology of the hand in an unsupervised, data-driven manner. (ii) A new forest-based, discriminative framework for structured search in images, as well as an error regression step to avoid error accumulation. (iii) A new multi-view hand pose dataset containing 180K annotated images from 10 different subjects. Our experiments show that the LRF out-performs state-of-the-art methods in both accuracy and efficiency.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tang et al_2014_Latent Regression Forest.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KXBN8XA5/abs_all.html}
}

@article{tang_phonetic_2018,
  title = {Phonetic {{Temporal Neural Model}} for {{Language Identification}}},
  author = {Tang, Z. and Wang, D. and Chen, Y. and Li, L. and Abel, A.},
  year = {2018},
  month = jan,
  volume = {26},
  pages = {134--144},
  issn = {2329-9290},
  doi = {10.1109/TASLP.2017.2764271},
  abstract = {Deep neural models, particularly the long short-term memory recurrent neural network (LSTM-RNN) model, have shown great potential for language identification (LID). However, the use of phonetic information has been largely overlooked by most existing neural LID methods, although this information has been used very successfully in conventional phonetic LID systems. We present a phonetic temporal neural model for LID, which is an LSTM-RNN LID system that accepts phonetic features produced by a phone-discriminative DNN as the input, rather than raw acoustic features. This new model is similar to traditional phonetic LID methods, but the phonetic knowledge here is much richer: It is at the frame level and involves compacted information of all phones. Our experiments conducted on the Babel database and the AP16-OLR database demonstrate that the temporal phonetic neural approach is very effective, and significantly outperforms existing acoustic neural models. It also outperforms the conventional i-vector approach on short utterances and in noisy conditions.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tang et al_2018_Phonetic Temporal Neural Model for Language Identification.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QQVHB9PM/8070977.html},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  number = {1}
}

@article{tang_real-time_2015,
  title = {A {{Real}}-{{Time Hand Posture Recognition System Using Deep Neural Networks}}},
  author = {Tang, Ao and Lu, Ke and Wang, Yufei and Huang, Jie and Li, Houqiang},
  year = {2015},
  volume = {6},
  pages = {21},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4TRXEBK3/citation.html},
  journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},
  number = {2}
}

@inproceedings{tang_realtime_2013,
  title = {Real-{{Time Articulated Hand Pose Estimation Using Semi}}-Supervised {{Transductive Regression Forests}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Tang, Danhang and Yu, Tsz-Ho and Kim, Tae-Kyun},
  year = {2013},
  month = dec,
  pages = {3224--3231},
  doi = {10.1109/ICCV.2013.400},
  abstract = {This paper presents the first semi-supervised transductive algorithm for real-time articulated hand pose estimation. Noisy data and occlusions are the major challenges of articulated hand pose estimation. In addition, the discrepancies among realistic and synthetic pose data undermine the performances of existing approaches that use synthetic data extensively in training. We therefore propose the Semi-supervised Transductive Regression (STR) forest which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset. We also design a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints. Our contributions include: (i) capturing the benefits of both realistic and synthetic data via transductive learning, (ii) showing accuracies can be improved by considering unlabelled data, and (iii) introducing a pseudo-kinematic technique to refine articulations efficiently. Experimental results show not only the promising performance of our method with respect to noise and occlusions, but also its superiority over state-of-the-arts in accuracy, robustness and speed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2013/Tang et al/Tang et al_2013_Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tang et al_2013_Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8RBUMA8Z/abs_all.html}
}

@inproceedings{tanibata_extraction_2002,
  title = {Extraction of {{Hand Features}} for {{Recognition}} of {{Sign Language Words}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Vision Interface}}},
  author = {Tanibata, Nobuhiko and Shimada, Nobutaka and Shirai, Yoshiaki},
  year = {2002},
  month = may,
  pages = {391--398},
  address = {{Calgary, Canada}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tanibata et al_2002_Extraction of hand features for recognition of sign language words.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=65;signer=1;sl=JSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=1,sl=JSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<100,vocab=65}
}

@article{tanielian_adversarial_2018,
  title = {Adversarial {{Training}} of {{Word2Vec}} for {{Basket Completion}}},
  author = {Tanielian, Ugo and Gartrell, Mike and Vasile, Flavian},
  year = {2018},
  month = may,
  abstract = {In recent years, the Word2Vec model trained with the Negative Sampling loss function has shown state-of-the-art results in a number of machine learning tasks, including language modeling tasks, such as word analogy and word similarity, and in recommendation tasks, through Prod2Vec, an extension that applies to modeling user shopping activity and user preferences. Several methods that aim to improve upon the standard Negative Sampling loss have been proposed. In our paper we pursue more sophisticated Negative Sampling, by leveraging ideas from the field of Generative Adversarial Networks (GANs), and propose Adversarial Negative Sampling. We build upon the recent progress made in stabilizing the training objective of GANs in the discrete data se ing, and introduce a new GAN-Word2Vec model. We evaluate our model on the task of basket completion, and show significant improvements in performance over Word2Vec trained using standard loss functions, including Noise Contrastive Estimation and Negative Sampling.},
  archivePrefix = {arXiv},
  eprint = {1805.08720},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/55ZUN27H/Tanielian et al. - 2018 - Adversarial Training of Word2Vec for Basket Comple.pdf},
  journal = {arXiv:1805.08720 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{taylor_dynamic_2012,
  title = {Dynamic Units of Visual Speech},
  booktitle = {Proc. {{ACM SIGGRAPH}}/{{Eurographics Symposium}} on {{Computer Animation}}},
  author = {Taylor, Sarah L. and Mahler, Moshe and Theobald, Barry-John and Matthews, Iain},
  year = {2012},
  pages = {275--284},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Taylor et al_2012_Dynamic units of visual speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZPSF8PXI/citation.html}
}

@inproceedings{taylor_userspecific_2014,
  title = {User-Specific Hand Modeling from Monocular Depth Sequences},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Taylor, James and Stebbing, Richard and Ramakrishna, Varun and Keskin, Cem and Shotton, Jamie and Izadi, Shahram and Hertzmann, Aaron and Fitzgibbon, Andrew},
  year = {2014},
  pages = {644--651},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Taylor et al_2014_User-specific hand modeling from monocular depth sequences.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5CBXQ5SB/abs_all.html}
}

@inproceedings{teixeira_accent_1996,
  title = {Accent {{Identification}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{ICSLP}})},
  author = {Teixeira, C. and Trancoso, I. and Serralheiro, A.},
  year = {1996},
  volume = {3},
  pages = {1784--1787}
}

@incollection{terissi_audio-visual_2015,
  title = {Audio-{{Visual Speech Recognition Scheme Based}} on {{Wavelets}} and {{Random Forests Classification}}},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Terissi, Lucas Daniel and Sad, Gonzalo D. and G{\'o}mez, Juan Carlos and Parodi, Marianela},
  year = {2015},
  pages = {567--574},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5GS9M324/978-3-319-25751-8_68.html}
}

@inproceedings{terissi_lip_2014,
  title = {Lip Reading Using Wavelet-Based Features and Random Forests Classification},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Terissi, Lucas D. and Parodi, Marianela and Gomez, Juan C.},
  year = {2014},
  pages = {791--796},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DS4NB7SW/cookiedetectresponse.html}
}

@inproceedings{thang_comparison_2017,
  title = {A {{Comparison}} of {{SimpSVM}} and {{RVM}} for {{Sign Language Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Machine Learning}} and {{Soft Computing}}},
  author = {Thang, Pham Quoc and Dung, Nguyen Duc and Thuy, Nguyen Thanh},
  year = {2017},
  month = jan,
  pages = {98--104},
  address = {{Ho Chi Minh City, Vietnam}},
  doi = {10.1145/3036290.3036322},
  abstract = {Sign language recognition is a rather new field and many challenges, especially when motion capture devices become more popular. In this paper, we study the feasibility and effectiveness of two classification methods, namely Simplification of Support Vector Machine (SimpSVM) and Relevance Vector Machine (RVM), and also give some comparative results of them for the sign language recognition problem. The experimental results on the Auslan data set and ASLID data set show that SimpSVM and RVM could achieve good predictive performances and SimpSVM is better as compared to RVM on sign recognition. They also pointed out that prediction behaviors of them are similar in terms of the prediction accuracy when the amount of data or the number of feature changed and sign discrimination. However, SimpSVM requires fewer training time than RVM in training phase.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Thang et al_2017_A Comparison of SimpSVM and RVM for Sign Language Recognition.pdf},
  isbn = {978-1-4503-4828-7},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,capturedBy=vision,isolated,isolated;vocab=45;signer=6;sl=ASL;capturedBy=vision,isolated;vocab=95;signer=5;sl=Auslan;capturedBy=electronicGlove;capturedBy=mocap,modal=location,modal=orientation,modal=shape,signer=5,signer=6,sl=ASL,sl=Auslan,slreco,slreco;modal=location;modal=orientation;modal=shape,vocab<100,vocab<50,vocab=45,vocab=95},
  series = {{{ICMLSC}} '17}
}

@inproceedings{thangali_alignment_2009,
  title = {An Alignment Based Similarity Measure for Hand Detection in Cluttered Sign Language Video},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Thangali, A. and Sclaroff, S.},
  year = {2009},
  month = jun,
  pages = {89--96},
  doi = {10.1109/CVPRW.2009.5204266},
  abstract = {Locating hands in sign language video is challenging due to a number of factors. Hand appearance varies widely across signers due to anthropometric variations and varying levels of signer proficiency. Video can be captured under varying illumination, camera resolutions, and levels of scene clutter, e.g., high-res video captured in a studio vs. low-res video gathered by a Web cam in a user's home. Moreover, the signers' clothing varies, e.g., skin-toned clothing vs. contrasting clothing, short-sleeved vs. long-sleeved shirts, etc. In this work, the hand detection problem is addressed in an appearance matching framework. The histogram of oriented gradient (HOG) based matching score function is reformulated to allow non-rigid alignment between pairs of images to account for hand shape variation. The resulting alignment score is used within a support vector machine hand/not-hand classifier for hand detection. The new matching score function yields improved performance (in ROC area and hand detection rate) over the vocabulary guided pyramid match kernel (VGPMK) and the traditional, rigid HOG distance on American Sign Language video gestured by expert signers. The proposed match score function is computationally less expensive (for training and testing), has fewer parameters and is less sensitive to parameter settings than VGPMK. The proposed detector works well on test sequences from an inexpert signer in a non-studio setting with cluttered background.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Thangali_Sclaroff_2009_An alignment based similarity measure for hand detection in cluttered sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/98M3SRA8/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HEJMFKNU/abs_all.html}
}

@inproceedings{thangali_exploiting_2011,
  title = {Exploiting Phonological Constraints for Handshape Inference in {{ASL}} Video},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Thangali, A. and Nash, J. P. and Sclaroff, S. and Neidle, C.},
  year = {2011},
  month = jun,
  pages = {521--528},
  address = {{Colorado Springs, Colorado, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Thangali et al_2011_Exploiting phonological constraints for handshape inference in ASL video.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/A5S5T2RV/abs_all.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=82;signer=2;sl=ASL;capturedBy=vision,modal=shape,signer=2,sl=ASL,slreco,slreco;modal=shape,vocab<100,vocab=82}
}

@phdthesis{thangali_exploiting_2013,
  title = {Exploiting Phonological Constraints for Handshape Recognition in Sign Language Video},
  author = {Thangali, Ashwin},
  year = {2013},
  address = {{Boston, Ma, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Thangali_2013_Exploiting phonological constraints for handshape recognition in sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BWDVXJ4A/14076.html},
  school = {Boston University}
}

@article{theodorakis_dynamicstatic_2014,
  title = {Dynamic-Static Unsupervised Sequentiality, Statistical Subunits and Lexicon for Sign Language Recognition},
  author = {Theodorakis, Stavros and Pitsikalis, Vassilis and Maragos, Petros},
  year = {2014},
  volume = {32},
  pages = {533--549},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Theodorakis et al_2014_Dynamic–static unsupervised sequentiality, statistical subunits and lexicon for.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8ITC5SGN/S0262885614000808.html},
  journal = {Image and Vision Computing},
  keywords = {capturedBy=vision,isolated,isolated;vocab=1046;signer=2;sl=GSL;capturedBy=vision,isolated;vocab=94;signer=1;sl=ASL;capturedBy=vision,isolated;vocab=97;signer=2;sl=ASL;capturedBy=vision,modal=location,modal=movement,modal=shape,signer=1,signer=2,sl=ASL,sl=GSL,slreco,slreco;modal=shape;modal=location;modal=movement,vocab<100,vocab=1046,vocab=94,vocab=97,vocab>=1000},
  number = {8}
}

@inproceedings{theodorakis_modellevel_2010,
  title = {Model-Level Data-Driven Sub-Units for Signs in Videos of Continuous Sign Language},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Theodorakis, S. and Pitsikalis, V. and Maragos, P.},
  year = {2010},
  month = mar,
  pages = {2262--2265},
  address = {{Dallas, Texas, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Theodorakis et al_2010_Model-level data-driven sub-units for signs in videos of continuous sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7R85CDAU/Theodorakis et al. - 2010 - Model-level data-driven sub-units for signs in vid.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=20;signer=1;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,signer=1,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=20}
}

@inproceedings{theodorakis_producthmms_2009,
  title = {Product-{{HMMs}} for Automatic Sign Language Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Theodorakis, Stavros and Katsamanis, Athanassios and Maragos, Petros},
  year = {2009},
  month = apr,
  pages = {1601--1604},
  address = {{Taipei, Taiwan}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Theodorakis et al_2009_Product-HMMs for automatic sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CN89EQX6/login.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QRM8NZ4E/abs_all.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=93;signer=1;sl=GSL;capturedBy=vision,modal=movement,modal=shape,signer=1,sl=GSL,slreco,slreco;modal=shape;modal=movement,vocab<100,vocab=93}
}

@inproceedings{thepade_sign_2013,
  title = {Sign Language Recognition Using Color Means of Gradient Slope Magnitude Edge Images},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Intelligent Systems}} and {{Signal Processing}} ({{ISSP}})},
  author = {Thepade, S. D. and Kulkarni, G. and Narkhede, A. and Kelvekar, P. and Tathe, S.},
  year = {2013},
  month = mar,
  pages = {216--220},
  doi = {10.1109/ISSP.2013.6526905},
  abstract = {Sign Language is a method of communication for hearing and speech impaired people in which hand movements, gestures and facial expressions are used to convey messages. The hearing and speech impaired people are deeply associated with Sign Language as it is their fundamental medium of communication. American Sign Language (ASL) is a complete, visual-gestural language that employs signs made by moving the hands combined with facial expressions and postures of the body. The paper discusses novel Sign Language Image Retrieval techniques using the edge images of the ASL signs. Edge images are obtained by applying gradient masks and Slope Magnitude Methods. The proposed image retrieval techniques are tested on generic image database with 312 images. Feature vector of sign images are extracted using color averaging techniques. In all 5 techniques are experimented and sign images are compared using 5 masks (Prewitt, Robert, Sobel, Laplace, and Canny) and 5 averaging techniques. The GAR (Genuine Acceptance Ratio) values indicate the best performance values.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Thepade et al_2013_Sign language recognition using color means of gradient slope magnitude edge.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TPHHE72K/6526905.html}
}

@inproceedings{thies2016face,
  title = {{{Face2Face}}: {{Real}}-Time {{Face Capture}} and {{Reenactment}} of {{RGB Videos}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Thies, J. and Zollh{\"o}fer, M. and Stamminger, M. and Theobalt, C. and Nie{\ss}ner, M.},
  year = {2016}
}

@article{tian_recognizing_2001,
  title = {Recognizing Action Units for Facial Expression Analysis},
  author = {Tian, Ying-Li and Kanade, T. and Cohn, J.F.},
  year = {2001},
  month = feb,
  volume = {23},
  pages = {97--115},
  issn = {0162-8828},
  doi = {10.1109/34.908962},
  abstract = {Most automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as happiness, anger, surprise, and fear. Such prototypic expressions, however, occur rather infrequently. Human emotions and intentions are more often communicated by changes in one or a few discrete facial features. In this paper, we develop an automatic face analysis (AFA) system to analyze facial expressions based on both permanent facial features (brows, eyes, mouth) and transient facial features (deepening of facial furrows) in a nearly frontal-view face image sequence. The AFA system recognizes fine-grained changes in facial expression into action units (AU) of the Facial Action Coding System (FACS), instead of a few prototypic expressions. Multistate face and facial component models are proposed for tracking and modeling the various facial features, including lips, eyes, brows, cheeks, and furrows. During tracking, detailed parametric descriptions of the facial features are extracted. With these parameters as the inputs, a group of action units (neutral expression, six upper face AU and 10 lower face AU) are recognized whether they occur alone or in combinations. The system has achieved average recognition rates of 96.4 percent (95.4 percent if neutral expressions are excluded) for upper face AU and 96.7 percent (95.6 percent with neutral expressions excluded) for lower face AU. The generalizability of the system has been tested by using independent image databases collected and FACS-coded for ground-truth by different research teams},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2001/Tian et al/Tian et al_2001_Recognizing action units for facial expression analysis.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tian et al_2001_Recognizing action units for facial expression analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CPK4726C/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F5QXRVI5/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {2}
}

@phdthesis{tillmann:phd01,
  title = {Word {{Re}}-{{Ordering}} and {{Dynamic Programming}} Based {{Search Algorithm}} for {{Statistical Machine Translation}}},
  author = {Tillmann, Christoph},
  year = {2001},
  month = may,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tillmann_2001_Word Re-Ordering and Dynamic Programming based Search Algorithm for Statistical.pdf},
  school = {RWTH Aachen University}
}

@incollection{tokuda_automatic_1998,
  title = {Towards Automatic Translation from {{Japanese}} into {{Japanese}} Sign Language},
  booktitle = {Assistive {{Technology}} and {{Artificial Intelligence}}},
  author = {Tokuda, Masaaki and Okumura, Manabu},
  editor = {Mittal, Vibhu O. and Yanco, Holly A. and Aronis, John and Simpson, Richard},
  year = {1998},
  month = jan,
  pages = {97--108},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {In this paper, we present a prototype translation system named SYUWAN which translates Japanese into Japanese sign language. One of the most important problems in this task is that there are very few entries in a sign language dictionary compared with a Japanese one. To solve this problem, when the original input word does not exist in a sign language dictionary SYUWAN applies several techniques to find a similar word from a Japanese dictionary and substitutes this word for the original word. As the result, SYUWAN can translate up to 82\% of words which are morphologically analyzed.},
  copyright = {\textcopyright 1998 Springer-Verlag},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tokuda_Okumura_1998_Towards automatic translation from Japanese into Japanese sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/588VVEJK/BFb0055973.html},
  isbn = {978-3-540-64790-4 978-3-540-68678-1},
  language = {en},
  number = {1458},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{tolba_image_2010,
  title = {Image Signature Improving by {{PCNN}} for {{Arabic}} Sign Language Recognition},
  author = {Tolba, M. Fahmy and Abdellwahab, M. S. and {Aboul-Ela}, Magdy and Samir, Ahmed},
  year = {2010},
  volume = {1},
  pages = {1--6},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tolba et al_2010_Image signature improving by PCNN for Arabic sign language recognition.pdf},
  journal = {Can. J. Artif. Intell. Mach. Learn. Pattern Recognit},
  keywords = {capturedBy=vision,isolated,isolated;vocab=28;sl=ArSL;capturedBy=vision,modal=shape,sl=ArSL,slreco,slreco;modal=shape,vocab<50,vocab=28},
  number = {1}
}

@inproceedings{tomlinson_integrating_1996,
  title = {Integrating Audio and Visual Information to Provide Highly Robust Speech Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tomlinson, M. J. and Russell, M. J. and Brooke, N. M.},
  year = {1996},
  month = may,
  volume = {2},
  pages = {821-824 vol. 2},
  doi = {10.1109/ICASSP.1996.543247},
  abstract = {There is a requirement in many human machine interactions to provide accurate automatic speech recognition in the presence of high levels of interfering noise. The the paper shows that performance improvements in recognition accuracy can be obtained by including data derived from a speaker's lip images. We describe the combination of the audio and visual data in the construction of composite feature vectors and a hidden Markov model structure which allows for asynchrony between the audio and visual components. These ideas are applied to a speaker dependent recognition task involving a small vocabulary and subject to interfering noise. The recognition results obtained using composite vectors and cross-product models are compared with those based on an audio-only feature vector. The benefit of this approach is shown to be an increased performance over a very wide range of noise levels},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tomlinson et al_1996_Integrating audio and visual information to provide highly robust speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K9736G8E/543247.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V8AJPWVK/543247.html}
}

@article{tompson_realtime_2014,
  title = {Real-Time Continuous Pose Recovery of Human Hands Using Convolutional Networks},
  author = {Tompson, Jonathan and Stein, Murphy and Lecun, Yann and Perlin, Ken},
  year = {2014},
  volume = {33},
  pages = {169},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tompson et al_2014_Real-time continuous pose recovery of human hands using convolutional networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SNSIAM3C/citation.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X6HTGFUV/citation.html},
  journal = {ACM Transactions on Graphics (TOG)},
  number = {5}
}

@inproceedings{tong_targetaware_2009,
  title = {Target-{{Aware Language Models}} for {{Spoken Language Recognition}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Tong, Rong and Ma, Bin and Li, Haizhou and Chng, Eng Siong and Lee, Kong-Aik},
  year = {2009},
  month = sep,
  pages = {200--203},
  address = {{Brighton, UK}}
}

@article{tong_unified_2010,
  title = {A Unified Probabilistic Framework for Spontaneous Facial Action Modeling and Understanding},
  author = {Tong, Yan and Chen, Jixu and Ji, Qiang},
  year = {2010},
  volume = {32},
  pages = {258--273},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tong et al_2010_A unified probabilistic framework for spontaneous facial action modeling and.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6JM9WMWD/cookiedetectresponse.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {2}
}

@inproceedings{tornay_hmmbased_2019,
  title = {{{HMM}}-Based {{Approaches}} to {{Model Multichannel Information}} in {{Sign Language}} Inspired from {{Articulatory Features}}-Based {{Speech Processing}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Tornay, Sandrine and Razavi, Marzieh and Camgoz, Necati Cihan and Bowden, Richard and {Magimai. -Doss}, Mathew},
  year = {2019},
  month = may,
  pages = {2817--2821},
  address = {{Brighton, UK}},
  abstract = {Tornay, Sandrine; Razavi, Marzieh; Camgoz, Necati Cihan; Bowden, Richard; Magimai.-Doss, Mathew},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tornay et al_2019_HMM-based Approaches to Model Multichannel Information in Sign Language.pdf},
  keywords = {capturedBy=depth,capturedBy=vision,dataset=SmileDSGS,isolated,isolated;vocab=94;signer=30;sl=DSGS;capturedBy=vision;capturedBy=depth;dataset=SmileDSGS,modal=movement,modal=shape,signer=30,sl=DSGS,slreco,slreco;modal=shape;modal=movement,vocab<100,vocab=94,vocab=96},
  language = {en}
}

@incollection{torre_facial_2011,
  title = {Facial {{Expression Analysis}}},
  booktitle = {Visual {{Analysis}} of {{Humans}}},
  author = {la Torre, Fernando De and Cohn, Jeffrey F.},
  editor = {Moeslund, Thomas B. and Hilton, Adrian and Kr{\"u}ger, Volker and Sigal, Leonid},
  year = {2011},
  pages = {377--409},
  publisher = {{Springer London}},
  abstract = {The face is one of the most powerful channels of nonverbal communication. Facial expression provides cues about emotion, intention, alertness, pain, personality, regulates interpersonal behavior, and communicates psychiatric and biomedical status among other functions. Within the past 15 years, there has been increasing interest in automated facial expression analysis within the computer vision and machine learning communities. This chapter reviews fundamental approaches to facial measurement by behavioral scientists and current efforts in automated facial expression recognition. We consider challenges, review databases available to the research community, approaches to feature detection, tracking, and representation, and both supervised and unsupervised learning.},
  copyright = {\textcopyright 2011 Springer-Verlag London Limited},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Torre_Cohn_2011_Facial Expression Analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4QVWHKT3/978-0-85729-997-0_19.html},
  isbn = {978-0-85729-996-3 978-0-85729-997-0},
  language = {en}
}

@inproceedings{toshev_deeppose_2014,
  title = {Deeppose: {{Human}} Pose Estimation via Deep Neural Networks},
  shorttitle = {Deeppose},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Toshev, Alexander and Szegedy, Christian},
  year = {2014},
  pages = {1653--1660},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Toshev_Szegedy_2014_Deeppose.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HH9B3T28/login.html}
}

@inproceedings{tran_closer_2018,
  title = {A {{Closer Look}} at {{Spatiotemporal Convolutions}} for {{Action Recognition}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  year = {2018},
  month = jun,
  pages = {6450--6459},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00675},
  abstract = {In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly gains in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block ``R(2+1)D'' which produces CNNs that achieve results comparable or superior to the state-of-theart on Sports-1M, Kinetics, UCF101, and HMDB51.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U8JYV8EV/Tran et al. - 2018 - A Closer Look at Spatiotemporal Convolutions for A.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@inproceedings{tran_fusion_2010,
  title = {Fusion of Human Posture Features for Continuous Action Recognition},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Tran, Khai and Kakadiaris, Ioannis A. and Shah, Shishir K.},
  year = {2010},
  pages = {244--257},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tran et al_2010_Fusion of human posture features for continuous action recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NNSW45XX/978-3-642-35749-7_19.html}
}

@incollection{tran_fusion_2012,
  title = {Fusion of Human Posture Features for Continuous Action Recognition},
  booktitle = {Trends and {{Topics}} in {{Computer Vision}}},
  author = {Tran, Khai and Kakadiaris, Ioannis A. and Shah, Shishir K.},
  year = {2012},
  pages = {244--257},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tran et al_2012_Fusion of human posture features for continuous action recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X5F6FRHR/978-3-642-35749-7_19.html}
}

@inproceedings{tran_learning_2015,
  title = {Learning {{Spatiotemporal Features}} with {{3D Convolutional Networks}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Tran, D. and Bourdev, L. and Fergus, R. and Torresani, L. and Paluri, M.},
  year = {2015},
  month = dec,
  pages = {4489--4497},
  doi = {10.1109/ICCV.2015.510},
  abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tran et al_2015_Learning Spatiotemporal Features with 3D Convolutional Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EK4N34BF/7410867.html}
}

@article{tran_posefree_,
  title = {Towards {{Pose}}-{{Free Tracking}} of {{Non}}-{{Rigid Face}} Using {{Synthetic Data}}},
  author = {Tran, Ngoc-Trung and Ababsa, Fakhreddine and Charbit, Maurice},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tran et al_Towards Pose-Free Tracking of Non-Rigid Face using Synthetic Data.pdf}
}

@article{triesch_classification_2002,
  title = {Classification of Hand Postures against Complex Backgrounds Using Elastic Graph Matching},
  author = {Triesch, J. and {von der Malsburgb}, C.},
  year = {2002},
  volume = {20},
  pages = {937--943},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Triesch_von der Malsburgb_2002_Classification of hand postures against complex backgrounds using elastic graph.pdf},
  journal = {Image and Vision Computing},
  keywords = {complex backgrounds,graph matching,read 2011-week50}
}

@inproceedings{trmal_feature_2008,
  title = {Feature Space Transforms for Czech Sign-Language Recognition},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Trmal, Jan and Hr{\'u}z, Marek and Zelinka, Jan and Campr, Pavel and M{\"u}ller, Lud{\v e}k},
  year = {2008},
  month = sep,
  pages = {2036--2039},
  address = {{Brisbane, Australia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Trmal et al_2008_Feature space transforms for czech sign-language recognition.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=25;signer=20;sl=CzSL;capturedBy=vision,modal=2hlocation,modal=2hshape,signer=20,sl=CzSL,slreco,slreco;modal=2hlocation;modal=2hshape,vocab<50,vocab=25}
}

@inproceedings{tsironi_gesture_2016,
  title = {Gesture {{Recognition}} with a {{Convolutional Long Short}}-{{Term Memory Recurrent Neural Network}}},
  booktitle = {European {{Symposium}} on {{Artificial Neural Networks}}, {{Computational Intelligence}} and {{Machine Learning}}},
  author = {Tsironi, Eleni and Barros, Pablo and Wermter, Stefan},
  year = {2016},
  address = {{Bruges, Belgium}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tsironi et al_2016_Gesture Recognition with a Convolutional Long Short-Term Memory Recurrent.pdf}
}

@article{tubaiz_glovebased_2015,
  title = {Glove-Based Continuous {{Arabic}} Sign Language Recognition in User-Dependent Mode},
  author = {Tubaiz, Noor and Shanableh, Tamer and Assaleh, Khaled},
  year = {2015},
  volume = {45},
  pages = {526--533},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tubaiz et al_2015_Glove-based continuous Arabic sign language recognition in user-dependent mode.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K9GMPH2R/7061411.html},
  journal = {IEEE Transactions on Human-Machine Systems},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=80;signer=1;sl=ArSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,signer=1,sl=ArSL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<100,vocab=40,vocab=80},
  number = {4}
}

@phdthesis{tuggener_incremental_2016,
  title = {Incremental {{Coreference Resolution}} for {{German}}},
  author = {Tuggener, Don},
  year = {2016},
  doi = {info:doi/10.5167/uzh-124915},
  abstract = {The main contributions of this thesis are as follows:
1. We introduce a general model for coreference and explore its application to German.
\textbullet{} The model features an incremental discourse processing algorithm which allows it to coherently address issues caused by underspecification of mentions, which is an especially pressing problem regarding certain German pronouns.
\textbullet{} We introduce novel features relevant for the resolution of German pronouns. A subset of these features are made accessible through the incremental architecture of the discourse processing model.
\textbullet{} In evaluation, we show that the coreference model combined with our features provides new state-of-the-art results for coreference and pronoun resolution for German.
2. We elaborate on the evaluation of coreference and pronoun resolution.
\textbullet{} We discuss evaluation from the view of prospective downstream applications that benefit from coreference resolution as a preprocessing component. Addressing the shortcomings of the general evaluation framework in this regard, we introduce an alternative framework, the Application Related Coreference Scores (ARCS).
\textbullet{} The ARCS framework enables a thorough comparison of different system outputs and the quantification of their similarities and differences beyond the common coreference evaluation. We demonstrate how the framework is applied to state-of-the-art coreference systems. This provides a method to track specific differences in system outputs, which assists researchers in comparing their approaches to related work in detail.
3. We explore semantics for pronoun resolution.
\textbullet{} Within the introduced coreference model, we explore distributional approaches to estimate the compatibility of an antecedent candidate and the occurrence context of a pronoun. We compare a state-of-the-art approach for word embeddings to syntactic co-occurrence profiles to this end.
\textbullet{} In comparison to related work, we extend the notion of context and thereby increase the applicability of our approach. We find that a combination of both compatibility models, coupled with the coreference model, provides a large potential for improving pronoun resolution performance.
We make available all our resources, including a web demo of the system, at: http://pub.cl.uzh.ch/purl/coreference-resolution},
  copyright = {info:eu-repo/semantics/openAccess},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tuggener_2016_Incremental Coreference Resolution for German.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3L97P37P/124915.html},
  language = {eng},
  school = {University of Zurich},
  type = {Dissertation}
}

@phdthesis{turkmani_visual_2008,
  title = {Visual {{Analysis}} of {{Viseme Dynamics}}},
  author = {Turkmani, A.},
  year = {2008},
  abstract = {Face-to-face dialogue is the most natural mode of communication between humans. The combination of human visual perception of expression and perception in changes in intonation provides semantic information that communicates idea, feelings and concepts. The realistic modelling of speech movements, through automatic facial animation, and maintaining audio-visual coherence is still a challenge in both the computer graphics and film industry.},
  file = {/user/cvsspmisc/m04702/Downloads/turkmani_phd08.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SS4ZV8NR/OrderDetails.html},
  language = {eng},
  school = {University of Surrey},
  type = {Ph.{{D}}.}
}

@article{turner_suicide_2007,
  title = {Suicide in Deaf Populations: A Literature Review},
  shorttitle = {Suicide in Deaf Populations},
  author = {Turner, Oliver and Windfuhr, Kirsten and Kapur, Navneet},
  year = {2007},
  volume = {6},
  pages = {26},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Turner et al_2007_Suicide in deaf populations.pdf},
  journal = {Annals of general psychiatry},
  number = {1}
}

@inproceedings{tuske_data_2014,
  title = {Data {{Augmentation}}, {{Feature Combination}}, and {{Multilingual Neural Networks}} to {{Improve ASR}} and {{KWS Performance}} for {{Low}}-{{Resource Languages}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{Interspeech}})},
  author = {Tuske, Zoltan and Golik, Pavel and Nolden, David and Schluter, Ralf and Ney, Hermann},
  year = {2014},
  pages = {5},
  publisher = {{ISCA}},
  abstract = {This paper presents the progress of acoustic models for lowresourced languages (Assamese, Bengali, Haitian Creole, Lao, Zulu) developed within the second evaluation campaign of the IARPA Babel project. This year, the main focus of the project is put on training high-performing automatic speech recognition (ASR) and keyword search (KWS) systems from language resources limited to about 10 hours of transcribed speech data. Optimizing the structure of Multilayer Perceptron (MLP) based feature extraction and switching from the sigmoid activation function to rectified linear units results in about 5\% relative improvement over baseline MLP features. Further improvements are obtained when the MLPs are trained on multiple feature streams and by exploiting label preserving data augmentation techniques like vocal tract length perturbation. Systematic application of these methods allows to improve the unilingual systems by 4-6\% absolute in WER and 0.064-0.105 absolute in MTWV. Transfer and adaptation of multilingually trained MLPs lead to additional gains, clearly exceeding the project goal of 0.3 MTWV even when only the limited language pack of the target language is used.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tuske et al_Data Augmentation, Feature Combination, and Multilingual Neural Networks to.pdf},
  language = {en}
}

@incollection{tyrone_instrumented_2015,
  title = {Instrumented {{Measures}} of {{Sign Production}} and {{Perception}}},
  booktitle = {Research {{Methods}} in {{Sign Language Studies}}},
  author = {Tyrone, Martha E.},
  year = {2015},
  pages = {89--104},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118346013.ch6},
  abstract = {One of the central goals of linguistic research on sign language is to elucidate the mechanisms of language perception and production. Instrumented studies of sign language are designed to measure such mechanisms precisely and quantitatively. Instrumented techniques and their associated measures can be applied to a broad range of research questions, but they are best suited to studies of adult sign language users. Particular techniques to be reviewed in this chapter include electronic motion capture, data glove systems, video-based motion analysis procedures, eye-tracking systems, and reaction-time paradigms. The motion capture systems that have been used most often for sign language research are optical. In addition to recording movement with a marker-based system, it is possible to record movement during signing by using ordinary video and applying a motion detection algorithm to the video data. The main advantage of instrumented techniques is that they allow precise quantitative measurements.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Tyrone_2015_Instrumented Measures of Sign Production and Perception.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/QECTXTP7/9781118346013.html},
  isbn = {978-1-118-34601-3},
  language = {en}
}

@article{tzirakis_endtoend_2017,
  title = {End-to-{{End Multimodal Emotion Recognition Using Deep Neural Networks}}},
  author = {Tzirakis, Panagiotis and Trigeorgis, George and Nicolaou, Mihalis A. and Schuller, Bjorn W. and Zafeiriou, Stefanos},
  year = {2017},
  month = dec,
  volume = {11},
  pages = {1301--1309},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2017.2764438},
  abstract = {Automatic affect recognition is a challenging task due to the various modalities emotions can be expressed with. Applications can be found in many domains including multimedia retrieval and human\textendash computer interaction. In recent years, deep neural networks have been used with great success in determining emotional states. Inspired by this success, we propose an emotion recognition system using auditory and visual modalities. To capture the emotional content for various styles of speaking, robust features need to be extracted. To this purpose, we utilize a convolutional neural network (CNN) to extract features from the speech, while for the visual modality a deep residual network of 50 layers is used. In addition to the importance of feature extraction, a machine learning algorithm needs also to be insensitive to outliers while being able to model the context. To tackle this problem, long short-term memory networks are utilized. The system is then trained in an end-toend fashion where\textemdash by also taking advantage of the correlations of each of the streams\textemdash we manage to significantly outperform, in terms of concordance correlation coefficient, traditional approaches based on auditory and visual handcrafted features for the prediction of spontaneous and natural emotions on the RECOLA database of the AVEC 2016 research challenge on emotion recognition.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SNFMEPH4/Tzirakis et al. - 2017 - End-to-End Multimodal Emotion Recognition Using De.pdf},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  language = {en},
  number = {8}
}

@inproceedings{uebersax_realtime_2011,
  title = {Real-Time Sign Language Letter and Word Recognition from Depth Data},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision Workshops}} ({{ICCVW}})},
  author = {Uebersax, D. and Gall, J. and den Bergh, M. Van and Gool, L. Van},
  year = {2011},
  month = nov,
  pages = {383--390},
  doi = {10.1109/ICCVW.2011.6130267},
  abstract = {In this work, we present a system for recognizing letters and finger-spelled words of the American sign language (ASL) in real-time. To this end, the system segments the hand and estimates the hand orientation from captured depth data. The letter classification is based on average neighborhood margin maximization and relies on the segmented depth data of the hands. For word recognition, the letter confidences are aggregated. Furthermore, the word recognition is used to improve the letter recognition by updating the training examples of the letter classifiers on-line.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Uebersax et al_2011_Real-time sign language letter and word recognition from depth data.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GL9CUBAI/6130267.html},
  keywords = {capturedBy=vision,isolated,isolated;vocab=26;signer=7;sl=ASL;capturedBy=vision,modal=orientation,modal=shape,signer=7,sl=ASL,slreco,slreco;modal=orientation;modal=shape,vocab<50,vocab=26}
}

@phdthesis{ueffing:phdthesis2006,
  title = {Word {{Confidence Measures}} for {{Machine Translation}}},
  author = {Ueffing, Nicola},
  year = {2006},
  month = mar,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ueffing_2006_Word Confidence Measures for Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{ulasik_ceasr_2020,
  title = {{{CEASR}}: {{A Corpus}} for {{Evaluating Automatic Speech Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Ulasik, Malgorzata Anna and H{\"u}rlimann, Manuela and Germann, Fabian and Gedik, Esin and Benites, Fernando and Cieliebak, Mark},
  year = {2020},
  month = may,
  pages = {9},
  address = {{Marseille, France}},
  abstract = {In this paper, we present CEASR, a Corpus for Evaluating the quality of Automatic Speech Recognition (ASR). It is a data set based on public speech corpora, containing metadata along with transcripts generated by several modern state-of-the-art ASR systems. CEASR provides this data in a unified structure, consistent across all corpora and systems, with normalised transcript texts and metadata.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ulasik et al_2020_CEASR.pdf},
  language = {en}
}

@inproceedings{vaezi_joze_msasl_2019,
  title = {{{MS}}-{{ASL}}: {{A}} Large-Scale Data Set and Benchmark for Understanding American Sign Language},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Vaezi Joze, Hamid and Koller, Oscar},
  year = {2019},
  month = sep,
  address = {{Cardiff, UK}},
  abstract = {Computer Vision has been improved significantly in the past few decades. It has enabled machine to do many human tasks. However, the real challenge is in enabling machine to carry out tasks that an average human does not have the skills for. One such challenge that we have tackled in this paper is providing accessibility for deaf individual by providing means of communication with others with the aid of computer vision. Unlike other frequent works focusing on multiple camera, depth camera, electrical glove or visual gloves, we focused on the sole use of RGB which allows everybody to communicate with a deaf individual through their personal devices. This is not a new approach but the lack of realistic large-scale data set prevented recent computer vision trends on video classification in this filed. In this paper, we propose the first large scale ASL data set that covers over 200 signers, signer independent sets, challenging and unconstrained recording conditions and a large class count of 1000 signs. We evaluate baselines from action recognition techniques on the data set. We propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition. We also propose new pre-trained model more appropriate for sign language recognition. Finally, We estimate the effect of number of classes and number of training samples on the recognition accuracy.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vaezi Joze_Koller_2019_MS-ASL.pdf},
  keywords = {capturedBy=vision,dataSet=MS-ASL,isolated,isolated;vocab=100;signer=189;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=1000;signer=222;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=200;signer=196;sl=ASL;capturedBy=vision;dataSet=MS-ASL,isolated;vocab=500;signer=222;sl=ASL;capturedBy=vision;dataSet=MS-ASL,modal=bodyjoints,modal=fullframe,signer=189,signer=196,signer=222,sl=ASL,slreco,slreco;modal=bodyjoints,slreco;modal=fullframe,vocab<1000,vocab<500,vocab=100,vocab=1000,vocab=200,vocab=500,vocab>=1000}
}

@article{vahdani_recognizing_2020,
  title = {Recognizing {{American Sign Language Nonmanual Signal Grammar Errors}} in {{Continuous Videos}}},
  author = {Vahdani, Elahe and Jing, Longlong and Tian, Yingli and Huenerfauth, Matt},
  year = {2020},
  month = may,
  abstract = {As part of the development of an educational tool that can help students achieve fluency in American Sign Language (ASL) through independent and interactive practice with immediate feedback, this paper introduces a near real-time system to recognize grammatical errors in continuous signing videos without necessarily identifying the entire sequence of signs. Our system automatically recognizes if performance of ASL sentences contains grammatical errors made by ASL students. We first recognize the ASL grammatical elements including both manual gestures and nonmanual signals independently from multiple modalities (i.e. hand gestures, facial expressions, and head movements) by 3D-ResNet networks. Then the temporal boundaries of grammatical elements from different modalities are examined to detect ASL grammatical mistakes by using a sliding window-based approach. We have collected a dataset of continuous sign language, ASL-HW-RGBD, covering different aspects of ASL grammars for training and testing. Our system is able to recognize grammatical elements on ASL-HW-RGBD from manual gestures, facial expressions, and head movements and successfully detect 8 ASL grammatical mistakes.},
  archivePrefix = {arXiv},
  eprint = {2005.00253},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vahdani et al_2020_Recognizing American Sign Language Nonmanual Signal Grammar Errors in.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8MI9YDPA/2005.html},
  journal = {arXiv:2005.00253 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{valstar_motion_2004,
  title = {Motion History for Facial Action Detection in Video},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Valstar, Michel and Pantic, Maja and Patras, Ioannis},
  year = {2004},
  volume = {1},
  pages = {635--640},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Valstar et al_2004_Motion history for facial action detection in video.pdf}
}

@inproceedings{vamplew_recognition_1996,
  title = {Recognition of Sign Language Gestures Using Neural Networks},
  booktitle = {European {{Conf}}. on {{Disabilities}}, {{Virtual Reality}} and {{Associated Technologies}}},
  author = {Vamplew, Peter and Adams, Anthony},
  year = {1996},
  month = jul,
  pages = {27--33},
  address = {{Maidenhead, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vamplew_Adams_1996_Recognition of sign language gestures using neural networks.pdf},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=52;signer=7;sl=Auslan;capturedBy=electronicGlove,modal=movement,modal=orientation,modal=shape,signer=7,sl=Auslan,slreco,slreco;modal=shape;modal=orientation;modal=movement,vocab<100,vocab=52}
}

@inproceedings{vamplew_slarti_1992,
  title = {The {{SLARTI}} System: {{Applying}} Artificial Neural Networks to Sign Language Recognition},
  shorttitle = {The {{SLARTI}} System},
  booktitle = {Proc. of the {{Conf}}. on {{Technology}} and {{Persons}} with {{Disabilities}}},
  author = {Vamplew, P. and Adams, A.},
  year = {1992},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vamplew_Adams_1992_The SLARTI system.pdf}
}

@book{vanderkooij_phonological_2002,
  title = {Phonological Categories in {{Sign Language}} of the {{Netherlands}}: {{The}} Role of Phonetic Implementation and Iconicity},
  shorttitle = {Phonological Categories in {{Sign Language}} of the {{Netherlands}}},
  author = {{Van der Kooij}, Els},
  year = {2002},
  publisher = {{Netherlands Graduate School of Linguistics}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Van der Kooij_2002_Phonological categories in Sign Language of the Netherlands.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NCXHV694/phonological-categories-in-sign-language-of-the-netherlands-phonological-categories-in-sign-lan.html}
}

@misc{vanherreweghe_het_2015,
  title = {Het {{Corpus VGT}}. {{Een}} Digitaal Open Access Corpus van Videos and Annotaties van {{Vlaamse Gebarentaal}}, Ontwikkeld Aan de {{Universiteit Gent}} Ism {{KU Leuven}}.},
  author = {Van Herreweghe, Mieke and Vermeerbergen, Myriam and Demey, Eline and De Durpel, Hannes and Nyffels, Hilde and Verstraete, Sam},
  year = {2015},
  howpublished = {www.corpusvgt.be}
}

@inproceedings{varga_hidden_1990,
  title = {Hidden {{Markov}} Model Decomposition of Speech and Noise},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Varga, A\_P and Moore, R. K.},
  year = {1990},
  month = apr,
  pages = {845--848},
  address = {{Albuquerque, NM, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Varga_Moore_1990_Hidden Markov model decomposition of speech and noise.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CEV74G7Q/115970.html}
}

@article{vaseghi_state_1995,
  title = {State Duration Modelling in Hidden {{Markov}} Models},
  author = {Vaseghi, S. V.},
  year = {1995},
  month = jan,
  volume = {41},
  pages = {31--41},
  issn = {0165-1684},
  doi = {10.1016/0165-1684(94)00088-H},
  abstract = {A new method is proposed for modelling state duration in hidden Markov model (HMM) speech recognition systems. State transition probabilities are expressed conditional on how long the current state has been occupied. The conventional fixed-state transition probabilities aij are replaced by duration-dependent variables aij(d) that depend on the time d already spent in state i. In this way, state transition and state duration probabilities are combined to form duration-dependent transition probabilities. The transition probabilities are derived from the cumulative density function (CDF) of state duration. The training of HMMs with duration-dependent transitions are based on maximum likelihood segmentation of training data, using the Viterbi algorithm. At each training iteration, the current HMM parameters are used to segment every training example. All the segments associated with each state are then used to update state observation and transition parameters. In experiments with a data set of spoken English alphabet, durational modelling improves the recognition accuracy by 5.6\%.
Zusammenfassung
Es wird eine neue Methode zur Modellierung der Zustandsdauer in Spracherkennungssystemen mit Hidden Markov Modellen (HMM) vorgeschlagen. Die Zustands\"ubergangswahrscheinlichkeiten werden von der Bedingung abh\"angig gemacht, wie lange der aktuelle Zustand bereits eingenommen worden ist, Die konventionellen festen Zustands\"ubergangs-wahrscheinlichkeiten aij werden durch dauerabh\"angige Variablen aij (d) ersetzt, die von der Zeit d abh\"angen, die bereits im Zustand i verbracht wurde, Auf diese Weise werden Zustands\"ubergangs- und Zustandsdauerwahrscheinlichkeiten kombiniert, um dauerabh\"angige \"Ubergangswahrscheinlichkeiten zu bilden. Die \"Ubergangswahrscheinlichkeiten werden von der kumulativen Dichtefunktion (CDF) der Zustandsdauer abgeleitet. Das Training der HMMs mit dauerabh\"angigen \"Uberg\"angen beruht auf der Maximum-Likelihood-Segmentierung der Trainingsdaten mit Hilfe des ViterbiAlgorithmus. In jedem Trainingsschritt werden die aktuellen HMM Parameter benutzt, um jeweils alle Trainingsmuster zu segmentieren. Alle Segmente, die mit je einem Zustand verbunden werden, dienen sodann zur Neuberechnung der Beobachtungs- und \"Ubergangsparameter dieses Zustands. Bei Versuchen mit dem gesprochenen englischen Alphabet als Datensatz verbessert die Dauermodellierung die Erkennungsgenauigkeit um 5.6\%.
R\'esum\'e
On propose une nouvelle m\'ethode pour mod\'eliser la dur\'ee d'\'etat dans les syst\`emes de reconnaissance de la parole par mod\`ele de Markov cach\'e (MMC). Les probabilit\'es de transition d'\'etat sont exprim\'ees comme d\'ependantes du temps pendant lequel l'\'etat courant a \'et\'e occup\'e. Les probabilit\'es de transition d'\'etat fixes aij, conventionnelles, sont remplac\'ees par des variables d\'ependant de la dur\'ee aij(d), qui sont fonction du temps d d\'ej\`a pass\'e dans l'\'etati. De cette mani\`ere, les probabilit\'es de transition d'\'etat et de dur\'ee dans un \'etat sont combin\'ees pour former des probabilit\'es de transition d\'ependantes de la dur\'ee. Les probabilit\'es de transition sont d\'eriv\'ees de la fonction de densit\'e cumulative (FDC) de la dur\'ee d'\'etat. L'apprentissage des MMC avec les transitions d\'ependantes de la dur\'ee est bas\'e sur une segmentation au maximum de vraisemblance des donn\'ees d'apprentissage, en utilisant l'algorithme de Viterbi. A chaque it\'eration d'apprentissage, les param\`etres courants du MMC sont utilis\'es pour segmenter chaque exemple d'apprentissage. Tous les segments associ\'es \`a chaque \'etat sont ensuite utilis\'es pour mettre \`a jour les observations d'\'etat et les param\`etres de transition. Dans les experiences faites avec un ensemble de donn\'ees de l'alphabet anglais parl\'e, la mod\'elisation avec dur\'ee ameliore la precision de reconnaissance de 5.6\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vaseghi_1995_State duration modelling in hidden Markov models.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X7GC98MC/016516849400088H.html},
  journal = {Signal Processing},
  number = {1}
}

@inproceedings{vasudevan_introduction_2020,
  title = {Introduction and {{Analysis}} of an {{Event}}-{{Based Sign Language Dataset}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Vasudevan, Ajay and Negri, Pablo and {Linares-Barranco}, Bernabe and {Serrano-Gotarredona}, Teresa},
  year = {2020},
  month = may,
  pages = {441--448},
  address = {{Buenos Aires, Argentina}},
  abstract = {Human gestures recognition is a complex visual recognition task where motion across time distinguishes the type of action. Automatic systems tackle this problem using complex machine learning architectures and training datasets. In recent years, the use and success of robust deep learning techniques was compatible with the availability of a great number of these sets. This paper presents SL-Animals-DVS, an event-based action dataset captured by a Dynamic Vision Sensor (DVS). The DVS records humans performing sign language gestures of various animals as a continuous spike flow at very low latency. This is especially suited for sign language gestures which are usually made at very high speeds. We also benchmark the recognition performance on this data using two state-of-the-art Spiking Neural Networks (SNN) recognition systems. SNNs are naturally compatible to make use of the temporal information that is provided by the DVS where the information is encoded in the spike times. The dataset has about 1100 samples of 58 subjects performing 19 sign language gestures in isolation at different scenarios, providing a challenging evaluation platform for this emerging technology.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IS4UTMPH/Vasudevan et al. - Introduction and Analysis of an Event-Based Sign L.pdf},
  keywords = {capturedBy=dynamicvisionsensor,capturedBy=vision,isolated,isolated;vocab=19;signer=58;sl=LSE;capturedBy=vision;capturedBy=dynamicvisionsensor,modal=fullframe,signer=58,sl=LSE,slreco,slreco;modal=fullframe,vocab<50,vocab=19},
  language = {en}
}

@inproceedings{vaswani_attention_2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
  year = {2017},
  pages = {5998--6008},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vaswani et al_2017_Attention is all you need.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3EPLK6UR/7181-attention-is-all-you-need.html}
}

@article{vedaldi_efficient_2012,
  title = {Efficient Additive Kernels via Explicit Feature Maps},
  author = {Vedaldi, Andrea and Zisserman, Andrew},
  year = {2012},
  volume = {34},
  pages = {480--492},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vedaldi_Zisserman_2012_Efficient additive kernels via explicit feature maps.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GCZXA2RE/login.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {3}
}

@inproceedings{venkataramani_mllr_2001,
  title = {\{\vphantom\}{{MLLR}}\vphantom\{\} {{Adaptation Techniques}} for {{Pronunciation Modeling}}},
  booktitle = {{{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Venkataramani, U. and Byrne, W.},
  year = {2001},
  pages = {421--424},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Venkataramani_Byrne_2001_ MLLR Adaptation Techniques for Pronunciation Modeling.pdf}
}

@article{venugopalan_translating_2014,
  title = {Translating Videos to Natural Language Using Deep Recurrent Neural Networks},
  author = {Venugopalan, Subhashini and Xu, Huijuan and Donahue, Jeff and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate},
  year = {2014},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Venugopalan et al_2014_Translating videos to natural language using deep recurrent neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/S7RB3A48/1412.html},
  journal = {arXiv preprint arXiv:1412.4729}
}

@incollection{verlinden_signing_2002,
  title = {A {{Signing Avatar}} on the {{WWW}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Verlinden, Margriet and Tijsseling, Corrie and Frowein, Han},
  year = {2002},
  pages = {169--172},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Verlinden et al_2002_A Signing Avatar on the WWW.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5D9S7W3G/3-540-47873-6_17.html}
}

@inproceedings{verma_late_1999,
  title = {Late Integration in Audio-Visual Continuous Speech Recognition},
  booktitle = {{{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Verma, Ashish and Faruquie, Tanveer and Neti, Chalapathy and Basu, Sankar and Senior, Andrew},
  year = {1999},
  volume = {1},
  pages = {71--74},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Verma et al_1999_Late integration in audio-visual continuous speech recognition.pdf}
}

@book{vermeerbergen_simultaneity_2007,
  title = {Simultaneity in Signed Languages: Form and Function},
  shorttitle = {Simultaneity in Signed Languages},
  author = {Vermeerbergen, Myriam and Leeson, Lorraine and Crasborn, Onno A.},
  year = {2007},
  volume = {281},
  publisher = {{John Benjamins Publishing}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vermeerbergen et al_2007_Simultaneity in signed languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CRFSS7S7/books.html}
}

@article{vernon_violence_1999,
  title = {Violence in Deaf and Hard-of-Hearing People: {{A}} Review of the Literature},
  shorttitle = {Violence in Deaf and Hard-of-Hearing People},
  author = {Vernon, McCay and Greenberg, Sheldon F},
  year = {1999},
  volume = {4},
  pages = {259--272},
  issn = {1359-1789},
  doi = {10.1016/S1359-1789(97)00058-X},
  abstract = {The few available studies on the relationship of hearing loss to violence indicate that the prevalence of brain damage, learning disability, communication disorders, educational retardation, unemployment, and underemployment in the deaf and hard-of-hearing population creates frustration which tends to manifest in disproportionate aggression, violence, and hostility. Relative to prevalence, hearing-impaired people are overly represented in the prison population, which is the major datum the literature on violence and hearing loss yields. Other than 12 studies and an American Speech and Hearing Association Committee report documenting the fact, there is an amazing paucity of literature on hearing loss and violence. The reasons for this deficit of information are discussed and ways to remediate the problem are suggested.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vernon_Greenberg_1999_Violence in deaf and hard-of-hearing people.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AABEG68G/S135917899700058X.html},
  journal = {Aggression and Violent Behavior},
  number = {3}
}

@inproceedings{vesely_sequencediscriminative_2013,
  title = {Sequence-Discriminative Training of Deep Neural Networks},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Vesely, Karel and Ghoshal, Arnab and Burget, Lukas{\textasciicaron} and Povey, Daniel},
  year = {2013},
  month = aug,
  pages = {5},
  address = {{Lyon, France}},
  abstract = {Sequence-discriminative training of deep neural networks (DNNs) is investigated on a standard 300 hour American English conversational telephone speech task. Different sequencediscriminative criteria \textemdash{} maximum mutual information (MMI), minimum phone error (MPE), state-level minimum Bayes risk (sMBR), and boosted MMI \textemdash{} are compared. Two different heuristics are investigated to improve the performance of the DNNs trained using sequence-based criteria \textemdash{} lattices are regenerated after the first iteration of training; and, for MMI and BMMI, the frames where the numerator and denominator hypotheses are disjoint are removed from the gradient computation. Starting from a competitive DNN baseline trained using cross-entropy, different sequence-discriminative criteria are shown to lower word error rates by 7-9\% relative, on average. Little difference is noticed between the different sequencebased criteria that are investigated. The experiments are done using the open-source Kaldi toolkit, which makes it possible for the wider community to reproduce these results.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vesely et al_2013_Sequence-discriminative training of deep neural networks.pdf},
  language = {en}
}

@incollection{vieira_stop_2012,
  title = {Stop: {{Space}}-Time Occupancy Patterns for 3d Action Recognition from Depth Map Sequences},
  shorttitle = {Stop},
  booktitle = {Progress in {{Pattern Recognition}}, {{Image Analysis}}, {{Computer Vision}}, and {{Applications}}},
  author = {Vieira, Antonio W. and Nascimento, Erickson R. and Oliveira, Gabriel L. and Liu, Zicheng and Campos, Mario FM},
  year = {2012},
  pages = {252--259},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vieira et al_2012_Stop.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U9EQDPHA/978-3-642-33275-3_31.html}
}

@incollection{viitaniemi_detecting_2013,
  title = {Detecting {{Hand}}-{{Head Occlusions}} in {{Sign Language Video}}},
  booktitle = {Image {{Analysis}}},
  author = {Viitaniemi, Ville and Karppa, Matti and Laaksonen, Jorma and Jantunen, Tommi},
  editor = {K{\"a}m{\"a}r{\"a}inen, Joni-Kristian and Koskela, Markus},
  year = {2013},
  pages = {361--372},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {A large body of current linguistic research on sign language is based on analyzing large corpora of video recordings. This requires either manual or automatic annotation of the videos. In this paper we introduce methods for automatically detecting and classifying hand-head occlusions in sign language videos. Linguistically, hand-head occlusions are an important and interesting subject of study as the head is a structural place of articulation in many signs. Our method combines easily calculable local video properties with more global hand tracking. The experiments carried out with videos of the Suvi on-line dictionary of Finnish Sign Language show that the sensitivity of the proposed local method in detecting occlusion events is 92.6\%. When global hand tracking is combined in the method, the specificity can reach the level of 93.7\% while still maintaining the detection sensitivity above 90\%.},
  copyright = {\textcopyright 2013 Springer-Verlag Berlin Heidelberg},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Viitaniemi et al_2013_Detecting Hand-Head Occlusions in Sign Language Video.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GB6XDRXS/10.html},
  isbn = {978-3-642-38885-9 978-3-642-38886-6},
  language = {en},
  number = {7944},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{viitaniemi_spota_2014,
  title = {S-Pot\textendash a Benchmark in Spotting Signs within Continuous Signing},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Viitaniemi, Ville and Jantunen, Tommi and Savolainen, Leena and Karppa, Matti and Laaksonen, Jorma},
  year = {2014},
  month = may,
  pages = {1892--1897},
  address = {{Reykjavik, Iceland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Viitaniemi et al_2014_S-pot–a benchmark in spotting signs within continuous signing.pdf}
}

@phdthesis{vilar_investigations_2011,
  title = {Investigations on {{Hierarchical Phrase}}-Based {{Machine Translation}}},
  author = {Vilar, David},
  year = {2011},
  month = nov,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vilar_2011_Investigations on Hierarchical Phrase-based Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@article{vincent_stacked_2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  shorttitle = {Stacked Denoising Autoencoders},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year = {2010},
  volume = {11},
  pages = {3371--3408},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DQK68JK2/citation.html},
  journal = {The Journal of Machine Learning Research}
}

@article{vintsyuk_elementwise_1971,
  title = {Element-Wise Recognition of Continuous Speech Composed of Words from a Specified Dictionary},
  author = {Vintsyuk, T. K.},
  year = {1971},
  month = mar,
  volume = {7},
  pages = {361--372},
  issn = {1573-8337},
  doi = {10.1007/BF01071812},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vintsyuk_1971_Element-wise recognition of continuous speech composed of words from a.pdf},
  journal = {Cybernetics},
  language = {en},
  number = {2}
}

@article{vintsyuk_speech_1968,
  title = {Speech Discrimination by Dynamic Programming},
  author = {Vintsyuk, T. K.},
  year = {1968},
  month = jan,
  volume = {4},
  pages = {52--57},
  issn = {1573-8337},
  doi = {10.1007/BF01074755},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vintsyuk_1968_Speech discrimination by dynamic programming.pdf},
  journal = {Cybernetics},
  language = {en},
  number = {1}
}

@article{vinyals_show_2014,
  title = {Show and {{Tell}}: {{A Neural Image Caption Generator}}},
  shorttitle = {Show and {{Tell}}},
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  year = {2014},
  month = nov,
  abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1411.4555},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vinyals et al_2014_Show and Tell.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JIQG3NZH/1411.html},
  journal = {arXiv:1411.4555 [cs]},
  primaryClass = {cs}
}

@article{viola_robust_2004,
  title = {Robust {{Real}}-{{Time Face Detection}}},
  author = {Viola, Paul and Jones, Michael J.},
  year = {2004},
  volume = {57},
  pages = {137--154},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000013087.49260.fb},
  abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the ``Integral Image'' which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a ``cascade'' which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
  journal = {International Journal of Computer Vision (IJCV)},
  number = {2}
}

@article{virkkunen_automatic_2018,
  title = {Automatic {{Speech Recognition}} for the {{Hearing Impaired}} in an {{Augmented Reality Application}}},
  author = {Virkkunen, Anja},
  year = {2018},
  month = dec,
  abstract = {People with hearing loss experience considerable difficulties in participating and understanding spoken communication, which has negative effects on many aspects of their life. In many proposed solutions to the problem the deaf or hard of hearing person has to take their attention away from the speaker. As a consequence the hearing impaired miss for instance gestures and expressions of the speaker. 
   
This thesis studied the use of augmented reality and automatic speech recognition technologies in an assistive mobile application for the hearing impaired. The application uses mobile augmented reality with video-based augmentations. Automatic speech recognition is done using modern neural network models. In the implementation, automatic speech recogniser transcriptions were placed in speech bubbles on top of an augmented reality view of the conversation partner. This minimised the distance between the speaker and the transcriptions, which help the hearing impaired follow the conversation. To validate the usefulness of the approach, user tests were organised with hearing impaired participants. 
   
The results show that the deaf and hard of hearing found the augmented reality view and the application helpful for following conversations. The most requested improvements by the user testers were support for visual separation and identification of speakers in group conversations and higher speech recognition accuracy.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Virkkunen_2018_Automatic Speech Recognition for the Hearing Impaired in an Augmented Reality.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WZ2A4K2D/35515.html},
  language = {en}
}

@inproceedings{virkkunen_user_2019,
  title = {A User Study to Compare Two Conversational Assistants Designed for People with Hearing Impairments},
  booktitle = {Proc. {{Workshop}} on {{Speech}} and {{Language Processing}} for {{Assistive Technologies}}},
  author = {Virkkunen, Anja and Lukkarila, Juri and Palom{\"a}ki, Kalle and Kurimo, Mikko},
  year = {2019},
  month = jun,
  pages = {1--8},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/W19-1701},
  abstract = {Participating in conversations can be difficult for people with hearing loss, especially in acoustically challenging environments. We studied the preferences the hearing impaired have for a personal conversation assistant based on automatic speech recognition (ASR) technology. We created two prototypes which were evaluated by hearing impaired test users. This paper qualitatively compares the two based on the feedback obtained from the tests. The first prototype was a proof-of-concept system running real-time ASR on a laptop. The second prototype was developed for a mobile device with the recognizer running on a separate server. In the mobile device, augmented reality (AR) was used to help the hearing impaired observe gestures and lip movements of the speaker simultaneously with the transcriptions. Several testers found the systems useful enough to use in their daily lives, with majority preferring the mobile AR version. The biggest concern of the testers was the accuracy of the transcriptions and the lack of speaker identification.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Virkkunen et al_2019_A user study to compare two conversational assistants designed for people with.pdf}
}

@article{viswanathan_svm_2015,
  title = {{{SVM Based Recognition}} of {{Facial Expressions Used In Indian Sign Language}}},
  author = {Viswanathan, Daleesha M. and Idicula, Sumam Mary},
  year = {2015},
  volume = {9},
  pages = {32},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Viswanathan_Idicula_2015_SVM Based Recognition of Facial Expressions Used In Indian Sign Language.pdf},
  journal = {International Journal of Image Processing (IJIP)},
  number = {1}
}

@article{viterbi_error_1967,
  title = {Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm},
  author = {Viterbi, A.},
  year = {1967},
  month = apr,
  volume = {13},
  pages = {260--269},
  issn = {0018-9448},
  doi = {10.1109/TIT.1967.1054010},
  abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above{$<$}tex{$>$}R\_0{$<$}/tex{$>$}, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above{$<$}tex{$>$}R\_0{$<$}/tex{$>$}and whose performance bears certain similarities to that of sequential decoding algorithms.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Viterbi_1967_Error bounds for convolutional codes and an asymptotically optimum decoding.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Q2YHC6HJ/1054010.html},
  journal = {IEEE Transactions on Information Theory},
  number = {2}
}

@phdthesis{vogel:smtdiss05,
  title = {Statistical {{Machine Translation}} with {{Cascaded Probabilistic Transducers}}},
  author = {Vogel, S.},
  year = {2005},
  month = dec,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogel_2005_Statistical Machine Translation with Cascaded Probabilistic Transducers.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{vogler_adapting_1997,
  title = {Adapting Hidden {{Markov}} Models for {{ASL}} Recognition by Using Three-Dimensional Computer Vision Methods},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Vogler, C. and Metaxas, D.},
  year = {1997},
  month = oct,
  pages = {156--161},
  address = {{Orlando, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogler_Metaxas_1997_Adapting hidden Markov models for ASL recognition by using three-dimensional.pdf},
  keywords = {capturedBy=mocap,capturedBy=vision,continuous,continuous;vocab=53;sl=ASL;capturedBy=vision;capturedBy=mocap,modal=location,modal=movement,modal=orientation,sl=ASL,slreco,slreco;modal=movement;modal=orientation;modal=location,vocab<100,vocab=53}
}

@inproceedings{vogler_best_2007,
  title = {The {{Best}} of {{Both Worlds}}: {{Combining 3D Deformable Models}} with {{Active Shape Models}}},
  shorttitle = {The {{Best}} of {{Both Worlds}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Vogler, C. and Li, Z. and Kanaujia, A. and Goldenstein, S. and Metaxas, D.},
  year = {2007},
  month = oct,
  pages = {1--7},
  doi = {10.1109/ICCV.2007.4409015},
  abstract = {Reliable 3D tracking is still a difficult task. Most parametrized 3D deformable models rely on the accurate extraction of image features for updating their parameters, and are prone to failures when the underlying feature distribution assumptions are invalid. Active Shape Models (ASMs), on the other hand, are based on learning, and thus require fewer reliable local image features than parametrized 3D models, but fail easily when they encounter a situation for which they were not trained. In this paper, we develop an integrated framework that combines the strengths of both 3D deformable models and ASMs. The 3D model governs the overall shape, orientation and location, and provides the basis for statistical inference on both the image features and the parameters. The ASMs, in contrast, provide the majority of reliable 2D image features over time, and aid in recovering from drift and total occlusions. The framework dynamically selects among different ASMs to compensate for large viewpoint changes due to head rotations. This integration allows the robust tracking effaces and the estimation of both their rigid and non- rigid motions. We demonstrate the strength of the framework in experiments that include automated 3D model fitting and facial expression tracking for a variety of applications, including sign language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogler et al_2007_The Best of Both Worlds.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ERY6C7E6/4409015.html}
}

@article{vogler_facial_2008,
  title = {Facial Movement Analysis in {{ASL}}},
  author = {Vogler, Christian and Goldenstein, Siome},
  year = {2008},
  volume = {6},
  pages = {363--374},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogler_Goldenstein_2008_Facial movement analysis in ASL.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7UE7BIVK/s10209-007-0096-6.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AWZG4D3X/10.html},
  journal = {Universal Access in the Information Society},
  number = {4}
}

@article{vogler_framework_2001,
  title = {A Framework for Recognizing the Simultaneous Aspects of American Sign Language},
  author = {Vogler, Christian and Metaxas, Dimitris},
  year = {2001},
  volume = {81},
  pages = {358--384},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogler_Metaxas_2001_A framework for recognizing the simultaneous aspects of american sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VKDT6CUB/S1077314200908956.html},
  journal = {Computer Vision and Image Understanding (CVIU)},
  keywords = {capturedBy=mocap,continuous,continuous;vocab=22;sl=ASL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=22},
  number = {3}
}

@inproceedings{vogler_handshapes_2004,
  title = {Handshapes and {{Movements}}: {{Multiple}}-{{Channel American Sign Language Recognition}}},
  shorttitle = {Handshapes and {{Movements}}},
  booktitle = {Gesture-{{Based Communication}} in {{Human}}-{{Computer Interaction}}},
  author = {Vogler, Christian and Metaxas, Dimitris},
  editor = {Camurri, Antonio and Volpe, Gualtiero},
  year = {2004},
  month = apr,
  volume = {2915},
  pages = {247--258},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24598-8_23},
  abstract = {In this paper we present a framework for recognizing American Sign Language (ASL). The main challenges in developing scalable recognition systems are to devise the basic building blocks from which to build up the signs, and to handle simultaneous events, such as signs where both the hand moves and the handshape changes. The latter challenge is particularly thorny, because a naive approach to handling them can quickly result in a combinatorial explosion.We loosely follow the Movement-Hold model to devise a breakdown of the signs into their constituent phonemes, which provide the fundamental building blocks. We also show how to integrate the handshape into this breakdown, and discuss what handshape representation works best. To handle simultaneous events, we split up the signs into a number of channels that are independent from one another. We validate our framework in experiments with a 22-sign vocabulary and up to three channels.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogler_Metaxas_2004_Handshapes and Movements.pdf},
  isbn = {978-3-540-24598-8},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=22;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hmovement,modal=shape,sl=ASL,slreco,slreco;modal=2hmovement;modal=shape,vocab<50,vocab=22},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{vogler_parallel_1999,
  title = {Parallel Hidden Markov Models for American Sign Language Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Vogler, Christian and Metaxas, Dimitris},
  year = {1999},
  month = sep,
  volume = {1},
  pages = {116--122},
  address = {{Corfu, Greece}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogler_Metaxas_1999_Parallel hidden markov models for american sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SXTNDPKU/abs_all.html},
  keywords = {capturedBy=mocap,continuous,continuous;vocab=22;sl=ASL;capturedBy=mocap,modal=2hlocation,modal=2hmovement,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<50,vocab=22}
}

@inproceedings{vogler_scalability_1999,
  title = {Toward {{Scalability}} in {{ASL Recognition}}: {{Breaking Down Signs}} into {{Phonemes}}},
  shorttitle = {Toward {{Scalability}} in {{ASL Recognition}}},
  booktitle = {Gesture-{{Based Communication}} in {{Human}}-{{Computer Interaction}}},
  author = {Vogler, Christian and Metaxas, Dimitris},
  year = {1999},
  month = mar,
  pages = {211--224},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-46616-9_19},
  abstract = {In this paper we present a novel approach to continuous, whole-sentence ASL recognition that uses phonemes instead of whole signs as the basic units. Our approach is based on a sequential phonological model of ASL. According to this model the ASL signs can be broken into movements and holds, which are both considered phonemes. This model does away with the distinction between whole signs and epenthesis movements that we made in previous work [17]. Instead, epenthesis movements are just like the other movements that constitute the signs.We subsequently train Hidden Markov Models (HMMs) to recognize the phonemes, instead of whole signs and epenthesis movements that we recognized previously [17]. Because the number of phonemes is limited, HMM-based training and recognition of the ASL signal becomes computationally more tractable and has the potential to lead to the recognition of large-scale vocabularies.We experimented with a 22 word vocabulary, and we achieved similar recognition rates with phoneme-and word-based approaches. This result is very promising for scaling the task in the future.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vogler_Metaxas_1999_Toward Scalability in ASL Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5BFVQ5M4/3-540-46616-9_19.html},
  isbn = {978-3-540-66935-7 978-3-540-46616-1},
  keywords = {capturedBy=mocap,continuous,continuous;vocab=22;sl=ASL;capturedBy=mocap,modal=location,modal=movement,sl=ASL,slreco,slreco;modal=location;modal=movement,vocab<50,vocab=22},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{voigtlaender_sequencediscriminative_2015,
  title = {Sequence-Discriminative Training of Recurrent Neural Networks},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Voigtlaender, Paul and Doetsch, Patrick and Wiesler, Simon and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2015},
  pages = {2100--2104},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Voigtlaender et al_2015_Sequence-discriminative training of recurrent neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BM8ZSI86/7178341.html}
}

@article{von_agris_recent_2008,
  title = {Recent Developments in Visual Sign Language Recognition},
  author = {{von Agris}, U. and Zieren, J. and Canzler, U. and Bauer, B. and Kraiss, K. F},
  year = {2008},
  volume = {6},
  pages = {323--362},
  issn = {1615-5289},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/von Agris et al_2008_Recent developments in visual sign language recognition.pdf},
  journal = {Universal Access in the Information Society},
  keywords = {capturedBy=coloredGlove,capturedBy=vision,continuous,continuous;vocab=152;sl=DGS;capturedBy=coloredGlove,isolated,isolated;vocab=229;signer=4;sl=BSL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=head,modal=mouth,signer=4,sl=BSL,sl=DGS,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=head;modal=mouth,vocab<500,vocab=152,vocab=229},
  number = {4}
}

@inproceedings{von_agris_significance_2008,
  title = {The Significance of Facial Features for Automatic Sign Language Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {{von Agris}, Ulrich and Knorr, Moritz and Kraiss, K.-F.},
  year = {2008},
  month = sep,
  pages = {1--6},
  address = {{Amsterdam, The Netherlands}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/von Agris et al_2008_The significance of facial features for automatic sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PVNUF6B4/abs_all.html},
  keywords = {capturedBy=vision,continuous,continuous;vocab=450;signer=25;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Signum,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=head,modal=mouth,signer=25,sl=DGS,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=head;modal=mouth,vocab<500,vocab=450}
}

@inproceedings{von_agris_video_2007,
  title = {Towards a {{Video Corpus}} for {{Signer}}-{{Independent Continuous Sign Language Recognition}}},
  booktitle = {{{GW}} 2007 {{The}} 7th {{International Workshop}} on {{Gesture}} in {{Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {{von Agris}, U. and Kraiss, K.-F.},
  editor = {Dias, Sales and {Jota}},
  year = {2007},
  month = may,
  pages = {10--11},
  address = {{Lisbon, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/von Agris_Kraiss_2007_Towards a Video Corpus for Signer-Independent Continuous Sign Language.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=450;signer=5;sl=DGS;capturedBy=vision;dataSet=Signum,dataSet=Signum,modal=shape,signer=20,signer=5,sl=DGS,slreco,slreco;modal=shape,vocab<500,vocab=450}
}

@inproceedings{vowels_nestedvae_2020,
  title = {{{NestedVAE}}: {{Isolating Common Factors}} via {{Weak Supervision}}},
  shorttitle = {{{NestedVAE}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Vowels, Matthew J. and Camgoz, Necati Cihan and Bowden, Richard},
  year = {2020},
  pages = {9202--9212},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Vowels et al_2020_NestedVAE.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/95XLWPC6/Vowels_NestedVAE_Isolating_Common_Factors_via_Weak_Supervision_CVPR_2020_paper.html}
}

@article{waldron_isolated_1995,
  title = {Isolated {{ASL}} Sign Recognition System for Deaf Persons},
  author = {Waldron, M. B. and Kim, Soowon},
  year = {1995},
  month = sep,
  volume = {3},
  pages = {261--271},
  issn = {1063-6528},
  doi = {10.1109/86.413199},
  abstract = {The design and evaluation of a two-stage neural network which can recognize isolated ASL signs is given. The input to this network is the hand shape and position data obtained from a DataGlove mounted with a Polhemus sensor. The first level consists of four backpropagation neural networks which can recognize the sign language phonology, namely, the 36 hand shapes, 10 locations, 11 orientations, and 11 hand movements. The recognized phonemes from the beginning, middle, and end of the sign are fed to the second stage which recognizes the actual signs. Both backpropagation and Kohonen's self-organizing neural work was used to compare the performance and the expandability of the learned vocabulary. In the current work, six signers with differing hand sizes signed 14 signs which included hand shape, position, and motion fragile and triple robust signs. When a backpropagation network was used for the second stage, the results show that the network was able to recognize these signs with an overall accuracy of 86\%. Further, the recognition results were linearly dependent on the size of the finger in relation to the metacarpophalangeal joint and the total length of the hand. When the second stage was a Kohonen's self-organizing network, the network could not only recognize the signs with 84\% accuracy, but also expand its learned vocabulary through relabeling},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Waldron_Kim_1995_Isolated ASL sign recognition system for deaf persons.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BT2P8GR9/413199.html},
  journal = {IEEE Transactions on Rehabilitation Engineering},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=14;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=location,modal=movement,modal=orientation,modal=shape,sl=ASL,slreco,slreco;modal=location;modal=orientation;modal=shape;modal=movement,vocab<50,vocab=14},
  number = {3}
}

@article{wan_explore_2015,
  title = {Explore {{Efficient Local Features}} from {{RGB}}-{{D Data}} for {{One}}-Shot {{Learning Gesture Recognition}}},
  author = {Wan, Jun and Guo, Guodong and Li, Stan},
  year = {2015},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/E5TZFZJQ/login.html}
}

@inproceedings{wan_photorealistic_2013,
  title = {Photo-Realistic Expressive Text to Talking Head Synthesis.},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Wan, Vincent and Anderson, Robert and Blokland, Art and Braunschweiler, Norbert and Chen, Langzhou and Kolluru, BalaKrishna and Latorre, Javier and Maia, Ranniery and Stenger, Bj{\"o}rn and Yanagisawa, Kayoko and others},
  year = {2013},
  pages = {2667--2669},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wan et al_2013_Photo-realistic expressive text to talking head synthesis.pdf}
}

@article{wang_american_2006,
  title = {American {{Sign Language Recognition Using Multi}}-Dimensional {{Hidden Markov Models}}.},
  author = {Wang, Honggang and Leu, Ming C. and Oz, Cemil},
  year = {2006},
  volume = {22},
  pages = {1109--1123},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2006/Wang et al/Wang et al_2006_American Sign Language Recognition Using Multi-dimensional Hidden Markov Models.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2006_American Sign Language Recognition Using Multi-dimensional Hidden Markov Models.pdf},
  journal = {Journal of Information Science and Engineering},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=26;sl=ASL;capturedBy=electronicGlove;capturedBy=mocap,modal=location,modal=movement,modal=orientation,modal=shape,sl=ASL,slreco,slreco;modal=location;modal=movement;modal=shape;modal=orientation,vocab<50,vocab=26},
  number = {5}
}

@inproceedings{wang_approach_2002,
  title = {An Approach Based on Phonemes to Large Vocabulary {{Chinese}} Sign Language Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Wang, Chunli and Gao, Wen and Shan, Shiguang},
  year = {2002},
  month = may,
  pages = {411--416},
  doi = {10.1109/AFGR.2002.1004188},
  abstract = {Hitherto, the major challenge to sign language recognition is how to develop approaches that scale well with increasing vocabulary size. We present an approach to large vocabulary, continuous Chinese sign language (CSL) recognition that uses phonemes instead of whole signs as the basic units. Since the number of phonemes is limited, HMM-based training and recognition of the CSL signal becomes more tractable and has the potential to recognize enlarged vocabularies. Furthermore, the proposed method facilitates the CSL recognition when the finger-alphabet is blended with gestures. About 2400 phonemes are defined for CSL. One HMM is built for each phoneme, and then the signs are encoded based on these phonemes. A decoder that uses a tree-structured network is presented. Clustering of the Gaussians on the states, the language model and N-best-pass is used to improve the performance of the system. Experiments on a 5119 sign vocabulary are carried out, and the result is exciting.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2002_An approach based on phonemes to large vocabulary Chinese sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JUGSRUIJ/abs_all.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,continuous,continuous;vocab=5119;signer=1;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,isolated,isolated;vocab=5119;signer=1;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2horientation,modal=2hshape,signer=1,sl=CSL,slreco,slreco;modal=2hshape;modal=2hlocation;modal=2horientation,vocab=5119,vocab>=1000}
}

@inproceedings{wang_bidirectional_2019,
  title = {A {{Bi}}-Directional {{Interactive System}} of {{Sign Language}} and {{Visual Speech Based}} on {{Portable Devices}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Robotics}} and {{Biomimetics}} ({{ROBIO}})},
  author = {Wang, Fei and Sun, Shizhuo and Liu, Yanjun},
  year = {2019},
  month = dec,
  pages = {1071--1076},
  issn = {null},
  doi = {10.1109/ROBIO49542.2019.8961831},
  abstract = {At present, the natural communication between deaf and normal people is a major challenge both in theoretical research and application. In addition, in the field of Human-Machine Interaction, there is little work on bi-directional communication based on sign language and visual speech. In this paper, a portable bi-directional interactive system of sign language and visual speech was proposed to help deaf people communicate naturally under certain circumstances. In the section of modeling, Network in Network model was employed to classify sign language words, while the method using a network combined DenseNet and LSTM together was used for lip reading. By wearing the portable device, people can achieve bi-directional translation and communication of visual speech and sign language. We built our own Chinese sign language database containing more than 100 categories of words and conducted experiments in the context of the airport for experimental verification. Eventually, the average time of each round of conversation is within 15 seconds, faster than normal communication without system assistance, which verified the effectiveness and advantage of the purposed system.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2019_A Bi-directional Interactive System of Sign Language and Visual Speech Based on.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/22VCSIM6/8961831.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,Chinese Sign Language Recognition,DenseNet,isolated,isolated;vocab=138;signer=70;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,LSTM,modal=location,modal=movement,modal=orientation,modal=shape,Network in Network,signer=70,sl=CSL,slreco,slreco;modal=location;modal=movement;modal=shape;modal=orientation,Visual Speech Recognition,vocab<500,vocab=138}
}

@inproceedings{wang_connectionist_2018,
  ids = {wang\_connectionist\_2018},
  title = {Connectionist {{Temporal Fusion}} for {{Sign Language Translation}}},
  booktitle = {Proc. {{ACM Int}}. {{Conf}}. on {{Multimedia}}},
  author = {Wang, Shuo and Guo, Dan and Zhou, Wen-gang and Zha, Zheng-Jun and Wang, Meng},
  year = {2018},
  month = oct,
  pages = {1483--1491},
  address = {{Seoul, Republic of Korea}},
  doi = {10.1145/3240508.3240671},
  abstract = {Continuous sign language translation (CSLT) is a weakly supervised problem aiming at translating vision-based videos into natural languages under complicated sign linguistics, where the ordered words in a sentence label have no exact boundary of each sign action in the video. This paper proposes a hybrid deep architecture which consists of a temporal convolution module (TCOV), a bidirectional gated recurrent unit module (BGRU), and a fusion layer module (FL) to address the CSLT problem. TCOV captures short-term temporal transition on adjacent clip features (local pattern), while BGRU keeps the long-term context transition across temporal dimension (global pattern). FL concatenates the feature embedding of TCOV and BGRU to learn their complementary relationship (mutual pattern). Thus we propose a joint connectionist temporal fusion (CTF) mechanism to utilize the merit of each module. The proposed joint CTC loss optimization and deep classification score-based decoding fusion strategy are designed to boost performance. With only once training, our model under the CTC constraints achieves comparable performance to other existing methods with multiple EM iterations. Experiments are tested and verified on a benchmark, i.e. the RWTH-PHOENIX-Weather dataset, which demonstrate the effectiveness of our proposed method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2018_Connectionist Temporal Fusion for Sign Language Translation.pdf},
  isbn = {978-1-4503-5665-7},
  keywords = {augment=?,authorgroup=Hefei Tech,authorgroup=USTC,bgru,capturedBy=vision,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,ctc,dataSet=CSL,dataSet=Phoenix14,loss=CTC,modal=fullframe,nn=1dcnn,nn=3dcnn,nn=bgru,nnarch=c3d-resnet,nntrain=3dcnn+1dcnn+bgru,paperinfo,paperinfo;authorgroup=Hefei Tech;authorgroup=USTC;shorttitle=Temporal Fusion,shorttitle=Temporal Fusion,sign language translation,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;dataSet=CSL;nn=3dcnn;nn=1dcnn;nn=bgru;nnarch=c3d-resnet;nntrain=3dcnn+1dcnn+bgru;loss=CTC;augment=?,slreco;modal=fullframe;dataSet=Phoenix14;wer=37.9/37.8;nn=3dcnn;nn=1dcnn;nn=bgru;nnarch=c3d-resnet;nntrain=3dcnn+1dcnn+bgru;loss=CTC;augment=?,temporal cov,vocab<500,vocab=1080,vocab=178,vocab>=1000,wer=37.9/37.8},
  series = {{{MM}} '18}
}

@inproceedings{wang_expanding_2006,
  title = {Expanding Training Set for Chinese Sign Language Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Wang, Chunli and Chen, Xilin and Gao, Wen},
  year = {2006},
  month = apr,
  pages = {323--328},
  address = {{Southampton, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2006_Expanding training set for chinese sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TR4RAKZU/cookiedetectresponse.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZZDEGFC2/abs_all.html},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=2435;signer=1;sl=CSL;capturedBy=electronicGlove,modal=2hlocation,modal=movement,modal=orientation,modal=shape,signer=1,sl=CSL,slreco,slreco;modal=2hlocation;modal=shape;modal=movement;modal=orientation,vocab=2435,vocab>=1000}
}

@inproceedings{wang_fast_2000,
  title = {A {{Fast Sign Word Recognition Method}} for {{Chinese Sign Language}}},
  booktitle = {Advances in {{Multimodal Interfaces}} \textemdash{} {{ICMI}} 2000},
  author = {Wang, Jiangqin and Gao, Wen},
  editor = {Tan, Tieniu and Shi, Yuanchun and Gao, Wen},
  year = {2000},
  month = oct,
  pages = {599--606},
  address = {{Beijing, China}},
  abstract = {Sign language is the language used by the deaf, which is a comparatively steadier expressive system composed of signs corresponding to postures and motions assisted by facial expression. The objective of sign language recognition research is to ``see'' the language of deaf. The integration of sign language recognition and sign language synthesis jointly comprise a ``human-computer sign language interpreter'', which facilitates the interaction between deaf and their surroundings. Considering the speed and performance of the recognition system, Cyberglove is selected as gesture input device in our sign language recognition system, Semi-Continuous Dynamic Gaussian Mixture Model (SCDGMM) is used as recognition technique, and a search scheme based on relative entropy is proposed and is applied to SCDGMM- based sign word recognition. Comparing with SCDGMM recognizer without searching scheme, the recognition time of SCDGMM recognizer with searching scheme reduces almost 15 times.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang_Gao_2000_A Fast Sign Word Recognition Method for Chinese Sign Language.pdf},
  isbn = {978-3-540-40063-9},
  keywords = {capturedBy=electronicGlove,isolated,isolated;vocab=274;signer=1;sl=CSL;capturedBy=electronicGlove,modal=2hshape,signer=1,sl=CSL,slreco,slreco;modal=2hshape,vocab<500,vocab=274},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wang_fast_2015,
  title = {Fast Sign Language Recognition Benefited from Low Rank Approximation},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Wang, Hanjie and Chai, Xiujuan and Zhou, Yu and Chen, Xilin},
  year = {2015},
  month = may,
  volume = {1},
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Ljubljana, Slovenia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2015_Fast sign language recognition benefited from low rank approximation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2GGSIRHN/7163092.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=1000;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=1000;signer=7;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=370;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hshape,modal=bodyjoints,signer=1,signer=7,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=bodyjoints,vocab<500,vocab=1000,vocab=370,vocab>=1000}
}

@article{wang_framelevel_2017,
  title = {Beyond {{Frame}}-Level {{CNN}}: {{Saliency}}-{{Aware}} 3-{{D CNN With LSTM}} for {{Video Action Recognition}}},
  shorttitle = {Beyond {{Frame}}-Level {{CNN}}},
  author = {Wang, X. and Gao, L. and Song, J. and Shen, H.},
  year = {2017},
  month = apr,
  volume = {24},
  pages = {510--514},
  issn = {1070-9908},
  doi = {10.1109/LSP.2016.2611485},
  abstract = {Human activity recognition in videos with convolutional neural network (CNN) features has received increasing attention in multimedia understanding. Taking videos as a sequence of frames, a new record was recently set on several benchmark datasets by feeding frame-level CNN sequence features to long short-term memory (LSTM) model for video activity recognition. This recurrent model-based visual recognition pipeline is a natural choice for perceptual problems with time-varying visual input or sequential outputs. However, the above-mentioned pipeline takes frame-level CNN sequence features as input for LSTM, which may fail to capture the rich motion information from adjacent frames or maybe multiple clips. Furthermore, an activity is conducted by a subject or multiple subjects. It is important to consider attention that allows for salient features, instead of mapping an entire frame into a static representation. To tackle these issues, we propose a novel pipeline, saliency-aware three-dimensional (3-D) CNN with LSTM, for video action recognition by integrating LSTM with salient-aware deep 3-D CNN features on videos shots. Specifically, we first apply saliency-aware methods to generate saliency-aware videos. Then, we design an end-to-end pipeline by integrating 3-D CNN with LSTM, followed by a time series pooling layer and a softmax layer to predict the activities. Noticeably, we set a new record on two benchmark datasets, i.e., UCF101 with 13 320 videos and HMDB-51 with 6766 videos. Our method outperforms the state-of-the-art end-to-end methods of action recognition by 3.8\% and 3.2\%, respectively on above two datasets.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2017_Beyond Frame-level CNN.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2NKTDB74/7572183.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3N6XYJKX/7572183.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BI2J5VDK/7572183.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/E9XURRN7/7572183.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/K8H26IMA/7572183.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/V25GURJA/7572183.html},
  journal = {IEEE Signal Processing Letters},
  keywords = {convolutional neural network features,frame-level CNN sequence features,HMDB-51 dataset,human activity recognition,long short-term memory model,recurrent model-based visual recognition pipeline,saliency-aware,saliency-aware three-dimensional CNN,salient-aware deep 3D CNN features,soft-max layer,static representation,three-dimensional (3-D) convolution,time series pooling layer,video action recognition,video activity recognition,video frame sequence,video shots},
  number = {4}
}

@inproceedings{wang_nonlocal_2018,
  title = {Non-Local Neural Networks},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  year = {2018},
  pages = {7794--7803},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2018_Non-local neural networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CECYFZX7/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html}
}

@article{wang_posed_2015,
  title = {Posed and Spontaneous Expression Recognition through Modeling Their Spatial Patterns},
  author = {Wang, Shangfei and Wu, Chongliang and He, Menghua and Wang, Jun and Ji, Qiang},
  year = {2015},
  month = jan,
  pages = {1--13},
  issn = {0932-8092, 1432-1769},
  doi = {10.1007/s00138-015-0657-2},
  abstract = {This paper presents a new method to recognize posed and spontaneous expressions through modeling their spatial patterns. Gender and expression categories are employed as privileged information to further improve the recognition. The proposed approach includes three steps. First, geometric features about facial shape and Action Unit variations are extracted from the differences between apex and onset facial images to capture the spatial facial variation. Second, statistical hypothesis testings are conducted to explore the differences between posed and spontaneous expressions using the defined geometric features from three aspects: all samples, samples given the gender information, and samples given expression categories. Third, several Bayesian networks are built to capture posed and spontaneous spatial facial patterns respectively given gender and expression categories. The statistical analysis results on the USTC-NVIE and SPOS databases both demonstrate the effectiveness of the proposed geometric features. The recognition results on the USTC-NVIE database indicate that the privileged information of gender and expression can help model the spatial patterns caused by posed and spontaneous expressions. The recognition results on both databases outperform those of the state of the art.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2015_Posed and spontaneous expression recognition through modeling their spatial.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/743GA2JU/10.html},
  journal = {Machine Vision and Applications},
  language = {en}
}

@inproceedings{wang_real-time_2004,
  title = {A Real-Time Automatic Lipreading System},
  booktitle = {Circuits and {{Systems}}, 2004. {{ISCAS}}'04. {{Proceedings}} of the 2004 {{International Symposium}} On},
  author = {Wang, Shi-Lin and Lau, Wing Hong and Leung, Shu Hung and Yan, H.},
  year = {2004},
  volume = {2},
  pages = {II--101},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KAB6RIZX/login.html}
}

@inproceedings{wang_realtime_2001,
  title = {A {{Real}}-{{Time Large Vocabulary Recognition System}} for {{Chinese Sign Language}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Wang, Chunli and Gao, Wen and Ma, Jiyong},
  year = {2001},
  month = apr,
  pages = {86--95},
  doi = {10.1007/3-540-47873-6_9},
  abstract = {The major challenge that faces Sign Language recognition now is to develop methods that will scale well with increasing vocabulary size. In this paper, a real-time system designed for recognizing Chinese Sign Language (CSL) signs with a 5100 sign vocabulary is presented. The raw data are collected from two CyberGlove and a 3-D tracker. An algorithm based on geometrical analysis for purpose of extracting invariant feature to signer position is proposed. Then the worked data are presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, some useful new ideas are proposed in design and implementation, including modifying the transferring probability, clustering the Gaussians and fast matching algorithm. Experiments show that techniques proposed in this paper are efficient on either recognition speed or recognition performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Chunli et al_2001_A Real-Time Large Vocabulary Recognition System for Chinese Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3STJRT7Q/3-540-47873-6_9.html},
  isbn = {978-3-540-43678-2 978-3-540-47873-7},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=5100;signer=1;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=1,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape,vocab=5100,vocab>=1000},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wang_similarity_2009,
  title = {A {{Similarity Measure}} for {{Vision}}-{{Based Sign Recognition}}},
  booktitle = {Universal {{Access}} in {{Human}}-{{Computer Interaction}}. {{Applications}} and {{Services}}},
  author = {Wang, Haijing and Stefan, Alexandra and Athitsos, Vassilis},
  editor = {Stephanidis, Constantine},
  year = {2009},
  pages = {607--616},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {When we encounter an English word that we do not understand, we can look it up in a dictionary. However, when an American Sign Language (ASL) user encounters an unknown sign, looking up the meaning of that sign is not a straightforward process. It has been recently proposed that this problem can be addressed using a computer vision system that helps users look up the meaning of a sign. In that approach, sign lookup can be treated as a video database retrieval problem. When the user encounters an unknown sign, the user provides a video example of that sign as a query, so as to retrieve the most similar signs in the database. A necessary component of such a sign lookup system is a similarity measure for comparing sign videos. Given a query video of a specific sign, the similarity measure should assign high similarity values to videos from the same sign, and low similarity values to videos from other signs. This paper evaluates a state-of-the-art video-based similarity measure called Dynamic Space-Time Warping (DSTW) for the purposes of sign retrieval. The paper also discusses how to specifically adapt DSTW so as to tolerate differences in translation and scale.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2009_A Similarity Measure for Vision-Based Sign Recognition.pdf},
  isbn = {978-3-642-02713-0},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wang_simultaneous_2009,
  title = {Simultaneous Image Classification and Annotation},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Chong and Blei, David and Li, Fei-Fei},
  year = {2009},
  month = jun,
  pages = {1903--1910},
  address = {{Miami, FL, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2009_Simultaneous image classification and annotation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6SM4JJGV/abs_all.html}
}

@article{wang_sparse_2016,
  title = {Sparse {{Observation}} ({{SO}}) {{Alignment}} for {{Sign Language Recognition}}},
  author = {Wang, Hanjie and Chai, Xiujuan and Chen, Xilin},
  year = {2016},
  month = jan,
  volume = {175},
  pages = {674--685},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2015.10.112},
  abstract = {In this paper, we propose a method for robust Sign Language Recognition from RGB-D data. A Sparse Observation (SO) description is proposed to character each sign in terms of the typical hand postures. Concretely speaking, the SOs are generated by considering the typical posture fragments, where hand motions are relatively slow and hand shapes are stable. Thus the matching between two sign words is converted to measure the similarity computing between two aligned SO sequences. The alignment is formulated as a variation of Stable Marriage Problem (SMP). The classical ``propose-engage'' idea is extended to get the order preserving matched SO pairs. In the training stage, the multiple instances from one sign are fused to generate single SO template. In the recognition stage, SOs of each probe sign ``propose'' to SOs of the templates for the purpose of reasonable similarity computing. To further speed up the SO alignment, hand posture relationship map is constructed as a strong prior to generate the distinguished low-dimensional feature of SO. Moreover, to get much better performance, the motion trajectory feature is integrated. Experiments on two large datasets and an extra Chalearn Multi-modal Gesture Dataset demonstrate that our algorithm has much higher accuracy with only 1/10 time cost compared with the HMM and DTW based methods.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2016_Sparse Observation (SO) Alignment for Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Z7BZ34ZZ/S0925231215016227.html},
  journal = {Neurocomputing},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=1000;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=370;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hmovement,modal=2hshape,signer=1,sl=CSL,slreco,slreco;modal=2hshape;modal=2hmovement,vocab<500,vocab=1000,vocab=370,vocab>=1000},
  number = {Part A}
}

@incollection{wang_spectral_2005,
  title = {Spectral Clustering for Time Series},
  booktitle = {Pattern {{Recognition}} and {{Data Mining}}},
  author = {Wang, Fei and Zhang, Changshui},
  year = {2005},
  pages = {345--354},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang_Zhang_2005_Spectral clustering for time series.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XS53ST2V/11551188_37.html}
}

@inproceedings{wang_system_2010,
  title = {A {{System}} for {{Large Vocabulary Sign Search}}},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Wang, Haijing and Stefan, Alexandra and Moradi, Sajjad and Athitsos, Vassilis and Neidle, Carol and Kamangar, Farhad},
  year = {2010},
  month = sep,
  pages = {342--353},
  address = {{Crete, Greece}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2010_A System for Large Vocabulary Sign Search.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=1113;signer=2;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=2,sl=ASL,slreco,slreco;modal=2hmovement;modal=2hlocation;modal=2hshape,vocab=1113,vocab>=1000}
}

@article{wang_temporal_2016,
  title = {Temporal {{Segment Networks}}: {{Towards Good Practices}} for {{Deep Action Recognition}}},
  shorttitle = {Temporal {{Segment Networks}}},
  author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
  year = {2016},
  month = aug,
  abstract = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( \$ 69.4\textbackslash\% \$) and UCF101 (\$ 94.2\textbackslash\% \$). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices.},
  archivePrefix = {arXiv},
  eprint = {1608.00859},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2016_Temporal Segment Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/LHDTF99G/1608.html},
  journal = {arXiv:1608.00859 [cs]},
  primaryClass = {cs}
}

@article{wang_temporal_2018,
  title = {Temporal {{Segment Networks}} for {{Action Recognition}} in {{Videos}}},
  author = {Wang, L. and Xiong, Y. and Wang, Z. and Qiao, Y. and Lin, D. and Tang, X. and Gool, L. Van},
  year = {2018},
  pages = {1--1},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2018.2868668},
  abstract = {Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0\%), UCF101 (94.9\%), THUMOS14 (80.1\%), and ActivityNet v1.2 (89.6\%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0 \%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2018_Temporal Segment Networks for Action Recognition in Videos.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WE7VE8NW/8454294.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@article{wang_twostream_,
  title = {Two-{{Stream SR}}-{{CNNs}} for {{Action Recognition}} in {{Videos}}},
  author = {Wang, Yifan and Song, Jie and Wang, Limin and Van Gool, Luc and Hilliges, Otmar},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_Two-Stream SR-CNNs for Action Recognition in Videos.pdf}
}

@inproceedings{wang_unsupervised_2007,
  title = {Unsupervised {{Training}} for {{Mandarin Broadcast News}} and {{Conversation Transcription}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, L. and Gales, M. J. F. and Woodland, P. C.},
  year = {2007},
  month = apr,
  volume = {4},
  pages = {IV-353-IV-356},
  doi = {10.1109/ICASSP.2007.366922},
  abstract = {A significant cost in obtaining acoustic training data is the generation of accurate transcriptions. For some sources close-caption data is available. This allows the use of lightly-supervised training techniques. However, for some sources and languages close-caption is not available. In these cases unsupervised training techniques must be used. This paper examines the use of unsupervised techniques for discriminative training. In unsupervised training automatic transcriptions from a recognition system are used for training. As these transcriptions may be errorful data selection may be useful. Two forms of selection are described, one to remove non-target language shows, the other to remove segments with low confidence. Experiments were carried out on a Mandarin transcriptions task. Two types of test data were considered, broadcast news (BN) and broadcast conversations (BC). Results show that the gains from unsupervised discriminative training are highly dependent on the accuracy of the automatic transcriptions.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2007_Unsupervised Training for Mandarin Broadcast News and Conversation Transcription.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KL9B8RKU/4218110.html}
}

@article{wang_unsupervised_2019,
  title = {Unsupervised {{Learning Helps Supervised Neural Word Segmentation}}},
  author = {Wang, Xiaobin and Cai, Deng and Li, Linlin and Xu, Guangwei and Zhao, Hai and Si, Luo},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {7200--7207},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33017200},
  abstract = {By exploiting unlabeled data for further performance improvement for Chinese word segmentation, this work makes the first attempt at exploring adding unsupervised segmentation information into neural supervised segmenter. We survey various effective strategies, including extending the character embedding, augmenting the word score and applying multi-task learning, for leveraging unsupervised information derived from abundant unlabeled data. Experiments on standard data sets show that the explored strategies indeed improve the recall rate of out-of-vocabulary words and thus boost the segmentation accuracy. Moreover, the model enhanced by the proposed methods outperforms state-of-theart models in closed test and shows promising improvement trend when adopting three different strategies with the help of a large unlabeled data set. Our thorough empirical study eventually verifies the proposed approach outperforms the widelyused pre-training approach in terms of effectively making use of freely abundant unlabeled data.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2019_Unsupervised Learning Helps Supervised Neural Word Segmentation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AXJGMMNH/Wang et al. - 2019 - Unsupervised Learning Helps Supervised Neural Word.pdf},
  journal = {Proc. of the  AAAI Conf. on Artificial Intelligence},
  language = {en}
}

@article{wang_viewpoint_2007,
  title = {Viewpoint Invariant Sign Language Recognition},
  author = {Wang, Qi and Chen, Xilin and Zhang, Liang-Guo and Wang, Chunli and Gao, Wen},
  year = {2007},
  month = oct,
  volume = {108},
  pages = {87--97},
  issn = {1077-3142},
  doi = {10.1016/j.cviu.2006.11.009},
  abstract = {Viewpoint invariance is a grand challenge for sign language recognition. In this paper, we propose a novel viewpoint invariant method for sign language recognition. The recognition task is converted to a verification task under the proposed method. This conversion is based on the geometric constraint that the fundamental matrix associated with two views SHOULD BE UNIQUE when the observation and template signs can be considered as obtained synchronously under a virtual stereo vision and vice versa. The Dempster\textendash Shafer theory is applied to improve the robustness of the geometry model. Our experiment demonstrates the efficiency of the proposed method. Furthermore, the proposed method can be extended to other recognition tasks, such as gait recognition and lip-reading recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wang et al_2007_Viewpoint invariant sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GN5U6BPX/S1077314206002244.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JHR4TNH9/S1077314206002244.html},
  journal = {Computer Vision and Image Understanding (CVIU)},
  keywords = {capturedBy=coloredGlove,isolated,isolated;vocab=100;signer=1;sl=CSL;capturedBy=coloredGlove,modal=location,modal=shape,signer=1,sl=CSL,slreco,slreco;modal=shape;modal=location,vocab<500,vocab=100},
  number = {1\textendash 2},
  series = {Special {{Issue}} on {{Vision}} for {{Human}}-{{Computer Interaction}}}
}

@article{wardjr_hierarchical_1963,
  title = {Hierarchical Grouping to Optimize an Objective Function},
  author = {Ward Jr, Joe H.},
  year = {1963},
  volume = {58},
  pages = {236--244},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ward Jr_1963_Hierarchical grouping to optimize an objective function.pdf},
  journal = {Journal of the American statistical association},
  number = {301}
}

@article{washabaugh_providence_1978,
  title = {Providence {{Island Sign}}: {{A Context}}-{{Dependent Language}}},
  shorttitle = {Providence {{Island Sign}}},
  author = {Washabaugh, William and Woodward, James C. and DeSantis, Susan},
  year = {1978},
  volume = {20},
  pages = {95--109},
  issn = {0003-5483},
  journal = {Anthropological Linguistics},
  number = {3}
}

@article{weber_imaginationaugmented_2017,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Th{\'e}ophane and Racani{\`e}re, S{\'e}bastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdom{\`e}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Silver, David and Wierstra, Daan},
  year = {2017},
  month = jul,
  abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  archivePrefix = {arXiv},
  eprint = {1707.06203},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Weber et al_2017_Imagination-Augmented Agents for Deep Reinforcement Learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VUR4IEDA/1707.html},
  journal = {arXiv:1707.06203 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wei_componentbased_2016,
  title = {A {{Component}}-{{Based Vocabulary}}-{{Extensible Sign Language Gesture Recognition Framework}}},
  author = {Wei, Shengjing and Chen, Xiang and Yang, Xidong and Cao, Shuai and Zhang, Xu},
  year = {2016},
  volume = {16},
  pages = {556},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2016/Wei et al/Wei et al_2016_A Component-Based Vocabulary-Extensible Sign Language Gesture Recognition.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wei et al_2016_A Component-Based Vocabulary-Extensible Sign Language Gesture Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/3FDFCI8T/html.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UWNUCJV4/html.html},
  journal = {Sensors},
  keywords = {capturedBy=emg,capturedBy=mocap,isolated,isolated;vocab=110;signer=5;sl=CSL;capturedBy=emg;capturedBy=mocap,modal=2hmovement,modal=2horientation,modal=2hshape,signer=5,sl=CSL,slreco,slreco;modal=2hshape;modal=2hmovement;modal=2horientation,vocab<500,vocab=110},
  number = {4}
}

@inproceedings{wei_deep_2019,
  ids = {wei2019deep},
  title = {Deep {{Grammatical Multi}}-Classifier for {{Continuous Sign Language Recognition}}},
  booktitle = {2019 {{IEEE Fifth International Conference}} on {{Multimedia Big Data}} ({{BigMM}})},
  author = {Wei, Chengcheng and Zhou, Wengang and Pu, Junfu and Li, Houqiang},
  year = {2019},
  month = sep,
  pages = {435--442},
  doi = {10.1109/BigMM.2019.00027},
  abstract = {In this paper, we propose a novel deep architecture with multiple classifiers for continuous sign language recognition. Representing the sign video with a 3D convolutional residual network and a bidirectional LSTM, we formulate continuous sign language recognition as a grammatical-rule-based classification problem. We first split a text sentence of sign language into isolated words and n-grams, where an n-gram is a sequence of consecutive n words in a sentence. Then, we propose a word-independent classifiers (WIC) module and an n-gram classifier (NGC) module to identify the words and n-grams in a sentence, respectively. A greedy decoding algorithm is employed to integrate words and n-grams into the sentence based on the confidence scores provided by both modules. Our method is evaluated on a Chinese continuous sign language recognition benchmark, and the experimental results demonstrate its effectiveness and superiority.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wei et al_2019_Deep Grammatical Multi-classifier for Continuous Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GCR2HQHW/8919458.html},
  keywords = {capturedBy=vision,continuous,Continuous Sign Language Recognition,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,modal=fullframe,signer=50,sl=CSL,slreco,slreco;modal=fullframe,vocab<500,vocab=178}
}

@inproceedings{wei_realtime_2013,
  title = {Real-Time Head Nod and Shake Detection for Continuous Human Affect Recognition},
  booktitle = {Image {{Analysis}} for {{Multimedia Interactive Services}} ({{WIAMIS}}), 2013 14th {{International Workshop}} On},
  author = {Wei, Haolin and Scanlon, Patricia and Li, Yingbo and Monaghan, David S. and O'Connor, Noel E.},
  year = {2013},
  pages = {1--4},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wei et al_2013_Real-time head nod and shake detection for continuous human affect recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VWBQ3J3H/login.html}
}

@inproceedings{weinzaepfel_deepflow_2013,
  title = {{{DeepFlow}}: {{Large Displacement Optical Flow}} with {{Deep Matching}}},
  shorttitle = {{{DeepFlow}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Weinzaepfel, Philippe and Revaud, Jerome and Harchaoui, Zaid and Schmid, Cordelia},
  year = {2013},
  pages = {1385--1392},
  address = {{Sydney, Australia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Weinzaepfel et al_2013_DeepFlow.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8W6BT96I/Weinzaepfel_DeepFlow_Large_Displacement_2013_ICCV_paper.html}
}

@phdthesis{weisenberg_audience_2009,
  title = {Audience Effects in {{American Sign Language}} Interpretation},
  author = {Weisenberg, Julia},
  year = {2009},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Weisenberg_2009_Audience effects in American Sign Language interpretation.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/D8IDKZTG/3401735.html},
  school = {State University of New York at Stony Brook}
}

@inproceedings{weiss_german_2005,
  title = {A {{German Viseme}}-{{Set}} for {{Automatic Transcription}} of {{Input Text Used}} for {{Audio}}-{{Visual Speech Synthesis}}},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Weiss, Christian and Aschenberner, Bianca},
  year = {2005},
  pages = {2945--2948},
  address = {{Lisbon, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Weiss_Aschenberner_2005_A German Viseme-Set for Automatic Transcription of Input Text Used for.PDF}
}

@inproceedings{wellekens_use_1998,
  title = {The Use of Meta-{{HMM}} in Multistream {{HMM}} Training for Automatic Speech Recognition.},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Spoken Language Processing}} ({{ICSLP}})},
  author = {Wellekens, Christian and Kangasharju, Jussi and Milesi, Cedric},
  year = {1998},
  month = dec,
  pages = {2991--2994},
  address = {{Sydney, Australia}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wellekens et al_1998_The use of meta-HMM in multistream HMM training for automatic speech recognition.pdf}
}

@inproceedings{weller_analyzing_2012,
  title = {Analyzing and {{Aligning German Compound Nouns}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Language Resources}} and {{Evaluation}} ({{LREC}})},
  author = {Weller, Marion and Heid, Ulrich},
  editor = {Chair), Nicoletta Calzolari (Conference and Choukri, Khalid and Declerck, Thierry and Do{\u g}an, Mehmet U{\u g}ur and Maegaard, Bente and Mariani, Joseph and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
  year = {2012},
  month = may,
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Istanbul, Turkey}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Weller_Heid_2012_Analyzing and aligning german compound nouns.pdf},
  isbn = {978-2-9517408-7-7},
  language = {English}
}

@phdthesis{welling:1999:diss,
  title = {Merkmalsextraktion in {{Spracherkennungssystemen}} F\"ur Gro\ss en {{Wortschatz}}},
  author = {Welling, L.},
  year = {1999},
  month = jan,
  address = {{Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Welling_1999_Merkmalsextraktion in Spracherkennungssystemen für großen Wortschatz.pdf},
  school = {Human Language Technology and Pattern Recognition Group, RWTH Aachen University}
}

@inproceedings{werbos_applications_1982,
  title = {Applications of Advances in Nonlinear Sensitivity Analysis},
  booktitle = {System {{Modeling}} and {{Optimization}}},
  author = {Werbos, Paul J.},
  editor = {Drenick, R. F. and Kozin, F.},
  year = {1982},
  pages = {762--770},
  abstract = {The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting ,,Sensitivity Analysis Methods for Nonlinear Systems`` from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Werbos_1982_Applications of advances in nonlinear sensitivity analysis.pdf},
  isbn = {978-3-540-39459-4},
  language = {en},
  series = {Lecture {{Notes}} in {{Control}} and {{Information Sciences}}}
}

@inproceedings{wessel_unsupervised_2001,
  title = {Unsupervised {{Training}} of {{Acoustic Models}} for {{Large Vocabulary Continuous Speech Recognition}}},
  booktitle = {Proc. {{IEEE Workshop}} on {{Automatic Speech Recognition}} and {{Understanding}}},
  author = {Wessel, Frank and Ney, Hermann},
  year = {2001},
  pages = {307--310}
}

@phdthesis{wessel:asrdiss02,
  title = {Word {{Posterior Probabilities}} for {{Large Vocabulary Continuous Speech Recognition}}},
  author = {Wessel, F.},
  year = {2002},
  month = jul,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wessel_2002_Word Posterior Probabilities for Large Vocabulary Continuous Speech Recognition.pdf},
  school = {RWTH Aachen University}
}

@article{wetzler_rule_2015,
  title = {Rule {{Of Thumb}}: {{Deep}} Derotation for Improved Fingertip Detection},
  shorttitle = {Rule {{Of Thumb}}},
  author = {Wetzler, Aaron and Slossberg, Ron and Kimmel, Ron},
  year = {2015},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wetzler et al_2015_Rule Of Thumb.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JDV2HQA5/1507.html},
  journal = {arXiv preprint arXiv:1507.05726}
}

@inproceedings{whitehill_haar_2006,
  title = {Haar Features for Facs Au Recognition},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Whitehill, Jacob and Omlin, Christian W.},
  year = {2006},
  pages = {5--pp},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Whitehill_Omlin_2006_Haar features for facs au recognition.pdf}
}

@inproceedings{wieghardt_learning_2006,
  title = {Learning the {{Topology}} of {{Object Views}}},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Wieghardt, Jan and W{\"u}rtz, Rolf and {von der Malsburg}, Christoph},
  editor = {Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
  year = {2006},
  volume = {2353},
  pages = {11--15},
  publisher = {{Springer Berlin / Heidelberg}},
  abstract = {A visual representation of an object must meet at least three basic requirements. First, it must allow identification of the object in the presence of slight but unpredictable changes in its visual appearance. Second, it must account for larger changes in appearance due to variations in the object's fundamental degrees of freedom, such as, e.g., changes in pose. And last, any object representation must be derivable from visual input alone, i.e., it must be learnable. We here construct such a representation by deriving transformations between the different views of a given object, so that they can be parameterized in terms of the object's physical degrees of freedom. Our method allows to automatically derive the appearance representations of an object in conjunction with their linear deformation model from example images. These are subsequently used to provide linear charts to the entire appearance manifold of a three-dimensional object. In contrast to approaches aiming at mere dimensionality reduction the local linear charts to the object's appearance manifold are estimated on a strictly local basis avoiding any reference to a metric embedding space to all views. A real understanding of the object's appearance in terms of its physical degrees of freedom is this way learned from single views alone.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wieghardt et al_2006_Learning the Topology of Object Views.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9DB3J2I4/1tekqjt2q74gtk8d.html},
  isbn = {978-3-540-43748-2},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wiesler_meannormalized_2014,
  title = {Mean-Normalized Stochastic Gradient for Large-Scale Deep Learning},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wiesler, Simon and Richard, Alexander and Schluter, Ralf and Ney, Hermann},
  year = {2014},
  pages = {180--184},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wiesler et al_2014_Mean-normalized stochastic gradient for large-scale deep learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/7G64V985/abs_all.html}
}

@phdthesis{wiesler2016:phd,
  title = {Optimization of {{Discriminative Models}} for {{Speech}} and {{Handwriting Recognition}}},
  author = {Wiesler, Simon Bernhard},
  year = {2016},
  month = dec,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wiesler_2016_Optimization of Discriminative Models for Speech and Handwriting Recognition.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{wiesner_pretraining_2019,
  title = {Pretraining by {{Backtranslation}} for {{End}}-to-{{End ASR}} in {{Low}}-{{Resource Settings}}},
  booktitle = {Interspeech 2019},
  author = {Wiesner, Matthew and Renduchintala, Adithya and Watanabe, Shinji and Liu, Chunxi and Dehak, Najim and Khudanpur, Sanjeev},
  year = {2019},
  month = sep,
  pages = {4375--4379},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-3254},
  abstract = {We explore training attention-based encoder-decoder ASR in low-resource settings. These models perform poorly when trained on small amounts of transcribed speech, in part because they depend on having sufficient target-side text to train the attention and decoder networks. In this paper we address this shortcoming by pretraining our network parameters using only text-based data and transcribed speech from other languages. We analyze the relative contributions of both sources of data. Across 3 test languages, our text-based approach resulted in a 20\% average relative improvement over a text-based augmentation technique without pretraining. Using transcribed speech from nearby languages gives a further 20-30\% relative reduction in character error rate.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/56DXG6D5/Wiesner et al. - 2019 - Pretraining by Backtranslation for End-to-End ASR .pdf},
  language = {en}
}

@incollection{wilbur_phonological_2000,
  title = {Phonological and Prosodic Layering of Nonmanuals in {{American Sign Language}}},
  booktitle = {The Signs of Language Revisited:  {{An}} Anthology to Honor {{Ursula Bellugi}} and {{Edward Klima}}},
  author = {Wilbur, Ronnie B.},
  year = {2000},
  pages = {215--244},
  publisher = {{Lawrence Erlbaum Associates Publishers}},
  address = {{Mahwah, NJ, US}},
  abstract = {Reviews the basic notion of layering and its importance for understanding sign language structure and then extends the notion to the nonmanuals that occur in American Sign Language. The author also reviews data from adult fluent users of signed English to support the suggestion that layering is absent in that kind of signing due to lack of historical adaptation to the modality. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wilbur_2000_Phonological and prosodic layering of nonmanuals in American Sign Language4.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UURLJ6XJ/2000-03924-009.html},
  isbn = {978-0-8058-3246-4}
}

@techreport{wilbur_purdue_2006,
  title = {Purdue {{RVL}}-{{SLLL American Sign Language Database}}},
  author = {Wilbur, Ronnie and Kak, Avinash C.},
  year = {2006},
  pages = {1--15},
  address = {{Purdue University, W. Lafayette, IN 47906}},
  institution = {{School of Electrical and Computer Engineering}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wilbur_Kak_2006_Purdue RVL-SLLL American Sign Language Database.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FU8SEGGV/338.html},
  number = {TR-06-12},
  type = {Technical {{Report}}}
}

@book{wilbur_reanalysis_2005,
  title = {A Reanalysis of Reduplication in {{American Sign Language}}},
  author = {Wilbur, Ronnie B.},
  year = {2005},
  publisher = {{na}}
}

@article{williams_gradientbased_1995,
  title = {Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity},
  author = {Williams, Ronald J. and Zipser, David},
  year = {1995},
  volume = {1},
  pages = {433--486},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Williams_Zipser_1995_Gradient-based learning algorithms for recurrent networks and their.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JFGPWWUA/books.html},
  journal = {Back-propagation: Theory, architectures and applications}
}

@inproceedings{wilson_classification_2019,
  title = {Classification of Mouth Gestures in {{German}} Sign Language Using {{3D}} Convolutional Neural Networks},
  booktitle = {10th {{International Conference}} on {{Pattern Recognition Systems}} ({{ICPRS}}-2019)},
  author = {Wilson, Nancy and Brumm, Maren and Grigat, Rolf-Rainer},
  year = {2019},
  month = jul,
  pages = {52--57},
  doi = {10.1049/cp.2019.0248},
  abstract = {Automatic recognition of sign language gestures is becoming necessary with an increased interest into human-computer interaction in sign language as well as automatic translation from sign language. Most of the research on sign language recognition focuses on hand gesture recognition. However, there are also non-manual signals in sign language. Mouth gestures represent mouth shapes that add information to the hand gestures not related to spoken language visemes. For German Sign Language, mouth gesture recognition would be an important addition to manual gesture recognition. This research work evaluates the method 3D convolutional neural networks for recognising mouth gestures in German Sign Language. For the recognition of certain mouth gestures, temporal information is mandatory and the extraction of both spatial and temporal features by 3D convolutional networks makes the classification of all gestures easier. Our research work compares how different initialisations affect learning and classification by the network. We achieve an accuracy of around 68\% on testing 10 classes of mouth gestures in German Sign Language.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wilson et al_2019_Classification of mouth gestures in German sign language using 3D convolutional.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/PL9THLLL/9037799.html},
  keywords = {3D Convolutional neural networks,Deep learning,Mouth gestures,Sign Language,Video classification}
}

@article{wilson_parametric_1999,
  title = {Parametric Hidden {{Markov}} Models for Gesture Recognition},
  author = {Wilson, A.D. and Bobick, A.F.},
  year = {1999},
  month = sep,
  volume = {21},
  pages = {884--900},
  issn = {0162-8828},
  doi = {10.1109/34.790429},
  abstract = {A method for the representation, recognition, and interpretation of parameterized gesture is presented. By parameterized gesture we mean gestures that exhibit a systematic spatial variation; one example is a point gesture where the relevant parameter is the two-dimensional direction. Our approach is to extend the standard hidden Markov model method of gesture recognition by including a global parametric variation in the output probabilities of the HMM states. Using a linear model of dependence, we formulate an expectation-maximization (EM) method for training the parametric HMM. During testing, a similar EM algorithm simultaneously maximizes the output likelihood of the PHMM for the given sequence and estimates the quantifying parameters. Using visually derived and directly measured three-dimensional hand position measurements as input, we present results that demonstrate the recognition superiority of the PHMM over standard HMM techniques, as well as greater robustness in parameter estimation with respect to noise in the input features. Finally, we extend the PHMM to handle arbitrary smooth (nonlinear) dependencies. The nonlinear formulation requires the use of a generalized expectation-maximization (GEM) algorithm for both training and the simultaneous recognition of the gesture and estimation of the value of the parameter. We present results on a pointing gesture, where the nonlinear approach permits the natural spherical coordinate parameterization of pointing direction},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wilson_Bobick_1999_Parametric hidden Markov models for gesture recognition.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {9}
}

@article{wimmer_computer_2014,
  title = {Computer {{Assisted Communication}} for the {{Hearing Impaired}} for an {{Emergency Room Scenario}}},
  author = {Wimmer, Hayden},
  year = {2014},
  volume = {41},
  pages = {14--2014},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wimmer_2014_COMPUTER ASSISTED COMMUNICATION FOR THE HEARING IMPAIRED FOR AN EMERGENCY ROOM.pdf},
  journal = {SAIS 2014 Proc.}
}

@incollection{windridge_induced_2004,
  title = {Induced {{Decision Fusion}} in {{Automated Sign Language Interpretation}}: {{Using ICA}} to {{Isolate}} the {{Underlying Components}} of {{Sign}}},
  shorttitle = {Induced {{Decision Fusion}} in {{Automated Sign Language Interpretation}}},
  booktitle = {Multiple {{Classifier Systems}}},
  author = {Windridge, David and Bowden, Richard},
  editor = {Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Roli, Fabio and Kittler, Josef and Windeatt, Terry},
  year = {2004},
  volume = {3077},
  pages = {303--313},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-25966-4_30},
  abstract = {We utilise the techniques of independent component analysis and principle component analysis to derive an independent set of gestural primitives for visual sign-language, employing existing sign linguistics as a reference point in the feature reduction.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5NTU4XV7/Windridge and Bowden - 2004 - Induced Decision Fusion in Automated Sign Language.pdf},
  isbn = {978-3-540-22144-9 978-3-540-25966-4},
  keywords = {capturedBy=vision,isolated,isolated;vocab=115;signer=1;sl=BSL;capturedBy=vision,modal=location,modal=movement,modal=shape,signer=1,sl=ASL,sl=BSL,slreco,slreco;modal=movement;modal=location;modal=shape,vocab<500,vocab=115},
  language = {en}
}

@inproceedings{wojdel_using_2001,
  title = {Using Aerial and Geometric Features in Automatic Lip-Reading},
  booktitle = {Proc. {{European Conference}} on {{Speech Communication}} and {{Technology}} ({{Eurospeech}})},
  author = {Wojdel, Jacek C. and Rothkrantz, Leon JM},
  year = {2001},
  pages = {2463--2466},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wojdel_Rothkrantz_2001_Using aerial and geometric features in automatic lip-reading.pdf}
}

@article{wolfe_special_2015,
  title = {Special Issue: Recent Advances in Sign Language Translation and Avatar Technology},
  shorttitle = {Special Issue},
  author = {Wolfe, Rosalee and Efthimiou, Eleni and Glauert, John and Hanke, Thomas and McDonald, John and Schnepp, Jerry},
  year = {2015},
  month = jun,
  pages = {1--2},
  issn = {1615-5289, 1615-5297},
  doi = {10.1007/s10209-015-0412-5},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2015/Wolfe et al/Wolfe et al_2015_Special issue.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wolfe et al_2015_Special issue.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GU4X4WQV/s10209-015-0412-5.html},
  journal = {Universal Access in the Information Society},
  language = {en}
}

@incollection{woll_how_2009,
  title = {How the {{Brain}} Processes Language in Different Modalities},
  booktitle = {Multimodal {{Signals}}: {{Cognitive}} and {{Algorithmic Issues}}},
  author = {Woll, Bencie},
  year = {2009},
  pages = {145--163},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Woll_2009_How the Brain processes language in different modalities.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GPDHMKRN/978-3-642-00525-1_15.html}
}

@techreport{woodin_access_2011,
  title = {Access to Specialised Victim Support Services for Women with  Disabilities  Who Have Experienced Violence},
  author = {Woodin, Sarah and Shah, Sonali},
  year = {2011},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Woodin_Shah_2011_Access to specialised victim support services for women with disabilities who.pdf}
}

@inproceedings{wu_deep_2014,
  title = {Deep Dynamic Neural Networks for Gesture Segmentation and Recognition},
  booktitle = {European {{Conference}} on {{Computer Vision Workshops}} ({{ECCVW}})},
  author = {Wu, Di and Shao, Ling},
  year = {2014},
  month = sep,
  pages = {552--571},
  address = {{Zurich, Switzerland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wu_Shao_2014_Deep dynamic neural networks for gesture segmentation and recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZSF2JTX3/978-3-319-16178-5_39.html}
}

@article{wu_deep_2016,
  title = {Deep {{Dynamic Neural Networks}} for {{Multimodal Gesture Segmentation}} and {{Recognition}}},
  author = {Wu, Di and Pigou, Lionel and Kindermans, Pieter-Jan and LE, Nam and Shao, Ling and Dambre, Joni and Odobez, Jean-Marc},
  year = {2016},
  month = mar,
  volume = {38},
  pages = {1583--1597},
  issn = {0162-8828},
  abstract = {This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatio-temporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wu et al_2016_Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {eng},
  number = {8}
}

@inproceedings{wu_fusing_2013,
  title = {Fusing {{Multi}}-Modal {{Features}} for {{Gesture Recognition}}},
  booktitle = {Proc. {{ACM Int}}. {{Conf}}. on {{Multimodal Interaction}}},
  author = {Wu, Jiaxiang and Cheng, Jian and Zhao, Chaoyang and Lu, Hanqing},
  year = {2013},
  pages = {453--460},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2522848.2532589},
  abstract = {This paper proposes a novel multi-modal gesture recognition framework and introduces its application to continuous sign language recognition. A Hidden Markov Model is used to construct the audio feature classifier. A skeleton feature classifier is trained to provided complementary information based on the Dynamic Time Warping model. The confidence scores generated by two classifiers are firstly normalized and then combined to produce a weighted sum for the final recognition. Experimental results have shown that the precision and recall scores for 20 classes of our multi-modal recognition framework can achieve 0.8829 and 0.8890 respectively, which proves that our method is able to correctly reject false detection caused by single classifier. Our approach scored 0.12756 in mean Levenshtein distance and was ranked 1st in the Multi-modal Gesture Recognition Challenge in 2013.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wu et al_2013_Fusing Multi-modal Features for Gesture Recognition.pdf},
  isbn = {978-1-4503-2129-7},
  series = {{{ICMI}} '13}
}

@article{wu_googles_2016,
  title = {Google's {{Neural Machine Translation System}}: {{Bridging}} the {{Gap}} between {{Human}} and {{Machine Translation}}},
  shorttitle = {Google's {{Neural Machine Translation System}}},
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2016},
  month = oct,
  abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Google's phrase-based production system.},
  archivePrefix = {arXiv},
  eprint = {1609.08144},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wu et al_2016_Google's Neural Machine Translation System.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HPXT93R2/1609.html},
  journal = {arXiv:1609.08144 [cs]},
  primaryClass = {cs}
}

@inproceedings{wu_learning_2014,
  title = {Learning the {{Deep Features}} for {{Eye Detection}} in {{Uncontrolled Conditions}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Wu, Yue and Ji, Qiang},
  year = {2014},
  pages = {455--459},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wu_Ji_2014_Learning the Deep Features for Eye Detection in Uncontrolled Conditions.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EIPAHMW5/cookiedetectresponse.html}
}

@inproceedings{wu_recognition_2001,
  title = {The {{Recognition}} of {{Finger}}-{{Spelling}} for {{Chinese Sign Language}}},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Wu, Jiangqin and Gao, Wen},
  editor = {Wachsmuth, Ipke and Sowa, Timo},
  year = {2001},
  month = apr,
  pages = {96--100},
  address = {{London, UK}},
  abstract = {In this paper 3-layer feedforward network is introduced to recognize Chinese manual alphabet, and Single Parameter Dynamic Search Algorithm(SPDS) is used to learn net parameters. In addition, a recognition algorithm for recognizing manual alphabets based on multifeatures and multi-classifiers is proposed to promote the recognition performance of finger-spelling. From experiment result, it is shown that Chinese finger-spelling recognition based on multi-features and multiclassifiers outperforms its recognition based on single-classifier.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Jiangqin_Wen_2002_The Recognition of Finger-Spelling for Chinese Sign Language.pdf},
  isbn = {978-3-540-47873-7},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=30;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=location,modal=movement,modal=orientation,modal=shape,sl=CSL,slreco,slreco;modal=location;modal=movement;modal=shape;modal=orientation,vocab<50,vocab=30},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{wu_selfsupervised_2001,
  title = {Self-Supervised Learning for Object Recognition Based on Kernel Discriminant-{{EM}} Algorithm},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Wu, Ying and Huang, T.S. and Toyama, K.},
  year = {2001},
  month = jul,
  volume = {1},
  pages = {275--280},
  address = {{Vancouver, Canada}},
  doi = {10.1109/ICCV.2001.937529},
  abstract = {It is often tedious and expensive to label large training data sets for learning-based object recognition systems. This problem could be alleviated by self-supervised learning techniques, which take a hybrid of labeled and unlabeled training data to learn classifiers. Discriminant-EM (D-EM) proposed a framework for such tasks and current D-EM algorithm employed linear discriminant analysis. However, the algorithm is limited by its dependence on linear transformations. This paper extends the linear D-EM to nonlinear kernel algorithm, Kernel D-EM, based on kernel multiple discriminant analysis (KMDA). KMDA provides better ability to simplify the probabilistic structures of data distributions in a discrimination space. We propose two novel data-sampling schemes for efficient training of kernel discriminants. Experimental results show that classifiers using KMDA learning compare with SVM performance on standard benchmark tests, and that Kernel D-EM outperforms a variety of supervised and semi-supervised learning algorithms for a hand-gesture recognition task and fingertip tracking task},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wu et al_2001_Self-supervised learning for object recognition based on kernel discriminant-EM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/65S3S5B6/abs_all.html}
}

@article{wu_signature_2008,
  title = {On Signature Invariants for Effective Motion Trajectory Recognition},
  author = {Wu, S. and Li, Y. F.},
  year = {2008},
  volume = {27},
  pages = {895--917},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wu_Li_2008_On signature invariants for effective motion trajectory recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BFAK85JM/Wu und Li - 2008 - On signature invariants for effective motion traje.html},
  journal = {The International Journal of Robotics Research},
  number = {8}
}

@phdthesis{wuebker2017:phd,
  title = {Effective {{Training}} and {{Effcient Decoding}} for {{Statistical Machine Translation}}},
  author = {W{\"u}bker, J{\"o}rn},
  year = {2017},
  month = feb,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Wübker_2017_Effective Training and Effcient Decoding for Statistical Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{xiao_realtime_2004,
  title = {Real-Time Combined {{2D}}+ {{3D}} Active Appearance Models},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xiao, Jing and Baker, Simon and Matthews, Iain and Kanade, Takeo},
  year = {2004},
  month = jun,
  pages = {535--542},
  address = {{Washington, DC, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xiao et al_2004_Real-time combined 2D+ 3D active appearance models.pdf}
}

@inproceedings{xiaodongcui_developing_2008,
  title = {Developing High Performance Asr in the {{IBM}} Multilingual Speech-to-Speech Translation System},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Xiaodong Cui} and {Liang Gu} and {Bing Xiang} and {Wei Zhang} and {Yuqing Gao}},
  year = {2008},
  month = mar,
  pages = {5121--5124},
  doi = {10.1109/ICASSP.2008.4518811},
  abstract = {This paper presents our recent development of the real-time speech recognition component in the IBM English/Iraqi Arabic speech-to-speech translation system for the DARPA Transtac project. We describe the details of the acoustic and language modeling that lead to high recognition accuracy and noise robustness and give the performance of the system on the evaluation sets of spontaneous conversational speech. We also introduce the streaming decoding structure and several speedup techniques that achieves best recognition accuracy at about 0.3 x RT recognition speed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xiaodong Cui et al_2008_Developing high performance asr in the IBM multilingual speech-to-speech.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BYXVCZ2C/4518811.html},
  keywords = {acoustic modeling,Acoustic noise,Automatic speech recognition,decoding,Decoding,discriminative training,Hidden Markov models,IBM multilingual speech translation,language modeling,large vocabulary spontaneous speech recognition,Linear discriminant analysis,multilingual speech translation,natural languages,Natural languages,noise robustness,Noise robustness,Real time systems,Speech enhancement,speech recognition,Speech recognition,streaming mode decoding}
}

@article{xie_rgbd_2018,
  title = {{{RGB}}-{{D}} Static Gesture Recognition Based on Convolutional Neural Network},
  author = {Xie, B. and He, X. and Li, Y.},
  year = {2018},
  volume = {2018},
  pages = {1515--1520},
  issn = {2051-3305},
  doi = {10.1049/joe.2018.8327},
  abstract = {In the area of human\textendash computer interaction (HCI) and computer vision, gesture recognition has always been a research hotspot. With the appearance of depth camera, gesture recognition using RGB-D camera has gradually become mainstream in this field. However, how to effectively use depth information to construct a robust gesture recognition system is still a problem. In this paper, an RGB-D static gesture recognition method based on fine-tuning Inception V3 is proposed, which can eliminate the steps of gesture segmentation and feature extraction in traditional algorithms. Compared with general CNN algorithms, the authors adopt a two-stage training strategy to fine-tune the model. This method sets a feature concatenate layer of RGB and depth images in the CNN structure, using depth information to promote the performance of gesture recognition. Finally, on the American Sign Language (ASL) Recognition dataset, the authors compared their method with other traditional machine learning methods, CNN algorithms, and the RGB input only method. Among three groups of comparative experiments, the authors' method reached the highest accuracy of 91.35\%, reaching the state-of-the-art currently on ASL dataset.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xie et al_2018_RGB-D static gesture recognition based on convolutional neural network.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XL4VECT7/8543720.html},
  journal = {The Journal of Engineering},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=24;signer=5;sl=ASL;capturedBy=vision;capturedBy=depth,modal=shape,signer=5,sl=ASL,slreco,slreco;modal=shape,vocab<50,vocab=24},
  number = {16}
}

@inproceedings{ximenes_spacetime_2019,
  title = {Space-{{Time Graphs Based}} on {{Interest Point Tracking}} for {{Sign Language}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Systems}}, {{Man}} and {{Cybernetics}} ({{SMC}})},
  author = {Ximenes, Elias and Pedrini, Helio},
  year = {2019},
  month = oct,
  pages = {3390--3395},
  issn = {2577-1655},
  doi = {10.1109/SMC.2019.8913921},
  abstract = {A hand tracking method is presented in this work, which achieves the best results found in the literature for the public RWTH-BOSTON-50 dataset, with a tracking error rate of 8.5\%. Its main contribution is the extraction of intrinsic features from RGB sign language movies. In order to avoid some common limitations of model and appearance-based tracking methods, a movement pattern analysis was used as a feature basis. Such feature can be succinctly described as a space-time graph of interest point movement similarity, which is arranged as trees of dense trajectory connections to track hands in RGB sign language movies. In addition to basic geometry operations, simple graph methods are employed in the process, making it effective for parallel processing of large movie datasets.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ximenes_Pedrini_2019_Space-Time Graphs Based on Interest Point Tracking for Sign Language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BX6VWMUA/8913921.html},
  keywords = {appearance-based tracking,Assistive technology,computational geometry,feature extraction,Feature extraction,Gesture recognition,graph theory,Hand gesture,hand tracking,image colour analysis,image motion analysis,improved trajectories,interest point movement similarity,interest point tracking,intrinsic feature extraction,Motion pictures,movement pattern analysis,object tracking,RGB sign language movies,RGB videos,RWTH-BOSTON-50 dataset,sign language,sign language recognition,space-time graph,Tracking,Trajectory,video signal processing}
}

@inproceedings{xing_multiview_2014,
  title = {Towards {{Multi}}-View and {{Partially}}-Occluded {{Face Alignment}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xing, Junliang and Niu, Zhiheng and Huang, Junshi and Hu, Weiming and Yan, Shuicheng},
  year = {2014},
  pages = {1829--1836},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xing et al_2014_Towards Multi-view and Partially-occluded Face Alignment.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CKIH7I5A/abs_all.html}
}

@article{xiong_achieving_2016,
  title = {Achieving Human Parity in Conversational Speech Recognition},
  author = {Xiong, Wayne and Droppo, Jasha and Huang, Xuedong and Seide, Frank and Seltzer, Mike and Stolcke, Andreas and Yu, Dong and Zweig, Geoffrey},
  year = {2016},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xiong et al_2016_Achieving human parity in conversational speech recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZPF9WBNI/1610.html},
  journal = {arXiv preprint arXiv:1610.05256}
}

@inproceedings{xiong_supervised_2013,
  title = {Supervised Descent Method and Its Applications to Face Alignment},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xiong, Xuehan and {De la Torre}, Fernando},
  year = {2013},
  pages = {532--539},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xiong_De la Torre_2013_Supervised descent method and its applications to face alignment.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IR9NPNQV/login.html}
}

@inproceedings{xu_msrvtt_2016,
  title = {Msr-Vtt: {{A}} Large Video Description Dataset for Bridging Video and Language},
  shorttitle = {Msr-Vtt},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  year = {2016},
  address = {{Las Vegas, NV, USA}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xu et al_2016_Msr-vtt.pdf}
}

@article{xu_show_2015,
  title = {Show, Attend and Tell: {{Neural}} Image Caption Generation with Visual Attention},
  shorttitle = {Show, Attend and Tell},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  year = {2015},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xu et al_2015_Show, attend and tell.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6PK8ZTVC/1502.html},
  journal = {arXiv preprint arXiv:1502.03044}
}

@article{xu_survey_2005,
  title = {Survey of Clustering Algorithms},
  author = {Xu, Rui and Wunsch, D., II},
  year = {2005},
  volume = {16},
  pages = {645--678},
  issn = {1045-9227},
  doi = {10.1109/TNN.2005.845141},
  abstract = {Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/Q5VXXV2S/login.html},
  journal = {IEEE Transactions on Neural Networks},
  number = {3}
}

@phdthesis{xu10:phd,
  title = {Sequence {{Segmentation}} for {{Statistical Machine Translation}}},
  author = {Xu, Jia},
  year = {2010},
  month = sep,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xu_2010_Sequence Segmentation for Statistical Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@article{xue_restructuring_,
  title = {Restructuring of {{Deep Neural Network Acoustic Models}} with {{Singular Value Decomposition}}},
  author = {Xue, Jian and Li, Jinyu and Gong, Yifan},
  pages = {5},
  abstract = {Recently proposed deep neural network (DNN) obtains significant accuracy improvements in many large vocabulary continuous speech recognition (LVCSR) tasks. However, DNN requires much more parameters than traditional systems, which brings huge cost during online evaluation, and also limits the application of DNN in a lot of scenarios. In this paper we present our new effort on DNN aiming at reducing the model size while keeping the accuracy improvements. We apply singular value decomposition (SVD) on the weight matrices in DNN, and then restructure the model based on the inherent sparseness of the original matrices. After restructuring we can reduce the DNN model size significantly with negligible accuracy loss. We also fine-tune the restructured model using the regular back-propagation method to get the accuracy back when reducing the DNN model size heavily. The proposed method has been evaluated on two LVCSR tasks, with context-dependent DNN hidden Markov model (CD-DNN-HMM). Experimental results show that the proposed approach dramatically reduces the DNN model size by more than 80\% without losing any accuracy.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Xue et al_Restructuring of Deep Neural Network Acoustic Models with Singular Value.pdf},
  language = {en}
}

@inproceedings{yamashita_hand_2014,
  title = {Hand Posture Recognition Based on Bottom-up Structured Deep Convolutional Neural Network with Curriculum Learning},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Yamashita, Takayoshi and Watasue, Taro},
  year = {2014},
  pages = {853--857},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014/Yamashita_Watasue/Yamashita_Watasue_2014_Hand posture recognition based on bottom-up structured deep convolutional.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yamashita_Watasue_2014_Hand posture recognition based on bottom-up structured deep convolutional.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/48WEWRK6/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KQRU9K2C/abs_all.html}
}

@inproceedings{yang_combination_2011,
  title = {Combination of Manual and Non-Manual Features for Sign Language Recognition Based on Conditional Random Field and Active Appearance Model},
  booktitle = {Int. {{Conf}}. on {{Machine Learning}} and {{Cybernetics}} ({{ICMLC}})},
  author = {Yang, Hee-Deok and Lee, Seong-Whan},
  year = {2011},
  month = jul,
  volume = {4},
  pages = {1726--1731},
  doi = {10.1109/ICMLC.2011.6016973},
  abstract = {Sign language recognition is the task of detection and recognition of manual signals (MSs) and non-manual signals (NMSs) in a signed utterance. In this paper, a novel method for recognizing MS and facial expressions as a NMS is proposed. This is achieved through a framework consisting of three components: (1) Candidate segments of MSs are discriminated using an hierarchical conditional random field (CRF) and Boost-Map embedding. It can distinguish signs, fingerspellings and non-sign patterns, and is robust to the various sizes, scales and rotations of the signer's hand. (2) Facial expressions as a NMS are recognized with support vector machine (SVM) and active appearance model (AAM), AAM is used to extract facial feature points. From these facial feature points, several measurements are computed to distinguish each facial component into defined facial expressions with SVM. (3) Finally, the recognition results of MSs and NMSs are fused in order to recognize signed sentences. Experiments demonstrate that the proposed method can successfully combine MSs and NMSs features for recognizing signed sentences from utterance data.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang_Lee_2011_Combination of manual and non-manual features for sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6P848N8X/abs_all.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DEK92BFD/abs_all.html},
  keywords = {capturedBy=coloredGlove,continuous,continuous;vocab=24;signer=1;sl=ASL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,modal=eyebrows,modal=eyes,modal=head,modal=mouth,signer=1,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=head;modal=eyebrows;modal=eyes;modal=mouth,vocab<50,vocab=24}
}

@article{yang_continuous_2016,
  title = {Continuous Sign Language Recognition Using Level Building Based on Fast Hidden {{Markov}} Model},
  author = {Yang, Wenwen and Tao, Jinxu and Ye, Zhongfu},
  year = {2016},
  volume = {78},
  pages = {28--35},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2016_Continuous sign language recognition using level building based on fast hidden.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/38322EPQ/S0167865516300344.html},
  journal = {Pattern Recognition Letters},
  keywords = {capturedBy=depth,capturedBy=vision,continuous,continuous;vocab=21;signer=2;sl=CSL;capturedBy=vision;capturedBy=depth,isolated,isolated;vocab=21;signer=8;sl=CSL;capturedBy=vision;capturedBy=depth,modal=bodyjoints,signer=2,signer=8,sl=CSL,slreco,slreco;modal=bodyjoints,vocab<50,vocab=21}
}

@article{yang_continuous_2017,
  title = {Continuous Hand Gesture Recognition Based on Trajectory Shape Information},
  author = {Yang, Cheoljong and Han, David K. and Ko, Hanseok},
  year = {2017},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2017.05.016},
  abstract = {In this paper, we propose a continuous hand gesture recognition method based on trajectory shape information. A key issue in recognizing continuous gestures is that performance of conventional recognition algorithms may be lowered by such factors as, unknown start and end points of a gesture or variations in gesture duration. These issues become particularly difficult for those methods that rely on temporal information. To alleviate the issues of continuous gesture recognition, we propose a framework that simultaneously performs both segmentation and recognition. Each component of the framework applies shape-based information to ensure robust performance for gestures with large temporal variation. A gesture trajectory is divided by a set of key frames by thresholding its tangential angular change. Variable-sized trajectory segments are then generated using the selected key frames. For recognition, these trajectory segments are examined to determine whether the segment belongs to a class among intended gestures or a non-gesture class based on fusion of shape information and temporal features. In order to assess performance, the proposed algorithm was evaluated with a database of digit hand gestures. The experimental results indicate that the proposed algorithm has a high recognition rate while maintaining its performance in the presence of continuous gestures.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_Continuous hand gesture recognition based on trajectory shape information.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/93CQR4IR/S0167865517301642.html},
  journal = {Pattern Recognition Letters}
}

@article{yang_discovering_2013,
  title = {Discovering {{Motion Primitives}} for {{Unsupervised Grouping}} and {{One}}-{{Shot Learning}} of {{Human Actions}}, {{Gestures}}, and {{Expressions}}},
  author = {Yang, Yang and Saleemi, I. and Shah, M.},
  year = {2013},
  month = jul,
  volume = {35},
  pages = {1635--1648},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2012.253},
  abstract = {This paper proposes a novel representation of articulated human actions and gestures and facial expressions. The main goals of the proposed approach are: 1) to enable recognition using very few examples, i.e., one or k-shot learning, and 2) meaningful organization of unlabeled datasets by unsupervised clustering. Our proposed representation is obtained by automatically discovering high-level subactions or motion primitives, by hierarchical clustering of observed optical flow in four-dimensional, spatial, and motion flow space. The completely unsupervised proposed method, in contrast to state-of-the-art representations like bag of video words, provides a meaningful representation conducive to visual interpretation and textual labeling. Each primitive action depicts an atomic subaction, like directional motion of limb or torso, and is represented by a mixture of four-dimensional Gaussian distributions. For one\textendash shot and k-shot learning, the sequence of primitive labels discovered in a test video are labeled using KL divergence, and can then be represented as a string and matched against similar strings of training videos. The same sequence can also be collapsed into a histogram of primitives or be used to learn a Hidden Markov model to represent classes. We have performed extensive experiments on recognition by one and k-shot learning as well as unsupervised action clustering on six human actions and gesture datasets, a composite dataset, and a database of facial expressions. These experiments confirm the validity and discriminative nature of the proposed representation.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2013_Discovering Motion Primitives for Unsupervised Grouping and One-Shot Learning.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DINCKW4K/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {7}
}

@inproceedings{yang_enhanced_2007,
  title = {Enhanced {{Level Building Algorithm}} for the {{Movement Epenthesis Problem}} in {{Sign Language Recognition}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Yang, Ruiduo and Sarkar, S. and Loeding, B.},
  year = {2007},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  doi = {10.1109/CVPR.2007.383347},
  abstract = {One of the hard problems in automated sign language recognition is the movement epenthesis (me) problem. Movement epenthesis is the gesture movement that bridges two consecutive signs. This effect can be over a long duration and involve variations in hand shape, position, and movement, making it hard to explicitly model these intervening segments. This creates a problem when trying to match individual signs to full sign sentences since for many chunks of the sentence, corresponding to these mes, we do not have models. We present an approach based on version of a dynamic programming framework, called Level Building, to simultaneously segment and match signs to continuous sign language sentences in the presence of movement epenthesis (me). We enhance the classical Level Building framework so that it can accommodate me labels for which we do not have explicit models. This enhanced Level Building algorithm is then coupled with a trigram grammar model to optimally segment and label sign language sentences. We demonstrate the efficiency of the algorithm using a single view video dataset of continuous sign language sentences. We obtain 83\% word level recognition rate with the enhanced Level Building approach, as opposed to a 20\% recognition rate using a classical Level Building framework on the same dataset. The proposed approach is novel since it does not need explicit models for movement epenthesis.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ruiduo Yang et al_2007_Enhanced Level Building Algorithm for the Movement Epenthesis Problem in Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/J5N3NRH2/abs_all.html},
  isbn = {1-4244-1180-7},
  keywords = {capturedBy=vision,continuous,continuous;vocab=39;sl=ASL;capturedBy=vision,modal=2hshape,sl=ASL,slreco,slreco;modal=2hshape,vocab<50,vocab=39},
  language = {English}
}

@article{yang_extraction_2002,
  title = {Extraction of {{2D}} Motion Trajectories and Its Application to Hand Gesture Recognition},
  author = {Yang, Ming-Hsuan and Ahuja, N. and Tabb, M.},
  year = {2002},
  month = aug,
  volume = {24},
  pages = {1061--1074},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2002.1023803},
  abstract = {We present an algorithm for extracting and classifying two-dimensional motion in an image sequence based on motion trajectories. First, a multiscale segmentation is performed to generate homogeneous regions in each frame. Regions between consecutive frames are then matched to obtain two-view correspondences. Affine transformations are computed from each pair of corresponding regions to define pixel matches. Pixels matches over consecutive image pairs are concatenated to obtain pixel-level motion trajectories across the image sequence. Motion patterns are learned from the extracted trajectories using a time-delay neural network. We apply the proposed method to recognize 40 hand gestures of American Sign Language. Experimental results show that motion patterns of hand gestures can be extracted and recognized accurately using motion trajectories.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ming-Hsuan Yang et al_2002_Extraction of 2D motion trajectories and its application to hand gesture.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CSDBYB5K/1023803.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {capturedBy=vision,isolated,isolated;vocab=40;sl=ASL;capturedBy=vision,modal=movement,modal=shape,sl=ASL,slreco,slreco;modal=movement;modal=shape,vocab<50,vocab=40},
  number = {8}
}

@article{yang_handling_2010,
  title = {Handling {{Movement Epenthesis}} and {{Hand Segmentation Ambiguities}} in {{Continuous Sign Language Recognition Using Nested Dynamic Programming}}},
  author = {Yang, R. and Sarkar, S. and Loeding, B.},
  year = {2010},
  month = mar,
  volume = {32},
  pages = {462--477},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2009.26},
  abstract = {We consider two crucial problems in continuous sign language recognition from unaided video sequences. At the sentence level, we consider the movement epenthesis (me) problem and at the feature level, we consider the problem of hand segmentation and grouping. We construct a framework that can handle both of these problems based on an enhanced, nested version of the dynamic programming approach. To address movement epenthesis, a dynamic programming (DP) process employs a virtual me option that does not need explicit models. We call this the enhanced level building (eLB) algorithm. This formulation also allows the incorporation of grammar models. Nested within this eLB is another DP that handles the problem of selecting among multiple hand candidates. We demonstrate our ideas on four American Sign Language data sets with simple background, with the signer wearing short sleeves, with complex background, and across signers. We compared the performance with conditional random fields (CRF) and latent dynamic-CRF-based approaches. The experiments show more than 40 percent improvement over CRF or LDCRF approaches in terms of the frame labeling rate. We show the flexibility of our approach when handling a changing context. We also find a 70 percent improvement in sign recognition rate over the unenhanced DP matching algorithm that does not accommodate the me effect.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2010_Handling Movement Epenthesis and Hand Segmentation Ambiguities in Continuous.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/M9XST8RR/abs_all.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {capturedBy=vision,continuous,continuous;vocab=39;signer=1;sl=ASL;capturedBy=vision,continuous;vocab=40;signer=1;sl=ASL;capturedBy=vision,continuous;vocab=99;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,signer=1,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<100,vocab<50,vocab=39,vocab=40,vocab=99},
  number = {3}
}

@inproceedings{yang_multilayer_2016,
  title = {Multilayer and {{Multimodal Fusion}} of {{Deep Neural Networks}} for {{Video Classification}}},
  booktitle = {Proc. {{ACM}} on {{Multimedia Conference}}},
  author = {Yang, Xiaodong and Molchanov, Pavlo and Kautz, Jan},
  year = {2016},
  pages = {978--987},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2964284.2964297},
  abstract = {This paper presents a novel framework to combine multiple layers and modalities of deep neural networks for video classification. We first propose a multilayer strategy to simultaneously capture a variety of levels of abstraction and invariance in a network, where the convolutional and fully connected layers are effectively represented by our proposed feature aggregation methods. We further introduce a multimodal scheme that includes four highly complementary modalities to extract diverse static and dynamic cues at multiple temporal scales. In particular, for modeling the long-term temporal information, we propose a new structure, FC-RNN, to effectively transform pre-trained fully connected layers into recurrent layers. A robust boosting model is then introduced to optimize the fusion of multiple layers and modalities in a unified way. In the extensive experiments, we achieve state-of-the-art results on two public benchmark datasets: UCF101 and HMDB51.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2016_Multilayer and Multimodal Fusion of Deep Neural Networks for Video.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/52TP4HC4/citation.html},
  isbn = {978-1-4503-3603-1},
  series = {{{MM}} '16}
}

@article{yang_neural_2017,
  title = {Neural Word Segmentation with Rich Pretraining},
  author = {Yang, Jie and Zhang, Yue and Dong, Fei},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2017_Neural word segmentation with rich pretraining.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JGCXZKHZ/1704.html},
  journal = {arXiv preprint arXiv:1704.08960}
}

@inproceedings{yang_neural_2017a,
  title = {Neural {{Word Segmentation}} with {{Rich Pretraining}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yang, Jie and Zhang, Yue and Dong, Fei},
  year = {2017},
  month = jul,
  pages = {839--849},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1078},
  abstract = {Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2017_Neural Word Segmentation with Rich Pretraining.pdf}
}

@article{yang_robust_2013,
  title = {Robust Sign Language Recognition by Combining Manual and Non-Manual Features Based on Conditional Random Field and Support Vector Machine},
  author = {Yang, Hee-Deok and Lee, Seong-Whan},
  year = {2013},
  month = dec,
  volume = {34},
  pages = {2051--2056},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2013.06.022},
  abstract = {The sign language is composed of two categories of signals: manual signals such as signs and fingerspellings and non-manual ones such as body gestures and facial expressions. This paper proposes a new method for recognizing manual signals and facial expressions as non-manual signals. The proposed method involves the following three steps: First, a hierarchical conditional random field is used to detect candidate segments of manual signals. Second, the BoostMap embedding method is used to verify hand shapes of segmented signs and to recognize fingerspellings. Finally, the support vector machine is used to recognize facial expressions as non-manual signals. This final step is taken when there is some ambiguity in the previous two steps. The experimental results indicate that the proposed method can accurately recognize the sign language at an 84\% rate based on utterance data.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang_Lee_2013_Robust sign language recognition by combining manual and non-manual features.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/P3PS8KAK/S0167865513002559.html},
  journal = {Pattern Recognition Letters},
  keywords = {capturedBy=vision,continuous,continuous;vocab=24;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=head,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=head,vocab<50,vocab=24},
  number = {16}
}

@article{yang_sfnet_2019,
  title = {{{SF}}-{{Net}}: {{Structured Feature Network}} for {{Continuous Sign Language Recognition}}},
  shorttitle = {{{SF}}-{{Net}}},
  author = {Yang, Zhaoyang and Shi, Zhenmei and Shen, Xiaoyong and Tai, Yu-Wing},
  year = {2019},
  month = aug,
  abstract = {Continuous sign language recognition (SLR) aims to translate a signing sequence into a sentence. It is very challenging as sign language is rich in vocabulary, while many among them contain similar gestures and motions. Moreover, it is weakly supervised as the alignment of signing glosses is not available. In this paper, we propose Structured Feature Network (SF-Net) to address these challenges by effectively learn multiple levels of semantic information in the data. The proposed SF-Net extracts features in a structured manner and gradually encodes information at the frame level, the gloss level and the sentence level into the feature representation. The proposed SF-Net can be trained end-to-end without the help of other models or pre-training. We tested the proposed SF-Net on two large scale public SLR datasets collected from different continuous SLR scenarios. Results show that the proposed SF-Net clearly outperforms previous sequence level supervision based methods in terms of both accuracy and adaptability.},
  archivePrefix = {arXiv},
  eprint = {1908.01341},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2019_SF-Net.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/H52H7CYP/1908.html},
  journal = {arXiv:1908.01341 [cs]},
  keywords = {augment=randomcrop,authorgroup=HKUST,authorgroup=Tencent,capturedBy=vision,cnnresiduals=1,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=35.6/34.9,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,dataSet=Phoenix14,loss=CTC,loss=KL,modal=fullframe,nn=2dcnn,nn=3dcnn,nn=blstm,nnarch=2d-resnet,nnarch=3d-resnet,paperinfo,paperinfo;authorgroup=Tencent;authorgroup=HKUST;shorttitle=SF-Net,re-align=0,shorttitle=SF-Net,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;nn=3dcnn;nn=2dcnn;nn=blstm;nnarch=3d-resnet;nnarch=2d-resnet;cnnresiduals=1;re-align=0;augment=randomcrop;loss=KL;loss=CTC,vocab<500,vocab=1080,vocab=178,vocab>=1000,wer=35.6/34.9},
  primaryClass = {cs}
}

@article{yang_sign_2009,
  title = {Sign {{Language Spotting}} with a {{Threshold Model Based}} on {{Conditional Random Fields}}},
  author = {Yang, Hee-Deok and Sclaroff, Stan and Lee, Seong-Whan},
  year = {2009},
  volume = {31},
  pages = {1264--1277},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang et al_2009_Sign Language Spotting with a Threshold Model Based on Conditional Random Fields.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {capturedBy=vision,isolated,isolated;vocab=48;signer=1;sl=ASL;capturedBy=vision,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=1,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=48},
  number = {7}
}

@article{yang_simultaneous_2010,
  title = {Simultaneous Spotting of Signs and Fingerspellings Based on Hierarchical Conditional Random Fields and Boostmap Embeddings},
  author = {Yang, Hee-Deok and Lee, Seong-Whan},
  year = {2010},
  month = aug,
  volume = {43},
  pages = {2858--2870},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2010.03.007},
  abstract = {A sign language consists of two types of action; signs and fingerspellings. Signs are dynamic gestures discriminated by continuous hand motions and hand configurations, while fingerspellings are a combination of continuous hand configurations. Sign language spotting is the task of detection and recognition of signs and fingerspellings in a signed utterance. The internal structures of signs and fingerspellings differ significantly. Therefore, it is difficult to spot signs and fingerspellings simultaneously. In this paper, a novel method for spotting signs and fingerspellings is proposed. It can distinguish signs, fingerspellings and non-sign patterns, and is robust to the various sizes, scales and rotations of the signer's hand. This is achieved through a hierarchical framework consisting of three steps: (1) Candidate segments of signs and fingerspellings are discriminated using a two-layer conditional random field (CRF). (2) Hand shapes of segmented signs and fingerspellings are verified using BoostMap embeddings. (3) The motions of fingerspellings are verified in order to distinguish those which have similar hand shapes and different hand motions. Experiments demonstrate that the proposed method can spot signs and fingerspellings from utterance data at rates of 83\% and 78\%, respectively.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yang_Lee_2010_Simultaneous spotting of signs and fingerspellings based on hierarchical.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/X8ZI38RS/S0031320310001275.html},
  journal = {Pattern Recognition},
  keywords = {continous;vocab=24;signer=1;sl=ASL;capturedBy=coloredGlove,modal=2hlocation,modal=2hmovement,slreco,slreco;modal=2hlocation;modal=2hmovement},
  number = {8}
}

@inproceedings{yao_describing_2015,
  title = {Describing {{Videos}} by {{Exploiting Temporal Structure}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Yao, Li and Torabi, Atousa and Cho, Kyunghyun and Ballas, Nicolas and Pal, Christopher and Larochelle, Hugo and Courville, Aaron},
  year = {2015},
  pages = {4507--4515},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yao et al_2015_Describing Videos by Exploiting Temporal Structure.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/U4XB9IPT/Yao_Describing_Videos_by_ICCV_2015_paper.html}
}

@inproceedings{yap_database_2013,
  title = {A Database for Facial Behavioural Analysis},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Yap, Moi Hoon and Ugail, Hassan and Zwiggelaar, Reyer},
  year = {2013},
  pages = {1--6},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4AQWSWEG/login.html}
}

@inproceedings{yasugahira_analysis_2009,
  title = {Analysis of {{Hand Movement Variation Related}} to {{Speed}} in {{Japanese Sign Language}}},
  booktitle = {Proceedings of the 3rd {{International Universal Communication Symposium}}},
  author = {Yasugahira, Yuta and Horiuchi, Yasuo and Kuroiwa, Shingo},
  year = {2009},
  pages = {331--334},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1667780.1667849},
  abstract = {To achieve the greater accessibility for deaf people, sign language recognition systems and sign language animation systems must be developed. In Japanese sign language (JSL), previous studies have suggested that emphasis and emotion cause changes in hand movements. However, the relationship between emphasis and emotion and the signing speed has not been researched enough. In this study, we analyzed the hand movement variation in relation to the signing speed. First, we recorded 20 signed sentences at three speeds (fast, normal, and slow) using a digital video recorder and a 3D position sensor. Second, we segmented sentences into three types of components (sign words, transitions, and pauses). In our previous study, we analyzed hand movement variations of sign words in relation to the signing speed. In this study, we analyzed transitions between adjacent sign words by a method similar to that in the previous study. As a result, sign words and transitions showed a similar tendency, and we found that the variation in signing speed mainly caused changes in the distance hands moved. Furthermore, we compared transitions with sign words and found that transitions were slower than sign words.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yasugahira et al_2009_Analysis of Hand Movement Variation Related to Speed in Japanese Sign Language.pdf},
  isbn = {978-1-60558-641-0},
  series = {{{IUCS}} '09}
}

@inproceedings{ye_recognizing_2018,
  title = {Recognizing {{American Sign Language Gestures From Within Continuous Videos}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Ye, Yuancheng and Tian, Yingli and Huenerfauth, Matt and Liu, Jingya and Ruiz, Nataniel and Chong, Eunji and Rehg, James M. and Palsson, Sveinn and Agustsson, Eirikur and Timofte, Radu},
  year = {2018},
  month = jun,
  pages = {2064--2073},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Ye et al_2018_Recognizing American Sign Language Gestures From Within Continuous Videos.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FHXUP79U/Ye_Recognizing_American_Sign_CVPR_2018_paper.html},
  keywords = {capturedBy=depth,capturedBy=vision,continuous,continuous;vocab=27;signer=14;sl=ASL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=14,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2horientation;modal=2hshape,vocab<50,vocab=27}
}

@article{yeasin_visual_2000,
  title = {Visual Understanding of Dynamic Hand Gestures},
  author = {Yeasin, M. and Chaudhuri, S.},
  year = {2000},
  month = nov,
  volume = {33},
  pages = {1805--1817},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(99)00175-2},
  abstract = {Analysis of a dynamic hand gesture requires processing a spatio-temporal image sequence. The actual length of the sequence varies with each instantiation of the gesture. The key idea behind solving the problem is to translate the richness of the human gestural communication power to a machine for a better man\textendash machine interaction. We propose a novel vision-based system for automatic interpretation of a limited set of dynamic hand gestures. This involves extracting the temporal signature of the hand motion from the performed gesture. The concept of motion energy is used to estimate the dominant motion from an image sequence. To achieve the desired result, we introduce the concept of modeling the dynamic hand gesture using a finite state machine. The temporal signature is subsequently analyzed by the finite state machine to interpret automatically the performed gesture.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yeasin_Chaudhuri_2000_Visual understanding of dynamic hand gestures.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/CMW3QJFI/S0031320399001752.html},
  journal = {Pattern Recognition},
  number = {11}
}

@incollection{yildirim_cooperative_2009,
  title = {Cooperative {{Sign Language Tutoring}}: {{A Multiagent Approach}}},
  shorttitle = {Cooperative {{Sign Language Tutoring}}},
  booktitle = {Engineering {{Societies}} in the {{Agents World X}}},
  author = {Y{\i}ld{\i}r{\i}m, Ilker and Aran, Oya and Yolum, P{\i}nar and Akarun, Lale},
  year = {2009},
  pages = {213--228},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yıldırım et al_2009_Cooperative Sign Language Tutoring.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/99TAQR9K/978-3-642-10203-5_18.html}
}

@article{yin_dreaming_2020,
  title = {Dreaming to {{Distill}}: {{Data}}-Free {{Knowledge Transfer}} via {{DeepInversion}}},
  shorttitle = {Dreaming to {{Distill}}},
  author = {Yin, Hongxu and Molchanov, Pavlo and Li, Zhizhong and Alvarez, Jose M. and Mallya, Arun and Hoiem, Derek and Jha, Niraj K. and Kautz, Jan},
  year = {2020},
  month = jun,
  abstract = {We introduce DeepInversion, a new method for synthesizing images from the image distribution used to train a deep neural network. We 'invert' a trained network (teacher) to synthesize class-conditional input images starting from random noise, without using any additional information about the training dataset. Keeping the teacher fixed, our method optimizes the input while regularizing the distribution of intermediate feature maps using information stored in the batch normalization layers of the teacher. Further, we improve the diversity of synthesized images using Adaptive DeepInversion, which maximizes the Jensen-Shannon divergence between the teacher and student network logits. The resulting synthesized images from networks trained on the CIFAR-10 and ImageNet datasets demonstrate high fidelity and degree of realism, and help enable a new breed of data-free applications - ones that do not require any real images or labeled data. We demonstrate the applicability of our proposed method to three tasks of immense practical importance -- (i) data-free network pruning, (ii) data-free knowledge transfer, and (iii) data-free continual learning. Code is available at https://github.com/NVlabs/DeepInversion},
  archivePrefix = {arXiv},
  eprint = {1912.08795},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yin et al_2020_Dreaming to Distill.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/YPVUNQYP/1912.html},
  journal = {arXiv:1912.08795 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{yin_iterative_2016,
  title = {Iterative {{Reference Driven Metric Learning}} for {{Signer Independent Isolated Sign Language Recognition}}},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Yin, Fang and Chai, Xiujuan and Chen, Xilin},
  year = {2016},
  month = oct,
  pages = {434--450},
  address = {{Amsterdam, The Netherlands}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yin et al_2016_Iterative Reference Driven Metric Learning for Signer Independent Isolated Sign.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/UI6QDRMI/978-3-319-46478-7_27.html},
  keywords = {capturedBy=depth,capturedBy=vision,dataSet=Devisign-L,isolated,isolated;vocab=1000;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=1000;signer=7;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=2000;signer=8;sl=CSL;capturedBy=vision;capturedBy=depth;dataSet=Devisign-L,modal=2hmovement,modal=2hshape,signer=1,signer=7,signer=8,sl=CSL,slreco,slreco;modal=2hmovement;modal=2hshape,vocab=1000,vocab=2000,vocab>=1000}
}

@inproceedings{yin_learning_2009,
  title = {Learning the Basic Units in {{American Sign Language}} Using Discriminative Segmental Feature Selection},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Yin, P. and Starner, T. and Hamilton, H. and Essa, I. and Rehg, J.M.},
  year = {2009},
  month = apr,
  pages = {4757--4760},
  address = {{Taipei, Taiwan}},
  abstract = {The natural language for most deaf signers in the United States is American Sign Language (ASL). ASL has internal structure like spoken languages, and ASL linguists have introduced several phonemic models. The study of ASL phonemes is not only interesting to linguists, but also useful for scalability in recognition by machines. Since machine perception is different than human perception, this paper learns the basic units for ASL directly from data. Comparing with previous studies, our approach computes a set of data-driven units (fenemes) discriminatively from the results of segmental feature selection. The learning iterates the following two steps: first apply discriminative feature selection segmentally to the signs, and then tie the most similar temporal segments to re-train. Intuitively, the sign parts indistinguishable to machines are merged to form basic units, which we call ASL fenemes. Experiments on publicly available ASL recognition data show that the extracted data-driven fenemes are meaningful, and recognition using those fenemes achieves improved accuracy at reduced model complexity.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yin et al_2009_Learning the basic units in American Sign Language using discriminative.pdf},
  keywords = {capturedBy=electronicGlove,continuous,continuous;vocab=141;signer=1;sl=ASL;capturedBy=electronicGlove,modal=2hlocation,modal=2hmovement,signer=1,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement,vocab<500,vocab=141}
}

@inproceedings{yin_semantics_2015,
  title = {Semantics {{Constrained Dictionary Learning}} for {{Signer}}-{{Independent Sign Language Recognition}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Yin, Fang and Chai, Xiujuan and Zhou, Yu and Chen, Xilin},
  year = {2015},
  month = sep,
  pages = {3310--3314},
  address = {{Quebec, Canada}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yin et al_2015_Semantics constrained dictionary learning for signer-independent sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/87WKECFA/login.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=1000;signer=7;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=370;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=1,signer=7,sl=CSL,slreco,slreco;modal=2hmovement;modal=2hshape;modal=bodyjoints,vocab<500,vocab=1000,vocab=370,vocab>=1000}
}

@article{yin_sign_2020,
  title = {Sign {{Language Translation}} with {{Transformers}}},
  author = {Yin, Kayo},
  year = {2020},
  month = apr,
  abstract = {Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR) system to extract sign language glosses from videos. Then, a translation system generates spoken language translations from the sign language glosses. Though SLT has gathered interest recently, little study has been performed on the translation system. This paper focuses on the translation system and improves performance by utilizing Transformer networks. We report a wide range of experimental results for various Transformer setups and introduce the use of Spatial-Temporal Multi-Cue (STMC) networks in an end-to-end SLT system with Transformer.},
  archivePrefix = {arXiv},
  eprint = {2004.00588},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yin_2020_Sign Language Translation with Transformers.pdf},
  journal = {arXiv:2004.00588 [cs]},
  keywords = {translation;vocab=21600;sl=ASL;capturedBy=text,translation;vocab=2887;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,translation;vocab=7712;sl=ASL;capturedBy=text},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{yin_weakly_2015,
  title = {Weakly {{Supervised Metric Learning}} towards {{Signer Adaptation}} for {{Sign Language Recognition}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Yin, Fang and Chai, Xiujuan and Zhou, Yu and Chen, Xilin},
  year = {2015},
  month = sep,
  pages = {35.1-35.12},
  address = {{Swansea, UK}},
  doi = {10.5244/C.29.35},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yin et al_2015_Weakly Supervised Metric Learning towards Signer Adaptation for Sign Language.pdf},
  isbn = {978-1-901725-53-7},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=1000;signer=7;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,signer=7,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=bodyjoints,vocab=1000,vocab>=1000},
  language = {en}
}

@article{you_large_2017,
  ids = {you\_large\_2017a},
  title = {Large {{Batch Training}} of {{Convolutional Networks}}},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  year = {2017},
  month = sep,
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1708.03888},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/76WWWT2P/You et al. - 2017 - Large Batch Training of Convolutional Networks.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SKES44XT/You et al. - 2017 - Large Batch Training of Convolutional Networks.pdf},
  journal = {arXiv:1708.03888 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{young_treebased_1994,
  title = {Tree-Based State Tying for High Accuracy Acoustic Modelling},
  booktitle = {Proc. of the Workshop on {{Human Language Technology}}},
  author = {Young, Steve J. and Odell, Julian J. and Woodland, Philip C.},
  year = {1994},
  pages = {307--312},
  publisher = {{Association for Computational Linguistics}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Young et al_1994_Tree-based state tying for high accuracy acoustic modelling.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/9UC87MHM/citation.html}
}

@inproceedings{yu_consensus_2014,
  title = {Consensus of Regression for Occlusion-Robust Facial Feature Localization},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Yu, Xiang and Lin, Zhe and Brandt, Jonathan and Metaxas, Dimitris N.},
  year = {2014},
  month = sep,
  pages = {105--118},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yu et al_2014_Consensus of regression for occlusion-robust facial feature localization.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/B4JRSRJQ/978-3-319-10593-2_8.html}
}

@inproceedings{yu_deep_2016,
  title = {Deep Convolutional Neural Networks with Layer-Wise Context Expansion and Attention},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Yu, Dong and Xiong, Wayne and Droppo, Jasha and Stolcke, Andreas and Ye, Guoli and Li, Jinyu and Zweig, Geoffrey},
  year = {2016},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yu et al_2016_Deep convolutional neural networks with layer-wise context expansion and.pdf}
}

@inproceedings{yu_direct_2015,
  title = {Direct, {{Dense}}, and {{Deformable}}: {{Template}}-{{Based Non}}-{{Rigid 3D Reconstruction}} from {{RGB Video}}},
  shorttitle = {Direct, {{Dense}}, and {{Deformable}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Computer Vision}} ({{ICCV}})},
  author = {Yu, Rui and Russell, Chris and Campbell, Neill and Agapito, Lourdes},
  year = {2015},
  pages = {918--926},
  publisher = {{University of Bath}},
  address = {{Santiago, Chile}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yu et al_2015_Direct, Dense, and Deformable.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GGN7A8FI/47979.html}
}

@inproceedings{yuan_large_2019,
  ids = {yuan\_large\_2019},
  title = {Large {{Scale Sign Language Interpretation}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Yuan, Tiantian and Sah, Shagan and Ananthanarayana, Tejaswini and Zhang, Chi and Bhat, Aneesh and Gandhi, Sahaj and Ptucha, Raymond},
  year = {2019},
  month = may,
  pages = {1--5},
  issn = {null},
  doi = {10.1109/FG.2019.8756506},
  abstract = {Sign language is the primary way of communication between deaf people, but the majority of hearing people do not know how to sign. The reliance of deaf people on interpreters is both inconvenient and cost inefficient. Many research groups have experimented with using machine learning to develop automatic translators. Largely, these efforts have been constrained to restrictive dictionaries or insufficiently small signers or signed content. We introduce the world's largest sign language dataset to date- a collection of 50,000 video snippets taken from a pool of 10,000 unique utterances signed by 50 signers. We further propose several sequence-to-sequence deep learning approaches to automatically translate from Chinese sign language to both English and Mandarin written text. These methods utilize body joint position, facial expression, as well as finger articulation. While models can overfit on training sets, generalization to unforeseen utterances remains challenging with real-world data. The introduced dataset and methods demonstrate how modern machine learning methods are able to close the communication gap between deaf and hearing people.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yuan et al_2019_Large Scale Sign Language Interpretation2.pdf}
}

@inproceedings{yuan_recognition_2002,
  title = {Recognition of Strong and Weak Connection Models in Continuous Sign Language},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Pattern Recognition}} ({{ICPR}})},
  author = {Yuan, Quan and Geo, Wen and Yao, Hongxun and Wang, Chunli},
  year = {2002},
  month = aug,
  volume = {1},
  pages = {75-78 vol.1},
  address = {{Quebec, Canada}},
  doi = {10.1109/ICPR.2002.1044616},
  abstract = {A new method to recognize continuous sign language based on hidden Markov model is proposed. According to the dependence of linguistic context, connections between elementary subwords are classified as strong connection and weak connection. The recognition of strong connection is accomplished with the aid of subword trees, which describe the connection of subwords in each sign language word. In weak connection, the main problem is how to extract the best matched subwords and find their end-points with little help of context information. The proposed method improves the summing process of the Viterbi decoding algorithm which is constrained in every individual model, and compares the end score at each frame to find the ending frame of a subword. Experimental results show an accuracy of 70\% for continuous sign sentences that comprise no more than 4 subwords.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Yuan et al_2002_Recognition of strong and weak connection models in continuous sign language.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6UFFJHQ8/abs_all.html},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=40;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,sl=CSL,slreco,slreco;modal=2hshape;modal=2horientation;modal=2hmovement;modal=2hlocation,vocab<50,vocab=40}
}

@inproceedings{yugopuspito_mobile_2018,
  title = {Mobile {{Sign Language Recognition}} for {{Bahasa Indonesia Using Convolutional Neural Network}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Advances}} in {{Mobile Computing}} and {{Multimedia}}},
  author = {Yugopuspito, Pujianto and Murwantara, I. Made and Sean, Jessica},
  year = {2018},
  month = nov,
  pages = {84--91},
  address = {{New York, NY, USA}},
  doi = {10.1145/3282353.3282356},
  abstract = {Hand gestures for speech impaired community have their usage for specific language. In Indonesia, hand gesture has their natural two hands sign and widely accepted usage, BISINDO (Bahasa Indonesia Sign Language). In this paper, we propose to use a mobile application to support people who want to communicate with speech impaired people based on BISINDO. We make use the Convolutional Neural Network method to identify the hand gesture in a real time Android mobile application. For training the image dataset, we make use of MobileNet algorithm that have satisfied us with good result, on top of a Machine Learning Framework, TensorFlow. The percentage of success has been influenced by the image reference size and the optimizer algorithm. The highest performance of implemented model reached 95.13\% on its accuracy rate from 23 hand gestures from 13.802 images as dataset, and achieved 100\% success on some hand gestures.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8U58EZDC/Yugopuspito et al. - 2018 - Mobile Sign Language Recognition for Bahasa Indone.pdf},
  isbn = {978-1-4503-6452-2},
  keywords = {capturedBy=vision,isolated,isolated;vocab=23;sl=IndonesianSL;capturedBy=vision,modal=shape,sl=IndonesianSL,slreco,slreco;modal=shape,vocab<50,vocab=23},
  series = {{{MoMM2018}}}
}

@inproceedings{zablotskiy_subword_2015,
  title = {Sub-Word {{Language Modeling}} for {{Russian LVCSR}}},
  booktitle = {Speech and {{Computer}}},
  author = {Zablotskiy, Sergey and Minker, Wolfgang},
  editor = {Ronzhin, Andrey and Potapova, Rodmonga and Fakotakis, Nikos},
  year = {2015},
  pages = {413--421},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23132-7_51},
  abstract = {Russian is a highly inflected language with rich morphology. It is characterized by the low lexical coverage, high out-of-vocabulary (OOV) rate and perplexity. Therefore, the large vocabulary continuous speech recognition (LVCSR) of Russian and languages with similar morphology still remains to be a challenging task. Augmenting the full-word language model by fragments is a well-known approach targeting this challenge which also allows us to recognize missing words in the lexicon (open vocabulary recognition). In this paper we suggest a novel ``double-sided'' approach for marking word fragments, which reduces the WER by up to 3.7 \% absolute (20.8 \% relative) compared to the full-word baseline and by up to 1.1 \% absolute (7.2 \% relative) compared to the corresponding sub-word baseline, tested on evaluation set. Moreover, the type of word decomposition (syllables or morpheme-like units), their smallest size and optimal number of non-fragmented words were also investigated for Russian LVCSR.},
  isbn = {978-3-319-23132-7},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{zadghorban_algorithm_2018,
  title = {An {{Algorithm}} on {{Sign Words Extraction}} and {{Recognition}} of {{Continuous Persian Sign Language Based}} on {{Motion}} and {{Shape Features}} of {{Hands}}},
  author = {Zadghorban, Masoud and Nahvi, Manoochehr},
  year = {2018},
  month = may,
  volume = {21},
  pages = {323--335},
  issn = {1433-7541},
  doi = {10.1007/s10044-016-0579-2},
  abstract = {Sign language is the most important means of communication for deaf people. Given the lack of familiarity of non-deaf people with the language of deaf people, designing a translator system which facilitates the communication of deaf people with the surrounding environment seems to be necessary. The system of translating the sign language into spoken languages should be able to identify the gestures in sign language videos. Consequently, this study provides a system based on machine vision to recognize the signs in continuous Persian sign language video. This system generally consists of two main phases of sign words extraction and their classification. Several stages, including tracking and separating the sign words, are conducted in the sign word extraction phase. The most challenging part of this process is separation of sign words from video sequences. To do this, a new algorithm is presented which is capable of detecting accurate boundaries of words in the Persian sign language video. This algorithm decomposes sign language video into the sign words using motion and hand shape features, leading to more favorable results compared to the other methods presented in the literature. In the classification phase, separated words are classified and recognized using hidden Markov model and hybrid KNN-DTW algorithm, respectively. Due to the lack of proper database on Persian sign language, the authors prepared a database including several sentences and words performed by three signers. Simulation of proposed words boundary detection and classification algorithms on the above database led to the promising results. The results indicated an average rate of 93.73 \% for accurate words boundary detection algorithm and the average rate of 92.4 and 92.3 \% for words recognition using hands motion and shape features, respectively.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zadghorban_Nahvi_2018_An Algorithm on Sign Words Extraction and Recognition of Continuous Persian.pdf},
  journal = {Pattern Anal. Appl.},
  keywords = {capturedBy=vision,isolated,isolated;vocab=46;signer=3;sl=PersianSL;capturedBy=vision,modal=movement,modal=shape,signer=3,sl=PersianSL,slreco,slreco;modal=shape;modal=movement,vocab<50,vocab=46},
  number = {2}
}

@inproceedings{zafrulla_american_2011,
  title = {American {{Sign Language Recognition}} with the {{Kinect}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Multimodal Interfaces}} ({{ICMI}})},
  author = {Zafrulla, Zahoor and Brashear, Helene and Starner, Thad and Hamilton, Harley and Presti, Peter},
  year = {2011},
  month = nov,
  pages = {279--286},
  address = {{Alicante, Spain}},
  doi = {10.1145/2070481.2070532},
  abstract = {We investigate the potential of the Kinect depth-mapping camera for sign language recognition and verification for educational games for deaf children. We compare a prototype Kinect-based system to our current CopyCat system which uses colored gloves and embedded accelerometers to track children's hand movements. If successful, a Kinect-based approach could improve interactivity, user comfort, system robustness, system sustainability, cost, and ease of deployment. We collected a total of 1000 American Sign Language (ASL) phrases across both systems. On adult data, the Kinect system resulted in 51.5\% and 76.12\% sentence verification rates when the users were seated and standing respectively. These rates are comparable to the 74.82\% verification rate when using the current(seated) CopyCat system. While the Kinect computer vision system requires more tuning for seated use, the results suggest that the Kinect may be a viable option for sign verification.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zafrulla et al_2011_American Sign Language Recognition with the Kinect.pdf},
  isbn = {978-1-4503-0641-6},
  keywords = {capturedBy=depth,capturedBy=vision,continuous,continuous;vocab=19;signer=7;sl=ASL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=7,sl=ASL,slreco,slreco;modal=2hshape;modal=2hmovement;modal=2hlocation,vocab<50,vocab=19}
}

@inproceedings{zafrulla_novel_2010,
  title = {A Novel Approach to {{American Sign Language}} ({{ASL}}) Phrase Verification Using Reversed Signing},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Zafrulla, Z. and Brashear, H. and Hamilton, H. and Starner, T.},
  year = {2010},
  month = jun,
  pages = {48--55},
  doi = {10.1109/CVPRW.2010.5543268},
  abstract = {We propose a novel approach for American Sign Langauge (ASL) phrase verification that combines confidence measures (CM) obtained from aligning forward sign models (the conventional approach) to the input data with the CM's obtained from aligning reversed sign models to the same input. To demonstrate our approach we have used two CM's, the Normalized likelihood score and the Log-Likelihood Ratio (LLR).We perform leave-one-signer-out cross validation on a dataset of 420 ASL phrases obtained from five deaf children playing an educational game called CopyCat. The results show that for the new method the alignment selected for signs in a test phrase has a significantly better match to the ground truth when compared to the traditional approach. Additionally, when a low false reject rate is desired the new technique can provide a better verification accuracy as compared to the conventional approach.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zafrulla et al_2010_A novel approach to American Sign Language (ASL) phrase verification using.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/6SHBPB3B/5543268.html},
  keywords = {capturedBy=coloredGlove,capturedBy=electronicGlove,continuous,continuous;vocab=19;signer=5;sl=ASL;capturedBy=coloredGlove;capturedBy=electronicGlove,modal=2hlocation,modal=2hmovement,modal=2hshape,signer=5,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape,vocab<50,vocab=19}
}

@inproceedings{Zagoruyko2016WRN,
  title = {Wide {{Residual Networks}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2016},
  address = {{York, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zagoruyko_Komodakis_2016_Wide Residual Networks.pdf}
}

@inproceedings{zahedi_appearancebased_2005,
  title = {Appearance-{{Based Recognition}} of {{Words}} in {{American Sign Language}}},
  booktitle = {Proc. {{Iberian Conf}}. on {{Pattern Recognition}} and {{Image Analysis}} ({{IbPRIA}})},
  author = {Zahedi, Morteza and Keysers, Daniel and Ney, Hermann},
  year = {2005},
  month = jun,
  pages = {513--520},
  address = {{Estoril, Portugal}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zahedi et al_2005_Appearance-Based Recognition of Words in American Sign Language.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=10;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,signer=3,sl=ASL,slreco,slreco;modal=2hlocation,vocab<50,vocab=10}
}

@inproceedings{zahedi_combination_2005,
  title = {Combination of {{Tangent Distance}} and an {{Image Distortion Model}} for {{Appearance}}-{{Based Sign Language Recognition}}},
  booktitle = {Deutsche {{Arbeitsgemeinschaft}} F\"ur {{Mustererkennung Symposium}}},
  author = {Zahedi, Morteza and Keysers, Daniel and Deselaers, Thomas and Ney, Hermann},
  year = {2005},
  month = aug,
  volume = {3663},
  pages = {401--408},
  address = {{Vienna, Austria}},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{zahedi_continuous_2006,
  title = {Continuous {{Sign Language Recognition}} - {{Approaches}} from {{Speech Recognition}} and {{Available Data Resources}}},
  booktitle = {{{LREC Workshop}} on the {{Representation}} and {{Processing}} of {{Sign Languages}}},
  author = {Zahedi, Morteza and Dreuw, Philippe and Rybach, David and Deselaers, Thomas and Bungeroth, Jan and Ney, Hermann},
  year = {2006},
  month = may,
  pages = {21--24},
  address = {{Genoa, Italy}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zahedi et al_2006_Continuous Sign Language Recognition - Approaches from Speech Recognition and.pdf}
}

@inproceedings{zahedi_pronunciation_2005,
  title = {Pronunciation {{Clustering}} and {{Modeling}} of {{Variability}} for {{Appearance}}-{{Based Sign Language Recognition}}},
  booktitle = {International {{Workshop}} on {{Gesture}} in {{Human}}-{{Computer Interaction}} and {{Simulation}}},
  author = {Zahedi, Morteza and Keysers, Daniel and Ney, Hermann},
  year = {2005},
  month = may,
  volume = {3881},
  pages = {68--79},
  address = {{Ile-de-Berder, France}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zahedi et al_2005_Pronunciation Clustering and Modeling of Variability for Appearance-Based Sign.pdf},
  keywords = {capturedBy=vision,isolated,isolated;vocab=50;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,signer=3,sl=ASL,slreco,slreco;modal=2hlocation,vocab<100,vocab=50}
}

@phdthesis{zahedi_robust_2007,
  title = {Robust {{Appearance}}-Based {{Sign Language Recognition}}},
  author = {Zahedi, Morteza},
  year = {2007},
  month = sep,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zahedi_2007_Robust Appearance-based Sign Language Recognition.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=102;signer=3;sl=ASL;capturedBy=vision;dataSet=BU-104,dataSet=BU-104,dataSet=BU-50,isolated,isolated;vocab=10;signer=2;sl=ASL;capturedBy=vision,isolated;vocab=50;signer=3;sl=ASL;capturedBy=vision;dataSet=BU-50,modal=2hlocation,modal=movement,modal=shape,signer=2,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=shape;modal=movement,vocab<100,vocab<50,vocab<500,vocab=10,vocab=102,vocab=50},
  school = {RWTH Aachen University}
}

@inproceedings{zahedi_using_2006,
  title = {Using {{Geometric Features}} to {{Improve Continuous Appearance}}-Based {{Sign Language Recognition}}},
  booktitle = {Proc. {{British Machine Vision Conference}} ({{BMVC}})},
  author = {Zahedi, Morteza and Dreuw, Philippe and Rybach, David and Deselaers, Thomas and Ney, Hermann},
  year = {2006},
  month = sep,
  volume = {3},
  pages = {1019--1028},
  address = {{Edinburgh, UK}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zahedi et al_2006_Using Geometric Features to Improve Continuous Appearance-based Sign Language.pdf},
  keywords = {capturedBy=vision,continuous,continuous;vocab=103;signer=3;sl=ASL;capturedBy=vision;dataSet=BU-104,dataSet=BU-104,modal=location,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=shape;modal=location,vocab<500,vocab=103}
}

@article{zahedi_using_2008,
  title = {Using {{Different Aspects}} of the {{Signings}} for {{Appearance}}-Based {{Sign Language Recognition}}},
  author = {Zahedi, Morteza and Dreuw, Philippe and Deselaers, Thomas and Ney, Hermann},
  year = {2008},
  month = sep,
  volume = {4},
  pages = {300--307},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zahedi et al_2008_Using Different Aspects of the Signings for Appearance-based Sign Language.pdf},
  journal = {International Journal of Computational Intelligence},
  keywords = {capturedBy=vision,continuous,continuous;vocab=102;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=movement,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=shape;modal=movement,vocab<500,vocab=102},
  number = {4}
}

@article{zaki_sign_2011,
  title = {Sign Language Recognition Using a Combination of New Vision Based Features},
  author = {Zaki, Mahmoud M. and Shaheen, Samir I.},
  year = {2011},
  volume = {32},
  pages = {572--577},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zaki_Shaheen_2011_Sign language recognition using a combination of new vision based features.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5FPUQ4KP/Zaki und Shaheen - 2011 - Sign language recognition using a combination of n.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/WPUZN8MX/S016786551000379X.html},
  journal = {Pattern Recognition Letters},
  keywords = {capturedBy=vision,isolated,isolated;vocab=30;signer=3;sl=ASL;capturedBy=vision,modal=2hlocation,modal=movement,modal=orientation,modal=shape,signer=3,sl=ASL,slreco,slreco;modal=2hlocation;modal=shape;modal=orientation;modal=movement,vocab<50,vocab=30},
  number = {4}
}

@article{zampieri_findings_2017,
  title = {Findings of the Vardial Evaluation Campaign 2017},
  author = {Zampieri, Marcos and Malmasi, Shervin and Ljube{\v s}i{\'c}, Nikola and Nakov, Preslav and Ali, Ahmed and Tiedemann, J{\"o}rg and Scherrer, Yves and Aepli, No{\"e}mi},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zampieri et al_2017_Findings of the vardial evaluation campaign 2017.pdf}
}

@inproceedings{zampieri_report_2019,
  title = {A {{Report}} on the {{Third VarDial Evaluation Campaign}}},
  booktitle = {Proceedings of the {{Sixth Workshop}} on {{NLP}} for {{Similar Languages}}, {{Varieties}} and {{Dialects}}},
  author = {Zampieri, Marcos and Malmasi, Shervin and Scherrer, Yves and Samard{\v z}i{\'c}, Tanja and Tyers, Francis and Silfverberg, Miikka and Klyueva, Natalia and Pan, Tung-Le and Huang, Chu-Ren and Ionescu, Radu Tudor and Butnaru, Andrei M. and Jauhiainen, Tommi},
  year = {2019},
  month = jun,
  pages = {1--16},
  publisher = {{Association for Computational Linguistics}},
  address = {{Ann Arbor, Michigan}},
  doi = {10.18653/v1/W19-1401},
  abstract = {In this paper, we present the findings of the Third VarDial Evaluation Campaign organized as part of the sixth edition of the workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with NAACL 2019. This year, the campaign included five shared tasks, including one task re-run \textendash{} German Dialect Identification (GDI) \textendash{} and four new tasks \textendash{} Cross-lingual Morphological Analysis (CMA), Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT), Moldavian vs. Romanian Cross-dialect Topic identification (MRC), and Cuneiform Language Identification (CLI). A total of 22 teams submitted runs across the five shared tasks. After the end of the competition, we received 14 system description papers, which are published in the VarDial workshop proceedings and referred to in this report.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zampieri et al_2019_A Report on the Third VarDial Evaluation Campaign.pdf}
}

@article{zaremba_recurrent_2014,
  title = {Recurrent Neural Network Regularization},
  author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  year = {2014},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zaremba et al_2014_Recurrent neural network regularization.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/38HN7ADK/1409.html},
  journal = {arXiv preprint arXiv:1409.2329}
}

@article{zbakh_online_2015,
  title = {An Online Reversed {{French Sign Language}} Dictionary Based on a Learning Approach for Signs Classification},
  author = {Zbakh, Mohammed and Haddad, Zehira and Lopez Krahe, Jaime},
  year = {2015},
  month = dec,
  volume = {67},
  pages = {28--38},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2015.07.041},
  abstract = {Sign Language contains very rich vocabulary units which can vary by a simple change in shape, position or movement. The characteristic of this language, to be practiced in space, has encouraged us to adopt a pragmatic approach, which facilitates access to its vocabulary. In this paper, we take advantage of digital techniques to build a human machine interface to create a dictionary of French Sign Language (FSL) to French as a web application. We notice that the FSL has not yet any fixed set of rules to learn this language. We hope that this application could contribute both in spreading and stabilizing the language. To this end, we develop a scalable research system, able to find the meaning of a FSL sign from some features of the sign itself. In this work, we set up a web platform for signs and we analyzed user requests that are connected to this platform. This analysis based on some experiments; aim to identify the parameters necessary to develop an application able to easily find the meaning of the sign in French. Therefore, by studying user behavior, we build a cognitive system that meets to needs.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zbakh et al_An online reversed French Sign Language dictionary based on a learning approach.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/L9RF6EIH/S0167865515002639.html},
  journal = {Pattern Recognition Letters},
  keywords = {Classification algorithms,French Sign Language,Human machine interface,Learning algorithms},
  language = {en},
  series = {Cognitive {{Systems}} for {{Knowledge Discovery}}}
}

@inproceedings{zelinka_neural_2020,
  title = {Neural {{Sign Language Synthesis}}: {{Words Are Our Glosses}}},
  shorttitle = {Neural {{Sign Language Synthesis}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Zelinka, Jan and Kanis, Jakub},
  year = {2020},
  pages = {3395--3403},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zelinka_Kanis_2020_Neural Sign Language Synthesis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/XU4VFFMZ/Zelinka_Neural_Sign_Language_Synthesis_Words_Are_Our_Glosses_WACV_2020_paper.html}
}

@inproceedings{zelinka_nnbased_2019,
  title = {{{NN}}-{{Based Czech Sign Language Synthesis}}},
  booktitle = {International {{Conference}} on {{Speech}} and {{Computer}}},
  author = {Zelinka, Jan and Kanis, Jakub and Salajka, Petr},
  year = {2019},
  pages = {559--568},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/36RUEVEL/978-3-030-26061-3_57.html}
}

@inproceedings{zeng_learning_2013,
  title = {Learning by Associating Ambiguously Labeled Images},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zeng, Zinan and Xiao, Shijie and Jia, Kui and Chan, Tsung-Han and Gao, Shenghua and Xu, Dong and Ma, Yi},
  year = {2013},
  pages = {708--715},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zeng et al_2013_Learning by associating ambiguously labeled images.pdf}
}

@article{zenkel_adding_2019,
  title = {Adding {{Interpretable Attention}} to {{Neural Translation Models Improves Word Alignment}}},
  author = {Zenkel, Thomas and Wuebker, Joern and DeNero, John},
  year = {2019},
  month = jan,
  abstract = {Multi-layer models with multiple attention heads per layer provide superior translation quality compared to simpler and shallower models, but determining what source context is most relevant to each target word is more challenging as a result. Therefore, deriving high-accuracy word alignments from the activations of a state-of-the-art neural machine translation model is an open challenge. We propose a simple model extension to the Transformer architecture that makes use of its hidden representations and is restricted to attend solely on encoder information to predict the next word. It can be trained on bilingual data without word-alignment information. We further introduce a novel alignment inference procedure which applies stochastic gradient descent to directly optimize the attention activations towards a given target word. The resulting alignments dramatically outperform the naive approach to interpreting Transformer attention activations, and are comparable to Giza++ on two publicly available data sets.},
  archivePrefix = {arXiv},
  eprint = {1901.11359},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zenkel et al_2019_Adding Interpretable Attention to Neural Translation Models Improves Word.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/JH7IIYPZ/1901.html},
  journal = {arXiv:1901.11359 [cs]},
  primaryClass = {cs}
}

@article{zenkel_endtoend_2020,
  title = {End-to-{{End Neural Word Alignment Outperforms GIZA}}++},
  author = {Zenkel, Thomas and Wuebker, Joern and DeNero, John},
  year = {2020},
  month = apr,
  abstract = {Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.},
  archivePrefix = {arXiv},
  eprint = {2004.14675},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zenkel et al_2020_End-to-End Neural Word Alignment Outperforms GIZA++.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/BM4MWLQ6/2004.html},
  journal = {arXiv:2004.14675 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@phdthesis{zens08:phd,
  title = {Phrase-Based {{Statistical Machine Translation}}: {{Models}}, {{Search}}, {{Training}}},
  author = {Zens, Richard},
  year = {2008},
  month = feb,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zens_2008_Phrase-based Statistical Machine Translation.pdf},
  school = {RWTH Aachen University}
}

@article{zeshan_hand_2004,
  title = {Hand, Head and Face-Negative Constructions in Sign Languages},
  author = {Zeshan, Ulrike},
  year = {2004},
  volume = {8},
  pages = {1--58},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zeshan_2004_Hand, head and face-negative constructions in sign languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C6Z76D8Z/viewItemOverviewPage.html},
  journal = {Linguistic Typology},
  number = {1}
}

@article{zeshan_interrogative_2004,
  title = {Interrogative Constructions in Signed Languages: {{Crosslinguistic}} Perspectives},
  shorttitle = {Interrogative Constructions in Signed Languages},
  author = {Zeshan, Ulrike},
  year = {2004},
  pages = {7--39},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zeshan_2004_Interrogative constructions in signed languages.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MQ59SWT8/4489610.html},
  journal = {Language}
}

@article{zeyer_comprehensive_2016,
  title = {A {{Comprehensive Study}} of {{Deep Bidirectional LSTM RNNs}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Zeyer, Albert and Doetsch, Patrick and Voigtlaender, Paul and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2016},
  month = jun,
  abstract = {We present a comprehensive study of deep bidirectional long short-term memory (LSTM) recurrent neural network (RNN) based acoustic models for automatic speech recognition (ASR). We study the effect of size and depth and train models of up to 8 layers. We investigate the training aspect and study different variants of optimization methods, batching, truncated backpropagation, different regularization techniques such as dropout and \$L\_2\$ regularization, and different gradient clipping variants. The major part of the experimental analysis was performed on the Quaero corpus. Additional experiments also were performed on the Switchboard corpus. Our best LSTM model has a relative improvement in word error rate of over 14\textbackslash\% compared to our best feed-forward neural network (FFNN) baseline on the Quaero task. On this task, we get our best result with an 8 layer bidirectional LSTM and we show that a pretraining scheme with layer-wise construction helps for deep LSTMs. Finally we compare the training calculation time of many of the presented experiments in relation with recognition performance. All the experiments were done with RETURNN, the RWTH extensible training framework for universal recurrent neural networks in combination with RASR, the RWTH ASR toolkit.},
  archivePrefix = {arXiv},
  eprint = {1606.06871},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zeyer et al_2016_A Comprehensive Study of Deep Bidirectional LSTM RNNs for Acoustic Modeling in.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/MZTBVEXF/1606.html},
  journal = {arXiv:1606.06871 [cs]},
  primaryClass = {cs}
}

@inproceedings{zeyer_ctc_2017,
  title = {{{CTC}} in the {{Context}} of {{Generalized Full}}-{{Sum HMM Training}}.},
  booktitle = {Proc. of the {{Ann}}. {{Conf}}. of the {{Int}}. {{Speech Commun}}. {{Assoc}}. ({{Interspeech}})},
  author = {Zeyer, Albert and Beck, Eugen and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2017},
  pages = {944--948},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zeyer et al_CTC in the Context of Generalized Full-Sum HMM Training.pdf}
}

@inproceedings{zhang_appearancebased_2015,
  title = {Appearance-{{Based Gaze Estimation}} in the {{Wild}}},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Xucong and Sugano, Yusuke and Fritz, Mario and Bulling, Andreas},
  year = {2015},
  pages = {4511--4520},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2015_Appearance-Based Gaze Estimation in the Wild.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/8BEETNIN/Zhang_Appearance-Based_Gaze_Estimation_2015_CVPR_paper.html}
}

@phdthesis{zhang_application_2012,
  title = {The {{Application}} of {{Source Language Information}} in {{Chinese}}-{{English Statistical Machine Translation}}},
  author = {Zhang, Yuqi},
  year = {2012},
  month = nov,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang_2012_The Application of Source Language Information in Chinese-English Statistical.pdf},
  school = {RWTH Aachen University}
}

@inproceedings{zhang_chinese_2016,
  title = {Chinese Sign Language Recognition with Adaptive {{HMM}}},
  booktitle = {{{IEEE Int}}. {{Conf}}. on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Zhang, J. and Zhou, W. and Xie, C. and Pu, J. and Li, H.},
  year = {2016},
  month = jul,
  pages = {1--6},
  doi = {10.1109/ICME.2016.7552950},
  abstract = {Sign Language Recognition (SLR) aims at translating the sign language into text or speech, so as to realize the communication between deaf-mute people and ordinary people. This paper proposes a framework based on the Hidden Markov Models (HMMs) benefited from the utilization of the trajectories and hand-shape features of the original sign videos, respectively. First, we propose a new trajectory feature (enhanced shape context), which can capture the spatio-temporal information well. Second, we fetch the hand regions by Kinect mapping functions and describe each frame by HOG (pre-processed by PCA). Moreover, in order to optimize predictions, rather than fixing the number of hidden states for each sign model, we independently determine it through the variation of the hand shapes. As for recognition, we propose a combination method to fuse the probabilities of trajectory and hand shape. At last, we evaluate our approach with our self-building Kinect-based dataset and the experiments demonstrate the effectiveness of our approach.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2016_Chinese sign language recognition with adaptive HMM.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TVW4W2WX/7552950.html},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=100;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,isolated;vocab=500;signer=1;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hshape,modal=bodyjoints,signer=1,sl=CSL,slreco,slreco;modal=2hlocation;modal=2hshape;modal=bodyjoints,vocab<1000,vocab<500,vocab=100,vocab=500}
}

@inproceedings{zhang_coarsetofine_2014,
  title = {Coarse-to-Fine Auto-Encoder Networks (Cfan) for Real-Time Face Alignment},
  booktitle = {European {{Conference}} on {{Computer Vision}} ({{ECCV}})},
  author = {Zhang, Jie and Shan, Shiguang and Kan, Meina and Chen, Xilin},
  year = {2014},
  month = sep,
  pages = {1--16},
  publisher = {{Springer}},
  address = {{Zurich, Switzerland}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2014/Zhang et al/Zhang et al_2014_Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2014_Coarse-to-fine auto-encoder networks (cfan) for real-time face alignment.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/VNUHMQ72/978-3-319-10605-2_1.html}
}

@inproceedings{zhang_continuous_2019,
  ids = {zhang\_continuous\_2019a},
  title = {Continuous {{Sign Language Recognition}} via {{Reinforcement Learning}}},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Zhang, Z. and Pu, J. and Zhuang, L. and Zhou, W. and Li, H.},
  year = {2019},
  month = sep,
  pages = {285--289},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2019.8802972},
  abstract = {In this paper, we propose an approach to apply the Transformer with reinforcement learning (RL) for continuous sign language recognition (CSLR) task. The Transformer has an encoder-decoder structure, where the encoder network encodes the sign video into the context vector representation, while the decoder network generates the target sentence word by word based on the context vector. To avoid the intrinsic defects of supervised learning (SL) in our task, e.g., the exposure bias and non-differentiable task metrics issues, we propose to train the Transformer directly on non-differentiable metrics, i.e., word error rate (WER), through RL. Moreover, a policy gradient algorithm with baseline, which we call Self-critic REINFORCE, is employed to reduce variance while training. Experimental results on RWTH-PHOENIX-Weather benchmark verify the effectiveness of our method and demonstrate that our method achieves the comparable performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2019_Continuous Sign Language Recognition via Reinforcement Learning.pdf;/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2019_Continuous Sign Language Recognition via Reinforcement Learning2.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FXK3IYVT/8802972.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/THVXTHCD/8802972.html},
  keywords = {augment=?,authorgroup=USTC,capturedBy=vision,cnnresiduals=1,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=38.0/38.3,dataSet=Phoenix14,loss=reinforce,modal=fullframe,nn=3dcnn,nn=transformer,nnarch=3d-resnet,paperinfo,paperinfo;authorgroup=USTC;shorttitle=Reinforcement Learning,re-align=0,RWTH-PHOENIX-weather benchmark,shorttitle=Reinforcement Learning,signer=9,sl=DGS,slreco,slreco;modal=fullframe;nn=3dcnn;nn=transformer;nnarch=3d-resnet;cnnresiduals=1;re-align=0;augment=?;loss=reinforce,Transformer,vocab=1080,vocab>=1000,wer=38.0/38.3}
}

@inproceedings{zhang_languageuniversal_2019,
  title = {Towards {{Language}}-{{Universal Mandarin}}-{{English Speech Recognition}}},
  booktitle = {Interspeech 2019},
  author = {Zhang, Shiliang and Liu, Yuan and Lei, Ming and Ma, Bin and Xie, Lei},
  year = {2019},
  month = sep,
  pages = {2170--2174},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1365},
  abstract = {Multilingual and code-switching speech recognition are two challenging tasks that are studied separately in many previous works. In this work, we jointly study multilingual and codeswitching problems, and present a language-universal bilingual system for Mandarin-English speech recognition. Specifically, we propose a novel bilingual acoustic model, which consists of two monolingual system initialized subnets and a shared output layer corresponding to the Character-Subword acoustic modeling units. The bilingual acoustic model is trained using a large Mandarin-English corpus with CTC and sMBR criteria. We find that this model, which is not given any information about language identity, can achieve comparable performance in monolingual Mandarin and English test sets compared to the well-trained language-specific Mandarin and English ASR systems, respectively. More importantly, the proposed bilingual model can automatically learn the language switching. Experimental results on a Mandarin-English code-switching test set show that it can achieve 11.8\% and 17.9\% relative error reduction on Mandarin and English parts, respectively.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/HTSGKIX2/Zhang et al. - 2019 - Towards Language-Universal Mandarin-English Speech.pdf},
  language = {en}
}

@inproceedings{zhang_motionpose_2017,
  title = {Motion-{{Pose Recurrent Neural Network}} with {{Instantaneous Kinematic Descriptor}} for {{Skeleton Based Gesture Detection}} and {{Recognition}}},
  booktitle = {Proc. {{Asian Conf}}. on {{Pattern Recognition}} ({{ACPR}})},
  author = {Zhang, Z. and Song, Y. and Zhang, Y.},
  year = {2017},
  month = nov,
  pages = {764--769},
  doi = {10.1109/ACPR.2017.14},
  abstract = {Skeleton based human gesture detection and recognition have been attracting increasing attention in the field of human action understanding. A number of approaches have been proposed to only explore skeleton inherent pose features by deep learning method. In this paper, we propose a novel instantaneous kinematic descriptor and a motion-pose recurrent neural network (RNN) for skeleton based gesture detection and recognition. Instead of the raw skeleton joint position as input, we propose instantaneous kinematic descriptor to represent not only the skeleton inherent pose but also the instantaneous movement at a current frame. Meanwhile, the proposed network is capable of transforming the gesture detection to the problem of frame labeling, which can model both the frame-wise dynamic motion and long-term temporal context of gestures. The proposed method is evaluated on the Chalearn LAP gesture dataset, and the result demonstrates that our method achieves the state-of-art performance in the task of skeleton-based gesture detection and recognition.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2017_Motion-Pose Recurrent Neural Network with Instantaneous Kinematic Descriptor.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F2VJS35C/8575919.html},
  keywords = {Acceleration,Chalearn LAP gesture dataset,Computer architecture,deep learning method,frame labeling,frame-wise dynamic motion,Gesture Detection,gesture recognition,image motion analysis,instantaneous kinematic descriptor,Instantaneous Kinematic Descriptor,Kinematics,learning (artificial intelligence),Logic gates,long-term temporal gesture context,Microsoft Windows,Motion-Pose,motion-pose recurrent neural network,pose estimation,pose features,raw skeleton joint position,recurrent neural nets,Recurrent Neural Network,RNN,Skeleton,Skeleton Based,skeleton based human gesture detection,skeleton based human gesture recognition,Three-dimensional displays}
}

@inproceedings{zhang_multimodality_2016,
  title = {Multi-Modality {{American Sign Language}} Recognition},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Image Processing}} ({{ICIP}})},
  author = {Zhang, C. and Tian, Y. and Huenerfauth, M.},
  year = {2016},
  month = sep,
  pages = {2881--2885},
  doi = {10.1109/ICIP.2016.7532886},
  abstract = {American Sign Language (ASL) is a visual gestural language which is used by many people who are deaf or hard-of-hearing. In this paper, we design a visual recognition system based on action recognition techniques to recognize individual ASL signs. Specifically, we focus on recognition of words in videos of continuous ASL signing. The proposed framework combines multiple signal modalities because ASL includes gestures of both hands, body movements, and facial expressions. We have collected a corpus of RBG + depth videos of multi-sentence ASL performances, from both fluent signers and ASL students; this corpus has served as a source for training and testing sets for multiple evaluation experiments reported in this paper. Experimental results demonstrate that the proposed framework can automatically recognize ASL.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2016_Multi-modality American Sign Language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/FNXVWCGJ/7532886.html},
  keywords = {capturedBy=depth,capturedBy=vision,continuous,continuous;vocab=99;signer=5;sl=ASL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=2hmovement,modal=2hshape,modal=bodyjoints,modal=eyes,modal=head,modal=mouth,signer=5,sl=ASL,slreco,slreco;modal=2hlocation;modal=2hmovement;modal=2hshape;modal=head;modal=eyes;modal=mouth;modal=bodyjoints,vocab<100,vocab=99}
}

@inproceedings{zhang_new_2015,
  title = {A New System for {{Chinese}} Sign Language Recognition},
  booktitle = {Proc. {{IEEE China Summit}} and {{Int}}. {{Conf}}. on {{Signal}} and {{Information Processing}} ({{ChinaSIP}})},
  author = {Zhang, Jihai and Zhou, Wengang and Li, Houqiang},
  year = {2015},
  month = jul,
  pages = {534--538},
  doi = {10.1109/ChinaSIP.2015.7230460},
  abstract = {In this paper, we propose a new system for isolated sign language recognition (SLR) and continuous SLR. In isolated SLR, Histogram of Oriented Displacement is used to describe the trajectories, and multi-SVM is adopted for classification. In continuous SLR, we propose a Dynamic Programming method with warping templates obtained by Dynamic Time Warping (DTW) algorithm. We evaluate our approach with 450 phrases and 180 sentences recorded by Kinect and compare with classical methods, including Hidden Markov Models and state-of-the-art Conditional Random Fields (CRF), Hidden CRF and Latent Dynamic CRF. The experiments demonstrate the effectiveness of our method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2015_A new system for Chinese sign language recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ZZJ2IR68/login.html},
  keywords = {capturedBy=depth,capturedBy=vision,continuous,continuous;vocab=30;signer=5;sl=CSL;capturedBy=vision;capturedBy=depth,isolated,isolated;vocab=30;signer=5;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hmovement,modal=bodyjoints,signer=5,sl=CSL,slreco,slreco;modal=2hmovement;modal=bodyjoints,vocab<50,vocab=30}
}

@inproceedings{zhang_thresholdbased_2014,
  title = {A {{Threshold}}-Based {{HMM}}-{{DTW Approach}} for {{Continuous Sign Language Recognition}}},
  booktitle = {Proc. of {{Int}}. {{Conf}}. on {{Internet Multimedia Computing}} and {{Service}}},
  author = {Zhang, Jihai and Zhou, Wengang and Li, Houqiang},
  year = {2014},
  month = jul,
  pages = {237:237--237:240},
  address = {{Xiamen, China}},
  doi = {10.1145/2632856.2632931},
  abstract = {Recently, great progress has been made in sign language recognition. Most approaches are based on the Hidden Markov Model (HMM) with various features, such as motion trajectory. Recognition for sign sentences is obtained from optimal path by Viterbi algorithm, however, some wrong jumps are usually caused by transitional movements between signs. To address the problem, in this paper, we propose an approach consisting of two stages: offline training and online recognition. In the offline training stage, we propose a threshold matrix and rate thresholds. Each element of the threshold matrix describes the minimal probability when a segment belongs to a sign, and rate thresholds are defined as the average probability for signs. So, if certain segment's evaluation is smaller than all the thresholds, it is regarded as a transitional movement and then it should be removed. In the online recognition stage, coarse segmentation, based on the threshold matrix, records the time interval for fine segmentation, and fine segmentation, based on Dynamic Time Warping(DTW) and Length-Root method, determines the endpoint for each candidate sign and selects the most possible one. The final recognition is obtained by concatenating the most possible signs. We evaluate our approach with Kinect-based dataset and the experiments demonstrate the effectiveness of our approach.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2014_A Threshold-based HMM-DTW Approach for Continuous Sign Language Recognition.pdf},
  isbn = {978-1-4503-2810-4},
  keywords = {capturedBy=depth,capturedBy=vision,continuous,continuous;vocab=34;signer=3;sl=CSL;capturedBy=vision;capturedBy=depth,isolated,isolated;vocab=34;signer=5;sl=CSL;capturedBy=vision;capturedBy=depth,modal=2hlocation,modal=bodyjoints,signer=3,signer=5,sl=CSL,slreco,slreco;modal=2hlocation;modal=bodyjoints,vocab<50,vocab=34}
}

@article{zhang_transitionbased_2018,
  title = {Transition-{{Based Neural Word Segmentation Using Word}}-{{Level Features}}},
  author = {Zhang, Meishan and Zhang, Yue and Fu, Guohong},
  year = {2018},
  volume = {63},
  pages = {923--953},
  doi = {10.1613/jair.1.11266},
  abstract = {Character-based and word-based methods are two different solutions for Chinese word segmentation, the former exploiting sequence labeling models over characters and the latter using word-level features. Neural models have been exploited for character-based Chinese word segmentation, giving high accuracies by making use of external character embeddings, yet requiring less feature engineering. In this paper, we study a neural model for wordbased Chinese word segmentation, by replacing the manually-designed discrete features with neural features in a transition-based word segmentation framework. Experimental results demonstrate that word features lead to comparable performance to the best systems in the literature, and a further combination of discrete and neural features obtains top accuracies on several benchmarks.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2018_Transition-Based Neural Word Segmentation Using Word-Level Features.pdf},
  journal = {J. Artif. Intell. Res.}
}

@inproceedings{zhang_understanding_2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  booktitle = {Int. {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhang et al_2016_Understanding deep learning requires rethinking generalization.pdf}
}

@inproceedings{zhao_3d_2009,
  title = {A 3d Statistical Facial Feature Model and Its Application on Locating Facial Landmarks},
  booktitle = {Advanced {{Concepts}} for {{Intelligent Vision Systems}}},
  author = {Zhao, Xi and Dellandr{\'e}a, Emmanuel and Chen, Liming},
  year = {2009},
  pages = {686--697},
  publisher = {{Springer}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhao et al_2009_A 3d statistical facial feature model and its application on locating facial.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/N2RA4ITR/978-3-642-04697-1_64.html}
}

@inproceedings{zhao_application_2018,
  title = {The {{Application}} of {{Convolution Neural Networks}} in {{Sign Language Recognition}}},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Intelligent Control}} and {{Information Processing}} ({{ICICIP}})},
  author = {Zhao, Y. and Wang, L.},
  year = {2018},
  month = nov,
  pages = {269--272},
  doi = {10.1109/ICICIP.2018.8606707},
  abstract = {There is a significant method provided by CNN in sign language recognition. In order to train the dataset efficiently, the effect of various parameters on training MNIST is discussed in detail at first. Then carry out an analog analysis, with the same method, the most appropriate parameters are selected from a series of pre-training on ASL dataset. After a formal training, a sign language recognize filter is born with an accuracy close to 90\%.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhao_Wang_2018_The Application of Convolution Neural Networks in Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5SM5YGZA/8606707.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/637J8M2X/8606707.html},
  keywords = {analog analysis,convolution neural networks,convolutional neural nets,Image classification,image classification.,Market research,sign language recognize filter,training MNIST}
}

@inproceedings{zhao_au_2010,
  title = {{{AU}} Recognition on {{3D}} Faces Based on an Extended Statistical Facial Feature Model},
  booktitle = {Proc. {{IEEE Int}}. {{Conf}}. on {{Biometrics}}: {{Theory Applications}} and {{Systems}} ({{BTAS}})},
  author = {Zhao, Xi and Dellandr{\'e}a, Emmanuel and Chen, Liming and Samaras, Dimitris},
  year = {2010},
  pages = {1--6},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhao et al_2010_AU recognition on 3D faces based on an extended statistical facial feature model.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/SRZ39B8M/cookiedetectresponse.html}
}

@article{zhao_lipreading_2009,
  title = {Lipreading {{With Local Spatiotemporal Descriptors}}},
  author = {Zhao, Guoying and Barnard, M. and Pietikainen, M.},
  year = {2009},
  month = nov,
  volume = {11},
  pages = {1254--1265},
  issn = {1520-9210},
  doi = {10.1109/TMM.2009.2030637},
  abstract = {Visual speech information plays an important role in lipreading under noisy conditions or for listeners with a hearing impairment. In this paper, we present local spatiotemporal descriptors to represent and recognize spoken isolated phrases based solely on visual input. Spatiotemporal local binary patterns extracted from mouth regions are used for describing isolated phrase sequences. In our experiments with 817 sequences from ten phrases and 20 speakers, promising accuracies of 62\% and 70\% were obtained in speaker-independent and speaker-dependent recognition, respectively. In comparison with other methods on AVLetters database, the accuracy, 62.8\%, of our method clearly outperforms the others. Analysis of the confusion matrix for 26 English letters shows the good clustering characteristics of visemes for the proposed descriptors. The advantages of our approach include local processing and robustness to monotonic gray-scale changes. Moreover, no error prone segmentation of moving lips is needed.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhao et al_2009_Lipreading With Local Spatiotemporal Descriptors.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/DHWSHTG9/articleDetails.html},
  journal = {IEEE Transactions on Multimedia},
  number = {7}
}

@article{zhao_loss_2017,
  title = {Loss {{Functions}} for {{Image Restoration With Neural Networks}}},
  author = {Zhao, H. and Gallo, O. and Frosio, I. and Kautz, J.},
  year = {2017},
  month = mar,
  volume = {3},
  pages = {47--57},
  issn = {2333-9403},
  doi = {10.1109/TCI.2016.2644865},
  abstract = {Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is {$\mathscr{l}$}2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/4D29ECKE/7797130.html},
  journal = {IEEE Transactions on Computational Imaging},
  number = {1}
}

@inproceedings{zhao_shallowfusion_2019,
  title = {Shallow-{{Fusion End}}-to-{{End Contextual Biasing}}},
  booktitle = {Interspeech 2019},
  author = {Zhao, Ding and Sainath, Tara N. and Rybach, David and Rondon, Pat and Bhatia, Deepti and Li, Bo and Pang, Ruoming},
  year = {2019},
  month = sep,
  pages = {1418--1422},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1209},
  abstract = {Contextual biasing to a specific domain, including a user's song names, app names and contact names, is an important component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in end-toend models because these models keep a small list of candidates during beam search, and also do poorly on proper nouns, which is the main source of biasing phrases. In this paper, we present various algorithmic and training improvements to shallow-fusionbased biasing for end-to-end models. We will show that the proposed approach obtains better performance than a state-ofthe-art conventional model across a variety of tasks, the first time this has been demonstrated.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/GAZYHB44/Zhao et al. - 2019 - Shallow-Fusion End-to-End Contextual Biasing.pdf},
  language = {en}
}

@inproceedings{zheng_recent_2017,
  title = {Recent {{Advances}} of {{Deep Learning}} for {{Sign Language Recognition}}},
  booktitle = {International {{Conference}} on {{Digital Image Computing}}: {{Techniques}} and {{Applications}} ({{DICTA}})},
  author = {Zheng, L. and Liang, B. and Jiang, A.},
  year = {2017},
  month = nov,
  pages = {1--7},
  doi = {10.1109/DICTA.2017.8227483},
  abstract = {To assist the social interaction of deaf and hearing impaired people, efficient interactive communication tools is expected. With the growing research interest in action and gesture recognition in the last years, many successful applications for sign language recognition comprise new types of sensors including low-cost depth camera and advanced machine learning technologies. In this paper, we present a complete overview of deep learning based methodologies for sign language recognition. We discuss various types of such approaches designed for the recognition from viewpoints of available modalities provided by depth sensors, feature extraction and classification. In addition, we summarise the currently available datasets of sign language, including gestures of finger spelling and vocabulary words, which can be used as an assessing tool for those people who are learning sign languages. We then discuss the main current research works with particular interest on how they treat the different types of data, discussing their main features and identify opportunities and challenges for future research.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zheng et al_2017_Recent Advances of Deep Learning for Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/2EEWXTQY/8227483.html}
}

@inproceedings{zheng_sign_2016,
  title = {Sign Language Recognition Using Depth Images},
  booktitle = {14th {{Int}}. {{Conf}}. on {{Control}}, {{Automation}}, {{Robotics}} and {{Vision}} ({{ICARCV}})},
  author = {Zheng, Lihong and Liang, Bin},
  year = {2016},
  month = nov,
  pages = {1--6},
  address = {{Phuket, Thailand}},
  doi = {10.1109/ICARCV.2016.7838572},
  abstract = {This paper presents a vision based sign language gesture recognition framework that can assist people with impaired hearing and speech with their social interaction and interactive communications. Utilizing a low-cost sensor, such as Microsoft Kinect combined with advanced machine learning analysis, it aims to ease the challenging issue of increasing demand for professional sign language interpreting services. Specifically, this paper discusses a powerful discriminating descriptor called 3D motion map based pyramid histograms of oriented gradient (M-PHOG) which is proposed for depth-based human gesture recognition. The 3D motion map is generated through the entire depth video sequence to encode additional motion information from three projected orthogonal planes. By adding pyramid representation, HOG descriptor is extended to M-PHOG which can characterize local shapes at different spatial grid sizes for gesture recognition. The proposed approach is evaluated on MSR Gesture3D and DEVISIGN two data sets captured by depth cameras. Experimental results show that the proposed approach outperforms the current state-of-the-art methods and demonstrates the effectiveness and robustness of the proposed 3D M-PHOG descriptor. The proposed approach can translate the meaning of captured gestures as professional interpreters currently do. The novelty framework has the potential to improve the quality of life for the deaf community and reduce the communication barriers they currently experience.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zheng_Liang_2016_Sign language recognition using depth images.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/92LW5I7L/7838572.html},
  keywords = {capturedBy=depth,capturedBy=vision,dataSet=Devisign-G,isolated,isolated;vocab=36;signer=8;sl=CSL;capturedBy=vision;capturedBy=depth;dataSet=Devisign-G,modal=fullframedepth,signer=8,sl=CSL,slreco,slreco;modal=fullframedepth,vocab<50,vocab=36}
}

@inproceedings{zhong_learning_2012,
  title = {Learning Active Facial Patches for Expression Analysis},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhong, Lin and Liu, Qingshan and Yang, Peng and Liu, Bo and Huang, Junzhou and Metaxas, Dimitris N.},
  year = {2012},
  pages = {2562--2569},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhong et al_2012_Learning active facial patches for expression analysis.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/KXA6VE4V/cookiedetectresponse.html}
}

@article{zhou_adaptive_2010,
  title = {Adaptive {{Sign Language Recognition With Exemplar Extraction}} and {{MAP}}/{{IVFS}}},
  author = {Zhou, Y. and Chen, X. and Zhao, D. and Yao, H. and Gao, W.},
  year = {2010},
  month = mar,
  volume = {17},
  pages = {297--300},
  issn = {1070-9908},
  doi = {10.1109/LSP.2009.2038251},
  abstract = {Sign language recognition systems suffer from the problem of signer dependence. In this letter, we propose a novel method that adapts the original model set to a specific signer with his/her small amount of training data. First, affinity propagation is used to extract the exemplars of signer independent hidden Markov models; then the adaptive training vocabulary can be automatically formed. Based on the collected sign gestures of the new vocabulary, the combination of maximum a posteriori and iterative vector field smoothing is utilized to generate signer-adapted models. Experimental results on six signers demonstrate that the proposed method can reduce the amount of the adaptation data and still can achieve high recognition performance.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhou et al_2010_Adaptive Sign Language Recognition With Exemplar Extraction and MAP-IVFS.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/NZUSBTFX/5350707.html},
  journal = {IEEE Signal Processing Letters},
  keywords = {capturedBy=electronicGlove,capturedBy=mocap,isolated,isolated;vocab=256;signer=6;sl=CSL;capturedBy=electronicGlove;capturedBy=mocap,modal=2hlocation,modal=2hmovement,modal=2horientation,modal=2hshape,signer=6,sl=CSL,slreco,slreco;modal=2hshape;modal=2hlocation;modal=2hmovement;modal=2horientation,vocab<500,vocab=256},
  number = {3}
}

@inproceedings{zhou_aligned_2008,
  title = {Aligned Cluster Analysis for Temporal Segmentation of Human Motion},
  booktitle = {Proc. {{Int}}. {{Conf}}. on {{Automatic Face}} and {{Gesture Recognition}} ({{FG}})},
  author = {Zhou, Feng and Torre, F. and Hodgins, Jessica K.},
  year = {2008},
  pages = {1--7},
  publisher = {{IEEE}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhou et al_2008_Aligned cluster analysis for temporal segmentation of human motion.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IE7KS239/abs_all.html}
}

@inproceedings{zhou_dynamic_2019,
  title = {Dynamic {{Pseudo Label Decoding}} for {{Continuous Sign Language Recognition}}},
  booktitle = {2019 {{IEEE International Conference}} on {{Multimedia}} and {{Expo}} ({{ICME}})},
  author = {Zhou, Hao and Zhou, Wengang and Li, Houqiang},
  year = {2019},
  month = jul,
  pages = {1282--1287},
  issn = {1945-788X},
  doi = {10.1109/ICME.2019.00223},
  abstract = {Continuous sign language recognition is a weakly supervised problem to translate video sequence to sign gloss sequence, where temporal boundary of each sign gloss is not annotated. The CNN-RNN-CTC framework shows effectiveness in this task by estimating pseudo label for each clip and retraining the feature extractor alternately. The quality of pseudo labels greatly impacts the final performance. In contrast of existing methods which select labels of maximum posterior probability, we propose a dynamic pseudo label decoding method to find a reasonable alignment path via dynamic-programming. Our approach filters out apparently wrong labels and generates pseudo labels which conform to natural word order of sign language. To further boost the performance after iterative optimization, we introduce a temporal ensemble module equipped with BGRU and 1D-CNN to integrate features from different time scales. Experiments on two continuous sign language benchmarks with large vocabulary show the effectiveness of our proposed method.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhou et al_2019_Dynamic Pseudo Label Decoding for Continuous Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/EZMEWTWB/8784863.html},
  keywords = {augment=?,authorgroup=USTC,capturedBy=vision,CNN-RNN-CTC framework,cnnresiduals=0,continuous,continuous sign language recognition,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=35.6/34.5,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,convolutional neural nets,CTC,dataSet=CSL,dataSet=Phoenix14,loss=CE,loss=CTC,loss=KL,modal=fullframe,nn=1dcnn,nn=3dcnn,nn=bgru,nnarch=i3d,paperinfo,paperinfo;authorgroup=USTC;shorttitle=Pseudo Label Decoding,re-align=1,shorttitle=Pseudo Label Decoding,sign language recognition,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;nn=3dcnn;nn=bgru;nn=1dcnn;nnarch=i3d;cnnresiduals=0;re-align=1;augment=?;loss=CTC;loss=CE;loss=KL,vocab<500,vocab=1080,vocab=178,vocab>=1000,wer=35.6/34.5}
}

@article{zhou_modelbased_,
  title = {Model-Based {{Deep Hand Pose Estimation}}},
  author = {Zhou, Xingyi and Wan, Qingfu and Zhang, Wei and Xue, Xiangyang and Wei, Yichen},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhou et al_Model-based Deep Hand Pose Estimation.pdf}
}

@inproceedings{zhou_practical_2011,
  title = {Towards a Practical Lipreading System},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Ziheng and Zhao, Guoying and Pietikainen, Matti},
  year = {2011},
  pages = {137--144},
  doi = {10.1109/CVPR.2011.5995345},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhou et al_2011_Towards a practical lipreading system.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/TIMRG642/abs_all.html}
}

@article{zhou_selfattentionbased_2020,
  title = {Self-{{Attention}}-Based {{Fully}}-{{Inception Networks}} for {{Continuous Sign Language Recognition}}},
  author = {Zhou, Mingjie and Ng, Michael and Cai, Zixin and Cheung, Ka Chun},
  year = {2020},
  pages = {8},
  abstract = {In hearing-loss community, sign language is a primary tool to communicate with people while there is communication gap between hearing-loss people with normal hearing people. Continuous sign language recognition, which can bridge the communication gap, is a challenging task because of the weakly supervised ordered annotations where no frame-level label is provided. To overcome this problem, connectionist temporal classification (CTC) is the most widely used method. However, CTC learning could perform bad if extracted features are not good. For better feature extraction, this work presents the novel self-attention-based fully-inception (SAFI) networks for vision-based end-to-end continuous sign language recognition. Considering the length of sign words differs from each other, we introduce fully inception network with different receptive field to extract dynamic clip-level features. To further boost the performance, the fully inception network with an auxiliary classifier is trained with aggregation cross entropy (ACE) loss. Then the selfattention networks as global sequential feature extractor is used to model the clip-level features with CTC. The proposed model is optimized by jointly training with ACE on clip-level feature learning and CTC on global sequential feature learning in an end-to-end fashion. The best method in the baselines achieves 35.6\% WER on validation set and 34.5\% WER on test set. It employs a better decoding algorithm for pseudo label to do the EM-like optimization to fine tune CNN module. In contrast, our approach focuses on the better feature extraction for end-to-end learning. To alleviate the overfitting on the limited dataset, we employ temporal elastic deformation to triple the real-world dataset RWTH-PHOENIX-Weather 2014. Experimental results on the real-world dataset RWTH-PHOENIX-Weather 2014 demonstrate the effectiveness of our approach which achieves 31.7\% WER on validation set and 31.3\% WER on test set.},
  file = {/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/F5FWPLFN/Zhou et al. - 2020 - Self-Attention-based Fully-Inception Networks for .pdf},
  journal = {Santiago de Compostela},
  keywords = {augment=randomcrop,augment=temp-scale,authorgroup=BJTU,authorgroup=HKBU,authorgroup=HKU,authorgroup=Nvidia,capturedBy=vision,cnnresiduals=0,continuous,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=31.7/31.3,dataSet=Phoenix14,loss=ACE,loss=CTC,modal=fullframe,nn=1dcnn,nn=2dcnn,nn=transformer,nnarch=inception,paperinfo,paperinfo;authorgroup=HKBU;authorgroup=HKU;authorgroup=BJTU;authorgroup=Nvidia;shorttitle=Fully-Inception Networks,re-align=0,shorttitle=Fully-Inception Networks,signer=9,sl=DGS,slreco,slreco;modal=fullframe;nn=2dcnn;nn=1dcnn;nn=transformer;nnarch=inception;cnnresiduals=0;re-align=0;augment=randomcrop;augment=temp-scale;loss=ACE;loss=CTC,vocab=1080,vocab>=1000,wer=31.7/31.3},
  language = {en}
}

@article{zhou_spatialtemporal_2020,
  title = {Spatial-{{Temporal Multi}}-{{Cue Network}} for {{Continuous Sign Language Recognition}}},
  author = {Zhou, Hao and Zhou, Wengang and Zhou, Yun and Li, Houqiang},
  year = {2020},
  month = feb,
  abstract = {Despite the recent success of deep learning in continuous sign language recognition (CSLR), deep models typically focus on the most discriminative features, ignoring other potentially non-trivial and informative contents. Such characteristic heavily constrains their capability to learn implicit visual grammars behind the collaboration of different visual cues (i,e., hand shape, facial expression and body posture). By injecting multi-cue learning into neural network design, we propose a spatial-temporal multi-cue (STMC) network to solve the vision-based sequence learning problem. Our STMC network consists of a spatial multi-cue (SMC) module and a temporal multi-cue (TMC) module. The SMC module is dedicated to spatial representation and explicitly decomposes visual features of different cues with the aid of a self-contained pose estimation branch. The TMC module models temporal correlations along two parallel paths, i.e., intra-cue and inter-cue, which aims to preserve the uniqueness and explore the collaboration of multiple cues. Finally, we design a joint optimization strategy to achieve the end-to-end sequence learning of the STMC network. To validate the effectiveness, we perform experiments on three large-scale CSLR benchmarks: PHOENIX-2014, CSL and PHOENIX-2014-T. Experimental results demonstrate that the proposed method achieves new state-of-the-art performance on all three benchmarks.},
  archivePrefix = {arXiv},
  eprint = {2002.03187},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhou et al_2020_Spatial-Temporal Multi-Cue Network for Continuous Sign Language Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/5446YUXM/2002.html},
  journal = {arXiv:2002.03187 [cs]},
  keywords = {augment=flip,augment=randomcrop,augment=randomframedrop,authorgroup=USTC,capturedBy=vision,cnnresiduals=0,Computer Science - Computer Vision and Pattern Recognition,continuous,continuous;vocab=1066;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14T,continuous;vocab=1080;signer=9;sl=DGS;capturedBy=vision;dataSet=Phoenix14;wer=21.1/20.7,continuous;vocab=178;signer=50;sl=CSL;capturedBy=vision;dataSet=CSL,dataSet=CSL,dataSet=Phoenix14,dataSet=Phoenix14T,loss=CE,loss=CTC,loss=other,modal=2hshape,modal=bodyjoints,modal=fullframe,modal=head,nn=1dcnn,nn=2dcnn,nn=blstm,nnarch=vgg-11,paperinfo,paperinfo;authorgroup=USTC;shorttitle=ST Multi-Cue Network,re-align=1,shorttitle=Spatial-Temporal Multi-Cue Network,shorttitle=ST Multi-Cue Network,signer=50,signer=9,sl=CSL,sl=DGS,slreco,slreco;modal=fullframe;modal=2hshape;modal=head;modal=bodyjoints;nn=2dcnn;nn=1dcnn;nn=blstm;nnarch=vgg-11;cnnresiduals=0;re-align=1;augment=randomcrop;augment=randomframedrop;augment=flip;loss=CTC;loss=other;loss=CE,vocab<500,vocab=1066,vocab=1080,vocab=178,vocab>=1000,wer=21.1/20.7},
  primaryClass = {cs}
}

@inproceedings{zhou_unsupervised_2010,
  title = {Unsupervised Discovery of Facial Events},
  booktitle = {Proc. {{IEEE Conf}}. on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Feng and {De la Torre}, F. and Cohn, J.F.},
  year = {2010},
  month = jun,
  pages = {2574--2581},
  doi = {10.1109/CVPR.2010.5539966},
  abstract = {Automatic facial image analysis has been a long standing research problem in computer vision. A key component in facial image analysis, largely conditioning the success of subsequent algorithms (e.g. facial expression recognition), is to define a vocabulary of possible dynamic facial events. To date, that vocabulary has come from the anatomically-based Facial Action Coding System (FACS) or more subjective approaches (i.e. emotion-specified expressions). The aim of this paper is to discover facial events directly from video of naturally occurring facial behavior, without recourse to FACS or other labeling schemes. To discover facial events, we propose a temporal clustering algorithm, Aligned Cluster Analysis (ACA), and a multi-subject correspondence algorithm for matching expressions. We use a variety of video sources: posed facial behavior (Cohn-Kanade database), unscripted facial behavior (RU-FACS database) and some video in infants. Accuracy of (unsupervised) ACA approached that of a supervised version, achieved moderate intersystem agreement with FACS, and proved informative as a visualization/summarization tool.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/2010/Zhou et al/Zhou et al_2010_Unsupervised discovery of facial events.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/AZ7K38A6/abs_all.html}
}

@article{zhou_unsupervised_2015,
  title = {Unsupervised Adaptive Sign Language Recognition Based on Hypothesis Comparison Guided Cross Validation and Linguistic Prior Filtering},
  author = {Zhou, Yu and Yang, Xiaokang and Zhang, Yongzheng and Xu, Xiang and Wang, Yipeng and Chai, Xiujuan and Lin, Weiyao},
  year = {2015},
  volume = {149},
  pages = {1604--1612},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhou et al_2015_Unsupervised adaptive sign language recognition based on hypothesis comparison.pdf},
  journal = {Neurocomputing},
  keywords = {capturedBy=depth,capturedBy=vision,isolated,isolated;vocab=20;signer=7;sl=CSL;capturedBy=vision;capturedBy=depth,modal=shape,signer=7,sl=CSL,slreco,slreco;modal=shape,vocab<50,vocab=20}
}

@inproceedings{zhu_cooccurrence_2016,
  title = {Co-{{Occurrence Feature Learning}} for {{Skeleton Based Action Recognition Using Regularized Deep LSTM Networks}}},
  booktitle = {Proc. of the  {{AAAI Conf}}. on {{Artificial Intelligence}}},
  author = {Zhu, Wentao and Lan, Cuiling and Xing, Junliang and Zeng, Wenjun and Li, Yanghao and Shen, Li and Xie, Xiaohui},
  year = {2016},
  month = mar,
  abstract = {Skeleton based action recognition distinguishes human actions using the trajectories of skeleton joints, which provide a very good representation for describing actions. Considering that recurrent neural networks (RNNs) with Long Short-Term Memory (LSTM) can learn feature representations and model long-term temporal dependencies automatically, we propose an end-to-end fully connected deep LSTM network for skeleton based action recognition. Inspired by the observation that the co-occurrences of the joints intrinsically characterize human actions, we take the skeleton as the input at each time slot and introduce a novel regularization scheme to learn the co-occurrence features of skeleton joints. To train the deep LSTM network effectively, we propose a new dropout algorithm which simultaneously operates on the gates, cells, and output responses of the LSTM neurons. Experimental results on three human action recognition datasets consistently demonstrate the effectiveness of the proposed model.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhu et al_2016_Co-Occurrence Feature Learning for Skeleton Based Action Recognition Using.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/C25G6CHA/11989.html;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/ED63BAY5/11989.html},
  language = {en}
}

@article{zhu_hidden_2017,
  title = {Hidden {{Two}}-{{Stream Convolutional Networks}} for {{Action Recognition}}},
  author = {Zhu, Yi and Lan, Zhenzhong and Newsam, Shawn and Hauptmann, Alexander G.},
  year = {2017},
  month = apr,
  abstract = {Analyzing videos of human actions involves understanding the temporal relationships among video frames. State-of-the-art action recognition approaches rely on traditional optical flow estimation methods to pre-compute motion information for CNNs. Such a two-stage approach is computationally expensive, storage demanding, and not end-to-end trainable. In this paper, we present a novel CNN architecture that implicitly captures motion information between adjacent frames. We name our approach hidden two-stream CNNs because it only takes raw video frames as input and directly predicts action classes without explicitly computing optical flow. Our end-to-end approach is 10x faster than its two-stage baseline. Experimental results on four challenging action recognition datasets: UCF101, HMDB51, THUMOS14 and ActivityNet v1.2 show that our approach significantly outperforms the previous best real-time approaches.},
  archivePrefix = {arXiv},
  eprint = {1704.00389},
  eprinttype = {arxiv},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhu et al_2017_Hidden Two-Stream Convolutional Networks for Action Recognition.pdf;/media/osetu/oseSD/prog/zotero-userfiles/h2r8ucs0.default/zotero/storage/IH7VLZL2/1704.html},
  journal = {arXiv:1704.00389 [cs]},
  primaryClass = {cs}
}

@article{zhu_multimodal_2017,
  title = {Multimodal {{Gesture Recognition Using}} 3-{{D Convolution}} and {{Convolutional LSTM}}},
  author = {Zhu, G. and Zhang, L. and Shen, P. and Song, J.},
  year = {2017},
  volume = {5},
  pages = {4517--4524},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2017.2684186},
  abstract = {Gesture recognition aims to recognize meaningful movements of human bodies, and is of utmost importance in intelligent human-computer/robot interactions. In this paper, we present a multimodal gesture recognition method based on 3-D convolution and convolutional long-short-term-memory (LSTM) networks. The proposed method first learns short-term spatiotemporal features of gestures through the 3-D convolutional neural network, and then learns long-term spatiotemporal features by convolutional LSTM networks based on the extracted short-term spatiotemporal features. In addition, fine-tuning among multimodal data is evaluated, and we find that it can be considered as an optional skill to prevent overfitting when no pre-trained models exist. The proposed method is verified on the ChaLearn LAP large-scale isolated gesture data set (IsoGD) and the Sheffield Kinect gesture (SKIG) data set. The results show that our proposed method can obtain the state-of-the-art recognition accuracy (51.02\% on the validation set of IsoGD and 98.89\% on SKIG).},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhu et al_2017_Multimodal Gesture Recognition Using 3-D Convolution and Convolutional LSTM.pdf},
  journal = {IEEE Access}
}

@techreport{zhu_semisupervised_2008,
  title = {Semi-{{Supervised Learning Literature Survey}}},
  author = {Zhu, Xiaojin},
  year = {2008},
  pages = {1--60},
  institution = {{Computer Sciences, University of Wisconsin -Madison}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zhu_2008_Semi-Supervised Learning Literature Survey.pdf},
  number = {1530}
}

@article{zissman_automatic_2001,
  title = {Automatic {{Language Identification}}},
  author = {Zissman, Marc A. and Berkling, Kay M.},
  year = {2001},
  volume = {35},
  pages = {115--124},
  journal = {Speech Communication},
  number = {1-2}
}

@article{zissman_comparison_1996,
  title = {Comparison of {{Four Approaches}} to {{Automatic Language Identification}} of {{Telephone Speech}}},
  author = {Zissman, M.A.},
  year = {1996},
  volume = {4},
  pages = {31--44},
  journal = {IEEE Transactions on Speech and Audio Processing},
  number = {1}
}

@phdthesis{zolnay_acoustic_2006,
  title = {Acoustic {{Feature Combination}} for {{Speech Recognition}}},
  author = {Zolnay, Andras},
  year = {2006},
  month = aug,
  address = {{Computer Science Department, RWTH Aachen University, Aachen, Germany}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zolnay_2006_Acoustic Feature Combination for Speech Recognition.pdf},
  school = {RWTH Aachen University}
}

@incollection{zwitserlood_classifiers_2012,
  title = {Classifiers},
  booktitle = {Sign Language: {{An}} International Handbook},
  author = {Zwitserlood, Inge},
  editor = {Pfau, Roland and Steinbach, Markus and Woll, Bencie},
  year = {2012},
  month = aug,
  pages = {158--186},
  publisher = {{Mouton de Gruyter}},
  address = {{Berlin}},
  file = {/media/osetu/oseSD/cloudstorage/onedrive/zotero-userfiles/zotero-attachments/Zwitserlood_2012_Classifiers.pdf}
}


